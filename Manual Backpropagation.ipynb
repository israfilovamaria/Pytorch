{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a412bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18610bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b049f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef8bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([ 8., 10., 21., 20., 32., 32., 35., 46., 46., 53.])\n"
     ]
    }
   ],
   "source": [
    "# creating some data to train Linear Regression \n",
    "x = torch.tensor(np.arange(10), dtype = torch.float32)\n",
    "y = x*5 + 10 + torch.randint(low = -5, high = 5, size = (10,), dtype = torch.float32)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d386c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbe7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# initializing weights\n",
    "# we have only one feature, so we need to find two coefficients: slope and intercept \n",
    "# let's initialize them as zeros \n",
    "\n",
    "w = torch.zeros(2, dtype = torch.float16, )\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8f3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def predict(w, x):\n",
    "    y_pred = w[0]*x + w[1]\n",
    "    return y_pred \n",
    "\n",
    "def mseerror(y, y_pred):\n",
    "    loss_val = ((y_pred - y)**2).mean()\n",
    "    return loss_val \n",
    "\n",
    "# derivative from mse with respect to slope: 2x*(y - y_pred)\n",
    "def beta1gradient(x, y, y_pred):\n",
    "    direction = np.dot(2*x, y_pred - y).mean()\n",
    "    return direction \n",
    "# derivative from mse with respect to intercept: 2x*(y - y_pred)\n",
    "def beta0gradient(x, y, y_pred):\n",
    "    direction = np.dot(2, y_pred - y).mean()\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2992c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1131.9000)\n"
     ]
    }
   ],
   "source": [
    "# let's check our predictions before training \n",
    "y_pred = predict(w, x)\n",
    "error = mseerror(y, y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b345c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174bbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1131.9000244140625, weights = tensor([2.1328, 0.0363], dtype=torch.float16)\n",
      "step №1: loss = 502.0381774902344, weights = tensor([3.5332, 0.0612], dtype=torch.float16)\n",
      "step №2: loss = 230.22238159179688, weights = tensor([4.4531, 0.0784], dtype=torch.float16)\n",
      "step №3: loss = 112.82142639160156, weights = tensor([5.0586, 0.0906], dtype=torch.float16)\n",
      "step №4: loss = 62.015830993652344, weights = tensor([5.4570, 0.0995], dtype=torch.float16)\n",
      "step №5: loss = 40.045127868652344, weights = tensor([5.7188, 0.1063], dtype=torch.float16)\n",
      "step №6: loss = 30.564132690429688, weights = tensor([5.8906, 0.1116], dtype=torch.float16)\n",
      "step №7: loss = 26.472360610961914, weights = tensor([6.0039, 0.1160], dtype=torch.float16)\n",
      "step №8: loss = 24.699848175048828, weights = tensor([6.0781, 0.1198], dtype=torch.float16)\n",
      "step №9: loss = 23.934911727905273, weights = tensor([6.1250, 0.1232], dtype=torch.float16)\n",
      "step №10: loss = 23.61081314086914, weights = tensor([6.1562, 0.1263], dtype=torch.float16)\n",
      "step №11: loss = 23.461820602416992, weights = tensor([6.1758, 0.1293], dtype=torch.float16)\n",
      "step №12: loss = 23.393178939819336, weights = tensor([6.1875, 0.1321], dtype=torch.float16)\n",
      "step №13: loss = 23.35805320739746, weights = tensor([6.1953, 0.1349], dtype=torch.float16)\n",
      "step №14: loss = 23.33504295349121, weights = tensor([6.2031, 0.1376], dtype=torch.float16)\n",
      "step №15: loss = 23.31647300720215, weights = tensor([6.2070, 0.1403], dtype=torch.float16)\n",
      "step №16: loss = 23.302654266357422, weights = tensor([6.2109, 0.1429], dtype=torch.float16)\n",
      "step №17: loss = 23.289905548095703, weights = tensor([6.2109, 0.1456], dtype=torch.float16)\n",
      "step №18: loss = 23.278053283691406, weights = tensor([6.2109, 0.1483], dtype=torch.float16)\n",
      "step №19: loss = 23.266216278076172, weights = tensor([6.2109, 0.1510], dtype=torch.float16)\n",
      "step №20: loss = 23.254396438598633, weights = tensor([6.2109, 0.1537], dtype=torch.float16)\n",
      "step №21: loss = 23.24258804321289, weights = tensor([6.2109, 0.1564], dtype=torch.float16)\n",
      "step №22: loss = 23.230792999267578, weights = tensor([6.2109, 0.1591], dtype=torch.float16)\n",
      "step №23: loss = 23.219013214111328, weights = tensor([6.2109, 0.1617], dtype=torch.float16)\n",
      "step №24: loss = 23.207250595092773, weights = tensor([6.2109, 0.1644], dtype=torch.float16)\n",
      "step №25: loss = 23.195499420166016, weights = tensor([6.2109, 0.1670], dtype=torch.float16)\n",
      "step №26: loss = 23.18429946899414, weights = tensor([6.2109, 0.1696], dtype=torch.float16)\n",
      "step №27: loss = 23.173107147216797, weights = tensor([6.2109, 0.1721], dtype=torch.float16)\n",
      "step №28: loss = 23.161930084228516, weights = tensor([6.2109, 0.1747], dtype=torch.float16)\n",
      "step №29: loss = 23.150768280029297, weights = tensor([6.2109, 0.1772], dtype=torch.float16)\n",
      "step №30: loss = 23.13961410522461, weights = tensor([6.2109, 0.1798], dtype=torch.float16)\n",
      "step №31: loss = 23.128482818603516, weights = tensor([6.2109, 0.1824], dtype=torch.float16)\n",
      "step №32: loss = 23.117355346679688, weights = tensor([6.2109, 0.1849], dtype=torch.float16)\n",
      "step №33: loss = 23.106245040893555, weights = tensor([6.2109, 0.1875], dtype=torch.float16)\n",
      "step №34: loss = 23.09514808654785, weights = tensor([6.2109, 0.1901], dtype=torch.float16)\n",
      "step №35: loss = 23.084064483642578, weights = tensor([6.2070, 0.1926], dtype=torch.float16)\n",
      "step №36: loss = 23.072032928466797, weights = tensor([6.2070, 0.1952], dtype=torch.float16)\n",
      "step №37: loss = 23.060884475708008, weights = tensor([6.2070, 0.1978], dtype=torch.float16)\n",
      "step №38: loss = 23.04974937438965, weights = tensor([6.2070, 0.2003], dtype=torch.float16)\n",
      "step №39: loss = 23.038625717163086, weights = tensor([6.2070, 0.2029], dtype=torch.float16)\n",
      "step №40: loss = 23.02751922607422, weights = tensor([6.2070, 0.2054], dtype=torch.float16)\n",
      "step №41: loss = 23.016422271728516, weights = tensor([6.2070, 0.2080], dtype=torch.float16)\n",
      "step №42: loss = 23.005340576171875, weights = tensor([6.2070, 0.2106], dtype=torch.float16)\n",
      "step №43: loss = 22.9942684173584, weights = tensor([6.2070, 0.2131], dtype=torch.float16)\n",
      "step №44: loss = 22.983211517333984, weights = tensor([6.2070, 0.2157], dtype=torch.float16)\n",
      "step №45: loss = 22.972171783447266, weights = tensor([6.2031, 0.2183], dtype=torch.float16)\n",
      "step №46: loss = 22.960147857666016, weights = tensor([6.2031, 0.2208], dtype=torch.float16)\n",
      "step №47: loss = 22.94904327392578, weights = tensor([6.2031, 0.2234], dtype=torch.float16)\n",
      "step №48: loss = 22.93794822692871, weights = tensor([6.2031, 0.2260], dtype=torch.float16)\n",
      "step №49: loss = 22.92686653137207, weights = tensor([6.2031, 0.2285], dtype=torch.float16)\n",
      "step №50: loss = 22.91579818725586, weights = tensor([6.2031, 0.2311], dtype=torch.float16)\n",
      "step №51: loss = 22.904743194580078, weights = tensor([6.2031, 0.2336], dtype=torch.float16)\n",
      "step №52: loss = 22.89370346069336, weights = tensor([6.2031, 0.2362], dtype=torch.float16)\n",
      "step №53: loss = 22.88267707824707, weights = tensor([6.2031, 0.2388], dtype=torch.float16)\n",
      "step №54: loss = 22.871660232543945, weights = tensor([6.1992, 0.2413], dtype=torch.float16)\n",
      "step №55: loss = 22.859725952148438, weights = tensor([6.1992, 0.2439], dtype=torch.float16)\n",
      "step №56: loss = 22.84864616394043, weights = tensor([6.1992, 0.2465], dtype=torch.float16)\n",
      "step №57: loss = 22.837583541870117, weights = tensor([6.1992, 0.2490], dtype=torch.float16)\n",
      "step №58: loss = 22.82653045654297, weights = tensor([6.1992, 0.2517], dtype=torch.float16)\n",
      "step №59: loss = 22.814966201782227, weights = tensor([6.1992, 0.2544], dtype=torch.float16)\n",
      "step №60: loss = 22.803415298461914, weights = tensor([6.1992, 0.2571], dtype=torch.float16)\n",
      "step №61: loss = 22.791879653930664, weights = tensor([6.1992, 0.2598], dtype=torch.float16)\n",
      "step №62: loss = 22.780353546142578, weights = tensor([6.1992, 0.2625], dtype=torch.float16)\n",
      "step №63: loss = 22.76885223388672, weights = tensor([6.1992, 0.2651], dtype=torch.float16)\n",
      "step №64: loss = 22.757356643676758, weights = tensor([6.1953, 0.2678], dtype=torch.float16)\n",
      "step №65: loss = 22.744884490966797, weights = tensor([6.1953, 0.2705], dtype=torch.float16)\n",
      "step №66: loss = 22.733327865600586, weights = tensor([6.1953, 0.2732], dtype=torch.float16)\n",
      "step №67: loss = 22.721782684326172, weights = tensor([6.1953, 0.2759], dtype=torch.float16)\n",
      "step №68: loss = 22.71025276184082, weights = tensor([6.1953, 0.2786], dtype=torch.float16)\n",
      "step №69: loss = 22.69873809814453, weights = tensor([6.1953, 0.2812], dtype=torch.float16)\n",
      "step №70: loss = 22.687236785888672, weights = tensor([6.1953, 0.2839], dtype=torch.float16)\n",
      "step №71: loss = 22.675750732421875, weights = tensor([6.1953, 0.2866], dtype=torch.float16)\n",
      "step №72: loss = 22.66427993774414, weights = tensor([6.1953, 0.2891], dtype=torch.float16)\n",
      "step №73: loss = 22.65386390686035, weights = tensor([6.1914, 0.2915], dtype=torch.float16)\n",
      "step №74: loss = 22.64250373840332, weights = tensor([6.1914, 0.2942], dtype=torch.float16)\n",
      "step №75: loss = 22.630977630615234, weights = tensor([6.1914, 0.2969], dtype=torch.float16)\n",
      "step №76: loss = 22.619464874267578, weights = tensor([6.1914, 0.2996], dtype=torch.float16)\n",
      "step №77: loss = 22.607967376708984, weights = tensor([6.1914, 0.3022], dtype=torch.float16)\n",
      "step №78: loss = 22.596487045288086, weights = tensor([6.1914, 0.3049], dtype=torch.float16)\n",
      "step №79: loss = 22.585018157958984, weights = tensor([6.1914, 0.3074], dtype=torch.float16)\n",
      "step №80: loss = 22.57460594177246, weights = tensor([6.1914, 0.3098], dtype=torch.float16)\n",
      "step №81: loss = 22.564205169677734, weights = tensor([6.1914, 0.3123], dtype=torch.float16)\n",
      "step №82: loss = 22.553815841674805, weights = tensor([6.1914, 0.3147], dtype=torch.float16)\n",
      "step №83: loss = 22.543439865112305, weights = tensor([6.1875, 0.3171], dtype=torch.float16)\n",
      "step №84: loss = 22.532085418701172, weights = tensor([6.1875, 0.3198], dtype=torch.float16)\n",
      "step №85: loss = 22.52060890197754, weights = tensor([6.1875, 0.3225], dtype=torch.float16)\n",
      "step №86: loss = 22.509138107299805, weights = tensor([6.1875, 0.3250], dtype=torch.float16)\n",
      "step №87: loss = 22.498722076416016, weights = tensor([6.1875, 0.3274], dtype=torch.float16)\n",
      "step №88: loss = 22.488325119018555, weights = tensor([6.1875, 0.3298], dtype=torch.float16)\n",
      "step №89: loss = 22.477935791015625, weights = tensor([6.1875, 0.3323], dtype=torch.float16)\n",
      "step №90: loss = 22.467559814453125, weights = tensor([6.1875, 0.3347], dtype=torch.float16)\n",
      "step №91: loss = 22.457189559936523, weights = tensor([6.1875, 0.3372], dtype=torch.float16)\n",
      "step №92: loss = 22.446842193603516, weights = tensor([6.1875, 0.3396], dtype=torch.float16)\n",
      "step №93: loss = 22.43649673461914, weights = tensor([6.1836, 0.3420], dtype=torch.float16)\n",
      "step №94: loss = 22.425174713134766, weights = tensor([6.1836, 0.3445], dtype=torch.float16)\n",
      "step №95: loss = 22.414770126342773, weights = tensor([6.1836, 0.3469], dtype=torch.float16)\n",
      "step №96: loss = 22.404380798339844, weights = tensor([6.1836, 0.3494], dtype=torch.float16)\n",
      "step №97: loss = 22.394001007080078, weights = tensor([6.1836, 0.3518], dtype=torch.float16)\n",
      "step №98: loss = 22.383634567260742, weights = tensor([6.1836, 0.3542], dtype=torch.float16)\n",
      "step №99: loss = 22.373279571533203, weights = tensor([6.1836, 0.3567], dtype=torch.float16)\n",
      "step №100: loss = 22.362934112548828, weights = tensor([6.1836, 0.3591], dtype=torch.float16)\n",
      "step №101: loss = 22.352602005004883, weights = tensor([6.1836, 0.3616], dtype=torch.float16)\n",
      "step №102: loss = 22.342281341552734, weights = tensor([6.1836, 0.3640], dtype=torch.float16)\n",
      "step №103: loss = 22.331974029541016, weights = tensor([6.1797, 0.3665], dtype=torch.float16)\n",
      "step №104: loss = 22.320697784423828, weights = tensor([6.1797, 0.3689], dtype=torch.float16)\n",
      "step №105: loss = 22.310327529907227, weights = tensor([6.1797, 0.3713], dtype=torch.float16)\n",
      "step №106: loss = 22.299968719482422, weights = tensor([6.1797, 0.3738], dtype=torch.float16)\n",
      "step №107: loss = 22.289623260498047, weights = tensor([6.1797, 0.3762], dtype=torch.float16)\n",
      "step №108: loss = 22.27928924560547, weights = tensor([6.1797, 0.3787], dtype=torch.float16)\n",
      "step №109: loss = 22.26896858215332, weights = tensor([6.1797, 0.3811], dtype=torch.float16)\n",
      "step №110: loss = 22.258655548095703, weights = tensor([6.1797, 0.3835], dtype=torch.float16)\n",
      "step №111: loss = 22.24835968017578, weights = tensor([6.1797, 0.3860], dtype=torch.float16)\n",
      "step №112: loss = 22.23807144165039, weights = tensor([6.1797, 0.3884], dtype=torch.float16)\n",
      "step №113: loss = 22.227798461914062, weights = tensor([6.1758, 0.3909], dtype=torch.float16)\n",
      "step №114: loss = 22.216564178466797, weights = tensor([6.1758, 0.3933], dtype=torch.float16)\n",
      "step №115: loss = 22.206226348876953, weights = tensor([6.1758, 0.3958], dtype=torch.float16)\n",
      "step №116: loss = 22.195903778076172, weights = tensor([6.1758, 0.3982], dtype=torch.float16)\n",
      "step №117: loss = 22.185590744018555, weights = tensor([6.1758, 0.4006], dtype=torch.float16)\n",
      "step №118: loss = 22.175289154052734, weights = tensor([6.1758, 0.4031], dtype=torch.float16)\n",
      "step №119: loss = 22.165000915527344, weights = tensor([6.1758, 0.4055], dtype=torch.float16)\n",
      "step №120: loss = 22.15472412109375, weights = tensor([6.1758, 0.4080], dtype=torch.float16)\n",
      "step №121: loss = 22.144458770751953, weights = tensor([6.1758, 0.4104], dtype=torch.float16)\n",
      "step №122: loss = 22.134206771850586, weights = tensor([6.1758, 0.4128], dtype=torch.float16)\n",
      "step №123: loss = 22.12396240234375, weights = tensor([6.1719, 0.4153], dtype=torch.float16)\n",
      "step №124: loss = 22.112775802612305, weights = tensor([6.1719, 0.4177], dtype=torch.float16)\n",
      "step №125: loss = 22.102474212646484, weights = tensor([6.1719, 0.4202], dtype=torch.float16)\n",
      "step №126: loss = 22.092182159423828, weights = tensor([6.1719, 0.4226], dtype=torch.float16)\n",
      "step №127: loss = 22.08190155029297, weights = tensor([6.1719, 0.4250], dtype=torch.float16)\n",
      "step №128: loss = 22.071636199951172, weights = tensor([6.1719, 0.4275], dtype=torch.float16)\n",
      "step №129: loss = 22.061378479003906, weights = tensor([6.1719, 0.4299], dtype=torch.float16)\n",
      "step №130: loss = 22.051136016845703, weights = tensor([6.1719, 0.4324], dtype=torch.float16)\n",
      "step №131: loss = 22.040904998779297, weights = tensor([6.1719, 0.4348], dtype=torch.float16)\n",
      "step №132: loss = 22.030685424804688, weights = tensor([6.1719, 0.4373], dtype=torch.float16)\n",
      "step №133: loss = 22.02048110961914, weights = tensor([6.1680, 0.4397], dtype=torch.float16)\n",
      "step №134: loss = 22.00933265686035, weights = tensor([6.1680, 0.4421], dtype=torch.float16)\n",
      "step №135: loss = 21.999065399169922, weights = tensor([6.1680, 0.4446], dtype=torch.float16)\n",
      "step №136: loss = 21.988807678222656, weights = tensor([6.1680, 0.4470], dtype=torch.float16)\n",
      "step №137: loss = 21.978559494018555, weights = tensor([6.1680, 0.4495], dtype=torch.float16)\n",
      "step №138: loss = 21.968326568603516, weights = tensor([6.1680, 0.4519], dtype=torch.float16)\n",
      "step №139: loss = 21.95810317993164, weights = tensor([6.1680, 0.4543], dtype=torch.float16)\n",
      "step №140: loss = 21.947895050048828, weights = tensor([6.1680, 0.4568], dtype=torch.float16)\n",
      "step №141: loss = 21.93769645690918, weights = tensor([6.1680, 0.4592], dtype=torch.float16)\n",
      "step №142: loss = 21.92751121520996, weights = tensor([6.1680, 0.4617], dtype=torch.float16)\n",
      "step №143: loss = 21.91733741760254, weights = tensor([6.1641, 0.4641], dtype=torch.float16)\n",
      "step №144: loss = 21.906238555908203, weights = tensor([6.1641, 0.4666], dtype=torch.float16)\n",
      "step №145: loss = 21.8960018157959, weights = tensor([6.1641, 0.4690], dtype=torch.float16)\n",
      "step №146: loss = 21.88577651977539, weights = tensor([6.1641, 0.4714], dtype=torch.float16)\n",
      "step №147: loss = 21.875564575195312, weights = tensor([6.1641, 0.4739], dtype=torch.float16)\n",
      "step №148: loss = 21.865360260009766, weights = tensor([6.1641, 0.4763], dtype=torch.float16)\n",
      "step №149: loss = 21.855175018310547, weights = tensor([6.1641, 0.4788], dtype=torch.float16)\n",
      "step №150: loss = 21.84499740600586, weights = tensor([6.1641, 0.4812], dtype=torch.float16)\n",
      "step №151: loss = 21.8348331451416, weights = tensor([6.1641, 0.4836], dtype=torch.float16)\n",
      "step №152: loss = 21.82468032836914, weights = tensor([6.1641, 0.4861], dtype=torch.float16)\n",
      "step №153: loss = 21.814538955688477, weights = tensor([6.1602, 0.4885], dtype=torch.float16)\n",
      "step №154: loss = 21.803485870361328, weights = tensor([6.1602, 0.4910], dtype=torch.float16)\n",
      "step №155: loss = 21.79328155517578, weights = tensor([6.1602, 0.4934], dtype=torch.float16)\n",
      "step №156: loss = 21.783090591430664, weights = tensor([6.1602, 0.4958], dtype=torch.float16)\n",
      "step №157: loss = 21.77291488647461, weights = tensor([6.1602, 0.4983], dtype=torch.float16)\n",
      "step №158: loss = 21.762746810913086, weights = tensor([6.1602, 0.5010], dtype=torch.float16)\n",
      "step №159: loss = 21.751575469970703, weights = tensor([6.1602, 0.5034], dtype=torch.float16)\n",
      "step №160: loss = 21.74143409729004, weights = tensor([6.1602, 0.5059], dtype=torch.float16)\n",
      "step №161: loss = 21.731300354003906, weights = tensor([6.1602, 0.5083], dtype=torch.float16)\n",
      "step №162: loss = 21.7211856842041, weights = tensor([6.1602, 0.5107], dtype=torch.float16)\n",
      "step №163: loss = 21.711078643798828, weights = tensor([6.1562, 0.5132], dtype=torch.float16)\n",
      "step №164: loss = 21.70005989074707, weights = tensor([6.1562, 0.5156], dtype=torch.float16)\n",
      "step №165: loss = 21.689891815185547, weights = tensor([6.1562, 0.5181], dtype=torch.float16)\n",
      "step №166: loss = 21.67973518371582, weights = tensor([6.1562, 0.5205], dtype=torch.float16)\n",
      "step №167: loss = 21.66959571838379, weights = tensor([6.1562, 0.5229], dtype=torch.float16)\n",
      "step №168: loss = 21.659460067749023, weights = tensor([6.1562, 0.5254], dtype=torch.float16)\n",
      "step №169: loss = 21.649335861206055, weights = tensor([6.1562, 0.5278], dtype=torch.float16)\n",
      "step №170: loss = 21.639232635498047, weights = tensor([6.1562, 0.5303], dtype=torch.float16)\n",
      "step №171: loss = 21.629131317138672, weights = tensor([6.1562, 0.5327], dtype=torch.float16)\n",
      "step №172: loss = 21.61904525756836, weights = tensor([6.1562, 0.5352], dtype=torch.float16)\n",
      "step №173: loss = 21.60897445678711, weights = tensor([6.1562, 0.5376], dtype=torch.float16)\n",
      "step №174: loss = 21.59891700744629, weights = tensor([6.1523, 0.5400], dtype=torch.float16)\n",
      "step №175: loss = 21.58786964416504, weights = tensor([6.1523, 0.5425], dtype=torch.float16)\n",
      "step №176: loss = 21.57774543762207, weights = tensor([6.1523, 0.5449], dtype=torch.float16)\n",
      "step №177: loss = 21.567630767822266, weights = tensor([6.1523, 0.5474], dtype=torch.float16)\n",
      "step №178: loss = 21.55753517150879, weights = tensor([6.1523, 0.5498], dtype=torch.float16)\n",
      "step №179: loss = 21.547447204589844, weights = tensor([6.1523, 0.5522], dtype=torch.float16)\n",
      "step №180: loss = 21.537372589111328, weights = tensor([6.1523, 0.5547], dtype=torch.float16)\n",
      "step №181: loss = 21.52730941772461, weights = tensor([6.1523, 0.5571], dtype=torch.float16)\n",
      "step №182: loss = 21.517255783081055, weights = tensor([6.1523, 0.5596], dtype=torch.float16)\n",
      "step №183: loss = 21.50721549987793, weights = tensor([6.1523, 0.5620], dtype=torch.float16)\n",
      "step №184: loss = 21.497188568115234, weights = tensor([6.1484, 0.5645], dtype=torch.float16)\n",
      "step №185: loss = 21.486186981201172, weights = tensor([6.1484, 0.5669], dtype=torch.float16)\n",
      "step №186: loss = 21.476099014282227, weights = tensor([6.1484, 0.5693], dtype=torch.float16)\n",
      "step №187: loss = 21.466020584106445, weights = tensor([6.1484, 0.5718], dtype=torch.float16)\n",
      "step №188: loss = 21.455955505371094, weights = tensor([6.1484, 0.5742], dtype=torch.float16)\n",
      "step №189: loss = 21.44590187072754, weights = tensor([6.1484, 0.5767], dtype=torch.float16)\n",
      "step №190: loss = 21.435861587524414, weights = tensor([6.1484, 0.5791], dtype=torch.float16)\n",
      "step №191: loss = 21.42582893371582, weights = tensor([6.1484, 0.5815], dtype=torch.float16)\n",
      "step №192: loss = 21.41581153869629, weights = tensor([6.1484, 0.5840], dtype=torch.float16)\n",
      "step №193: loss = 21.405803680419922, weights = tensor([6.1484, 0.5864], dtype=torch.float16)\n",
      "step №194: loss = 21.39581298828125, weights = tensor([6.1445, 0.5889], dtype=torch.float16)\n",
      "step №195: loss = 21.38485336303711, weights = tensor([6.1445, 0.5913], dtype=torch.float16)\n",
      "step №196: loss = 21.374799728393555, weights = tensor([6.1445, 0.5938], dtype=torch.float16)\n",
      "step №197: loss = 21.36475372314453, weights = tensor([6.1445, 0.5962], dtype=torch.float16)\n",
      "step №198: loss = 21.354721069335938, weights = tensor([6.1445, 0.5986], dtype=torch.float16)\n",
      "step №199: loss = 21.344701766967773, weights = tensor([6.1445, 0.6011], dtype=torch.float16)\n",
      "step №200: loss = 21.334692001342773, weights = tensor([6.1445, 0.6035], dtype=torch.float16)\n",
      "step №201: loss = 21.32469367980957, weights = tensor([6.1445, 0.6060], dtype=torch.float16)\n",
      "step №202: loss = 21.314712524414062, weights = tensor([6.1445, 0.6084], dtype=torch.float16)\n",
      "step №203: loss = 21.304738998413086, weights = tensor([6.1445, 0.6108], dtype=torch.float16)\n",
      "step №204: loss = 21.294775009155273, weights = tensor([6.1406, 0.6133], dtype=torch.float16)\n",
      "step №205: loss = 21.283864974975586, weights = tensor([6.1406, 0.6157], dtype=torch.float16)\n",
      "step №206: loss = 21.273845672607422, weights = tensor([6.1406, 0.6182], dtype=torch.float16)\n",
      "step №207: loss = 21.263830184936523, weights = tensor([6.1406, 0.6206], dtype=torch.float16)\n",
      "step №208: loss = 21.253829956054688, weights = tensor([6.1406, 0.6230], dtype=torch.float16)\n",
      "step №209: loss = 21.24384307861328, weights = tensor([6.1406, 0.6255], dtype=torch.float16)\n",
      "step №210: loss = 21.233869552612305, weights = tensor([6.1406, 0.6279], dtype=torch.float16)\n",
      "step №211: loss = 21.223909378051758, weights = tensor([6.1406, 0.6304], dtype=torch.float16)\n",
      "step №212: loss = 21.213956832885742, weights = tensor([6.1406, 0.6328], dtype=torch.float16)\n",
      "step №213: loss = 21.204015731811523, weights = tensor([6.1406, 0.6353], dtype=torch.float16)\n",
      "step №214: loss = 21.194089889526367, weights = tensor([6.1367, 0.6377], dtype=torch.float16)\n",
      "step №215: loss = 21.18321990966797, weights = tensor([6.1367, 0.6401], dtype=torch.float16)\n",
      "step №216: loss = 21.17323112487793, weights = tensor([6.1367, 0.6426], dtype=torch.float16)\n",
      "step №217: loss = 21.163251876831055, weights = tensor([6.1367, 0.6450], dtype=torch.float16)\n",
      "step №218: loss = 21.153289794921875, weights = tensor([6.1367, 0.6475], dtype=torch.float16)\n",
      "step №219: loss = 21.14333724975586, weights = tensor([6.1367, 0.6499], dtype=torch.float16)\n",
      "step №220: loss = 21.133392333984375, weights = tensor([6.1367, 0.6523], dtype=torch.float16)\n",
      "step №221: loss = 21.123462677001953, weights = tensor([6.1367, 0.6548], dtype=torch.float16)\n",
      "step №222: loss = 21.113544464111328, weights = tensor([6.1367, 0.6572], dtype=torch.float16)\n",
      "step №223: loss = 21.103641510009766, weights = tensor([6.1367, 0.6597], dtype=torch.float16)\n",
      "step №224: loss = 21.093746185302734, weights = tensor([6.1328, 0.6621], dtype=torch.float16)\n",
      "step №225: loss = 21.08292007446289, weights = tensor([6.1328, 0.6646], dtype=torch.float16)\n",
      "step №226: loss = 21.07296371459961, weights = tensor([6.1328, 0.6670], dtype=torch.float16)\n",
      "step №227: loss = 21.063024520874023, weights = tensor([6.1328, 0.6694], dtype=torch.float16)\n",
      "step №228: loss = 21.05309295654297, weights = tensor([6.1328, 0.6719], dtype=torch.float16)\n",
      "step №229: loss = 21.043170928955078, weights = tensor([6.1328, 0.6743], dtype=torch.float16)\n",
      "step №230: loss = 21.033262252807617, weights = tensor([6.1328, 0.6768], dtype=torch.float16)\n",
      "step №231: loss = 21.023366928100586, weights = tensor([6.1328, 0.6792], dtype=torch.float16)\n",
      "step №232: loss = 21.01348114013672, weights = tensor([6.1328, 0.6816], dtype=torch.float16)\n",
      "step №233: loss = 21.003604888916016, weights = tensor([6.1328, 0.6841], dtype=torch.float16)\n",
      "step №234: loss = 20.99374771118164, weights = tensor([6.1289, 0.6865], dtype=torch.float16)\n",
      "step №235: loss = 20.98297119140625, weights = tensor([6.1289, 0.6890], dtype=torch.float16)\n",
      "step №236: loss = 20.973047256469727, weights = tensor([6.1289, 0.6914], dtype=torch.float16)\n",
      "step №237: loss = 20.963136672973633, weights = tensor([6.1289, 0.6938], dtype=torch.float16)\n",
      "step №238: loss = 20.953235626220703, weights = tensor([6.1289, 0.6963], dtype=torch.float16)\n",
      "step №239: loss = 20.94335174560547, weights = tensor([6.1289, 0.6987], dtype=torch.float16)\n",
      "step №240: loss = 20.933475494384766, weights = tensor([6.1289, 0.7012], dtype=torch.float16)\n",
      "step №241: loss = 20.92361068725586, weights = tensor([6.1289, 0.7036], dtype=torch.float16)\n",
      "step №242: loss = 20.913761138916016, weights = tensor([6.1289, 0.7061], dtype=torch.float16)\n",
      "step №243: loss = 20.903919219970703, weights = tensor([6.1289, 0.7085], dtype=torch.float16)\n",
      "step №244: loss = 20.894094467163086, weights = tensor([6.1289, 0.7109], dtype=torch.float16)\n",
      "step №245: loss = 20.884279251098633, weights = tensor([6.1250, 0.7134], dtype=torch.float16)\n",
      "step №246: loss = 20.87347412109375, weights = tensor([6.1250, 0.7158], dtype=torch.float16)\n",
      "step №247: loss = 20.863597869873047, weights = tensor([6.1250, 0.7183], dtype=torch.float16)\n",
      "step №248: loss = 20.853729248046875, weights = tensor([6.1250, 0.7207], dtype=torch.float16)\n",
      "step №249: loss = 20.843873977661133, weights = tensor([6.1250, 0.7231], dtype=torch.float16)\n",
      "step №250: loss = 20.83403205871582, weights = tensor([6.1250, 0.7256], dtype=torch.float16)\n",
      "step №251: loss = 20.824203491210938, weights = tensor([6.1250, 0.7280], dtype=torch.float16)\n",
      "step №252: loss = 20.814388275146484, weights = tensor([6.1250, 0.7305], dtype=torch.float16)\n",
      "step №253: loss = 20.804580688476562, weights = tensor([6.1250, 0.7329], dtype=torch.float16)\n",
      "step №254: loss = 20.79478645324707, weights = tensor([6.1250, 0.7354], dtype=torch.float16)\n",
      "step №255: loss = 20.785003662109375, weights = tensor([6.1211, 0.7378], dtype=torch.float16)\n",
      "step №256: loss = 20.774242401123047, weights = tensor([6.1211, 0.7402], dtype=torch.float16)\n",
      "step №257: loss = 20.76439666748047, weights = tensor([6.1211, 0.7427], dtype=torch.float16)\n",
      "step №258: loss = 20.754566192626953, weights = tensor([6.1211, 0.7451], dtype=torch.float16)\n",
      "step №259: loss = 20.744747161865234, weights = tensor([6.1211, 0.7476], dtype=torch.float16)\n",
      "step №260: loss = 20.734939575195312, weights = tensor([6.1211, 0.7500], dtype=torch.float16)\n",
      "step №261: loss = 20.725141525268555, weights = tensor([6.1211, 0.7524], dtype=torch.float16)\n",
      "step №262: loss = 20.71535873413086, weights = tensor([6.1211, 0.7549], dtype=torch.float16)\n",
      "step №263: loss = 20.705585479736328, weights = tensor([6.1211, 0.7573], dtype=torch.float16)\n",
      "step №264: loss = 20.695823669433594, weights = tensor([6.1211, 0.7598], dtype=torch.float16)\n",
      "step №265: loss = 20.686073303222656, weights = tensor([6.1172, 0.7622], dtype=torch.float16)\n",
      "step №266: loss = 20.675357818603516, weights = tensor([6.1172, 0.7646], dtype=torch.float16)\n",
      "step №267: loss = 20.665546417236328, weights = tensor([6.1172, 0.7671], dtype=torch.float16)\n",
      "step №268: loss = 20.655750274658203, weights = tensor([6.1172, 0.7695], dtype=torch.float16)\n",
      "step №269: loss = 20.64596176147461, weights = tensor([6.1172, 0.7720], dtype=torch.float16)\n",
      "step №270: loss = 20.636188507080078, weights = tensor([6.1172, 0.7744], dtype=torch.float16)\n",
      "step №271: loss = 20.626422882080078, weights = tensor([6.1172, 0.7769], dtype=torch.float16)\n",
      "step №272: loss = 20.61667251586914, weights = tensor([6.1172, 0.7793], dtype=torch.float16)\n",
      "step №273: loss = 20.606931686401367, weights = tensor([6.1172, 0.7817], dtype=torch.float16)\n",
      "step №274: loss = 20.59720802307129, weights = tensor([6.1172, 0.7842], dtype=torch.float16)\n",
      "step №275: loss = 20.587491989135742, weights = tensor([6.1133, 0.7866], dtype=torch.float16)\n",
      "step №276: loss = 20.576818466186523, weights = tensor([6.1133, 0.7891], dtype=torch.float16)\n",
      "step №277: loss = 20.56704330444336, weights = tensor([6.1133, 0.7915], dtype=torch.float16)\n",
      "step №278: loss = 20.55727767944336, weights = tensor([6.1133, 0.7939], dtype=torch.float16)\n",
      "step №279: loss = 20.547523498535156, weights = tensor([6.1133, 0.7964], dtype=torch.float16)\n",
      "step №280: loss = 20.537782669067383, weights = tensor([6.1133, 0.7988], dtype=torch.float16)\n",
      "step №281: loss = 20.52804946899414, weights = tensor([6.1133, 0.8013], dtype=torch.float16)\n",
      "step №282: loss = 20.518335342407227, weights = tensor([6.1133, 0.8037], dtype=torch.float16)\n",
      "step №283: loss = 20.508630752563477, weights = tensor([6.1133, 0.8062], dtype=torch.float16)\n",
      "step №284: loss = 20.49893569946289, weights = tensor([6.1133, 0.8086], dtype=torch.float16)\n",
      "step №285: loss = 20.489253997802734, weights = tensor([6.1094, 0.8110], dtype=torch.float16)\n",
      "step №286: loss = 20.478626251220703, weights = tensor([6.1094, 0.8135], dtype=torch.float16)\n",
      "step №287: loss = 20.468881607055664, weights = tensor([6.1094, 0.8159], dtype=torch.float16)\n",
      "step №288: loss = 20.459152221679688, weights = tensor([6.1094, 0.8184], dtype=torch.float16)\n",
      "step №289: loss = 20.44942855834961, weights = tensor([6.1094, 0.8208], dtype=torch.float16)\n",
      "step №290: loss = 20.439722061157227, weights = tensor([6.1094, 0.8232], dtype=torch.float16)\n",
      "step №291: loss = 20.430028915405273, weights = tensor([6.1094, 0.8257], dtype=torch.float16)\n",
      "step №292: loss = 20.420337677001953, weights = tensor([6.1094, 0.8281], dtype=torch.float16)\n",
      "step №293: loss = 20.410669326782227, weights = tensor([6.1094, 0.8306], dtype=torch.float16)\n",
      "step №294: loss = 20.401004791259766, weights = tensor([6.1094, 0.8330], dtype=torch.float16)\n",
      "step №295: loss = 20.3913631439209, weights = tensor([6.1055, 0.8354], dtype=torch.float16)\n",
      "step №296: loss = 20.38077735900879, weights = tensor([6.1055, 0.8379], dtype=torch.float16)\n",
      "step №297: loss = 20.371065139770508, weights = tensor([6.1055, 0.8403], dtype=torch.float16)\n",
      "step №298: loss = 20.361370086669922, weights = tensor([6.1055, 0.8428], dtype=torch.float16)\n",
      "step №299: loss = 20.351682662963867, weights = tensor([6.1055, 0.8452], dtype=torch.float16)\n",
      "step №300: loss = 20.34200668334961, weights = tensor([6.1055, 0.8477], dtype=torch.float16)\n",
      "step №301: loss = 20.33234405517578, weights = tensor([6.1055, 0.8501], dtype=torch.float16)\n",
      "step №302: loss = 20.322694778442383, weights = tensor([6.1055, 0.8525], dtype=torch.float16)\n",
      "step №303: loss = 20.31305503845215, weights = tensor([6.1055, 0.8550], dtype=torch.float16)\n",
      "step №304: loss = 20.30342674255371, weights = tensor([6.1055, 0.8574], dtype=torch.float16)\n",
      "step №305: loss = 20.29380989074707, weights = tensor([6.1016, 0.8599], dtype=torch.float16)\n",
      "step №306: loss = 20.283275604248047, weights = tensor([6.1016, 0.8623], dtype=torch.float16)\n",
      "step №307: loss = 20.273597717285156, weights = tensor([6.1016, 0.8647], dtype=torch.float16)\n",
      "step №308: loss = 20.26392936706543, weights = tensor([6.1016, 0.8672], dtype=torch.float16)\n",
      "step №309: loss = 20.2542781829834, weights = tensor([6.1016, 0.8696], dtype=torch.float16)\n",
      "step №310: loss = 20.24463653564453, weights = tensor([6.1016, 0.8721], dtype=torch.float16)\n",
      "step №311: loss = 20.235008239746094, weights = tensor([6.1016, 0.8745], dtype=torch.float16)\n",
      "step №312: loss = 20.225391387939453, weights = tensor([6.1016, 0.8770], dtype=torch.float16)\n",
      "step №313: loss = 20.215782165527344, weights = tensor([6.1016, 0.8794], dtype=torch.float16)\n",
      "step №314: loss = 20.20619010925293, weights = tensor([6.1016, 0.8818], dtype=torch.float16)\n",
      "step №315: loss = 20.19660758972168, weights = tensor([6.0977, 0.8843], dtype=torch.float16)\n",
      "step №316: loss = 20.18611717224121, weights = tensor([6.0977, 0.8867], dtype=torch.float16)\n",
      "step №317: loss = 20.176471710205078, weights = tensor([6.0977, 0.8892], dtype=torch.float16)\n",
      "step №318: loss = 20.166841506958008, weights = tensor([6.0977, 0.8916], dtype=torch.float16)\n",
      "step №319: loss = 20.157222747802734, weights = tensor([6.0977, 0.8940], dtype=torch.float16)\n",
      "step №320: loss = 20.147613525390625, weights = tensor([6.0977, 0.8965], dtype=torch.float16)\n",
      "step №321: loss = 20.13801383972168, weights = tensor([6.0977, 0.8989], dtype=torch.float16)\n",
      "step №322: loss = 20.12843132019043, weights = tensor([6.0977, 0.9014], dtype=torch.float16)\n",
      "step №323: loss = 20.118860244750977, weights = tensor([6.0977, 0.9038], dtype=torch.float16)\n",
      "step №324: loss = 20.10930061340332, weights = tensor([6.0977, 0.9062], dtype=torch.float16)\n",
      "step №325: loss = 20.099750518798828, weights = tensor([6.0977, 0.9087], dtype=torch.float16)\n",
      "step №326: loss = 20.0902156829834, weights = tensor([6.0938, 0.9111], dtype=torch.float16)\n",
      "step №327: loss = 20.079696655273438, weights = tensor([6.0938, 0.9136], dtype=torch.float16)\n",
      "step №328: loss = 20.070098876953125, weights = tensor([6.0938, 0.9160], dtype=torch.float16)\n",
      "step №329: loss = 20.060504913330078, weights = tensor([6.0938, 0.9185], dtype=torch.float16)\n",
      "step №330: loss = 20.050935745239258, weights = tensor([6.0938, 0.9209], dtype=torch.float16)\n",
      "step №331: loss = 20.041370391845703, weights = tensor([6.0938, 0.9233], dtype=torch.float16)\n",
      "step №332: loss = 20.031818389892578, weights = tensor([6.0938, 0.9258], dtype=torch.float16)\n",
      "step №333: loss = 20.022281646728516, weights = tensor([6.0938, 0.9282], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №334: loss = 20.01275634765625, weights = tensor([6.0938, 0.9307], dtype=torch.float16)\n",
      "step №335: loss = 20.003238677978516, weights = tensor([6.0938, 0.9331], dtype=torch.float16)\n",
      "step №336: loss = 19.993732452392578, weights = tensor([6.0898, 0.9355], dtype=torch.float16)\n",
      "step №337: loss = 19.983257293701172, weights = tensor([6.0898, 0.9380], dtype=torch.float16)\n",
      "step №338: loss = 19.973695755004883, weights = tensor([6.0898, 0.9404], dtype=torch.float16)\n",
      "step №339: loss = 19.964139938354492, weights = tensor([6.0898, 0.9429], dtype=torch.float16)\n",
      "step №340: loss = 19.954599380493164, weights = tensor([6.0898, 0.9453], dtype=torch.float16)\n",
      "step №341: loss = 19.945070266723633, weights = tensor([6.0898, 0.9478], dtype=torch.float16)\n",
      "step №342: loss = 19.9355525970459, weights = tensor([6.0898, 0.9502], dtype=torch.float16)\n",
      "step №343: loss = 19.926044464111328, weights = tensor([6.0898, 0.9526], dtype=torch.float16)\n",
      "step №344: loss = 19.916553497314453, weights = tensor([6.0898, 0.9551], dtype=torch.float16)\n",
      "step №345: loss = 19.90707015991211, weights = tensor([6.0898, 0.9575], dtype=torch.float16)\n",
      "step №346: loss = 19.897602081298828, weights = tensor([6.0859, 0.9600], dtype=torch.float16)\n",
      "step №347: loss = 19.887168884277344, weights = tensor([6.0859, 0.9624], dtype=torch.float16)\n",
      "step №348: loss = 19.87763786315918, weights = tensor([6.0859, 0.9648], dtype=torch.float16)\n",
      "step №349: loss = 19.868118286132812, weights = tensor([6.0859, 0.9673], dtype=torch.float16)\n",
      "step №350: loss = 19.858612060546875, weights = tensor([6.0859, 0.9697], dtype=torch.float16)\n",
      "step №351: loss = 19.84911346435547, weights = tensor([6.0859, 0.9722], dtype=torch.float16)\n",
      "step №352: loss = 19.839630126953125, weights = tensor([6.0859, 0.9746], dtype=torch.float16)\n",
      "step №353: loss = 19.830156326293945, weights = tensor([6.0859, 0.9771], dtype=torch.float16)\n",
      "step №354: loss = 19.820697784423828, weights = tensor([6.0859, 0.9795], dtype=torch.float16)\n",
      "step №355: loss = 19.81125259399414, weights = tensor([6.0859, 0.9819], dtype=torch.float16)\n",
      "step №356: loss = 19.801815032958984, weights = tensor([6.0820, 0.9844], dtype=torch.float16)\n",
      "step №357: loss = 19.791425704956055, weights = tensor([6.0820, 0.9868], dtype=torch.float16)\n",
      "step №358: loss = 19.78192901611328, weights = tensor([6.0820, 0.9893], dtype=torch.float16)\n",
      "step №359: loss = 19.772441864013672, weights = tensor([6.0820, 0.9917], dtype=torch.float16)\n",
      "step №360: loss = 19.76296615600586, weights = tensor([6.0820, 0.9941], dtype=torch.float16)\n",
      "step №361: loss = 19.753501892089844, weights = tensor([6.0820, 0.9966], dtype=torch.float16)\n",
      "step №362: loss = 19.744054794311523, weights = tensor([6.0820, 0.9990], dtype=torch.float16)\n",
      "step №363: loss = 19.734615325927734, weights = tensor([6.0820, 1.0010], dtype=torch.float16)\n",
      "step №364: loss = 19.727073669433594, weights = tensor([6.0820, 1.0029], dtype=torch.float16)\n",
      "step №365: loss = 19.71953582763672, weights = tensor([6.0820, 1.0049], dtype=torch.float16)\n",
      "step №366: loss = 19.712011337280273, weights = tensor([6.0820, 1.0068], dtype=torch.float16)\n",
      "step №367: loss = 19.704492568969727, weights = tensor([6.0781, 1.0088], dtype=torch.float16)\n",
      "step №368: loss = 19.696025848388672, weights = tensor([6.0781, 1.0107], dtype=torch.float16)\n",
      "step №369: loss = 19.688457489013672, weights = tensor([6.0781, 1.0127], dtype=torch.float16)\n",
      "step №370: loss = 19.680891036987305, weights = tensor([6.0781, 1.0146], dtype=torch.float16)\n",
      "step №371: loss = 19.673330307006836, weights = tensor([6.0781, 1.0166], dtype=torch.float16)\n",
      "step №372: loss = 19.665781021118164, weights = tensor([6.0781, 1.0186], dtype=torch.float16)\n",
      "step №373: loss = 19.658241271972656, weights = tensor([6.0781, 1.0205], dtype=torch.float16)\n",
      "step №374: loss = 19.650707244873047, weights = tensor([6.0781, 1.0225], dtype=torch.float16)\n",
      "step №375: loss = 19.643177032470703, weights = tensor([6.0781, 1.0244], dtype=torch.float16)\n",
      "step №376: loss = 19.635656356811523, weights = tensor([6.0781, 1.0264], dtype=torch.float16)\n",
      "step №377: loss = 19.628149032592773, weights = tensor([6.0781, 1.0283], dtype=torch.float16)\n",
      "step №378: loss = 19.620643615722656, weights = tensor([6.0781, 1.0303], dtype=torch.float16)\n",
      "step №379: loss = 19.613143920898438, weights = tensor([6.0742, 1.0322], dtype=torch.float16)\n",
      "step №380: loss = 19.604751586914062, weights = tensor([6.0742, 1.0342], dtype=torch.float16)\n",
      "step №381: loss = 19.597200393676758, weights = tensor([6.0742, 1.0361], dtype=torch.float16)\n",
      "step №382: loss = 19.589656829833984, weights = tensor([6.0742, 1.0381], dtype=torch.float16)\n",
      "step №383: loss = 19.582124710083008, weights = tensor([6.0742, 1.0400], dtype=torch.float16)\n",
      "step №384: loss = 19.574596405029297, weights = tensor([6.0742, 1.0420], dtype=torch.float16)\n",
      "step №385: loss = 19.56707763671875, weights = tensor([6.0742, 1.0439], dtype=torch.float16)\n",
      "step №386: loss = 19.55956268310547, weights = tensor([6.0742, 1.0459], dtype=torch.float16)\n",
      "step №387: loss = 19.552061080932617, weights = tensor([6.0742, 1.0479], dtype=torch.float16)\n",
      "step №388: loss = 19.544565200805664, weights = tensor([6.0742, 1.0498], dtype=torch.float16)\n",
      "step №389: loss = 19.53707504272461, weights = tensor([6.0742, 1.0518], dtype=torch.float16)\n",
      "step №390: loss = 19.529592514038086, weights = tensor([6.0742, 1.0537], dtype=torch.float16)\n",
      "step №391: loss = 19.52212142944336, weights = tensor([6.0742, 1.0557], dtype=torch.float16)\n",
      "step №392: loss = 19.5146541595459, weights = tensor([6.0703, 1.0576], dtype=torch.float16)\n",
      "step №393: loss = 19.506267547607422, weights = tensor([6.0703, 1.0596], dtype=torch.float16)\n",
      "step №394: loss = 19.498746871948242, weights = tensor([6.0703, 1.0615], dtype=torch.float16)\n",
      "step №395: loss = 19.491233825683594, weights = tensor([6.0703, 1.0635], dtype=torch.float16)\n",
      "step №396: loss = 19.48373031616211, weights = tensor([6.0703, 1.0654], dtype=torch.float16)\n",
      "step №397: loss = 19.476234436035156, weights = tensor([6.0703, 1.0674], dtype=torch.float16)\n",
      "step №398: loss = 19.468746185302734, weights = tensor([6.0703, 1.0693], dtype=torch.float16)\n",
      "step №399: loss = 19.461261749267578, weights = tensor([6.0703, 1.0713], dtype=torch.float16)\n",
      "step №400: loss = 19.45378875732422, weights = tensor([6.0703, 1.0732], dtype=torch.float16)\n",
      "step №401: loss = 19.446325302124023, weights = tensor([6.0703, 1.0752], dtype=torch.float16)\n",
      "step №402: loss = 19.438865661621094, weights = tensor([6.0703, 1.0771], dtype=torch.float16)\n",
      "step №403: loss = 19.431413650512695, weights = tensor([6.0703, 1.0791], dtype=torch.float16)\n",
      "step №404: loss = 19.423969268798828, weights = tensor([6.0703, 1.0811], dtype=torch.float16)\n",
      "step №405: loss = 19.416536331176758, weights = tensor([6.0664, 1.0830], dtype=torch.float16)\n",
      "step №406: loss = 19.408157348632812, weights = tensor([6.0664, 1.0850], dtype=torch.float16)\n",
      "step №407: loss = 19.40066909790039, weights = tensor([6.0664, 1.0869], dtype=torch.float16)\n",
      "step №408: loss = 19.393186569213867, weights = tensor([6.0664, 1.0889], dtype=torch.float16)\n",
      "step №409: loss = 19.385711669921875, weights = tensor([6.0664, 1.0908], dtype=torch.float16)\n",
      "step №410: loss = 19.378246307373047, weights = tensor([6.0664, 1.0928], dtype=torch.float16)\n",
      "step №411: loss = 19.37078857421875, weights = tensor([6.0664, 1.0947], dtype=torch.float16)\n",
      "step №412: loss = 19.363338470458984, weights = tensor([6.0664, 1.0967], dtype=torch.float16)\n",
      "step №413: loss = 19.355892181396484, weights = tensor([6.0664, 1.0986], dtype=torch.float16)\n",
      "step №414: loss = 19.348459243774414, weights = tensor([6.0664, 1.1006], dtype=torch.float16)\n",
      "step №415: loss = 19.34103012084961, weights = tensor([6.0664, 1.1025], dtype=torch.float16)\n",
      "step №416: loss = 19.33361053466797, weights = tensor([6.0664, 1.1045], dtype=torch.float16)\n",
      "step №417: loss = 19.326196670532227, weights = tensor([6.0625, 1.1064], dtype=torch.float16)\n",
      "step №418: loss = 19.317888259887695, weights = tensor([6.0625, 1.1084], dtype=torch.float16)\n",
      "step №419: loss = 19.3104190826416, weights = tensor([6.0625, 1.1104], dtype=torch.float16)\n",
      "step №420: loss = 19.302959442138672, weights = tensor([6.0625, 1.1123], dtype=torch.float16)\n",
      "step №421: loss = 19.295513153076172, weights = tensor([6.0625, 1.1143], dtype=torch.float16)\n",
      "step №422: loss = 19.288070678710938, weights = tensor([6.0625, 1.1162], dtype=torch.float16)\n",
      "step №423: loss = 19.280630111694336, weights = tensor([6.0625, 1.1182], dtype=torch.float16)\n",
      "step №424: loss = 19.273202896118164, weights = tensor([6.0625, 1.1201], dtype=torch.float16)\n",
      "step №425: loss = 19.26578712463379, weights = tensor([6.0625, 1.1221], dtype=torch.float16)\n",
      "step №426: loss = 19.258373260498047, weights = tensor([6.0625, 1.1240], dtype=torch.float16)\n",
      "step №427: loss = 19.250965118408203, weights = tensor([6.0625, 1.1260], dtype=torch.float16)\n",
      "step №428: loss = 19.243566513061523, weights = tensor([6.0625, 1.1279], dtype=torch.float16)\n",
      "step №429: loss = 19.23617935180664, weights = tensor([6.0625, 1.1299], dtype=torch.float16)\n",
      "step №430: loss = 19.22879981994629, weights = tensor([6.0586, 1.1318], dtype=torch.float16)\n",
      "step №431: loss = 19.22049331665039, weights = tensor([6.0586, 1.1338], dtype=torch.float16)\n",
      "step №432: loss = 19.213058471679688, weights = tensor([6.0586, 1.1357], dtype=torch.float16)\n",
      "step №433: loss = 19.205631256103516, weights = tensor([6.0586, 1.1377], dtype=torch.float16)\n",
      "step №434: loss = 19.19820785522461, weights = tensor([6.0586, 1.1396], dtype=torch.float16)\n",
      "step №435: loss = 19.190797805786133, weights = tensor([6.0586, 1.1416], dtype=torch.float16)\n",
      "step №436: loss = 19.183391571044922, weights = tensor([6.0586, 1.1436], dtype=torch.float16)\n",
      "step №437: loss = 19.175992965698242, weights = tensor([6.0586, 1.1455], dtype=torch.float16)\n",
      "step №438: loss = 19.168603897094727, weights = tensor([6.0586, 1.1475], dtype=torch.float16)\n",
      "step №439: loss = 19.161222457885742, weights = tensor([6.0586, 1.1494], dtype=torch.float16)\n",
      "step №440: loss = 19.15384864807129, weights = tensor([6.0586, 1.1514], dtype=torch.float16)\n",
      "step №441: loss = 19.146480560302734, weights = tensor([6.0586, 1.1533], dtype=torch.float16)\n",
      "step №442: loss = 19.139118194580078, weights = tensor([6.0586, 1.1553], dtype=torch.float16)\n",
      "step №443: loss = 19.131771087646484, weights = tensor([6.0547, 1.1572], dtype=torch.float16)\n",
      "step №444: loss = 19.123472213745117, weights = tensor([6.0547, 1.1592], dtype=torch.float16)\n",
      "step №445: loss = 19.116069793701172, weights = tensor([6.0547, 1.1611], dtype=torch.float16)\n",
      "step №446: loss = 19.108673095703125, weights = tensor([6.0547, 1.1631], dtype=torch.float16)\n",
      "step №447: loss = 19.101280212402344, weights = tensor([6.0547, 1.1650], dtype=torch.float16)\n",
      "step №448: loss = 19.09389877319336, weights = tensor([6.0547, 1.1670], dtype=torch.float16)\n",
      "step №449: loss = 19.08652687072754, weights = tensor([6.0547, 1.1689], dtype=torch.float16)\n",
      "step №450: loss = 19.079158782958984, weights = tensor([6.0547, 1.1709], dtype=torch.float16)\n",
      "step №451: loss = 19.071796417236328, weights = tensor([6.0547, 1.1729], dtype=torch.float16)\n",
      "step №452: loss = 19.06444549560547, weights = tensor([6.0547, 1.1748], dtype=torch.float16)\n",
      "step №453: loss = 19.05710220336914, weights = tensor([6.0547, 1.1768], dtype=torch.float16)\n",
      "step №454: loss = 19.049768447875977, weights = tensor([6.0547, 1.1787], dtype=torch.float16)\n",
      "step №455: loss = 19.042436599731445, weights = tensor([6.0508, 1.1807], dtype=torch.float16)\n",
      "step №456: loss = 19.034212112426758, weights = tensor([6.0508, 1.1826], dtype=torch.float16)\n",
      "step №457: loss = 19.02682876586914, weights = tensor([6.0508, 1.1846], dtype=torch.float16)\n",
      "step №458: loss = 19.019453048706055, weights = tensor([6.0508, 1.1865], dtype=torch.float16)\n",
      "step №459: loss = 19.012088775634766, weights = tensor([6.0508, 1.1885], dtype=torch.float16)\n",
      "step №460: loss = 19.004728317260742, weights = tensor([6.0508, 1.1904], dtype=torch.float16)\n",
      "step №461: loss = 18.997373580932617, weights = tensor([6.0508, 1.1924], dtype=torch.float16)\n",
      "step №462: loss = 18.990032196044922, weights = tensor([6.0508, 1.1943], dtype=torch.float16)\n",
      "step №463: loss = 18.982696533203125, weights = tensor([6.0508, 1.1963], dtype=torch.float16)\n",
      "step №464: loss = 18.97536849975586, weights = tensor([6.0508, 1.1982], dtype=torch.float16)\n",
      "step №465: loss = 18.96804428100586, weights = tensor([6.0508, 1.2002], dtype=torch.float16)\n",
      "step №466: loss = 18.960731506347656, weights = tensor([6.0508, 1.2021], dtype=torch.float16)\n",
      "step №467: loss = 18.953426361083984, weights = tensor([6.0508, 1.2041], dtype=torch.float16)\n",
      "step №468: loss = 18.946128845214844, weights = tensor([6.0469, 1.2061], dtype=torch.float16)\n",
      "step №469: loss = 18.937910079956055, weights = tensor([6.0469, 1.2080], dtype=torch.float16)\n",
      "step №470: loss = 18.930561065673828, weights = tensor([6.0469, 1.2100], dtype=torch.float16)\n",
      "step №471: loss = 18.9232120513916, weights = tensor([6.0469, 1.2119], dtype=torch.float16)\n",
      "step №472: loss = 18.915874481201172, weights = tensor([6.0469, 1.2139], dtype=torch.float16)\n",
      "step №473: loss = 18.908550262451172, weights = tensor([6.0469, 1.2158], dtype=torch.float16)\n",
      "step №474: loss = 18.901227951049805, weights = tensor([6.0469, 1.2178], dtype=torch.float16)\n",
      "step №475: loss = 18.893911361694336, weights = tensor([6.0469, 1.2197], dtype=torch.float16)\n",
      "step №476: loss = 18.886606216430664, weights = tensor([6.0469, 1.2217], dtype=torch.float16)\n",
      "step №477: loss = 18.879310607910156, weights = tensor([6.0469, 1.2236], dtype=torch.float16)\n",
      "step №478: loss = 18.872020721435547, weights = tensor([6.0469, 1.2256], dtype=torch.float16)\n",
      "step №479: loss = 18.864734649658203, weights = tensor([6.0469, 1.2275], dtype=torch.float16)\n",
      "step №480: loss = 18.857458114624023, weights = tensor([6.0469, 1.2295], dtype=torch.float16)\n",
      "step №481: loss = 18.850194931030273, weights = tensor([6.0430, 1.2314], dtype=torch.float16)\n",
      "step №482: loss = 18.841978073120117, weights = tensor([6.0430, 1.2334], dtype=torch.float16)\n",
      "step №483: loss = 18.834659576416016, weights = tensor([6.0430, 1.2354], dtype=torch.float16)\n",
      "step №484: loss = 18.827346801757812, weights = tensor([6.0430, 1.2373], dtype=torch.float16)\n",
      "step №485: loss = 18.820039749145508, weights = tensor([6.0430, 1.2393], dtype=torch.float16)\n",
      "step №486: loss = 18.812740325927734, weights = tensor([6.0430, 1.2412], dtype=torch.float16)\n",
      "step №487: loss = 18.805452346801758, weights = tensor([6.0430, 1.2432], dtype=torch.float16)\n",
      "step №488: loss = 18.798168182373047, weights = tensor([6.0430, 1.2451], dtype=torch.float16)\n",
      "step №489: loss = 18.7908935546875, weights = tensor([6.0430, 1.2471], dtype=torch.float16)\n",
      "step №490: loss = 18.78362274169922, weights = tensor([6.0430, 1.2490], dtype=torch.float16)\n",
      "step №491: loss = 18.776365280151367, weights = tensor([6.0430, 1.2510], dtype=torch.float16)\n",
      "step №492: loss = 18.769113540649414, weights = tensor([6.0430, 1.2529], dtype=torch.float16)\n",
      "step №493: loss = 18.76186752319336, weights = tensor([6.0391, 1.2549], dtype=torch.float16)\n",
      "step №494: loss = 18.753726959228516, weights = tensor([6.0391, 1.2568], dtype=torch.float16)\n",
      "step №495: loss = 18.74642562866211, weights = tensor([6.0391, 1.2588], dtype=torch.float16)\n",
      "step №496: loss = 18.739133834838867, weights = tensor([6.0391, 1.2607], dtype=torch.float16)\n",
      "step №497: loss = 18.731853485107422, weights = tensor([6.0391, 1.2627], dtype=torch.float16)\n",
      "step №498: loss = 18.724576950073242, weights = tensor([6.0391, 1.2646], dtype=torch.float16)\n",
      "step №499: loss = 18.717308044433594, weights = tensor([6.0391, 1.2666], dtype=torch.float16)\n",
      "step №500: loss = 18.71004867553711, weights = tensor([6.0391, 1.2686], dtype=torch.float16)\n",
      "step №501: loss = 18.702796936035156, weights = tensor([6.0391, 1.2705], dtype=torch.float16)\n",
      "step №502: loss = 18.695552825927734, weights = tensor([6.0391, 1.2725], dtype=torch.float16)\n",
      "step №503: loss = 18.688312530517578, weights = tensor([6.0391, 1.2744], dtype=torch.float16)\n",
      "step №504: loss = 18.68108367919922, weights = tensor([6.0391, 1.2764], dtype=torch.float16)\n",
      "step №505: loss = 18.673864364624023, weights = tensor([6.0391, 1.2783], dtype=torch.float16)\n",
      "step №506: loss = 18.666648864746094, weights = tensor([6.0352, 1.2803], dtype=torch.float16)\n",
      "step №507: loss = 18.65851402282715, weights = tensor([6.0352, 1.2822], dtype=torch.float16)\n",
      "step №508: loss = 18.651247024536133, weights = tensor([6.0352, 1.2842], dtype=torch.float16)\n",
      "step №509: loss = 18.643983840942383, weights = tensor([6.0352, 1.2861], dtype=torch.float16)\n",
      "step №510: loss = 18.636734008789062, weights = tensor([6.0352, 1.2881], dtype=torch.float16)\n",
      "step №511: loss = 18.62948989868164, weights = tensor([6.0352, 1.2900], dtype=torch.float16)\n",
      "step №512: loss = 18.622251510620117, weights = tensor([6.0352, 1.2920], dtype=torch.float16)\n",
      "step №513: loss = 18.615020751953125, weights = tensor([6.0352, 1.2939], dtype=torch.float16)\n",
      "step №514: loss = 18.607799530029297, weights = tensor([6.0352, 1.2959], dtype=torch.float16)\n",
      "step №515: loss = 18.6005859375, weights = tensor([6.0352, 1.2979], dtype=torch.float16)\n",
      "step №516: loss = 18.593379974365234, weights = tensor([6.0352, 1.2998], dtype=torch.float16)\n",
      "step №517: loss = 18.586177825927734, weights = tensor([6.0352, 1.3018], dtype=torch.float16)\n",
      "step №518: loss = 18.578989028930664, weights = tensor([6.0352, 1.3037], dtype=torch.float16)\n",
      "step №519: loss = 18.57180404663086, weights = tensor([6.0312, 1.3057], dtype=torch.float16)\n",
      "step №520: loss = 18.563674926757812, weights = tensor([6.0312, 1.3076], dtype=torch.float16)\n",
      "step №521: loss = 18.556442260742188, weights = tensor([6.0312, 1.3096], dtype=torch.float16)\n",
      "step №522: loss = 18.549211502075195, weights = tensor([6.0312, 1.3115], dtype=torch.float16)\n",
      "step №523: loss = 18.5419864654541, weights = tensor([6.0312, 1.3135], dtype=torch.float16)\n",
      "step №524: loss = 18.534770965576172, weights = tensor([6.0312, 1.3154], dtype=torch.float16)\n",
      "step №525: loss = 18.527568817138672, weights = tensor([6.0312, 1.3174], dtype=torch.float16)\n",
      "step №526: loss = 18.520370483398438, weights = tensor([6.0312, 1.3193], dtype=torch.float16)\n",
      "step №527: loss = 18.513174057006836, weights = tensor([6.0312, 1.3213], dtype=torch.float16)\n",
      "step №528: loss = 18.505990982055664, weights = tensor([6.0312, 1.3232], dtype=torch.float16)\n",
      "step №529: loss = 18.49881935119629, weights = tensor([6.0312, 1.3252], dtype=torch.float16)\n",
      "step №530: loss = 18.491649627685547, weights = tensor([6.0312, 1.3271], dtype=torch.float16)\n",
      "step №531: loss = 18.484485626220703, weights = tensor([6.0273, 1.3291], dtype=torch.float16)\n",
      "step №532: loss = 18.476428985595703, weights = tensor([6.0273, 1.3311], dtype=torch.float16)\n",
      "step №533: loss = 18.46921157836914, weights = tensor([6.0273, 1.3330], dtype=torch.float16)\n",
      "step №534: loss = 18.462005615234375, weights = tensor([6.0273, 1.3350], dtype=torch.float16)\n",
      "step №535: loss = 18.45480728149414, weights = tensor([6.0273, 1.3369], dtype=torch.float16)\n",
      "step №536: loss = 18.447616577148438, weights = tensor([6.0273, 1.3389], dtype=torch.float16)\n",
      "step №537: loss = 18.440433502197266, weights = tensor([6.0273, 1.3408], dtype=torch.float16)\n",
      "step №538: loss = 18.43325424194336, weights = tensor([6.0273, 1.3428], dtype=torch.float16)\n",
      "step №539: loss = 18.426088333129883, weights = tensor([6.0273, 1.3447], dtype=torch.float16)\n",
      "step №540: loss = 18.418926239013672, weights = tensor([6.0273, 1.3467], dtype=torch.float16)\n",
      "step №541: loss = 18.411771774291992, weights = tensor([6.0273, 1.3486], dtype=torch.float16)\n",
      "step №542: loss = 18.404626846313477, weights = tensor([6.0273, 1.3506], dtype=torch.float16)\n",
      "step №543: loss = 18.397489547729492, weights = tensor([6.0273, 1.3525], dtype=torch.float16)\n",
      "step №544: loss = 18.39035987854004, weights = tensor([6.0234, 1.3545], dtype=torch.float16)\n",
      "step №545: loss = 18.382308959960938, weights = tensor([6.0234, 1.3564], dtype=torch.float16)\n",
      "step №546: loss = 18.375123977661133, weights = tensor([6.0234, 1.3584], dtype=torch.float16)\n",
      "step №547: loss = 18.36794662475586, weights = tensor([6.0234, 1.3604], dtype=torch.float16)\n",
      "step №548: loss = 18.360776901245117, weights = tensor([6.0234, 1.3623], dtype=torch.float16)\n",
      "step №549: loss = 18.353618621826172, weights = tensor([6.0234, 1.3643], dtype=torch.float16)\n",
      "step №550: loss = 18.346466064453125, weights = tensor([6.0234, 1.3662], dtype=torch.float16)\n",
      "step №551: loss = 18.339317321777344, weights = tensor([6.0234, 1.3682], dtype=torch.float16)\n",
      "step №552: loss = 18.33218002319336, weights = tensor([6.0234, 1.3701], dtype=torch.float16)\n",
      "step №553: loss = 18.32505226135254, weights = tensor([6.0234, 1.3721], dtype=torch.float16)\n",
      "step №554: loss = 18.317928314208984, weights = tensor([6.0234, 1.3740], dtype=torch.float16)\n",
      "step №555: loss = 18.310810089111328, weights = tensor([6.0234, 1.3760], dtype=torch.float16)\n",
      "step №556: loss = 18.30370330810547, weights = tensor([6.0234, 1.3779], dtype=torch.float16)\n",
      "step №557: loss = 18.29660415649414, weights = tensor([6.0195, 1.3799], dtype=torch.float16)\n",
      "step №558: loss = 18.28856086730957, weights = tensor([6.0195, 1.3818], dtype=torch.float16)\n",
      "step №559: loss = 18.28140640258789, weights = tensor([6.0195, 1.3838], dtype=torch.float16)\n",
      "step №560: loss = 18.274263381958008, weights = tensor([6.0195, 1.3857], dtype=torch.float16)\n",
      "step №561: loss = 18.26712417602539, weights = tensor([6.0195, 1.3877], dtype=torch.float16)\n",
      "step №562: loss = 18.259992599487305, weights = tensor([6.0195, 1.3896], dtype=torch.float16)\n",
      "step №563: loss = 18.252872467041016, weights = tensor([6.0195, 1.3916], dtype=torch.float16)\n",
      "step №564: loss = 18.24575424194336, weights = tensor([6.0195, 1.3936], dtype=torch.float16)\n",
      "step №565: loss = 18.2386474609375, weights = tensor([6.0195, 1.3955], dtype=torch.float16)\n",
      "step №566: loss = 18.231548309326172, weights = tensor([6.0195, 1.3975], dtype=torch.float16)\n",
      "step №567: loss = 18.224454879760742, weights = tensor([6.0195, 1.3994], dtype=torch.float16)\n",
      "step №568: loss = 18.21737289428711, weights = tensor([6.0195, 1.4014], dtype=torch.float16)\n",
      "step №569: loss = 18.21029281616211, weights = tensor([6.0156, 1.4033], dtype=torch.float16)\n",
      "step №570: loss = 18.202320098876953, weights = tensor([6.0156, 1.4053], dtype=torch.float16)\n",
      "step №571: loss = 18.195186614990234, weights = tensor([6.0156, 1.4072], dtype=torch.float16)\n",
      "step №572: loss = 18.188064575195312, weights = tensor([6.0156, 1.4092], dtype=torch.float16)\n",
      "step №573: loss = 18.180952072143555, weights = tensor([6.0156, 1.4111], dtype=torch.float16)\n",
      "step №574: loss = 18.173843383789062, weights = tensor([6.0156, 1.4131], dtype=torch.float16)\n",
      "step №575: loss = 18.1667423248291, weights = tensor([6.0156, 1.4150], dtype=torch.float16)\n",
      "step №576: loss = 18.159648895263672, weights = tensor([6.0156, 1.4170], dtype=torch.float16)\n",
      "step №577: loss = 18.15256690979004, weights = tensor([6.0156, 1.4189], dtype=torch.float16)\n",
      "step №578: loss = 18.145490646362305, weights = tensor([6.0156, 1.4209], dtype=torch.float16)\n",
      "step №579: loss = 18.138418197631836, weights = tensor([6.0156, 1.4229], dtype=torch.float16)\n",
      "step №580: loss = 18.131357192993164, weights = tensor([6.0156, 1.4248], dtype=torch.float16)\n",
      "step №581: loss = 18.124305725097656, weights = tensor([6.0156, 1.4268], dtype=torch.float16)\n",
      "step №582: loss = 18.117258071899414, weights = tensor([6.0117, 1.4287], dtype=torch.float16)\n",
      "step №583: loss = 18.109291076660156, weights = tensor([6.0117, 1.4307], dtype=torch.float16)\n",
      "step №584: loss = 18.102190017700195, weights = tensor([6.0117, 1.4326], dtype=torch.float16)\n",
      "step №585: loss = 18.095096588134766, weights = tensor([6.0117, 1.4346], dtype=torch.float16)\n",
      "step №586: loss = 18.0880126953125, weights = tensor([6.0117, 1.4365], dtype=torch.float16)\n",
      "step №587: loss = 18.080936431884766, weights = tensor([6.0117, 1.4385], dtype=torch.float16)\n",
      "step №588: loss = 18.073867797851562, weights = tensor([6.0117, 1.4404], dtype=torch.float16)\n",
      "step №589: loss = 18.066804885864258, weights = tensor([6.0117, 1.4424], dtype=torch.float16)\n",
      "step №590: loss = 18.059749603271484, weights = tensor([6.0117, 1.4443], dtype=torch.float16)\n",
      "step №591: loss = 18.052705764770508, weights = tensor([6.0117, 1.4463], dtype=torch.float16)\n",
      "step №592: loss = 18.045665740966797, weights = tensor([6.0117, 1.4482], dtype=torch.float16)\n",
      "step №593: loss = 18.038633346557617, weights = tensor([6.0117, 1.4502], dtype=torch.float16)\n",
      "step №594: loss = 18.0316104888916, weights = tensor([6.0117, 1.4521], dtype=torch.float16)\n",
      "step №595: loss = 18.024593353271484, weights = tensor([6.0078, 1.4541], dtype=torch.float16)\n",
      "step №596: loss = 18.016633987426758, weights = tensor([6.0078, 1.4561], dtype=torch.float16)\n",
      "step №597: loss = 18.009565353393555, weights = tensor([6.0078, 1.4580], dtype=torch.float16)\n",
      "step №598: loss = 18.002506256103516, weights = tensor([6.0078, 1.4600], dtype=torch.float16)\n",
      "step №599: loss = 17.99544906616211, weights = tensor([6.0078, 1.4619], dtype=torch.float16)\n",
      "step №600: loss = 17.988401412963867, weights = tensor([6.0078, 1.4639], dtype=torch.float16)\n",
      "step №601: loss = 17.981365203857422, weights = tensor([6.0078, 1.4658], dtype=torch.float16)\n",
      "step №602: loss = 17.974332809448242, weights = tensor([6.0078, 1.4678], dtype=torch.float16)\n",
      "step №603: loss = 17.967308044433594, weights = tensor([6.0078, 1.4697], dtype=torch.float16)\n",
      "step №604: loss = 17.96029281616211, weights = tensor([6.0078, 1.4717], dtype=torch.float16)\n",
      "step №605: loss = 17.953285217285156, weights = tensor([6.0078, 1.4736], dtype=torch.float16)\n",
      "step №606: loss = 17.946285247802734, weights = tensor([6.0078, 1.4756], dtype=torch.float16)\n",
      "step №607: loss = 17.939289093017578, weights = tensor([6.0039, 1.4775], dtype=torch.float16)\n",
      "step №608: loss = 17.931400299072266, weights = tensor([6.0039, 1.4795], dtype=torch.float16)\n",
      "step №609: loss = 17.924352645874023, weights = tensor([6.0039, 1.4814], dtype=torch.float16)\n",
      "step №610: loss = 17.917314529418945, weights = tensor([6.0039, 1.4834], dtype=torch.float16)\n",
      "step №611: loss = 17.9102840423584, weights = tensor([6.0039, 1.4854], dtype=torch.float16)\n",
      "step №612: loss = 17.90325927734375, weights = tensor([6.0039, 1.4873], dtype=torch.float16)\n",
      "step №613: loss = 17.896244049072266, weights = tensor([6.0039, 1.4893], dtype=torch.float16)\n",
      "step №614: loss = 17.88923454284668, weights = tensor([6.0039, 1.4912], dtype=torch.float16)\n",
      "step №615: loss = 17.882234573364258, weights = tensor([6.0039, 1.4932], dtype=torch.float16)\n",
      "step №616: loss = 17.875242233276367, weights = tensor([6.0039, 1.4951], dtype=torch.float16)\n",
      "step №617: loss = 17.868255615234375, weights = tensor([6.0039, 1.4971], dtype=torch.float16)\n",
      "step №618: loss = 17.861278533935547, weights = tensor([6.0039, 1.4990], dtype=torch.float16)\n",
      "step №619: loss = 17.85430908203125, weights = tensor([6.0039, 1.5010], dtype=torch.float16)\n",
      "step №620: loss = 17.84734535217285, weights = tensor([6.0000, 1.5029], dtype=torch.float16)\n",
      "step №621: loss = 17.83946418762207, weights = tensor([6.0000, 1.5049], dtype=torch.float16)\n",
      "step №622: loss = 17.832447052001953, weights = tensor([6.0000, 1.5068], dtype=torch.float16)\n",
      "step №623: loss = 17.825435638427734, weights = tensor([6.0000, 1.5088], dtype=torch.float16)\n",
      "step №624: loss = 17.818435668945312, weights = tensor([6.0000, 1.5107], dtype=torch.float16)\n",
      "step №625: loss = 17.811443328857422, weights = tensor([6.0000, 1.5127], dtype=torch.float16)\n",
      "step №626: loss = 17.804460525512695, weights = tensor([6.0000, 1.5146], dtype=torch.float16)\n",
      "step №627: loss = 17.7974796295166, weights = tensor([6.0000, 1.5166], dtype=torch.float16)\n",
      "step №628: loss = 17.790508270263672, weights = tensor([6.0000, 1.5186], dtype=torch.float16)\n",
      "step №629: loss = 17.783550262451172, weights = tensor([6.0000, 1.5205], dtype=torch.float16)\n",
      "step №630: loss = 17.776592254638672, weights = tensor([6.0000, 1.5225], dtype=torch.float16)\n",
      "step №631: loss = 17.769643783569336, weights = tensor([6.0000, 1.5244], dtype=torch.float16)\n",
      "step №632: loss = 17.762704849243164, weights = tensor([6.0000, 1.5264], dtype=torch.float16)\n",
      "step №633: loss = 17.755773544311523, weights = tensor([5.9961, 1.5283], dtype=torch.float16)\n",
      "step №634: loss = 17.74789810180664, weights = tensor([5.9961, 1.5303], dtype=torch.float16)\n",
      "step №635: loss = 17.74091148376465, weights = tensor([5.9961, 1.5322], dtype=torch.float16)\n",
      "step №636: loss = 17.733936309814453, weights = tensor([5.9961, 1.5342], dtype=torch.float16)\n",
      "step №637: loss = 17.72696304321289, weights = tensor([5.9961, 1.5361], dtype=torch.float16)\n",
      "step №638: loss = 17.720001220703125, weights = tensor([5.9961, 1.5381], dtype=torch.float16)\n",
      "step №639: loss = 17.71304702758789, weights = tensor([5.9961, 1.5400], dtype=torch.float16)\n",
      "step №640: loss = 17.706098556518555, weights = tensor([5.9961, 1.5420], dtype=torch.float16)\n",
      "step №641: loss = 17.699159622192383, weights = tensor([5.9961, 1.5439], dtype=torch.float16)\n",
      "step №642: loss = 17.69222640991211, weights = tensor([5.9961, 1.5459], dtype=torch.float16)\n",
      "step №643: loss = 17.685302734375, weights = tensor([5.9961, 1.5479], dtype=torch.float16)\n",
      "step №644: loss = 17.678386688232422, weights = tensor([5.9961, 1.5498], dtype=torch.float16)\n",
      "step №645: loss = 17.671476364135742, weights = tensor([5.9922, 1.5518], dtype=torch.float16)\n",
      "step №646: loss = 17.663671493530273, weights = tensor([5.9922, 1.5537], dtype=torch.float16)\n",
      "step №647: loss = 17.656705856323242, weights = tensor([5.9922, 1.5557], dtype=torch.float16)\n",
      "step №648: loss = 17.649751663208008, weights = tensor([5.9922, 1.5576], dtype=torch.float16)\n",
      "step №649: loss = 17.642807006835938, weights = tensor([5.9922, 1.5596], dtype=torch.float16)\n",
      "step №650: loss = 17.635866165161133, weights = tensor([5.9922, 1.5615], dtype=torch.float16)\n",
      "step №651: loss = 17.62893295288086, weights = tensor([5.9922, 1.5635], dtype=torch.float16)\n",
      "step №652: loss = 17.622007369995117, weights = tensor([5.9922, 1.5654], dtype=torch.float16)\n",
      "step №653: loss = 17.615093231201172, weights = tensor([5.9922, 1.5674], dtype=torch.float16)\n",
      "step №654: loss = 17.608184814453125, weights = tensor([5.9922, 1.5693], dtype=torch.float16)\n",
      "step №655: loss = 17.601280212402344, weights = tensor([5.9922, 1.5713], dtype=torch.float16)\n",
      "step №656: loss = 17.59438705444336, weights = tensor([5.9922, 1.5732], dtype=torch.float16)\n",
      "step №657: loss = 17.58750343322754, weights = tensor([5.9922, 1.5752], dtype=torch.float16)\n",
      "step №658: loss = 17.580623626708984, weights = tensor([5.9883, 1.5771], dtype=torch.float16)\n",
      "step №659: loss = 17.572824478149414, weights = tensor([5.9883, 1.5791], dtype=torch.float16)\n",
      "step №660: loss = 17.56589126586914, weights = tensor([5.9883, 1.5811], dtype=torch.float16)\n",
      "step №661: loss = 17.5589656829834, weights = tensor([5.9883, 1.5830], dtype=torch.float16)\n",
      "step №662: loss = 17.55204963684082, weights = tensor([5.9883, 1.5850], dtype=torch.float16)\n",
      "step №663: loss = 17.54513931274414, weights = tensor([5.9883, 1.5869], dtype=torch.float16)\n",
      "step №664: loss = 17.538240432739258, weights = tensor([5.9883, 1.5889], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №665: loss = 17.53134536743164, weights = tensor([5.9883, 1.5908], dtype=torch.float16)\n",
      "step №666: loss = 17.524457931518555, weights = tensor([5.9883, 1.5928], dtype=torch.float16)\n",
      "step №667: loss = 17.517581939697266, weights = tensor([5.9883, 1.5947], dtype=torch.float16)\n",
      "step №668: loss = 17.51070785522461, weights = tensor([5.9883, 1.5967], dtype=torch.float16)\n",
      "step №669: loss = 17.50384521484375, weights = tensor([5.9883, 1.5986], dtype=torch.float16)\n",
      "step №670: loss = 17.496990203857422, weights = tensor([5.9883, 1.6006], dtype=torch.float16)\n",
      "step №671: loss = 17.490140914916992, weights = tensor([5.9844, 1.6025], dtype=torch.float16)\n",
      "step №672: loss = 17.482349395751953, weights = tensor([5.9844, 1.6045], dtype=torch.float16)\n",
      "step №673: loss = 17.475448608398438, weights = tensor([5.9844, 1.6064], dtype=torch.float16)\n",
      "step №674: loss = 17.468555450439453, weights = tensor([5.9844, 1.6084], dtype=torch.float16)\n",
      "step №675: loss = 17.461666107177734, weights = tensor([5.9844, 1.6104], dtype=torch.float16)\n",
      "step №676: loss = 17.454788208007812, weights = tensor([5.9844, 1.6123], dtype=torch.float16)\n",
      "step №677: loss = 17.447919845581055, weights = tensor([5.9844, 1.6143], dtype=torch.float16)\n",
      "step №678: loss = 17.441055297851562, weights = tensor([5.9844, 1.6162], dtype=torch.float16)\n",
      "step №679: loss = 17.4341983795166, weights = tensor([5.9844, 1.6182], dtype=torch.float16)\n",
      "step №680: loss = 17.427349090576172, weights = tensor([5.9844, 1.6201], dtype=torch.float16)\n",
      "step №681: loss = 17.42051124572754, weights = tensor([5.9844, 1.6221], dtype=torch.float16)\n",
      "step №682: loss = 17.413679122924805, weights = tensor([5.9844, 1.6240], dtype=torch.float16)\n",
      "step №683: loss = 17.406850814819336, weights = tensor([5.9805, 1.6260], dtype=torch.float16)\n",
      "step №684: loss = 17.39912986755371, weights = tensor([5.9805, 1.6279], dtype=torch.float16)\n",
      "step №685: loss = 17.392250061035156, weights = tensor([5.9805, 1.6299], dtype=torch.float16)\n",
      "step №686: loss = 17.385379791259766, weights = tensor([5.9805, 1.6318], dtype=torch.float16)\n",
      "step №687: loss = 17.378517150878906, weights = tensor([5.9805, 1.6338], dtype=torch.float16)\n",
      "step №688: loss = 17.371660232543945, weights = tensor([5.9805, 1.6357], dtype=torch.float16)\n",
      "step №689: loss = 17.364810943603516, weights = tensor([5.9805, 1.6377], dtype=torch.float16)\n",
      "step №690: loss = 17.35797119140625, weights = tensor([5.9805, 1.6396], dtype=torch.float16)\n",
      "step №691: loss = 17.351139068603516, weights = tensor([5.9805, 1.6416], dtype=torch.float16)\n",
      "step №692: loss = 17.344314575195312, weights = tensor([5.9805, 1.6436], dtype=torch.float16)\n",
      "step №693: loss = 17.337495803833008, weights = tensor([5.9805, 1.6455], dtype=torch.float16)\n",
      "step №694: loss = 17.330684661865234, weights = tensor([5.9805, 1.6475], dtype=torch.float16)\n",
      "step №695: loss = 17.323884963989258, weights = tensor([5.9805, 1.6494], dtype=torch.float16)\n",
      "step №696: loss = 17.317089080810547, weights = tensor([5.9766, 1.6514], dtype=torch.float16)\n",
      "step №697: loss = 17.309375762939453, weights = tensor([5.9766, 1.6533], dtype=torch.float16)\n",
      "step №698: loss = 17.30252456665039, weights = tensor([5.9766, 1.6553], dtype=torch.float16)\n",
      "step №699: loss = 17.295682907104492, weights = tensor([5.9766, 1.6572], dtype=torch.float16)\n",
      "step №700: loss = 17.288850784301758, weights = tensor([5.9766, 1.6592], dtype=torch.float16)\n",
      "step №701: loss = 17.282026290893555, weights = tensor([5.9766, 1.6611], dtype=torch.float16)\n",
      "step №702: loss = 17.275211334228516, weights = tensor([5.9766, 1.6631], dtype=torch.float16)\n",
      "step №703: loss = 17.26839828491211, weights = tensor([5.9766, 1.6650], dtype=torch.float16)\n",
      "step №704: loss = 17.261594772338867, weights = tensor([5.9766, 1.6670], dtype=torch.float16)\n",
      "step №705: loss = 17.254802703857422, weights = tensor([5.9766, 1.6689], dtype=torch.float16)\n",
      "step №706: loss = 17.248014450073242, weights = tensor([5.9766, 1.6709], dtype=torch.float16)\n",
      "step №707: loss = 17.241233825683594, weights = tensor([5.9766, 1.6729], dtype=torch.float16)\n",
      "step №708: loss = 17.23446273803711, weights = tensor([5.9766, 1.6748], dtype=torch.float16)\n",
      "step №709: loss = 17.227699279785156, weights = tensor([5.9727, 1.6768], dtype=torch.float16)\n",
      "step №710: loss = 17.219989776611328, weights = tensor([5.9727, 1.6787], dtype=torch.float16)\n",
      "step №711: loss = 17.213172912597656, weights = tensor([5.9727, 1.6807], dtype=torch.float16)\n",
      "step №712: loss = 17.206363677978516, weights = tensor([5.9727, 1.6826], dtype=torch.float16)\n",
      "step №713: loss = 17.199560165405273, weights = tensor([5.9727, 1.6846], dtype=torch.float16)\n",
      "step №714: loss = 17.192766189575195, weights = tensor([5.9727, 1.6865], dtype=torch.float16)\n",
      "step №715: loss = 17.18597984313965, weights = tensor([5.9727, 1.6885], dtype=torch.float16)\n",
      "step №716: loss = 17.17919921875, weights = tensor([5.9727, 1.6904], dtype=torch.float16)\n",
      "step №717: loss = 17.172428131103516, weights = tensor([5.9727, 1.6924], dtype=torch.float16)\n",
      "step №718: loss = 17.16566276550293, weights = tensor([5.9727, 1.6943], dtype=torch.float16)\n",
      "step №719: loss = 17.158906936645508, weights = tensor([5.9727, 1.6963], dtype=torch.float16)\n",
      "step №720: loss = 17.152158737182617, weights = tensor([5.9727, 1.6982], dtype=torch.float16)\n",
      "step №721: loss = 17.145416259765625, weights = tensor([5.9688, 1.7002], dtype=torch.float16)\n",
      "step №722: loss = 17.137779235839844, weights = tensor([5.9688, 1.7021], dtype=torch.float16)\n",
      "step №723: loss = 17.1309814453125, weights = tensor([5.9688, 1.7041], dtype=torch.float16)\n",
      "step №724: loss = 17.124195098876953, weights = tensor([5.9688, 1.7061], dtype=torch.float16)\n",
      "step №725: loss = 17.11741828918457, weights = tensor([5.9688, 1.7080], dtype=torch.float16)\n",
      "step №726: loss = 17.110645294189453, weights = tensor([5.9688, 1.7100], dtype=torch.float16)\n",
      "step №727: loss = 17.103878021240234, weights = tensor([5.9688, 1.7119], dtype=torch.float16)\n",
      "step №728: loss = 17.097122192382812, weights = tensor([5.9688, 1.7139], dtype=torch.float16)\n",
      "step №729: loss = 17.090373992919922, weights = tensor([5.9688, 1.7158], dtype=torch.float16)\n",
      "step №730: loss = 17.083635330200195, weights = tensor([5.9688, 1.7178], dtype=torch.float16)\n",
      "step №731: loss = 17.0768985748291, weights = tensor([5.9688, 1.7197], dtype=torch.float16)\n",
      "step №732: loss = 17.070171356201172, weights = tensor([5.9688, 1.7217], dtype=torch.float16)\n",
      "step №733: loss = 17.063457489013672, weights = tensor([5.9688, 1.7236], dtype=torch.float16)\n",
      "step №734: loss = 17.056743621826172, weights = tensor([5.9648, 1.7256], dtype=torch.float16)\n",
      "step №735: loss = 17.049114227294922, weights = tensor([5.9648, 1.7275], dtype=torch.float16)\n",
      "step №736: loss = 17.042346954345703, weights = tensor([5.9648, 1.7295], dtype=torch.float16)\n",
      "step №737: loss = 17.03559112548828, weights = tensor([5.9648, 1.7314], dtype=torch.float16)\n",
      "step №738: loss = 17.02884292602539, weights = tensor([5.9648, 1.7334], dtype=torch.float16)\n",
      "step №739: loss = 17.0221004486084, weights = tensor([5.9648, 1.7354], dtype=torch.float16)\n",
      "step №740: loss = 17.015369415283203, weights = tensor([5.9648, 1.7373], dtype=torch.float16)\n",
      "step №741: loss = 17.00864028930664, weights = tensor([5.9648, 1.7393], dtype=torch.float16)\n",
      "step №742: loss = 17.001922607421875, weights = tensor([5.9648, 1.7412], dtype=torch.float16)\n",
      "step №743: loss = 16.99521255493164, weights = tensor([5.9648, 1.7432], dtype=torch.float16)\n",
      "step №744: loss = 16.988508224487305, weights = tensor([5.9648, 1.7451], dtype=torch.float16)\n",
      "step №745: loss = 16.981813430786133, weights = tensor([5.9648, 1.7471], dtype=torch.float16)\n",
      "step №746: loss = 16.97512435913086, weights = tensor([5.9648, 1.7490], dtype=torch.float16)\n",
      "step №747: loss = 16.96844482421875, weights = tensor([5.9609, 1.7510], dtype=torch.float16)\n",
      "step №748: loss = 16.960819244384766, weights = tensor([5.9609, 1.7529], dtype=torch.float16)\n",
      "step №749: loss = 16.95408821105957, weights = tensor([5.9609, 1.7549], dtype=torch.float16)\n",
      "step №750: loss = 16.947362899780273, weights = tensor([5.9609, 1.7568], dtype=torch.float16)\n",
      "step №751: loss = 16.940641403198242, weights = tensor([5.9609, 1.7588], dtype=torch.float16)\n",
      "step №752: loss = 16.933931350708008, weights = tensor([5.9609, 1.7607], dtype=torch.float16)\n",
      "step №753: loss = 16.927230834960938, weights = tensor([5.9609, 1.7627], dtype=torch.float16)\n",
      "step №754: loss = 16.920534133911133, weights = tensor([5.9609, 1.7646], dtype=torch.float16)\n",
      "step №755: loss = 16.91384506225586, weights = tensor([5.9609, 1.7666], dtype=torch.float16)\n",
      "step №756: loss = 16.907163619995117, weights = tensor([5.9609, 1.7686], dtype=torch.float16)\n",
      "step №757: loss = 16.900493621826172, weights = tensor([5.9609, 1.7705], dtype=torch.float16)\n",
      "step №758: loss = 16.893829345703125, weights = tensor([5.9609, 1.7725], dtype=torch.float16)\n",
      "step №759: loss = 16.887168884277344, weights = tensor([5.9570, 1.7744], dtype=torch.float16)\n",
      "step №760: loss = 16.879615783691406, weights = tensor([5.9570, 1.7764], dtype=torch.float16)\n",
      "step №761: loss = 16.87290382385254, weights = tensor([5.9570, 1.7783], dtype=torch.float16)\n",
      "step №762: loss = 16.866199493408203, weights = tensor([5.9570, 1.7803], dtype=torch.float16)\n",
      "step №763: loss = 16.859506607055664, weights = tensor([5.9570, 1.7822], dtype=torch.float16)\n",
      "step №764: loss = 16.85281753540039, weights = tensor([5.9570, 1.7842], dtype=torch.float16)\n",
      "step №765: loss = 16.84613609313965, weights = tensor([5.9570, 1.7861], dtype=torch.float16)\n",
      "step №766: loss = 16.83946418762207, weights = tensor([5.9570, 1.7881], dtype=torch.float16)\n",
      "step №767: loss = 16.83279800415039, weights = tensor([5.9570, 1.7900], dtype=torch.float16)\n",
      "step №768: loss = 16.826143264770508, weights = tensor([5.9570, 1.7920], dtype=torch.float16)\n",
      "step №769: loss = 16.81949234008789, weights = tensor([5.9570, 1.7939], dtype=torch.float16)\n",
      "step №770: loss = 16.812849044799805, weights = tensor([5.9570, 1.7959], dtype=torch.float16)\n",
      "step №771: loss = 16.806217193603516, weights = tensor([5.9570, 1.7979], dtype=torch.float16)\n",
      "step №772: loss = 16.79958724975586, weights = tensor([5.9531, 1.7998], dtype=torch.float16)\n",
      "step №773: loss = 16.792041778564453, weights = tensor([5.9531, 1.8018], dtype=torch.float16)\n",
      "step №774: loss = 16.78536033630371, weights = tensor([5.9531, 1.8037], dtype=torch.float16)\n",
      "step №775: loss = 16.7786865234375, weights = tensor([5.9531, 1.8057], dtype=torch.float16)\n",
      "step №776: loss = 16.772022247314453, weights = tensor([5.9531, 1.8076], dtype=torch.float16)\n",
      "step №777: loss = 16.765365600585938, weights = tensor([5.9531, 1.8096], dtype=torch.float16)\n",
      "step №778: loss = 16.758716583251953, weights = tensor([5.9531, 1.8115], dtype=torch.float16)\n",
      "step №779: loss = 16.752071380615234, weights = tensor([5.9531, 1.8135], dtype=torch.float16)\n",
      "step №780: loss = 16.745437622070312, weights = tensor([5.9531, 1.8154], dtype=torch.float16)\n",
      "step №781: loss = 16.738813400268555, weights = tensor([5.9531, 1.8174], dtype=torch.float16)\n",
      "step №782: loss = 16.732192993164062, weights = tensor([5.9531, 1.8193], dtype=torch.float16)\n",
      "step №783: loss = 16.7255802154541, weights = tensor([5.9531, 1.8213], dtype=torch.float16)\n",
      "step №784: loss = 16.718975067138672, weights = tensor([5.9531, 1.8232], dtype=torch.float16)\n",
      "step №785: loss = 16.71238136291504, weights = tensor([5.9492, 1.8252], dtype=torch.float16)\n",
      "step №786: loss = 16.7048397064209, weights = tensor([5.9492, 1.8271], dtype=torch.float16)\n",
      "step №787: loss = 16.698190689086914, weights = tensor([5.9492, 1.8291], dtype=torch.float16)\n",
      "step №788: loss = 16.69154930114746, weights = tensor([5.9492, 1.8311], dtype=torch.float16)\n",
      "step №789: loss = 16.684913635253906, weights = tensor([5.9492, 1.8330], dtype=torch.float16)\n",
      "step №790: loss = 16.678287506103516, weights = tensor([5.9492, 1.8350], dtype=torch.float16)\n",
      "step №791: loss = 16.671669006347656, weights = tensor([5.9492, 1.8369], dtype=torch.float16)\n",
      "step №792: loss = 16.665056228637695, weights = tensor([5.9492, 1.8389], dtype=torch.float16)\n",
      "step №793: loss = 16.658451080322266, weights = tensor([5.9492, 1.8408], dtype=torch.float16)\n",
      "step №794: loss = 16.65185546875, weights = tensor([5.9492, 1.8428], dtype=torch.float16)\n",
      "step №795: loss = 16.645267486572266, weights = tensor([5.9492, 1.8447], dtype=torch.float16)\n",
      "step №796: loss = 16.638687133789062, weights = tensor([5.9492, 1.8467], dtype=torch.float16)\n",
      "step №797: loss = 16.632112503051758, weights = tensor([5.9453, 1.8486], dtype=torch.float16)\n",
      "step №798: loss = 16.624643325805664, weights = tensor([5.9453, 1.8506], dtype=torch.float16)\n",
      "step №799: loss = 16.618013381958008, weights = tensor([5.9453, 1.8525], dtype=torch.float16)\n",
      "step №800: loss = 16.611392974853516, weights = tensor([5.9453, 1.8545], dtype=torch.float16)\n",
      "step №801: loss = 16.604785919189453, weights = tensor([5.9453, 1.8564], dtype=torch.float16)\n",
      "step №802: loss = 16.59817886352539, weights = tensor([5.9453, 1.8584], dtype=torch.float16)\n",
      "step №803: loss = 16.591581344604492, weights = tensor([5.9453, 1.8604], dtype=torch.float16)\n",
      "step №804: loss = 16.584993362426758, weights = tensor([5.9453, 1.8623], dtype=torch.float16)\n",
      "step №805: loss = 16.578413009643555, weights = tensor([5.9453, 1.8643], dtype=torch.float16)\n",
      "step №806: loss = 16.571842193603516, weights = tensor([5.9453, 1.8662], dtype=torch.float16)\n",
      "step №807: loss = 16.56527328491211, weights = tensor([5.9453, 1.8682], dtype=torch.float16)\n",
      "step №808: loss = 16.558713912963867, weights = tensor([5.9453, 1.8701], dtype=torch.float16)\n",
      "step №809: loss = 16.552165985107422, weights = tensor([5.9453, 1.8721], dtype=torch.float16)\n",
      "step №810: loss = 16.545621871948242, weights = tensor([5.9414, 1.8740], dtype=torch.float16)\n",
      "step №811: loss = 16.538158416748047, weights = tensor([5.9414, 1.8760], dtype=torch.float16)\n",
      "step №812: loss = 16.53156089782715, weights = tensor([5.9414, 1.8779], dtype=torch.float16)\n",
      "step №813: loss = 16.524972915649414, weights = tensor([5.9414, 1.8799], dtype=torch.float16)\n",
      "step №814: loss = 16.518390655517578, weights = tensor([5.9414, 1.8818], dtype=torch.float16)\n",
      "step №815: loss = 16.511817932128906, weights = tensor([5.9414, 1.8838], dtype=torch.float16)\n",
      "step №816: loss = 16.505252838134766, weights = tensor([5.9414, 1.8857], dtype=torch.float16)\n",
      "step №817: loss = 16.498693466186523, weights = tensor([5.9414, 1.8877], dtype=torch.float16)\n",
      "step №818: loss = 16.492143630981445, weights = tensor([5.9414, 1.8896], dtype=torch.float16)\n",
      "step №819: loss = 16.4856014251709, weights = tensor([5.9414, 1.8916], dtype=torch.float16)\n",
      "step №820: loss = 16.47906494140625, weights = tensor([5.9414, 1.8936], dtype=torch.float16)\n",
      "step №821: loss = 16.472537994384766, weights = tensor([5.9414, 1.8955], dtype=torch.float16)\n",
      "step №822: loss = 16.46601676940918, weights = tensor([5.9414, 1.8975], dtype=torch.float16)\n",
      "step №823: loss = 16.459505081176758, weights = tensor([5.9375, 1.8994], dtype=torch.float16)\n",
      "step №824: loss = 16.45204734802246, weights = tensor([5.9375, 1.9014], dtype=torch.float16)\n",
      "step №825: loss = 16.445484161376953, weights = tensor([5.9375, 1.9033], dtype=torch.float16)\n",
      "step №826: loss = 16.438926696777344, weights = tensor([5.9375, 1.9053], dtype=torch.float16)\n",
      "step №827: loss = 16.432373046875, weights = tensor([5.9375, 1.9072], dtype=torch.float16)\n",
      "step №828: loss = 16.425830841064453, weights = tensor([5.9375, 1.9092], dtype=torch.float16)\n",
      "step №829: loss = 16.41929817199707, weights = tensor([5.9375, 1.9111], dtype=torch.float16)\n",
      "step №830: loss = 16.412769317626953, weights = tensor([5.9375, 1.9131], dtype=torch.float16)\n",
      "step №831: loss = 16.406246185302734, weights = tensor([5.9375, 1.9150], dtype=torch.float16)\n",
      "step №832: loss = 16.399734497070312, weights = tensor([5.9375, 1.9170], dtype=torch.float16)\n",
      "step №833: loss = 16.393230438232422, weights = tensor([5.9375, 1.9189], dtype=torch.float16)\n",
      "step №834: loss = 16.386735916137695, weights = tensor([5.9375, 1.9209], dtype=torch.float16)\n",
      "step №835: loss = 16.3802433013916, weights = tensor([5.9336, 1.9229], dtype=torch.float16)\n",
      "step №836: loss = 16.37285804748535, weights = tensor([5.9336, 1.9248], dtype=torch.float16)\n",
      "step №837: loss = 16.366313934326172, weights = tensor([5.9336, 1.9268], dtype=torch.float16)\n",
      "step №838: loss = 16.359777450561523, weights = tensor([5.9336, 1.9287], dtype=torch.float16)\n",
      "step №839: loss = 16.353252410888672, weights = tensor([5.9336, 1.9307], dtype=torch.float16)\n",
      "step №840: loss = 16.346729278564453, weights = tensor([5.9336, 1.9326], dtype=torch.float16)\n",
      "step №841: loss = 16.34021759033203, weights = tensor([5.9336, 1.9346], dtype=torch.float16)\n",
      "step №842: loss = 16.33371353149414, weights = tensor([5.9336, 1.9365], dtype=torch.float16)\n",
      "step №843: loss = 16.32721519470215, weights = tensor([5.9336, 1.9385], dtype=torch.float16)\n",
      "step №844: loss = 16.320728302001953, weights = tensor([5.9336, 1.9404], dtype=torch.float16)\n",
      "step №845: loss = 16.31424331665039, weights = tensor([5.9336, 1.9424], dtype=torch.float16)\n",
      "step №846: loss = 16.307769775390625, weights = tensor([5.9336, 1.9443], dtype=torch.float16)\n",
      "step №847: loss = 16.30130386352539, weights = tensor([5.9336, 1.9463], dtype=torch.float16)\n",
      "step №848: loss = 16.294843673706055, weights = tensor([5.9297, 1.9482], dtype=torch.float16)\n",
      "step №849: loss = 16.287466049194336, weights = tensor([5.9297, 1.9502], dtype=torch.float16)\n",
      "step №850: loss = 16.28095245361328, weights = tensor([5.9297, 1.9521], dtype=torch.float16)\n",
      "step №851: loss = 16.274446487426758, weights = tensor([5.9297, 1.9541], dtype=torch.float16)\n",
      "step №852: loss = 16.267948150634766, weights = tensor([5.9297, 1.9561], dtype=torch.float16)\n",
      "step №853: loss = 16.26146125793457, weights = tensor([5.9297, 1.9580], dtype=torch.float16)\n",
      "step №854: loss = 16.254980087280273, weights = tensor([5.9297, 1.9600], dtype=torch.float16)\n",
      "step №855: loss = 16.248502731323242, weights = tensor([5.9297, 1.9619], dtype=torch.float16)\n",
      "step №856: loss = 16.242036819458008, weights = tensor([5.9297, 1.9639], dtype=torch.float16)\n",
      "step №857: loss = 16.235580444335938, weights = tensor([5.9297, 1.9658], dtype=torch.float16)\n",
      "step №858: loss = 16.229127883911133, weights = tensor([5.9297, 1.9678], dtype=torch.float16)\n",
      "step №859: loss = 16.22268295288086, weights = tensor([5.9297, 1.9697], dtype=torch.float16)\n",
      "step №860: loss = 16.216245651245117, weights = tensor([5.9297, 1.9717], dtype=torch.float16)\n",
      "step №861: loss = 16.209819793701172, weights = tensor([5.9258, 1.9736], dtype=torch.float16)\n",
      "step №862: loss = 16.20244598388672, weights = tensor([5.9258, 1.9756], dtype=torch.float16)\n",
      "step №863: loss = 16.195964813232422, weights = tensor([5.9258, 1.9775], dtype=torch.float16)\n",
      "step №864: loss = 16.189491271972656, weights = tensor([5.9258, 1.9795], dtype=torch.float16)\n",
      "step №865: loss = 16.18302345275879, weights = tensor([5.9258, 1.9814], dtype=torch.float16)\n",
      "step №866: loss = 16.176563262939453, weights = tensor([5.9258, 1.9834], dtype=torch.float16)\n",
      "step №867: loss = 16.170114517211914, weights = tensor([5.9258, 1.9854], dtype=torch.float16)\n",
      "step №868: loss = 16.16366958618164, weights = tensor([5.9258, 1.9873], dtype=torch.float16)\n",
      "step №869: loss = 16.1572322845459, weights = tensor([5.9258, 1.9893], dtype=torch.float16)\n",
      "step №870: loss = 16.15080451965332, weights = tensor([5.9258, 1.9912], dtype=torch.float16)\n",
      "step №871: loss = 16.14438247680664, weights = tensor([5.9258, 1.9932], dtype=torch.float16)\n",
      "step №872: loss = 16.137971878051758, weights = tensor([5.9258, 1.9951], dtype=torch.float16)\n",
      "step №873: loss = 16.13156509399414, weights = tensor([5.9219, 1.9971], dtype=torch.float16)\n",
      "step №874: loss = 16.124263763427734, weights = tensor([5.9219, 1.9990], dtype=torch.float16)\n",
      "step №875: loss = 16.117801666259766, weights = tensor([5.9219, 2.0020], dtype=torch.float16)\n",
      "step №876: loss = 16.108125686645508, weights = tensor([5.9219, 2.0039], dtype=torch.float16)\n",
      "step №877: loss = 16.101688385009766, weights = tensor([5.9219, 2.0059], dtype=torch.float16)\n",
      "step №878: loss = 16.095252990722656, weights = tensor([5.9219, 2.0078], dtype=torch.float16)\n",
      "step №879: loss = 16.088830947875977, weights = tensor([5.9219, 2.0098], dtype=torch.float16)\n",
      "step №880: loss = 16.08241081237793, weights = tensor([5.9219, 2.0117], dtype=torch.float16)\n",
      "step №881: loss = 16.076004028320312, weights = tensor([5.9219, 2.0137], dtype=torch.float16)\n",
      "step №882: loss = 16.069599151611328, weights = tensor([5.9219, 2.0156], dtype=torch.float16)\n",
      "step №883: loss = 16.063207626342773, weights = tensor([5.9219, 2.0176], dtype=torch.float16)\n",
      "step №884: loss = 16.056819915771484, weights = tensor([5.9219, 2.0195], dtype=torch.float16)\n",
      "step №885: loss = 16.05044174194336, weights = tensor([5.9219, 2.0215], dtype=torch.float16)\n",
      "step №886: loss = 16.044069290161133, weights = tensor([5.9180, 2.0234], dtype=torch.float16)\n",
      "step №887: loss = 16.036745071411133, weights = tensor([5.9180, 2.0254], dtype=torch.float16)\n",
      "step №888: loss = 16.030317306518555, weights = tensor([5.9180, 2.0273], dtype=torch.float16)\n",
      "step №889: loss = 16.023902893066406, weights = tensor([5.9180, 2.0293], dtype=torch.float16)\n",
      "step №890: loss = 16.01749038696289, weights = tensor([5.9180, 2.0312], dtype=torch.float16)\n",
      "step №891: loss = 16.011091232299805, weights = tensor([5.9180, 2.0332], dtype=torch.float16)\n",
      "step №892: loss = 16.004695892333984, weights = tensor([5.9180, 2.0352], dtype=torch.float16)\n",
      "step №893: loss = 15.998311042785645, weights = tensor([5.9180, 2.0371], dtype=torch.float16)\n",
      "step №894: loss = 15.99193000793457, weights = tensor([5.9180, 2.0391], dtype=torch.float16)\n",
      "step №895: loss = 15.985560417175293, weights = tensor([5.9180, 2.0410], dtype=torch.float16)\n",
      "step №896: loss = 15.979194641113281, weights = tensor([5.9180, 2.0430], dtype=torch.float16)\n",
      "step №897: loss = 15.972841262817383, weights = tensor([5.9180, 2.0449], dtype=torch.float16)\n",
      "step №898: loss = 15.966489791870117, weights = tensor([5.9141, 2.0469], dtype=torch.float16)\n",
      "step №899: loss = 15.959234237670898, weights = tensor([5.9141, 2.0488], dtype=torch.float16)\n",
      "step №900: loss = 15.95283031463623, weights = tensor([5.9141, 2.0508], dtype=torch.float16)\n",
      "step №901: loss = 15.946438789367676, weights = tensor([5.9141, 2.0527], dtype=torch.float16)\n",
      "step №902: loss = 15.94005012512207, weights = tensor([5.9141, 2.0547], dtype=torch.float16)\n",
      "step №903: loss = 15.933672904968262, weights = tensor([5.9141, 2.0566], dtype=torch.float16)\n",
      "step №904: loss = 15.927299499511719, weights = tensor([5.9141, 2.0586], dtype=torch.float16)\n",
      "step №905: loss = 15.920938491821289, weights = tensor([5.9141, 2.0605], dtype=torch.float16)\n",
      "step №906: loss = 15.914579391479492, weights = tensor([5.9141, 2.0625], dtype=torch.float16)\n",
      "step №907: loss = 15.908233642578125, weights = tensor([5.9141, 2.0645], dtype=torch.float16)\n",
      "step №908: loss = 15.901890754699707, weights = tensor([5.9141, 2.0664], dtype=torch.float16)\n",
      "step №909: loss = 15.895559310913086, weights = tensor([5.9141, 2.0684], dtype=torch.float16)\n",
      "step №910: loss = 15.88923168182373, weights = tensor([5.9141, 2.0703], dtype=torch.float16)\n",
      "step №911: loss = 15.882916450500488, weights = tensor([5.9102, 2.0723], dtype=torch.float16)\n",
      "step №912: loss = 15.875663757324219, weights = tensor([5.9102, 2.0742], dtype=torch.float16)\n",
      "step №913: loss = 15.869295120239258, weights = tensor([5.9102, 2.0762], dtype=torch.float16)\n",
      "step №914: loss = 15.86292839050293, weights = tensor([5.9102, 2.0781], dtype=torch.float16)\n",
      "step №915: loss = 15.856575012207031, weights = tensor([5.9102, 2.0801], dtype=torch.float16)\n",
      "step №916: loss = 15.850224494934082, weights = tensor([5.9102, 2.0820], dtype=torch.float16)\n",
      "step №917: loss = 15.84388542175293, weights = tensor([5.9102, 2.0840], dtype=torch.float16)\n",
      "step №918: loss = 15.837550163269043, weights = tensor([5.9102, 2.0859], dtype=torch.float16)\n",
      "step №919: loss = 15.83122730255127, weights = tensor([5.9102, 2.0879], dtype=torch.float16)\n",
      "step №920: loss = 15.824907302856445, weights = tensor([5.9102, 2.0898], dtype=torch.float16)\n",
      "step №921: loss = 15.818598747253418, weights = tensor([5.9102, 2.0918], dtype=torch.float16)\n",
      "step №922: loss = 15.812294006347656, weights = tensor([5.9102, 2.0938], dtype=torch.float16)\n",
      "step №923: loss = 15.806001663208008, weights = tensor([5.9102, 2.0957], dtype=torch.float16)\n",
      "step №924: loss = 15.799711227416992, weights = tensor([5.9062, 2.0977], dtype=torch.float16)\n",
      "step №925: loss = 15.792470932006836, weights = tensor([5.9062, 2.0996], dtype=torch.float16)\n",
      "step №926: loss = 15.786128044128418, weights = tensor([5.9062, 2.1016], dtype=torch.float16)\n",
      "step №927: loss = 15.779797554016113, weights = tensor([5.9062, 2.1035], dtype=torch.float16)\n",
      "step №928: loss = 15.773469924926758, weights = tensor([5.9062, 2.1055], dtype=torch.float16)\n",
      "step №929: loss = 15.7671537399292, weights = tensor([5.9062, 2.1074], dtype=torch.float16)\n",
      "step №930: loss = 15.760841369628906, weights = tensor([5.9062, 2.1094], dtype=torch.float16)\n",
      "step №931: loss = 15.754541397094727, weights = tensor([5.9062, 2.1113], dtype=torch.float16)\n",
      "step №932: loss = 15.74824333190918, weights = tensor([5.9062, 2.1133], dtype=torch.float16)\n",
      "step №933: loss = 15.741958618164062, weights = tensor([5.9062, 2.1152], dtype=torch.float16)\n",
      "step №934: loss = 15.735676765441895, weights = tensor([5.9062, 2.1172], dtype=torch.float16)\n",
      "step №935: loss = 15.729406356811523, weights = tensor([5.9062, 2.1191], dtype=torch.float16)\n",
      "step №936: loss = 15.723139762878418, weights = tensor([5.9023, 2.1211], dtype=torch.float16)\n",
      "step №937: loss = 15.715968132019043, weights = tensor([5.9023, 2.1230], dtype=torch.float16)\n",
      "step №938: loss = 15.709648132324219, weights = tensor([5.9023, 2.1250], dtype=torch.float16)\n",
      "step №939: loss = 15.703340530395508, weights = tensor([5.9023, 2.1270], dtype=torch.float16)\n",
      "step №940: loss = 15.69703483581543, weights = tensor([5.9023, 2.1289], dtype=torch.float16)\n",
      "step №941: loss = 15.690742492675781, weights = tensor([5.9023, 2.1309], dtype=torch.float16)\n",
      "step №942: loss = 15.684453010559082, weights = tensor([5.9023, 2.1328], dtype=torch.float16)\n",
      "step №943: loss = 15.67817497253418, weights = tensor([5.9023, 2.1348], dtype=torch.float16)\n",
      "step №944: loss = 15.671900749206543, weights = tensor([5.9023, 2.1367], dtype=torch.float16)\n",
      "step №945: loss = 15.66563892364502, weights = tensor([5.9023, 2.1387], dtype=torch.float16)\n",
      "step №946: loss = 15.659379959106445, weights = tensor([5.9023, 2.1406], dtype=torch.float16)\n",
      "step №947: loss = 15.653132438659668, weights = tensor([5.9023, 2.1426], dtype=torch.float16)\n",
      "step №948: loss = 15.646888732910156, weights = tensor([5.9023, 2.1445], dtype=torch.float16)\n",
      "step №949: loss = 15.640657424926758, weights = tensor([5.8984, 2.1465], dtype=torch.float16)\n",
      "step №950: loss = 15.633488655090332, weights = tensor([5.8984, 2.1484], dtype=torch.float16)\n",
      "step №951: loss = 15.627202987670898, weights = tensor([5.8984, 2.1504], dtype=torch.float16)\n",
      "step №952: loss = 15.62092113494873, weights = tensor([5.8984, 2.1523], dtype=torch.float16)\n",
      "step №953: loss = 15.614651679992676, weights = tensor([5.8984, 2.1543], dtype=torch.float16)\n",
      "step №954: loss = 15.60838508605957, weights = tensor([5.8984, 2.1562], dtype=torch.float16)\n",
      "step №955: loss = 15.602129936218262, weights = tensor([5.8984, 2.1582], dtype=torch.float16)\n",
      "step №956: loss = 15.595878601074219, weights = tensor([5.8984, 2.1602], dtype=torch.float16)\n",
      "step №957: loss = 15.589639663696289, weights = tensor([5.8984, 2.1621], dtype=torch.float16)\n",
      "step №958: loss = 15.583402633666992, weights = tensor([5.8984, 2.1641], dtype=torch.float16)\n",
      "step №959: loss = 15.577178955078125, weights = tensor([5.8984, 2.1660], dtype=torch.float16)\n",
      "step №960: loss = 15.570958137512207, weights = tensor([5.8984, 2.1680], dtype=torch.float16)\n",
      "step №961: loss = 15.564748764038086, weights = tensor([5.8984, 2.1699], dtype=torch.float16)\n",
      "step №962: loss = 15.55854320526123, weights = tensor([5.8945, 2.1719], dtype=torch.float16)\n",
      "step №963: loss = 15.551386833190918, weights = tensor([5.8945, 2.1738], dtype=torch.float16)\n",
      "step №964: loss = 15.545127868652344, weights = tensor([5.8945, 2.1758], dtype=torch.float16)\n",
      "step №965: loss = 15.538881301879883, weights = tensor([5.8945, 2.1777], dtype=torch.float16)\n",
      "step №966: loss = 15.532636642456055, weights = tensor([5.8945, 2.1797], dtype=torch.float16)\n",
      "step №967: loss = 15.526405334472656, weights = tensor([5.8945, 2.1816], dtype=torch.float16)\n",
      "step №968: loss = 15.520176887512207, weights = tensor([5.8945, 2.1836], dtype=torch.float16)\n",
      "step №969: loss = 15.513959884643555, weights = tensor([5.8945, 2.1855], dtype=torch.float16)\n",
      "step №970: loss = 15.507746696472168, weights = tensor([5.8945, 2.1875], dtype=torch.float16)\n",
      "step №971: loss = 15.501545906066895, weights = tensor([5.8945, 2.1895], dtype=torch.float16)\n",
      "step №972: loss = 15.49534797668457, weights = tensor([5.8945, 2.1914], dtype=torch.float16)\n",
      "step №973: loss = 15.489161491394043, weights = tensor([5.8945, 2.1934], dtype=torch.float16)\n",
      "step №974: loss = 15.482978820800781, weights = tensor([5.8906, 2.1953], dtype=torch.float16)\n",
      "step №975: loss = 15.47589111328125, weights = tensor([5.8906, 2.1973], dtype=torch.float16)\n",
      "step №976: loss = 15.46965503692627, weights = tensor([5.8906, 2.1992], dtype=torch.float16)\n",
      "step №977: loss = 15.463430404663086, weights = tensor([5.8906, 2.2012], dtype=torch.float16)\n",
      "step №978: loss = 15.457209587097168, weights = tensor([5.8906, 2.2031], dtype=torch.float16)\n",
      "step №979: loss = 15.451001167297363, weights = tensor([5.8906, 2.2051], dtype=torch.float16)\n",
      "step №980: loss = 15.444795608520508, weights = tensor([5.8906, 2.2070], dtype=torch.float16)\n",
      "step №981: loss = 15.43860149383545, weights = tensor([5.8906, 2.2090], dtype=torch.float16)\n",
      "step №982: loss = 15.432411193847656, weights = tensor([5.8906, 2.2109], dtype=torch.float16)\n",
      "step №983: loss = 15.426233291625977, weights = tensor([5.8906, 2.2129], dtype=torch.float16)\n",
      "step №984: loss = 15.42005729675293, weights = tensor([5.8906, 2.2148], dtype=torch.float16)\n",
      "step №985: loss = 15.413894653320312, weights = tensor([5.8906, 2.2168], dtype=torch.float16)\n",
      "step №986: loss = 15.407734870910645, weights = tensor([5.8906, 2.2188], dtype=torch.float16)\n",
      "step №987: loss = 15.401586532592773, weights = tensor([5.8867, 2.2207], dtype=torch.float16)\n",
      "step №988: loss = 15.394502639770508, weights = tensor([5.8867, 2.2227], dtype=torch.float16)\n",
      "step №989: loss = 15.388300895690918, weights = tensor([5.8867, 2.2246], dtype=torch.float16)\n",
      "step №990: loss = 15.382102966308594, weights = tensor([5.8867, 2.2266], dtype=torch.float16)\n",
      "step №991: loss = 15.375917434692383, weights = tensor([5.8867, 2.2285], dtype=torch.float16)\n",
      "step №992: loss = 15.369733810424805, weights = tensor([5.8867, 2.2305], dtype=torch.float16)\n",
      "step №993: loss = 15.363563537597656, weights = tensor([5.8867, 2.2324], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №994: loss = 15.357396125793457, weights = tensor([5.8867, 2.2344], dtype=torch.float16)\n",
      "step №995: loss = 15.351240158081055, weights = tensor([5.8867, 2.2363], dtype=torch.float16)\n",
      "step №996: loss = 15.345088005065918, weights = tensor([5.8867, 2.2383], dtype=torch.float16)\n",
      "step №997: loss = 15.338948249816895, weights = tensor([5.8867, 2.2402], dtype=torch.float16)\n",
      "step №998: loss = 15.33281135559082, weights = tensor([5.8867, 2.2422], dtype=torch.float16)\n",
      "step №999: loss = 15.326685905456543, weights = tensor([5.8867, 2.2441], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#first let's train our model with a stopping criteria as numbers of steps \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793f260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative loss = 15.326685905456543')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1693c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 271.8157958984375\n",
      "did one more step, loss reduced by 117.40095520019531\n",
      "did one more step, loss reduced by 50.80559539794922\n",
      "did one more step, loss reduced by 21.970703125\n",
      "did one more step, loss reduced by 9.480995178222656\n",
      "did one more step, loss reduced by 4.091772079467773\n",
      "did one more step, loss reduced by 1.772512435913086\n",
      "did one more step, loss reduced by 0.7649364471435547\n",
      "did one more step, loss reduced by 0.3240985870361328\n",
      "did one more step, loss reduced by 0.14899253845214844\n",
      "did one more step, loss reduced by 0.06864166259765625\n",
      "did one more step, loss reduced by 0.035125732421875\n",
      "did one more step, loss reduced by 0.02301025390625\n",
      "did one more step, loss reduced by 0.0185699462890625\n",
      "did one more step, loss reduced by 0.013818740844726562\n",
      "did one more step, loss reduced by 0.01274871826171875\n",
      "did one more step, loss reduced by 0.011852264404296875\n",
      "did one more step, loss reduced by 0.011837005615234375\n",
      "did one more step, loss reduced by 0.011819839477539062\n",
      "did one more step, loss reduced by 0.011808395385742188\n",
      "did one more step, loss reduced by 0.0117950439453125\n",
      "did one more step, loss reduced by 0.01177978515625\n",
      "did one more step, loss reduced by 0.011762619018554688\n",
      "did one more step, loss reduced by 0.011751174926757812\n",
      "did one more step, loss reduced by 0.011199951171875\n",
      "did one more step, loss reduced by 0.01119232177734375\n",
      "did one more step, loss reduced by 0.01117706298828125\n",
      "did one more step, loss reduced by 0.01116180419921875\n",
      "did one more step, loss reduced by 0.0111541748046875\n",
      "did one more step, loss reduced by 0.01113128662109375\n",
      "did one more step, loss reduced by 0.011127471923828125\n",
      "did one more step, loss reduced by 0.011110305786132812\n",
      "did one more step, loss reduced by 0.011096954345703125\n",
      "did one more step, loss reduced by 0.011083602905273438\n",
      "did one more step, loss reduced by 0.01203155517578125\n",
      "did one more step, loss reduced by 0.011148452758789062\n",
      "did one more step, loss reduced by 0.011135101318359375\n",
      "did one more step, loss reduced by 0.0111236572265625\n",
      "did one more step, loss reduced by 0.011106491088867188\n",
      "did one more step, loss reduced by 0.011096954345703125\n",
      "did one more step, loss reduced by 0.011081695556640625\n",
      "did one more step, loss reduced by 0.011072158813476562\n",
      "did one more step, loss reduced by 0.011056900024414062\n",
      "did one more step, loss reduced by 0.01103973388671875\n",
      "did one more step, loss reduced by 0.01202392578125\n",
      "did one more step, loss reduced by 0.011104583740234375\n",
      "did one more step, loss reduced by 0.011095046997070312\n",
      "did one more step, loss reduced by 0.011081695556640625\n",
      "did one more step, loss reduced by 0.011068344116210938\n",
      "did one more step, loss reduced by 0.01105499267578125\n",
      "did one more step, loss reduced by 0.01103973388671875\n",
      "did one more step, loss reduced by 0.011026382446289062\n",
      "did one more step, loss reduced by 0.011016845703125\n",
      "did one more step, loss reduced by 0.011934280395507812\n",
      "did one more step, loss reduced by 0.011079788208007812\n",
      "did one more step, loss reduced by 0.0110626220703125\n",
      "did one more step, loss reduced by 0.011053085327148438\n",
      "did one more step, loss reduced by 0.011564254760742188\n",
      "did one more step, loss reduced by 0.0115509033203125\n",
      "did one more step, loss reduced by 0.01153564453125\n",
      "did one more step, loss reduced by 0.011526107788085938\n",
      "did one more step, loss reduced by 0.011501312255859375\n",
      "did one more step, loss reduced by 0.011495590209960938\n",
      "did one more step, loss reduced by 0.012472152709960938\n",
      "did one more step, loss reduced by 0.011556625366210938\n",
      "did one more step, loss reduced by 0.011545181274414062\n",
      "did one more step, loss reduced by 0.011529922485351562\n",
      "did one more step, loss reduced by 0.011514663696289062\n",
      "did one more step, loss reduced by 0.011501312255859375\n",
      "did one more step, loss reduced by 0.011486053466796875\n",
      "did one more step, loss reduced by 0.011470794677734375\n",
      "did one more step, loss reduced by 0.010416030883789062\n",
      "did one more step, loss reduced by 0.01136016845703125\n",
      "did one more step, loss reduced by 0.011526107788085938\n",
      "did one more step, loss reduced by 0.01151275634765625\n",
      "did one more step, loss reduced by 0.01149749755859375\n",
      "did one more step, loss reduced by 0.011480331420898438\n",
      "did one more step, loss reduced by 0.011468887329101562\n",
      "did one more step, loss reduced by 0.010412216186523438\n",
      "did one more step, loss reduced by 0.010400772094726562\n",
      "did one more step, loss reduced by 0.010389328002929688\n",
      "did one more step, loss reduced by 0.0103759765625\n",
      "did one more step, loss reduced by 0.011354446411132812\n",
      "did one more step, loss reduced by 0.011476516723632812\n",
      "did one more step, loss reduced by 0.011470794677734375\n",
      "did one more step, loss reduced by 0.010416030883789062\n",
      "did one more step, loss reduced by 0.010396957397460938\n",
      "did one more step, loss reduced by 0.010389328002929688\n",
      "did one more step, loss reduced by 0.0103759765625\n",
      "did one more step, loss reduced by 0.010370254516601562\n",
      "did one more step, loss reduced by 0.010347366333007812\n",
      "did one more step, loss reduced by 0.010345458984375\n",
      "did one more step, loss reduced by 0.011322021484375\n",
      "did one more step, loss reduced by 0.010404586791992188\n",
      "did one more step, loss reduced by 0.010389328002929688\n",
      "did one more step, loss reduced by 0.010379791259765625\n",
      "did one more step, loss reduced by 0.010366439819335938\n",
      "did one more step, loss reduced by 0.010354995727539062\n",
      "did one more step, loss reduced by 0.010345458984375\n",
      "did one more step, loss reduced by 0.010332107543945312\n",
      "did one more step, loss reduced by 0.010320663452148438\n",
      "did one more step, loss reduced by 0.01030731201171875\n",
      "did one more step, loss reduced by 0.0112762451171875\n",
      "did one more step, loss reduced by 0.010370254516601562\n",
      "did one more step, loss reduced by 0.010358810424804688\n",
      "did one more step, loss reduced by 0.010345458984375\n",
      "did one more step, loss reduced by 0.010334014892578125\n",
      "did one more step, loss reduced by 0.010320663452148438\n",
      "did one more step, loss reduced by 0.010313034057617188\n",
      "did one more step, loss reduced by 0.010295867919921875\n",
      "did one more step, loss reduced by 0.010288238525390625\n",
      "did one more step, loss reduced by 0.010272979736328125\n",
      "did one more step, loss reduced by 0.011234283447265625\n",
      "did one more step, loss reduced by 0.01033782958984375\n",
      "did one more step, loss reduced by 0.01032257080078125\n",
      "did one more step, loss reduced by 0.010313034057617188\n",
      "did one more step, loss reduced by 0.010301589965820312\n",
      "did one more step, loss reduced by 0.010288238525390625\n",
      "did one more step, loss reduced by 0.01027679443359375\n",
      "did one more step, loss reduced by 0.010265350341796875\n",
      "did one more step, loss reduced by 0.010251998901367188\n",
      "did one more step, loss reduced by 0.010244369506835938\n",
      "did one more step, loss reduced by 0.011186599731445312\n",
      "did one more step, loss reduced by 0.010301589965820312\n",
      "did one more step, loss reduced by 0.01029205322265625\n",
      "did one more step, loss reduced by 0.010280609130859375\n",
      "did one more step, loss reduced by 0.010265350341796875\n",
      "did one more step, loss reduced by 0.010257720947265625\n",
      "did one more step, loss reduced by 0.010242462158203125\n",
      "did one more step, loss reduced by 0.01023101806640625\n",
      "did one more step, loss reduced by 0.010219573974609375\n",
      "did one more step, loss reduced by 0.010204315185546875\n",
      "did one more step, loss reduced by 0.011148452758789062\n",
      "did one more step, loss reduced by 0.010267257690429688\n",
      "did one more step, loss reduced by 0.010257720947265625\n",
      "did one more step, loss reduced by 0.010248184204101562\n",
      "did one more step, loss reduced by 0.010232925415039062\n",
      "did one more step, loss reduced by 0.010223388671875\n",
      "did one more step, loss reduced by 0.0102081298828125\n",
      "did one more step, loss reduced by 0.010198593139648438\n",
      "did one more step, loss reduced by 0.01018524169921875\n",
      "did one more step, loss reduced by 0.010173797607421875\n",
      "did one more step, loss reduced by 0.011098861694335938\n",
      "did one more step, loss reduced by 0.010236740112304688\n",
      "did one more step, loss reduced by 0.010225296020507812\n",
      "did one more step, loss reduced by 0.010211944580078125\n",
      "did one more step, loss reduced by 0.010204315185546875\n",
      "did one more step, loss reduced by 0.01018524169921875\n",
      "did one more step, loss reduced by 0.0101776123046875\n",
      "did one more step, loss reduced by 0.010164260864257812\n",
      "did one more step, loss reduced by 0.010152816772460938\n",
      "did one more step, loss reduced by 0.010141372680664062\n",
      "did one more step, loss reduced by 0.011053085327148438\n",
      "did one more step, loss reduced by 0.010204315185546875\n",
      "did one more step, loss reduced by 0.010190963745117188\n",
      "did one more step, loss reduced by 0.010175704956054688\n",
      "did one more step, loss reduced by 0.010168075561523438\n",
      "did one more step, loss reduced by 0.011171340942382812\n",
      "did one more step, loss reduced by 0.010141372680664062\n",
      "did one more step, loss reduced by 0.010133743286132812\n",
      "did one more step, loss reduced by 0.010114669799804688\n",
      "did one more step, loss reduced by 0.010107040405273438\n",
      "did one more step, loss reduced by 0.011018753051757812\n",
      "did one more step, loss reduced by 0.010168075561523438\n",
      "did one more step, loss reduced by 0.010156631469726562\n",
      "did one more step, loss reduced by 0.01013946533203125\n",
      "did one more step, loss reduced by 0.010135650634765625\n",
      "did one more step, loss reduced by 0.01012420654296875\n",
      "did one more step, loss reduced by 0.010103225708007812\n",
      "did one more step, loss reduced by 0.010101318359375\n",
      "did one more step, loss reduced by 0.0100860595703125\n",
      "did one more step, loss reduced by 0.01007080078125\n",
      "did one more step, loss reduced by 0.010057449340820312\n",
      "did one more step, loss reduced by 0.01104736328125\n",
      "did one more step, loss reduced by 0.01012420654296875\n",
      "did one more step, loss reduced by 0.010114669799804688\n",
      "did one more step, loss reduced by 0.010095596313476562\n",
      "did one more step, loss reduced by 0.010087966918945312\n",
      "did one more step, loss reduced by 0.010074615478515625\n",
      "did one more step, loss reduced by 0.01006317138671875\n",
      "did one more step, loss reduced by 0.010053634643554688\n",
      "did one more step, loss reduced by 0.010040283203125\n",
      "did one more step, loss reduced by 0.010026931762695312\n",
      "did one more step, loss reduced by 0.0110015869140625\n",
      "did one more step, loss reduced by 0.010087966918945312\n",
      "did one more step, loss reduced by 0.01007843017578125\n",
      "did one more step, loss reduced by 0.010065078735351562\n",
      "did one more step, loss reduced by 0.010053634643554688\n",
      "did one more step, loss reduced by 0.010040283203125\n",
      "did one more step, loss reduced by 0.01003265380859375\n",
      "did one more step, loss reduced by 0.01001739501953125\n",
      "did one more step, loss reduced by 0.010007858276367188\n",
      "did one more step, loss reduced by 0.009990692138671875\n",
      "did one more step, loss reduced by 0.010959625244140625\n",
      "did one more step, loss reduced by 0.010053634643554688\n",
      "did one more step, loss reduced by 0.010046005249023438\n",
      "did one more step, loss reduced by 0.01003265380859375\n",
      "did one more step, loss reduced by 0.010019302368164062\n",
      "did one more step, loss reduced by 0.010009765625\n",
      "did one more step, loss reduced by 0.009998321533203125\n",
      "did one more step, loss reduced by 0.009981155395507812\n",
      "did one more step, loss reduced by 0.009973526000976562\n",
      "did one more step, loss reduced by 0.0099639892578125\n",
      "did one more step, loss reduced by 0.0109100341796875\n",
      "did one more step, loss reduced by 0.010019302368164062\n",
      "did one more step, loss reduced by 0.010015487670898438\n",
      "did one more step, loss reduced by 0.010000228881835938\n",
      "did one more step, loss reduced by 0.00998687744140625\n",
      "did one more step, loss reduced by 0.009973526000976562\n",
      "did one more step, loss reduced by 0.009960174560546875\n",
      "did one more step, loss reduced by 0.009952545166015625\n",
      "did one more step, loss reduced by 0.00994110107421875\n",
      "did one more step, loss reduced by 0.00992584228515625\n",
      "did one more step, loss reduced by 0.010869979858398438\n",
      "did one more step, loss reduced by 0.009988784790039062\n",
      "did one more step, loss reduced by 0.009979248046875\n",
      "did one more step, loss reduced by 0.009962081909179688\n",
      "did one more step, loss reduced by 0.009952545166015625\n",
      "did one more step, loss reduced by 0.009944915771484375\n",
      "did one more step, loss reduced by 0.009929656982421875\n",
      "did one more step, loss reduced by 0.009918212890625\n",
      "did one more step, loss reduced by 0.0099029541015625\n",
      "did one more step, loss reduced by 0.00989532470703125\n",
      "did one more step, loss reduced by 0.01082611083984375\n",
      "did one more step, loss reduced by 0.00995635986328125\n",
      "did one more step, loss reduced by 0.009939193725585938\n",
      "did one more step, loss reduced by 0.009931564331054688\n",
      "did one more step, loss reduced by 0.009922027587890625\n",
      "did one more step, loss reduced by 0.009908676147460938\n",
      "did one more step, loss reduced by 0.00989532470703125\n",
      "did one more step, loss reduced by 0.009885787963867188\n",
      "did one more step, loss reduced by 0.009876251220703125\n",
      "did one more step, loss reduced by 0.009857177734375\n",
      "did one more step, loss reduced by 0.010776519775390625\n",
      "did one more step, loss reduced by 0.009923934936523438\n",
      "did one more step, loss reduced by 0.00991058349609375\n",
      "did one more step, loss reduced by 0.009901046752929688\n",
      "did one more step, loss reduced by 0.009883880615234375\n",
      "did one more step, loss reduced by 0.009876251220703125\n",
      "did one more step, loss reduced by 0.00986480712890625\n",
      "did one more step, loss reduced by 0.00984954833984375\n",
      "did one more step, loss reduced by 0.0098419189453125\n",
      "did one more step, loss reduced by 0.009824752807617188\n",
      "did one more step, loss reduced by 0.009815216064453125\n",
      "did one more step, loss reduced by 0.010805130004882812\n",
      "did one more step, loss reduced by 0.009876251220703125\n",
      "did one more step, loss reduced by 0.009868621826171875\n",
      "did one more step, loss reduced by 0.009855270385742188\n",
      "did one more step, loss reduced by 0.0098419189453125\n",
      "did one more step, loss reduced by 0.009828567504882812\n",
      "did one more step, loss reduced by 0.009815216064453125\n",
      "did one more step, loss reduced by 0.009807586669921875\n",
      "did one more step, loss reduced by 0.009794235229492188\n",
      "did one more step, loss reduced by 0.009782791137695312\n",
      "did one more step, loss reduced by 0.010761260986328125\n",
      "did one more step, loss reduced by 0.009845733642578125\n",
      "did one more step, loss reduced by 0.009830474853515625\n",
      "did one more step, loss reduced by 0.00981903076171875\n",
      "did one more step, loss reduced by 0.009807586669921875\n",
      "did one more step, loss reduced by 0.009798049926757812\n",
      "did one more step, loss reduced by 0.009782791137695312\n",
      "did one more step, loss reduced by 0.00977325439453125\n",
      "did one more step, loss reduced by 0.009761810302734375\n",
      "did one more step, loss reduced by 0.0097503662109375\n",
      "did one more step, loss reduced by 0.010715484619140625\n",
      "did one more step, loss reduced by 0.0098114013671875\n",
      "did one more step, loss reduced by 0.009796142578125\n",
      "did one more step, loss reduced by 0.00978851318359375\n",
      "did one more step, loss reduced by 0.00977325439453125\n",
      "did one more step, loss reduced by 0.009765625\n",
      "did one more step, loss reduced by 0.0097503662109375\n",
      "did one more step, loss reduced by 0.009740829467773438\n",
      "did one more step, loss reduced by 0.009723663330078125\n",
      "did one more step, loss reduced by 0.009716033935546875\n",
      "did one more step, loss reduced by 0.01067352294921875\n",
      "did one more step, loss reduced by 0.009775161743164062\n",
      "did one more step, loss reduced by 0.009765625\n",
      "did one more step, loss reduced by 0.009754180908203125\n",
      "did one more step, loss reduced by 0.009740829467773438\n",
      "did one more step, loss reduced by 0.009733200073242188\n",
      "did one more step, loss reduced by 0.009714126586914062\n",
      "did one more step, loss reduced by 0.00970458984375\n",
      "did one more step, loss reduced by 0.009695053100585938\n",
      "did one more step, loss reduced by 0.00968170166015625\n",
      "did one more step, loss reduced by 0.01062774658203125\n",
      "did one more step, loss reduced by 0.009744644165039062\n",
      "did one more step, loss reduced by 0.009729385375976562\n",
      "did one more step, loss reduced by 0.009723663330078125\n",
      "did one more step, loss reduced by 0.009706497192382812\n",
      "did one more step, loss reduced by 0.009693145751953125\n",
      "did one more step, loss reduced by 0.009691238403320312\n",
      "did one more step, loss reduced by 0.009668350219726562\n",
      "did one more step, loss reduced by 0.009664535522460938\n",
      "did one more step, loss reduced by 0.009641647338867188\n",
      "did one more step, loss reduced by 0.010585784912109375\n",
      "did one more step, loss reduced by 0.00971221923828125\n",
      "did one more step, loss reduced by 0.009695053100585938\n",
      "did one more step, loss reduced by 0.009687423706054688\n",
      "did one more step, loss reduced by 0.009675979614257812\n",
      "did one more step, loss reduced by 0.009662628173828125\n",
      "did one more step, loss reduced by 0.009649276733398438\n",
      "did one more step, loss reduced by 0.009639739990234375\n",
      "did one more step, loss reduced by 0.0096282958984375\n",
      "did one more step, loss reduced by 0.009616851806640625\n",
      "did one more step, loss reduced by 0.010534286499023438\n",
      "did one more step, loss reduced by 0.009677886962890625\n",
      "did one more step, loss reduced by 0.009668350219726562\n",
      "did one more step, loss reduced by 0.00965118408203125\n",
      "did one more step, loss reduced by 0.009641647338867188\n",
      "did one more step, loss reduced by 0.0096282958984375\n",
      "did one more step, loss reduced by 0.009616851806640625\n",
      "did one more step, loss reduced by 0.009609222412109375\n",
      "did one more step, loss reduced by 0.009592056274414062\n",
      "did one more step, loss reduced by 0.00958251953125\n",
      "did one more step, loss reduced by 0.01049041748046875\n",
      "did one more step, loss reduced by 0.009645462036132812\n",
      "did one more step, loss reduced by 0.009630203247070312\n",
      "did one more step, loss reduced by 0.009618759155273438\n",
      "did one more step, loss reduced by 0.009609222412109375\n",
      "did one more step, loss reduced by 0.009599685668945312\n",
      "did one more step, loss reduced by 0.00958251953125\n",
      "did one more step, loss reduced by 0.009571075439453125\n",
      "did one more step, loss reduced by 0.00955963134765625\n",
      "did one more step, loss reduced by 0.009550094604492188\n",
      "did one more step, loss reduced by 0.009534835815429688\n",
      "did one more step, loss reduced by 0.010519027709960938\n",
      "did one more step, loss reduced by 0.0095977783203125\n",
      "did one more step, loss reduced by 0.009593963623046875\n",
      "did one more step, loss reduced by 0.009569168090820312\n",
      "did one more step, loss reduced by 0.009565353393554688\n",
      "did one more step, loss reduced by 0.009552001953125\n",
      "did one more step, loss reduced by 0.0095367431640625\n",
      "did one more step, loss reduced by 0.009525299072265625\n",
      "did one more step, loss reduced by 0.009517669677734375\n",
      "did one more step, loss reduced by 0.0095062255859375\n",
      "did one more step, loss reduced by 0.01047515869140625\n",
      "did one more step, loss reduced by 0.009561538696289062\n",
      "did one more step, loss reduced by 0.009555816650390625\n",
      "did one more step, loss reduced by 0.009540557861328125\n",
      "did one more step, loss reduced by 0.00952911376953125\n",
      "did one more step, loss reduced by 0.009517669677734375\n",
      "did one more step, loss reduced by 0.009508132934570312\n",
      "did one more step, loss reduced by 0.009490966796875\n",
      "did one more step, loss reduced by 0.00948333740234375\n",
      "did one more step, loss reduced by 0.00946807861328125\n",
      "did one more step, loss reduced by 0.010433197021484375\n",
      "did one more step, loss reduced by 0.009531021118164062\n",
      "did one more step, loss reduced by 0.009519577026367188\n",
      "did one more step, loss reduced by 0.0095062255859375\n",
      "did one more step, loss reduced by 0.00949859619140625\n",
      "did one more step, loss reduced by 0.00948333740234375\n",
      "did one more step, loss reduced by 0.009473800659179688\n",
      "did one more step, loss reduced by 0.009458541870117188\n",
      "did one more step, loss reduced by 0.0094451904296875\n",
      "did one more step, loss reduced by 0.00943756103515625\n",
      "did one more step, loss reduced by 0.010389328002929688\n",
      "did one more step, loss reduced by 0.009496688842773438\n",
      "did one more step, loss reduced by 0.009487152099609375\n",
      "did one more step, loss reduced by 0.0094757080078125\n",
      "did one more step, loss reduced by 0.009464263916015625\n",
      "did one more step, loss reduced by 0.009447097778320312\n",
      "did one more step, loss reduced by 0.009439468383789062\n",
      "did one more step, loss reduced by 0.007541656494140625\n",
      "did one more step, loss reduced by 0.007537841796875\n",
      "did one more step, loss reduced by 0.0075244903564453125\n",
      "did one more step, loss reduced by 0.007518768310546875\n",
      "did one more step, loss reduced by 0.008466720581054688\n",
      "did one more step, loss reduced by 0.007568359375\n",
      "did one more step, loss reduced by 0.0075664520263671875\n",
      "did one more step, loss reduced by 0.00756072998046875\n",
      "did one more step, loss reduced by 0.007549285888671875\n",
      "did one more step, loss reduced by 0.0075397491455078125\n",
      "did one more step, loss reduced by 0.007534027099609375\n",
      "did one more step, loss reduced by 0.00753021240234375\n",
      "did one more step, loss reduced by 0.0075206756591796875\n",
      "did one more step, loss reduced by 0.00750732421875\n",
      "did one more step, loss reduced by 0.0075054168701171875\n",
      "did one more step, loss reduced by 0.00749969482421875\n",
      "did one more step, loss reduced by 0.008392333984375\n",
      "did one more step, loss reduced by 0.0075511932373046875\n",
      "did one more step, loss reduced by 0.0075435638427734375\n",
      "did one more step, loss reduced by 0.0075321197509765625\n",
      "did one more step, loss reduced by 0.0075283050537109375\n",
      "did one more step, loss reduced by 0.007518768310546875\n",
      "did one more step, loss reduced by 0.00751495361328125\n",
      "did one more step, loss reduced by 0.0075016021728515625\n",
      "did one more step, loss reduced by 0.007495880126953125\n",
      "did one more step, loss reduced by 0.0074901580810546875\n",
      "did one more step, loss reduced by 0.0074825286865234375\n",
      "did one more step, loss reduced by 0.0074710845947265625\n",
      "did one more step, loss reduced by 0.0074672698974609375\n",
      "did one more step, loss reduced by 0.008386611938476562\n",
      "did one more step, loss reduced by 0.0075206756591796875\n",
      "did one more step, loss reduced by 0.0075130462646484375\n",
      "did one more step, loss reduced by 0.007503509521484375\n",
      "did one more step, loss reduced by 0.007495880126953125\n",
      "did one more step, loss reduced by 0.007488250732421875\n",
      "did one more step, loss reduced by 0.00748443603515625\n",
      "did one more step, loss reduced by 0.007472991943359375\n",
      "did one more step, loss reduced by 0.0074634552001953125\n",
      "did one more step, loss reduced by 0.0074596405029296875\n",
      "did one more step, loss reduced by 0.0074520111083984375\n",
      "did one more step, loss reduced by 0.0074443817138671875\n",
      "did one more step, loss reduced by 0.0074329376220703125\n",
      "did one more step, loss reduced by 0.008378982543945312\n",
      "did one more step, loss reduced by 0.007488250732421875\n",
      "did one more step, loss reduced by 0.0074825286865234375\n",
      "did one more step, loss reduced by 0.0074748992919921875\n",
      "did one more step, loss reduced by 0.007465362548828125\n",
      "did one more step, loss reduced by 0.007457733154296875\n",
      "did one more step, loss reduced by 0.007450103759765625\n",
      "did one more step, loss reduced by 0.0074462890625\n",
      "did one more step, loss reduced by 0.0074329376220703125\n",
      "did one more step, loss reduced by 0.0074291229248046875\n",
      "did one more step, loss reduced by 0.007419586181640625\n",
      "did one more step, loss reduced by 0.0074138641357421875\n",
      "did one more step, loss reduced by 0.00830841064453125\n",
      "did one more step, loss reduced by 0.00746917724609375\n",
      "did one more step, loss reduced by 0.0074596405029296875\n",
      "did one more step, loss reduced by 0.0074462890625\n",
      "did one more step, loss reduced by 0.007442474365234375\n",
      "did one more step, loss reduced by 0.0074405670166015625\n",
      "did one more step, loss reduced by 0.007427215576171875\n",
      "did one more step, loss reduced by 0.007415771484375\n",
      "did one more step, loss reduced by 0.0074138641357421875\n",
      "did one more step, loss reduced by 0.00740814208984375\n",
      "did one more step, loss reduced by 0.0073986053466796875\n",
      "did one more step, loss reduced by 0.0073871612548828125\n",
      "did one more step, loss reduced by 0.0073795318603515625\n",
      "did one more step, loss reduced by 0.008306503295898438\n",
      "did one more step, loss reduced by 0.007434844970703125\n",
      "did one more step, loss reduced by 0.007427215576171875\n",
      "did one more step, loss reduced by 0.00742340087890625\n",
      "did one more step, loss reduced by 0.0074100494384765625\n",
      "did one more step, loss reduced by 0.0074062347412109375\n",
      "did one more step, loss reduced by 0.0073986053466796875\n",
      "did one more step, loss reduced by 0.007389068603515625\n",
      "did one more step, loss reduced by 0.007381439208984375\n",
      "did one more step, loss reduced by 0.007373809814453125\n",
      "did one more step, loss reduced by 0.0073680877685546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.00736236572265625\n",
      "did one more step, loss reduced by 0.00734710693359375\n",
      "did one more step, loss reduced by 0.008298873901367188\n",
      "did one more step, loss reduced by 0.0074024200439453125\n",
      "did one more step, loss reduced by 0.007396697998046875\n",
      "did one more step, loss reduced by 0.00739288330078125\n",
      "did one more step, loss reduced by 0.007381439208984375\n",
      "did one more step, loss reduced by 0.0073719024658203125\n",
      "did one more step, loss reduced by 0.0073680877685546875\n",
      "did one more step, loss reduced by 0.00736236572265625\n",
      "did one more step, loss reduced by 0.007350921630859375\n",
      "did one more step, loss reduced by 0.007343292236328125\n",
      "did one more step, loss reduced by 0.0073337554931640625\n",
      "did one more step, loss reduced by 0.00733184814453125\n",
      "did one more step, loss reduced by 0.0082244873046875\n",
      "did one more step, loss reduced by 0.0073833465576171875\n",
      "did one more step, loss reduced by 0.0073757171630859375\n",
      "did one more step, loss reduced by 0.0073642730712890625\n",
      "did one more step, loss reduced by 0.0073604583740234375\n",
      "did one more step, loss reduced by 0.007354736328125\n",
      "did one more step, loss reduced by 0.0073413848876953125\n",
      "did one more step, loss reduced by 0.007335662841796875\n",
      "did one more step, loss reduced by 0.007328033447265625\n",
      "did one more step, loss reduced by 0.00732421875\n",
      "did one more step, loss reduced by 0.007312774658203125\n",
      "did one more step, loss reduced by 0.007305145263671875\n",
      "did one more step, loss reduced by 0.007297515869140625\n",
      "did one more step, loss reduced by 0.008218765258789062\n",
      "did one more step, loss reduced by 0.0073490142822265625\n",
      "did one more step, loss reduced by 0.0073490142822265625\n",
      "did one more step, loss reduced by 0.0073375701904296875\n",
      "did one more step, loss reduced by 0.00732421875\n",
      "did one more step, loss reduced by 0.0073223114013671875\n",
      "did one more step, loss reduced by 0.00731658935546875\n",
      "did one more step, loss reduced by 0.007305145263671875\n",
      "did one more step, loss reduced by 0.0072956085205078125\n",
      "did one more step, loss reduced by 0.007289886474609375\n",
      "did one more step, loss reduced by 0.00728607177734375\n",
      "did one more step, loss reduced by 0.0072765350341796875\n",
      "did one more step, loss reduced by 0.00726318359375\n",
      "did one more step, loss reduced by 0.00821685791015625\n",
      "did one more step, loss reduced by 0.0073184967041015625\n",
      "did one more step, loss reduced by 0.007312774658203125\n",
      "did one more step, loss reduced by 0.0073070526123046875\n",
      "did one more step, loss reduced by 0.0072994232177734375\n",
      "did one more step, loss reduced by 0.0072879791259765625\n",
      "did one more step, loss reduced by 0.0072841644287109375\n",
      "did one more step, loss reduced by 0.007274627685546875\n",
      "did one more step, loss reduced by 0.00727081298828125\n",
      "did one more step, loss reduced by 0.0072574615478515625\n",
      "did one more step, loss reduced by 0.007251739501953125\n",
      "did one more step, loss reduced by 0.0072460174560546875\n",
      "did one more step, loss reduced by 0.00814056396484375\n",
      "did one more step, loss reduced by 0.00730133056640625\n",
      "did one more step, loss reduced by 0.0072917938232421875\n",
      "did one more step, loss reduced by 0.0072803497314453125\n",
      "did one more step, loss reduced by 0.0072765350341796875\n",
      "did one more step, loss reduced by 0.0072689056396484375\n",
      "did one more step, loss reduced by 0.007259368896484375\n",
      "did one more step, loss reduced by 0.007251739501953125\n",
      "did one more step, loss reduced by 0.007244110107421875\n",
      "did one more step, loss reduced by 0.00724029541015625\n",
      "did one more step, loss reduced by 0.007228851318359375\n",
      "did one more step, loss reduced by 0.0072193145751953125\n",
      "did one more step, loss reduced by 0.0072154998779296875\n",
      "did one more step, loss reduced by 0.008134841918945312\n",
      "did one more step, loss reduced by 0.007266998291015625\n",
      "did one more step, loss reduced by 0.00726318359375\n",
      "did one more step, loss reduced by 0.0072498321533203125\n",
      "did one more step, loss reduced by 0.007244110107421875\n",
      "did one more step, loss reduced by 0.0072383880615234375\n",
      "did one more step, loss reduced by 0.0072307586669921875\n",
      "did one more step, loss reduced by 0.007221221923828125\n",
      "did one more step, loss reduced by 0.007213592529296875\n",
      "did one more step, loss reduced by 0.007205963134765625\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.0071887969970703125\n",
      "did one more step, loss reduced by 0.0071849822998046875\n",
      "did one more step, loss reduced by 0.008129119873046875\n",
      "did one more step, loss reduced by 0.007232666015625\n",
      "did one more step, loss reduced by 0.0072307586669921875\n",
      "did one more step, loss reduced by 0.00722503662109375\n",
      "did one more step, loss reduced by 0.0072154998779296875\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.007198333740234375\n",
      "did one more step, loss reduced by 0.0071964263916015625\n",
      "did one more step, loss reduced by 0.007183074951171875\n",
      "did one more step, loss reduced by 0.007171630859375\n",
      "did one more step, loss reduced by 0.0071697235107421875\n",
      "did one more step, loss reduced by 0.00716400146484375\n",
      "did one more step, loss reduced by 0.008056640625\n",
      "did one more step, loss reduced by 0.0072174072265625\n",
      "did one more step, loss reduced by 0.007205963134765625\n",
      "did one more step, loss reduced by 0.007198333740234375\n",
      "did one more step, loss reduced by 0.007190704345703125\n",
      "did one more step, loss reduced by 0.007183074951171875\n",
      "did one more step, loss reduced by 0.00717926025390625\n",
      "did one more step, loss reduced by 0.0071659088134765625\n",
      "did one more step, loss reduced by 0.0071620941162109375\n",
      "did one more step, loss reduced by 0.0071544647216796875\n",
      "did one more step, loss reduced by 0.007144927978515625\n",
      "did one more step, loss reduced by 0.007137298583984375\n",
      "did one more step, loss reduced by 0.007129669189453125\n",
      "did one more step, loss reduced by 0.008050918579101562\n",
      "did one more step, loss reduced by 0.0071849822998046875\n",
      "did one more step, loss reduced by 0.0071773529052734375\n",
      "did one more step, loss reduced by 0.0071697235107421875\n",
      "did one more step, loss reduced by 0.0071582794189453125\n",
      "did one more step, loss reduced by 0.007152557373046875\n",
      "did one more step, loss reduced by 0.00714874267578125\n",
      "did one more step, loss reduced by 0.007137298583984375\n",
      "did one more step, loss reduced by 0.0071277618408203125\n",
      "did one more step, loss reduced by 0.0071239471435546875\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.007106781005859375\n",
      "did one more step, loss reduced by 0.007099151611328125\n",
      "did one more step, loss reduced by 0.008043289184570312\n",
      "did one more step, loss reduced by 0.0071544647216796875\n",
      "did one more step, loss reduced by 0.0071430206298828125\n",
      "did one more step, loss reduced by 0.0071392059326171875\n",
      "did one more step, loss reduced by 0.0071315765380859375\n",
      "did one more step, loss reduced by 0.0071201324462890625\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.007106781005859375\n",
      "did one more step, loss reduced by 0.007099151611328125\n",
      "did one more step, loss reduced by 0.0070934295654296875\n",
      "did one more step, loss reduced by 0.0070819854736328125\n",
      "did one more step, loss reduced by 0.007080078125\n",
      "did one more step, loss reduced by 0.00797271728515625\n",
      "did one more step, loss reduced by 0.00713348388671875\n",
      "did one more step, loss reduced by 0.007122039794921875\n",
      "did one more step, loss reduced by 0.0071125030517578125\n",
      "did one more step, loss reduced by 0.0071086883544921875\n",
      "did one more step, loss reduced by 0.0071010589599609375\n",
      "did one more step, loss reduced by 0.0070934295654296875\n",
      "did one more step, loss reduced by 0.0070819854736328125\n",
      "did one more step, loss reduced by 0.007076263427734375\n",
      "did one more step, loss reduced by 0.00707244873046875\n",
      "did one more step, loss reduced by 0.007061004638671875\n",
      "did one more step, loss reduced by 0.0070514678955078125\n",
      "did one more step, loss reduced by 0.0070476531982421875\n",
      "did one more step, loss reduced by 0.007966995239257812\n",
      "did one more step, loss reduced by 0.0071010589599609375\n",
      "did one more step, loss reduced by 0.0070934295654296875\n",
      "did one more step, loss reduced by 0.007083892822265625\n",
      "did one more step, loss reduced by 0.007076263427734375\n",
      "did one more step, loss reduced by 0.007068634033203125\n",
      "did one more step, loss reduced by 0.0070629119873046875\n",
      "did one more step, loss reduced by 0.0070552825927734375\n",
      "did one more step, loss reduced by 0.0070438385009765625\n",
      "did one more step, loss reduced by 0.0070400238037109375\n",
      "did one more step, loss reduced by 0.0070323944091796875\n",
      "did one more step, loss reduced by 0.007022857666015625\n",
      "did one more step, loss reduced by 0.0070171356201171875\n",
      "did one more step, loss reduced by 0.007959365844726562\n",
      "did one more step, loss reduced by 0.007068634033203125\n",
      "did one more step, loss reduced by 0.0070590972900390625\n",
      "did one more step, loss reduced by 0.00705718994140625\n",
      "did one more step, loss reduced by 0.0070476531982421875\n",
      "did one more step, loss reduced by 0.0070362091064453125\n",
      "did one more step, loss reduced by 0.0070323944091796875\n",
      "did one more step, loss reduced by 0.0070247650146484375\n",
      "did one more step, loss reduced by 0.007015228271484375\n",
      "did one more step, loss reduced by 0.007007598876953125\n",
      "did one more step, loss reduced by 0.006999969482421875\n",
      "did one more step, loss reduced by 0.00699615478515625\n",
      "did one more step, loss reduced by 0.0078887939453125\n",
      "did one more step, loss reduced by 0.0070476531982421875\n",
      "did one more step, loss reduced by 0.007038116455078125\n",
      "did one more step, loss reduced by 0.007030487060546875\n",
      "did one more step, loss reduced by 0.0070247650146484375\n",
      "did one more step, loss reduced by 0.007015228271484375\n",
      "did one more step, loss reduced by 0.0070095062255859375\n",
      "did one more step, loss reduced by 0.006999969482421875\n",
      "did one more step, loss reduced by 0.006992340087890625\n",
      "did one more step, loss reduced by 0.0069866180419921875\n",
      "did one more step, loss reduced by 0.006977081298828125\n",
      "did one more step, loss reduced by 0.006969451904296875\n",
      "did one more step, loss reduced by 0.0069637298583984375\n",
      "did one more step, loss reduced by 0.00788116455078125\n",
      "did one more step, loss reduced by 0.0070171356201171875\n",
      "did one more step, loss reduced by 0.00701141357421875\n",
      "did one more step, loss reduced by 0.006999969482421875\n",
      "did one more step, loss reduced by 0.006992340087890625\n",
      "did one more step, loss reduced by 0.0069828033447265625\n",
      "did one more step, loss reduced by 0.00698089599609375\n",
      "did one more step, loss reduced by 0.0069713592529296875\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.0069484710693359375\n",
      "did one more step, loss reduced by 0.006938934326171875\n",
      "did one more step, loss reduced by 0.006931304931640625\n",
      "did one more step, loss reduced by 0.007875442504882812\n",
      "did one more step, loss reduced by 0.0069866180419921875\n",
      "did one more step, loss reduced by 0.0069751739501953125\n",
      "did one more step, loss reduced by 0.0069732666015625\n",
      "did one more step, loss reduced by 0.006961822509765625\n",
      "did one more step, loss reduced by 0.006954193115234375\n",
      "did one more step, loss reduced by 0.0069484710693359375\n",
      "did one more step, loss reduced by 0.006938934326171875\n",
      "did one more step, loss reduced by 0.0069332122802734375\n",
      "did one more step, loss reduced by 0.006923675537109375\n",
      "did one more step, loss reduced by 0.006916046142578125\n",
      "did one more step, loss reduced by 0.0069103240966796875\n",
      "did one more step, loss reduced by 0.00780487060546875\n",
      "did one more step, loss reduced by 0.00696563720703125\n",
      "did one more step, loss reduced by 0.006954193115234375\n",
      "did one more step, loss reduced by 0.0069446563720703125\n",
      "did one more step, loss reduced by 0.0069408416748046875\n",
      "did one more step, loss reduced by 0.0069332122802734375\n",
      "did one more step, loss reduced by 0.0069255828857421875\n",
      "did one more step, loss reduced by 0.0069141387939453125\n",
      "did one more step, loss reduced by 0.006908416748046875\n",
      "did one more step, loss reduced by 0.00690460205078125\n",
      "did one more step, loss reduced by 0.006893157958984375\n",
      "did one more step, loss reduced by 0.0068836212158203125\n",
      "did one more step, loss reduced by 0.0068798065185546875\n",
      "did one more step, loss reduced by 0.0077991485595703125\n",
      "did one more step, loss reduced by 0.0069332122802734375\n",
      "did one more step, loss reduced by 0.0069255828857421875\n",
      "did one more step, loss reduced by 0.006916046142578125\n",
      "did one more step, loss reduced by 0.0069103240966796875\n",
      "did one more step, loss reduced by 0.0068988800048828125\n",
      "did one more step, loss reduced by 0.0068950653076171875\n",
      "did one more step, loss reduced by 0.0068874359130859375\n",
      "did one more step, loss reduced by 0.0068759918212890625\n",
      "did one more step, loss reduced by 0.00687408447265625\n",
      "did one more step, loss reduced by 0.006862640380859375\n",
      "did one more step, loss reduced by 0.006855010986328125\n",
      "did one more step, loss reduced by 0.0068492889404296875\n",
      "did one more step, loss reduced by 0.0077915191650390625\n",
      "did one more step, loss reduced by 0.006900787353515625\n",
      "did one more step, loss reduced by 0.006893157958984375\n",
      "did one more step, loss reduced by 0.00688934326171875\n",
      "did one more step, loss reduced by 0.006877899169921875\n",
      "did one more step, loss reduced by 0.0068683624267578125\n",
      "did one more step, loss reduced by 0.0068645477294921875\n",
      "did one more step, loss reduced by 0.0068569183349609375\n",
      "did one more step, loss reduced by 0.0068492889404296875\n",
      "did one more step, loss reduced by 0.0068378448486328125\n",
      "did one more step, loss reduced by 0.006832122802734375\n",
      "did one more step, loss reduced by 0.00682830810546875\n",
      "did one more step, loss reduced by 0.007720947265625\n",
      "did one more step, loss reduced by 0.0068798065185546875\n",
      "did one more step, loss reduced by 0.006870269775390625\n",
      "did one more step, loss reduced by 0.006862640380859375\n",
      "did one more step, loss reduced by 0.0068569183349609375\n",
      "did one more step, loss reduced by 0.0068492889404296875\n",
      "did one more step, loss reduced by 0.006839752197265625\n",
      "did one more step, loss reduced by 0.006832122802734375\n",
      "did one more step, loss reduced by 0.006824493408203125\n",
      "did one more step, loss reduced by 0.0068187713623046875\n",
      "did one more step, loss reduced by 0.0068111419677734375\n",
      "did one more step, loss reduced by 0.0067996978759765625\n",
      "did one more step, loss reduced by 0.0067958831787109375\n",
      "did one more step, loss reduced by 0.00771331787109375\n",
      "did one more step, loss reduced by 0.0068511962890625\n",
      "did one more step, loss reduced by 0.0068416595458984375\n",
      "did one more step, loss reduced by 0.006832122802734375\n",
      "did one more step, loss reduced by 0.006824493408203125\n",
      "did one more step, loss reduced by 0.0068149566650390625\n",
      "did one more step, loss reduced by 0.00681304931640625\n",
      "did one more step, loss reduced by 0.0068035125732421875\n",
      "did one more step, loss reduced by 0.0067920684814453125\n",
      "did one more step, loss reduced by 0.0067882537841796875\n",
      "did one more step, loss reduced by 0.0067806243896484375\n",
      "did one more step, loss reduced by 0.006771087646484375\n",
      "did one more step, loss reduced by 0.006763458251953125\n",
      "did one more step, loss reduced by 0.007709503173828125\n",
      "did one more step, loss reduced by 0.006816864013671875\n",
      "did one more step, loss reduced by 0.006809234619140625\n",
      "did one more step, loss reduced by 0.0068035125732421875\n",
      "did one more step, loss reduced by 0.006793975830078125\n",
      "did one more step, loss reduced by 0.006786346435546875\n",
      "did one more step, loss reduced by 0.0067806243896484375\n",
      "did one more step, loss reduced by 0.006771087646484375\n",
      "did one more step, loss reduced by 0.0067653656005859375\n",
      "did one more step, loss reduced by 0.006755828857421875\n",
      "did one more step, loss reduced by 0.006748199462890625\n",
      "did one more step, loss reduced by 0.0067424774169921875\n",
      "did one more step, loss reduced by 0.00763702392578125\n",
      "did one more step, loss reduced by 0.00679779052734375\n",
      "did one more step, loss reduced by 0.006786346435546875\n",
      "did one more step, loss reduced by 0.0067768096923828125\n",
      "did one more step, loss reduced by 0.0067729949951171875\n",
      "did one more step, loss reduced by 0.00676727294921875\n",
      "did one more step, loss reduced by 0.006755828857421875\n",
      "did one more step, loss reduced by 0.006748199462890625\n",
      "did one more step, loss reduced by 0.0067386627197265625\n",
      "did one more step, loss reduced by 0.00673675537109375\n",
      "did one more step, loss reduced by 0.0067272186279296875\n",
      "did one more step, loss reduced by 0.0067138671875\n",
      "did one more step, loss reduced by 0.0067138671875\n",
      "did one more step, loss reduced by 0.00762939453125\n",
      "did one more step, loss reduced by 0.00676727294921875\n",
      "did one more step, loss reduced by 0.006755828857421875\n",
      "did one more step, loss reduced by 0.006748199462890625\n",
      "did one more step, loss reduced by 0.0067424774169921875\n",
      "did one more step, loss reduced by 0.0067310333251953125\n",
      "did one more step, loss reduced by 0.0067291259765625\n",
      "did one more step, loss reduced by 0.006717681884765625\n",
      "did one more step, loss reduced by 0.006710052490234375\n",
      "did one more step, loss reduced by 0.0067043304443359375\n",
      "did one more step, loss reduced by 0.006694793701171875\n",
      "did one more step, loss reduced by 0.0066890716552734375\n",
      "did one more step, loss reduced by 0.006679534912109375\n",
      "did one more step, loss reduced by 0.007625579833984375\n",
      "did one more step, loss reduced by 0.0067310333251953125\n",
      "did one more step, loss reduced by 0.006725311279296875\n",
      "did one more step, loss reduced by 0.00672149658203125\n",
      "did one more step, loss reduced by 0.006710052490234375\n",
      "did one more step, loss reduced by 0.0067005157470703125\n",
      "did one more step, loss reduced by 0.0066967010498046875\n",
      "did one more step, loss reduced by 0.0066890716552734375\n",
      "did one more step, loss reduced by 0.0066814422607421875\n",
      "did one more step, loss reduced by 0.0066699981689453125\n",
      "did one more step, loss reduced by 0.006664276123046875\n",
      "did one more step, loss reduced by 0.00666046142578125\n",
      "did one more step, loss reduced by 0.0075531005859375\n",
      "did one more step, loss reduced by 0.0067119598388671875\n",
      "did one more step, loss reduced by 0.0067043304443359375\n",
      "did one more step, loss reduced by 0.0066928863525390625\n",
      "did one more step, loss reduced by 0.0066890716552734375\n",
      "did one more step, loss reduced by 0.0066814422607421875\n",
      "did one more step, loss reduced by 0.006671905517578125\n",
      "did one more step, loss reduced by 0.0066661834716796875\n",
      "did one more step, loss reduced by 0.0066547393798828125\n",
      "did one more step, loss reduced by 0.0066509246826171875\n",
      "did one more step, loss reduced by 0.0066432952880859375\n",
      "did one more step, loss reduced by 0.0066318511962890625\n",
      "did one more step, loss reduced by 0.00662994384765625\n",
      "did one more step, loss reduced by 0.00754547119140625\n",
      "did one more step, loss reduced by 0.0066814422607421875\n",
      "did one more step, loss reduced by 0.0066738128662109375\n",
      "did one more step, loss reduced by 0.006664276123046875\n",
      "did one more step, loss reduced by 0.006656646728515625\n",
      "did one more step, loss reduced by 0.006649017333984375\n",
      "did one more step, loss reduced by 0.00664520263671875\n",
      "did one more step, loss reduced by 0.006633758544921875\n",
      "did one more step, loss reduced by 0.0066242218017578125\n",
      "did one more step, loss reduced by 0.0066204071044921875\n",
      "did one more step, loss reduced by 0.0066127777099609375\n",
      "did one more step, loss reduced by 0.0066051483154296875\n",
      "did one more step, loss reduced by 0.0065937042236328125\n",
      "did one more step, loss reduced by 0.007541656494140625\n",
      "did one more step, loss reduced by 0.006649017333984375\n",
      "did one more step, loss reduced by 0.006641387939453125\n",
      "did one more step, loss reduced by 0.0066356658935546875\n",
      "did one more step, loss reduced by 0.006626129150390625\n",
      "did one more step, loss reduced by 0.006618499755859375\n",
      "did one more step, loss reduced by 0.0066127777099609375\n",
      "did one more step, loss reduced by 0.0066051483154296875\n",
      "did one more step, loss reduced by 0.006595611572265625\n",
      "did one more step, loss reduced by 0.006587982177734375\n",
      "did one more step, loss reduced by 0.006580352783203125\n",
      "did one more step, loss reduced by 0.0065746307373046875\n",
      "did one more step, loss reduced by 0.00746917724609375\n",
      "did one more step, loss reduced by 0.00662994384765625\n",
      "did one more step, loss reduced by 0.0066204071044921875\n",
      "did one more step, loss reduced by 0.0066070556640625\n",
      "did one more step, loss reduced by 0.0066070556640625\n",
      "did one more step, loss reduced by 0.0065975189208984375\n",
      "did one more step, loss reduced by 0.006587982177734375\n",
      "did one more step, loss reduced by 0.006580352783203125\n",
      "did one more step, loss reduced by 0.0065708160400390625\n",
      "did one more step, loss reduced by 0.00656890869140625\n",
      "did one more step, loss reduced by 0.0065593719482421875\n",
      "did one more step, loss reduced by 0.0065479278564453125\n",
      "did one more step, loss reduced by 0.0065441131591796875\n",
      "did one more step, loss reduced by 0.0074634552001953125\n",
      "did one more step, loss reduced by 0.0065975189208984375\n",
      "did one more step, loss reduced by 0.006587982177734375\n",
      "did one more step, loss reduced by 0.0065822601318359375\n",
      "did one more step, loss reduced by 0.006572723388671875\n",
      "did one more step, loss reduced by 0.006565093994140625\n",
      "did one more step, loss reduced by 0.0065593719482421875\n",
      "did one more step, loss reduced by 0.006549835205078125\n",
      "did one more step, loss reduced by 0.006542205810546875\n",
      "did one more step, loss reduced by 0.0065364837646484375\n",
      "did one more step, loss reduced by 0.006526947021484375\n",
      "did one more step, loss reduced by 0.0065212249755859375\n",
      "did one more step, loss reduced by 0.006511688232421875\n",
      "did one more step, loss reduced by 0.007457733154296875\n",
      "did one more step, loss reduced by 0.0065631866455078125\n",
      "did one more step, loss reduced by 0.006557464599609375\n",
      "did one more step, loss reduced by 0.00655364990234375\n",
      "did one more step, loss reduced by 0.006542205810546875\n",
      "did one more step, loss reduced by 0.0065326690673828125\n",
      "did one more step, loss reduced by 0.0065288543701171875\n",
      "did one more step, loss reduced by 0.00652313232421875\n",
      "did one more step, loss reduced by 0.006511688232421875\n",
      "did one more step, loss reduced by 0.006504058837890625\n",
      "did one more step, loss reduced by 0.0064945220947265625\n",
      "did one more step, loss reduced by 0.00649261474609375\n",
      "did one more step, loss reduced by 0.00738525390625\n",
      "did one more step, loss reduced by 0.0065441131591796875\n",
      "did one more step, loss reduced by 0.0065364837646484375\n",
      "did one more step, loss reduced by 0.0065250396728515625\n",
      "did one more step, loss reduced by 0.00652313232421875\n",
      "did one more step, loss reduced by 0.006511688232421875\n",
      "did one more step, loss reduced by 0.006504058837890625\n",
      "did one more step, loss reduced by 0.0064983367919921875\n",
      "did one more step, loss reduced by 0.0064868927001953125\n",
      "did one more step, loss reduced by 0.0064849853515625\n",
      "did one more step, loss reduced by 0.006473541259765625\n",
      "did one more step, loss reduced by 0.006465911865234375\n",
      "did one more step, loss reduced by 0.0064601898193359375\n",
      "did one more step, loss reduced by 0.00737762451171875\n",
      "did one more step, loss reduced by 0.0065135955810546875\n",
      "did one more step, loss reduced by 0.0065059661865234375\n",
      "did one more step, loss reduced by 0.0064983367919921875\n",
      "did one more step, loss reduced by 0.0064868927001953125\n",
      "did one more step, loss reduced by 0.006481170654296875\n",
      "did one more step, loss reduced by 0.00647735595703125\n",
      "did one more step, loss reduced by 0.006465911865234375\n",
      "did one more step, loss reduced by 0.0064563751220703125\n",
      "did one more step, loss reduced by 0.0064525604248046875\n",
      "did one more step, loss reduced by 0.0064449310302734375\n",
      "did one more step, loss reduced by 0.0064373016357421875\n",
      "did one more step, loss reduced by 0.0064258575439453125\n",
      "did one more step, loss reduced by 0.007373809814453125\n",
      "did one more step, loss reduced by 0.006481170654296875\n",
      "did one more step, loss reduced by 0.006473541259765625\n",
      "did one more step, loss reduced by 0.0064678192138671875\n",
      "did one more step, loss reduced by 0.0064601898193359375\n",
      "did one more step, loss reduced by 0.0064487457275390625\n",
      "did one more step, loss reduced by 0.0064449310302734375\n",
      "did one more step, loss reduced by 0.0064373016357421875\n",
      "did one more step, loss reduced by 0.006427764892578125\n",
      "did one more step, loss reduced by 0.0064220428466796875\n",
      "did one more step, loss reduced by 0.0064105987548828125\n",
      "did one more step, loss reduced by 0.0064067840576171875\n",
      "did one more step, loss reduced by 0.00730133056640625\n",
      "did one more step, loss reduced by 0.00646209716796875\n",
      "did one more step, loss reduced by 0.009675979614257812\n",
      "did one more step, loss reduced by 0.0064373016357421875\n",
      "did one more step, loss reduced by 0.006435394287109375\n",
      "did one more step, loss reduced by 0.0064220428466796875\n",
      "did one more step, loss reduced by 0.006420135498046875\n",
      "did one more step, loss reduced by 0.0064067840576171875\n",
      "did one more step, loss reduced by 0.006404876708984375\n",
      "did one more step, loss reduced by 0.0063915252685546875\n",
      "did one more step, loss reduced by 0.0063877105712890625\n",
      "did one more step, loss reduced by 0.006378173828125\n",
      "did one more step, loss reduced by 0.0063724517822265625\n",
      "did one more step, loss reduced by 0.00732421875\n",
      "did one more step, loss reduced by 0.006427764892578125\n",
      "did one more step, loss reduced by 0.0064144134521484375\n",
      "did one more step, loss reduced by 0.006412506103515625\n",
      "did one more step, loss reduced by 0.0063991546630859375\n",
      "did one more step, loss reduced by 0.0063953399658203125\n",
      "did one more step, loss reduced by 0.006384849548339844\n",
      "did one more step, loss reduced by 0.006381034851074219\n",
      "did one more step, loss reduced by 0.006369590759277344\n",
      "did one more step, loss reduced by 0.006365776062011719\n",
      "did one more step, loss reduced by 0.0063533782958984375\n",
      "did one more step, loss reduced by 0.006351470947265625\n",
      "did one more step, loss reduced by 0.00725555419921875\n",
      "did one more step, loss reduced by 0.006403923034667969\n",
      "did one more step, loss reduced by 0.0063915252685546875\n",
      "did one more step, loss reduced by 0.006388664245605469\n",
      "did one more step, loss reduced by 0.006377220153808594\n",
      "did one more step, loss reduced by 0.006373405456542969\n",
      "did one more step, loss reduced by 0.0063610076904296875\n",
      "did one more step, loss reduced by 0.006359100341796875\n",
      "did one more step, loss reduced by 0.0063457489013671875\n",
      "did one more step, loss reduced by 0.006342887878417969\n",
      "did one more step, loss reduced by 0.006331443786621094\n",
      "did one more step, loss reduced by 0.006327629089355469\n",
      "did one more step, loss reduced by 0.0063152313232421875\n",
      "did one more step, loss reduced by 0.007252693176269531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.0063686370849609375\n",
      "did one more step, loss reduced by 0.006366729736328125\n",
      "did one more step, loss reduced by 0.0063533782958984375\n",
      "did one more step, loss reduced by 0.006350517272949219\n",
      "did one more step, loss reduced by 0.006339073181152344\n",
      "did one more step, loss reduced by 0.006335258483886719\n",
      "did one more step, loss reduced by 0.0063228607177734375\n",
      "did one more step, loss reduced by 0.006319999694824219\n",
      "did one more step, loss reduced by 0.006308555603027344\n",
      "did one more step, loss reduced by 0.006304740905761719\n",
      "did one more step, loss reduced by 0.0062923431396484375\n",
      "did one more step, loss reduced by 0.006290435791015625\n",
      "did one more step, loss reduced by 0.00724029541015625\n",
      "did one more step, loss reduced by 0.006342887878417969\n",
      "did one more step, loss reduced by 0.0063304901123046875\n",
      "did one more step, loss reduced by 0.006327629089355469\n",
      "did one more step, loss reduced by 0.006316184997558594\n",
      "did one more step, loss reduced by 0.006312370300292969\n",
      "did one more step, loss reduced by 0.0062999725341796875\n",
      "did one more step, loss reduced by 0.006298065185546875\n",
      "did one more step, loss reduced by 0.0062847137451171875\n",
      "did one more step, loss reduced by 0.006281852722167969\n",
      "did one more step, loss reduced by 0.006270408630371094\n",
      "did one more step, loss reduced by 0.006266593933105469\n",
      "did one more step, loss reduced by 0.007171630859375\n",
      "did one more step, loss reduced by 0.006319999694824219\n",
      "did one more step, loss reduced by 0.0063076019287109375\n",
      "did one more step, loss reduced by 0.006305694580078125\n",
      "did one more step, loss reduced by 0.0062923431396484375\n",
      "did one more step, loss reduced by 0.006289482116699219\n",
      "did one more step, loss reduced by 0.006278038024902344\n",
      "did one more step, loss reduced by 0.006274223327636719\n",
      "did one more step, loss reduced by 0.0062618255615234375\n",
      "did one more step, loss reduced by 0.006258964538574219\n",
      "did one more step, loss reduced by 0.006247520446777344\n",
      "did one more step, loss reduced by 0.006243705749511719\n",
      "did one more step, loss reduced by 0.0062313079833984375\n",
      "did one more step, loss reduced by 0.007168769836425781\n",
      "did one more step, loss reduced by 0.006285667419433594\n",
      "did one more step, loss reduced by 0.006281852722167969\n",
      "did one more step, loss reduced by 0.0062694549560546875\n",
      "did one more step, loss reduced by 0.006266593933105469\n",
      "did one more step, loss reduced by 0.006255149841308594\n",
      "did one more step, loss reduced by 0.006251335144042969\n",
      "did one more step, loss reduced by 0.0062389373779296875\n",
      "did one more step, loss reduced by 0.006237030029296875\n",
      "did one more step, loss reduced by 0.0062236785888671875\n",
      "did one more step, loss reduced by 0.006220817565917969\n",
      "did one more step, loss reduced by 0.006209373474121094\n",
      "did one more step, loss reduced by 0.006205558776855469\n",
      "did one more step, loss reduced by 0.0071563720703125\n",
      "did one more step, loss reduced by 0.006258964538574219\n",
      "did one more step, loss reduced by 0.0062465667724609375\n",
      "did one more step, loss reduced by 0.006244659423828125\n",
      "did one more step, loss reduced by 0.0062313079833984375\n",
      "did one more step, loss reduced by 0.006228446960449219\n",
      "did one more step, loss reduced by 0.006217002868652344\n",
      "did one more step, loss reduced by 0.006213188171386719\n",
      "did one more step, loss reduced by 0.0062007904052734375\n",
      "did one more step, loss reduced by 0.006197929382324219\n",
      "did one more step, loss reduced by 0.006186485290527344\n",
      "did one more step, loss reduced by 0.006182670593261719\n",
      "did one more step, loss reduced by 0.00708770751953125\n",
      "did one more step, loss reduced by 0.006236076354980469\n",
      "did one more step, loss reduced by 0.006224632263183594\n",
      "did one more step, loss reduced by 0.006220817565917969\n",
      "did one more step, loss reduced by 0.0062084197998046875\n",
      "did one more step, loss reduced by 0.006205558776855469\n",
      "did one more step, loss reduced by 0.006194114685058594\n",
      "did one more step, loss reduced by 0.006190299987792969\n",
      "did one more step, loss reduced by 0.0061779022216796875\n",
      "did one more step, loss reduced by 0.006175994873046875\n",
      "did one more step, loss reduced by 0.0061626434326171875\n",
      "did one more step, loss reduced by 0.006159782409667969\n",
      "did one more step, loss reduced by 0.006148338317871094\n",
      "did one more step, loss reduced by 0.007083892822265625\n",
      "did one more step, loss reduced by 0.006201744079589844\n",
      "did one more step, loss reduced by 0.006197929382324219\n",
      "did one more step, loss reduced by 0.0061855316162109375\n",
      "did one more step, loss reduced by 0.006183624267578125\n",
      "did one more step, loss reduced by 0.0061702728271484375\n",
      "did one more step, loss reduced by 0.006167411804199219\n",
      "did one more step, loss reduced by 0.006155967712402344\n",
      "did one more step, loss reduced by 0.006152153015136719\n",
      "did one more step, loss reduced by 0.0061397552490234375\n",
      "did one more step, loss reduced by 0.006136894226074219\n",
      "did one more step, loss reduced by 0.006125450134277344\n",
      "did one more step, loss reduced by 0.006121635437011719\n",
      "did one more step, loss reduced by 0.00707244873046875\n",
      "did one more step, loss reduced by 0.006175041198730469\n",
      "did one more step, loss reduced by 0.006163597106933594\n",
      "did one more step, loss reduced by 0.006159782409667969\n",
      "did one more step, loss reduced by 0.0061473846435546875\n",
      "did one more step, loss reduced by 0.006144523620605469\n",
      "did one more step, loss reduced by 0.006133079528808594\n",
      "did one more step, loss reduced by 0.006129264831542969\n",
      "did one more step, loss reduced by 0.0061168670654296875\n",
      "did one more step, loss reduced by 0.006114959716796875\n",
      "did one more step, loss reduced by 0.0061016082763671875\n",
      "did one more step, loss reduced by 0.006098747253417969\n",
      "did one more step, loss reduced by 0.0070037841796875\n",
      "did one more step, loss reduced by 0.006152153015136719\n",
      "did one more step, loss reduced by 0.006140708923339844\n",
      "did one more step, loss reduced by 0.006136894226074219\n",
      "did one more step, loss reduced by 0.0061244964599609375\n",
      "did one more step, loss reduced by 0.006122589111328125\n",
      "did one more step, loss reduced by 0.0061092376708984375\n",
      "did one more step, loss reduced by 0.006106376647949219\n",
      "did one more step, loss reduced by 0.006094932556152344\n",
      "did one more step, loss reduced by 0.006091117858886719\n",
      "did one more step, loss reduced by 0.0060787200927734375\n",
      "did one more step, loss reduced by 0.006075859069824219\n",
      "did one more step, loss reduced by 0.006064414978027344\n",
      "did one more step, loss reduced by 0.007000923156738281\n",
      "did one more step, loss reduced by 0.0061168670654296875\n",
      "did one more step, loss reduced by 0.006114006042480469\n",
      "did one more step, loss reduced by 0.006102561950683594\n",
      "did one more step, loss reduced by 0.006098747253417969\n",
      "did one more step, loss reduced by 0.0060863494873046875\n",
      "did one more step, loss reduced by 0.006083488464355469\n",
      "did one more step, loss reduced by 0.006072044372558594\n",
      "did one more step, loss reduced by 0.006068229675292969\n",
      "did one more step, loss reduced by 0.0060558319091796875\n",
      "did one more step, loss reduced by 0.006053924560546875\n",
      "did one more step, loss reduced by 0.0060405731201171875\n",
      "did one more step, loss reduced by 0.006037712097167969\n",
      "did one more step, loss reduced by 0.006988525390625\n",
      "did one more step, loss reduced by 0.006091117858886719\n",
      "did one more step, loss reduced by 0.006079673767089844\n",
      "did one more step, loss reduced by 0.006075859069824219\n",
      "did one more step, loss reduced by 0.0060634613037109375\n",
      "did one more step, loss reduced by 0.006061553955078125\n",
      "did one more step, loss reduced by 0.0060482025146484375\n",
      "did one more step, loss reduced by 0.006045341491699219\n",
      "did one more step, loss reduced by 0.006033897399902344\n",
      "did one more step, loss reduced by 0.006030082702636719\n",
      "did one more step, loss reduced by 0.0060176849365234375\n",
      "did one more step, loss reduced by 0.006014823913574219\n",
      "did one more step, loss reduced by 0.00691986083984375\n",
      "did one more step, loss reduced by 0.006069183349609375\n",
      "did one more step, loss reduced by 0.0060558319091796875\n",
      "did one more step, loss reduced by 0.006052970886230469\n",
      "did one more step, loss reduced by 0.006041526794433594\n",
      "did one more step, loss reduced by 0.006037712097167969\n",
      "did one more step, loss reduced by 0.0060253143310546875\n",
      "did one more step, loss reduced by 0.006022453308105469\n",
      "did one more step, loss reduced by 0.006011009216308594\n",
      "did one more step, loss reduced by 0.006007194519042969\n",
      "did one more step, loss reduced by 0.0059947967529296875\n",
      "did one more step, loss reduced by 0.005992889404296875\n",
      "did one more step, loss reduced by 0.0059795379638671875\n",
      "did one more step, loss reduced by 0.006916999816894531\n",
      "did one more step, loss reduced by 0.0060329437255859375\n",
      "did one more step, loss reduced by 0.006030082702636719\n",
      "did one more step, loss reduced by 0.006018638610839844\n",
      "did one more step, loss reduced by 0.006014823913574219\n",
      "did one more step, loss reduced by 0.0060024261474609375\n",
      "did one more step, loss reduced by 0.006000518798828125\n",
      "did one more step, loss reduced by 0.0059871673583984375\n",
      "did one more step, loss reduced by 0.005984306335449219\n",
      "did one more step, loss reduced by 0.005972862243652344\n",
      "did one more step, loss reduced by 0.005969047546386719\n",
      "did one more step, loss reduced by 0.0059566497802734375\n",
      "did one more step, loss reduced by 0.005953788757324219\n",
      "did one more step, loss reduced by 0.00690460205078125\n",
      "did one more step, loss reduced by 0.006008148193359375\n",
      "did one more step, loss reduced by 0.0059947967529296875\n",
      "did one more step, loss reduced by 0.005991935729980469\n",
      "did one more step, loss reduced by 0.005980491638183594\n",
      "did one more step, loss reduced by 0.005976676940917969\n",
      "did one more step, loss reduced by 0.0059642791748046875\n",
      "did one more step, loss reduced by 0.005961418151855469\n",
      "did one more step, loss reduced by 0.005949974060058594\n",
      "did one more step, loss reduced by 0.005946159362792969\n",
      "did one more step, loss reduced by 0.0059337615966796875\n",
      "did one more step, loss reduced by 0.005931854248046875\n",
      "did one more step, loss reduced by 0.0068359375\n",
      "did one more step, loss reduced by 0.005984306335449219\n",
      "did one more step, loss reduced by 0.0059719085693359375\n",
      "did one more step, loss reduced by 0.005969047546386719\n",
      "did one more step, loss reduced by 0.005957603454589844\n",
      "did one more step, loss reduced by 0.005953788757324219\n",
      "did one more step, loss reduced by 0.0059413909912109375\n",
      "did one more step, loss reduced by 0.005939483642578125\n",
      "did one more step, loss reduced by 0.0059261322021484375\n",
      "did one more step, loss reduced by 0.005923271179199219\n",
      "did one more step, loss reduced by 0.005911827087402344\n",
      "did one more step, loss reduced by 0.005908012390136719\n",
      "did one more step, loss reduced by 0.0058956146240234375\n",
      "did one more step, loss reduced by 0.006833076477050781\n",
      "did one more step, loss reduced by 0.0059490203857421875\n",
      "did one more step, loss reduced by 0.005947113037109375\n",
      "did one more step, loss reduced by 0.0059337615966796875\n",
      "did one more step, loss reduced by 0.005930900573730469\n",
      "did one more step, loss reduced by 0.005919456481933594\n",
      "did one more step, loss reduced by 0.005915641784667969\n",
      "did one more step, loss reduced by 0.0059032440185546875\n",
      "did one more step, loss reduced by 0.005900382995605469\n",
      "did one more step, loss reduced by 0.005888938903808594\n",
      "did one more step, loss reduced by 0.005885124206542969\n",
      "did one more step, loss reduced by 0.0058727264404296875\n",
      "did one more step, loss reduced by 0.005870819091796875\n",
      "did one more step, loss reduced by 0.0068206787109375\n",
      "did one more step, loss reduced by 0.005923271179199219\n",
      "did one more step, loss reduced by 0.0059108734130859375\n",
      "did one more step, loss reduced by 0.005908012390136719\n",
      "did one more step, loss reduced by 0.005896568298339844\n",
      "did one more step, loss reduced by 0.005892753601074219\n",
      "did one more step, loss reduced by 0.0058803558349609375\n",
      "did one more step, loss reduced by 0.005878448486328125\n",
      "did one more step, loss reduced by 0.0058650970458984375\n",
      "did one more step, loss reduced by 0.005862236022949219\n",
      "did one more step, loss reduced by 0.005850791931152344\n",
      "did one more step, loss reduced by 0.005846977233886719\n",
      "did one more step, loss reduced by 0.00675201416015625\n",
      "did one more step, loss reduced by 0.005900382995605469\n",
      "did one more step, loss reduced by 0.0058879852294921875\n",
      "did one more step, loss reduced by 0.005886077880859375\n",
      "did one more step, loss reduced by 0.0058727264404296875\n",
      "did one more step, loss reduced by 0.005869865417480469\n",
      "did one more step, loss reduced by 0.005858421325683594\n",
      "did one more step, loss reduced by 0.005854606628417969\n",
      "did one more step, loss reduced by 0.0058422088623046875\n",
      "did one more step, loss reduced by 0.005839347839355469\n",
      "did one more step, loss reduced by 0.005827903747558594\n",
      "did one more step, loss reduced by 0.005824089050292969\n",
      "did one more step, loss reduced by 0.0058116912841796875\n",
      "did one more step, loss reduced by 0.006749153137207031\n",
      "did one more step, loss reduced by 0.005866050720214844\n",
      "did one more step, loss reduced by 0.005862236022949219\n",
      "did one more step, loss reduced by 0.0058498382568359375\n",
      "did one more step, loss reduced by 0.005846977233886719\n",
      "did one more step, loss reduced by 0.005835533142089844\n",
      "did one more step, loss reduced by 0.005831718444824219\n",
      "did one more step, loss reduced by 0.0058193206787109375\n",
      "did one more step, loss reduced by 0.005817413330078125\n",
      "did one more step, loss reduced by 0.0058040618896484375\n",
      "did one more step, loss reduced by 0.005801200866699219\n",
      "did one more step, loss reduced by 0.005789756774902344\n",
      "did one more step, loss reduced by 0.005785942077636719\n",
      "did one more step, loss reduced by 0.00673675537109375\n",
      "did one more step, loss reduced by 0.005839347839355469\n",
      "did one more step, loss reduced by 0.0058269500732421875\n",
      "did one more step, loss reduced by 0.005825042724609375\n",
      "did one more step, loss reduced by 0.0058116912841796875\n",
      "did one more step, loss reduced by 0.005808830261230469\n",
      "did one more step, loss reduced by 0.005797386169433594\n",
      "did one more step, loss reduced by 0.005793571472167969\n",
      "did one more step, loss reduced by 0.0057811737060546875\n",
      "did one more step, loss reduced by 0.005778312683105469\n",
      "did one more step, loss reduced by 0.005766868591308594\n",
      "did one more step, loss reduced by 0.005763053894042969\n",
      "did one more step, loss reduced by 0.0066680908203125\n",
      "did one more step, loss reduced by 0.005816459655761719\n",
      "did one more step, loss reduced by 0.005805015563964844\n",
      "did one more step, loss reduced by 0.005801200866699219\n",
      "did one more step, loss reduced by 0.0057888031005859375\n",
      "did one more step, loss reduced by 0.005785942077636719\n",
      "did one more step, loss reduced by 0.005774497985839844\n",
      "did one more step, loss reduced by 0.005770683288574219\n",
      "did one more step, loss reduced by 0.0057582855224609375\n",
      "did one more step, loss reduced by 0.005756378173828125\n",
      "did one more step, loss reduced by 0.0057430267333984375\n",
      "did one more step, loss reduced by 0.005740165710449219\n",
      "did one more step, loss reduced by 0.005728721618652344\n",
      "did one more step, loss reduced by 0.006664276123046875\n",
      "did one more step, loss reduced by 0.005782127380371094\n",
      "did one more step, loss reduced by 0.005778312683105469\n",
      "did one more step, loss reduced by 0.0057659149169921875\n",
      "did one more step, loss reduced by 0.005764007568359375\n",
      "did one more step, loss reduced by 0.0057506561279296875\n",
      "did one more step, loss reduced by 0.005747795104980469\n",
      "did one more step, loss reduced by 0.005736351013183594\n",
      "did one more step, loss reduced by 0.005732536315917969\n",
      "did one more step, loss reduced by 0.0057201385498046875\n",
      "did one more step, loss reduced by 0.005717277526855469\n",
      "did one more step, loss reduced by 0.005705833435058594\n",
      "did one more step, loss reduced by 0.005702018737792969\n",
      "did one more step, loss reduced by 0.00665283203125\n",
      "did one more step, loss reduced by 0.005755424499511719\n",
      "did one more step, loss reduced by 0.005743980407714844\n",
      "did one more step, loss reduced by 0.005740165710449219\n",
      "did one more step, loss reduced by 0.0057277679443359375\n",
      "did one more step, loss reduced by 0.005724906921386719\n",
      "did one more step, loss reduced by 0.005713462829589844\n",
      "did one more step, loss reduced by 0.005709648132324219\n",
      "did one more step, loss reduced by 0.0056972503662109375\n",
      "did one more step, loss reduced by 0.005695343017578125\n",
      "did one more step, loss reduced by 0.0056819915771484375\n",
      "did one more step, loss reduced by 0.005679130554199219\n",
      "did one more step, loss reduced by 0.00658416748046875\n",
      "did one more step, loss reduced by 0.005732536315917969\n",
      "did one more step, loss reduced by 0.005721092224121094\n",
      "did one more step, loss reduced by 0.005717277526855469\n",
      "did one more step, loss reduced by 0.0057048797607421875\n",
      "did one more step, loss reduced by 0.005702972412109375\n",
      "did one more step, loss reduced by 0.0056896209716796875\n",
      "did one more step, loss reduced by 0.005686759948730469\n",
      "did one more step, loss reduced by 0.005675315856933594\n",
      "did one more step, loss reduced by 0.005671501159667969\n",
      "did one more step, loss reduced by 0.0056591033935546875\n",
      "did one more step, loss reduced by 0.005656242370605469\n",
      "did one more step, loss reduced by 0.005644798278808594\n",
      "did one more step, loss reduced by 0.006581306457519531\n",
      "did one more step, loss reduced by 0.0056972503662109375\n",
      "did one more step, loss reduced by 0.005694389343261719\n",
      "did one more step, loss reduced by 0.005682945251464844\n",
      "did one more step, loss reduced by 0.005679130554199219\n",
      "did one more step, loss reduced by 0.0056667327880859375\n",
      "did one more step, loss reduced by 0.005663871765136719\n",
      "did one more step, loss reduced by 0.005652427673339844\n",
      "did one more step, loss reduced by 0.005648612976074219\n",
      "did one more step, loss reduced by 0.0056362152099609375\n",
      "did one more step, loss reduced by 0.005634307861328125\n",
      "did one more step, loss reduced by 0.0056209564208984375\n",
      "did one more step, loss reduced by 0.005618095397949219\n",
      "did one more step, loss reduced by 0.00656890869140625\n",
      "did one more step, loss reduced by 0.005671501159667969\n",
      "did one more step, loss reduced by 0.005660057067871094\n",
      "did one more step, loss reduced by 0.005656242370605469\n",
      "did one more step, loss reduced by 0.0056438446044921875\n",
      "did one more step, loss reduced by 0.005641937255859375\n",
      "did one more step, loss reduced by 0.0056285858154296875\n",
      "did one more step, loss reduced by 0.005625724792480469\n",
      "did one more step, loss reduced by 0.005614280700683594\n",
      "did one more step, loss reduced by 0.005610466003417969\n",
      "did one more step, loss reduced by 0.0055980682373046875\n",
      "did one more step, loss reduced by 0.005595207214355469\n",
      "did one more step, loss reduced by 0.006500244140625\n",
      "did one more step, loss reduced by 0.005649566650390625\n",
      "did one more step, loss reduced by 0.0056362152099609375\n",
      "did one more step, loss reduced by 0.005633354187011719\n",
      "did one more step, loss reduced by 0.005621910095214844\n",
      "did one more step, loss reduced by 0.005618095397949219\n",
      "did one more step, loss reduced by 0.0056056976318359375\n",
      "did one more step, loss reduced by 0.005602836608886719\n",
      "did one more step, loss reduced by 0.005591392517089844\n",
      "did one more step, loss reduced by 0.005587577819824219\n",
      "did one more step, loss reduced by 0.0055751800537109375\n",
      "did one more step, loss reduced by 0.005573272705078125\n",
      "did one more step, loss reduced by 0.0055599212646484375\n",
      "did one more step, loss reduced by 0.006497383117675781\n",
      "did one more step, loss reduced by 0.0056133270263671875\n",
      "did one more step, loss reduced by 0.005610466003417969\n",
      "did one more step, loss reduced by 0.005599021911621094\n",
      "did one more step, loss reduced by 0.005595207214355469\n",
      "did one more step, loss reduced by 0.0055828094482421875\n",
      "did one more step, loss reduced by 0.005580902099609375\n",
      "did one more step, loss reduced by 0.0055675506591796875\n",
      "did one more step, loss reduced by 0.005564689636230469\n",
      "did one more step, loss reduced by 0.005553245544433594\n",
      "did one more step, loss reduced by 0.005549430847167969\n",
      "did one more step, loss reduced by 0.0055370330810546875\n",
      "did one more step, loss reduced by 0.005534172058105469\n",
      "did one more step, loss reduced by 0.0064849853515625\n",
      "did one more step, loss reduced by 0.005588531494140625\n",
      "did one more step, loss reduced by 0.0055751800537109375\n",
      "did one more step, loss reduced by 0.005572319030761719\n",
      "did one more step, loss reduced by 0.005560874938964844\n",
      "did one more step, loss reduced by 0.005557060241699219\n",
      "did one more step, loss reduced by 0.0055446624755859375\n",
      "did one more step, loss reduced by 0.005541801452636719\n",
      "did one more step, loss reduced by 0.005530357360839844\n",
      "did one more step, loss reduced by 0.005526542663574219\n",
      "did one more step, loss reduced by 0.0055141448974609375\n",
      "did one more step, loss reduced by 0.005512237548828125\n",
      "did one more step, loss reduced by 0.00641632080078125\n",
      "did one more step, loss reduced by 0.005564689636230469\n",
      "did one more step, loss reduced by 0.0055522918701171875\n",
      "did one more step, loss reduced by 0.005549430847167969\n",
      "did one more step, loss reduced by 0.005537986755371094\n",
      "did one more step, loss reduced by 0.005534172058105469\n",
      "did one more step, loss reduced by 0.0055217742919921875\n",
      "did one more step, loss reduced by 0.005519866943359375\n",
      "did one more step, loss reduced by 0.0055065155029296875\n",
      "did one more step, loss reduced by 0.005503654479980469\n",
      "did one more step, loss reduced by 0.005492210388183594\n",
      "did one more step, loss reduced by 0.005488395690917969\n",
      "did one more step, loss reduced by 0.0054759979248046875\n",
      "did one more step, loss reduced by 0.006413459777832031\n",
      "did one more step, loss reduced by 0.0055294036865234375\n",
      "did one more step, loss reduced by 0.005527496337890625\n",
      "did one more step, loss reduced by 0.0055141448974609375\n",
      "did one more step, loss reduced by 0.005511283874511719\n",
      "did one more step, loss reduced by 0.005499839782714844\n",
      "did one more step, loss reduced by 0.005496025085449219\n",
      "did one more step, loss reduced by 0.0054836273193359375\n",
      "did one more step, loss reduced by 0.005480766296386719\n",
      "did one more step, loss reduced by 0.005469322204589844\n",
      "did one more step, loss reduced by 0.005465507507324219\n",
      "did one more step, loss reduced by 0.0054531097412109375\n",
      "did one more step, loss reduced by 0.005451202392578125\n",
      "did one more step, loss reduced by 0.00640106201171875\n",
      "did one more step, loss reduced by 0.005503654479980469\n",
      "did one more step, loss reduced by 0.0054912567138671875\n",
      "did one more step, loss reduced by 0.005488395690917969\n",
      "did one more step, loss reduced by 0.005476951599121094\n",
      "did one more step, loss reduced by 0.005473136901855469\n",
      "did one more step, loss reduced by 0.0054607391357421875\n",
      "did one more step, loss reduced by 0.005458831787109375\n",
      "did one more step, loss reduced by 0.0054454803466796875\n",
      "did one more step, loss reduced by 0.005442619323730469\n",
      "did one more step, loss reduced by 0.005431175231933594\n",
      "did one more step, loss reduced by 0.005427360534667969\n",
      "did one more step, loss reduced by 0.0063323974609375\n",
      "did one more step, loss reduced by 0.005480766296386719\n",
      "did one more step, loss reduced by 0.0054683685302734375\n",
      "did one more step, loss reduced by 0.005466461181640625\n",
      "did one more step, loss reduced by 0.0054531097412109375\n",
      "did one more step, loss reduced by 0.005450248718261719\n",
      "did one more step, loss reduced by 0.005438804626464844\n",
      "did one more step, loss reduced by 0.005434989929199219\n",
      "did one more step, loss reduced by 0.0054225921630859375\n",
      "did one more step, loss reduced by 0.005419731140136719\n",
      "did one more step, loss reduced by 0.005408287048339844\n",
      "did one more step, loss reduced by 0.005404472351074219\n",
      "did one more step, loss reduced by 0.0053920745849609375\n",
      "did one more step, loss reduced by 0.006329536437988281\n",
      "did one more step, loss reduced by 0.005446434020996094\n",
      "did one more step, loss reduced by 0.005442619323730469\n",
      "did one more step, loss reduced by 0.0054302215576171875\n",
      "did one more step, loss reduced by 0.005427360534667969\n",
      "did one more step, loss reduced by 0.005415916442871094\n",
      "did one more step, loss reduced by 0.005412101745605469\n",
      "did one more step, loss reduced by 0.0053997039794921875\n",
      "did one more step, loss reduced by 0.005397796630859375\n",
      "did one more step, loss reduced by 0.0053844451904296875\n",
      "did one more step, loss reduced by 0.005381584167480469\n",
      "did one more step, loss reduced by 0.005370140075683594\n",
      "did one more step, loss reduced by 0.005366325378417969\n",
      "did one more step, loss reduced by 0.006317138671875\n",
      "did one more step, loss reduced by 0.005419731140136719\n",
      "did one more step, loss reduced by 0.0054073333740234375\n",
      "did one more step, loss reduced by 0.005405426025390625\n",
      "did one more step, loss reduced by 0.0053920745849609375\n",
      "did one more step, loss reduced by 0.005389213562011719\n",
      "did one more step, loss reduced by 0.005377769470214844\n",
      "did one more step, loss reduced by 0.005373954772949219\n",
      "did one more step, loss reduced by 0.0053615570068359375\n",
      "did one more step, loss reduced by 0.005358695983886719\n",
      "did one more step, loss reduced by 0.005347251892089844\n",
      "did one more step, loss reduced by 0.005343437194824219\n",
      "did one more step, loss reduced by 0.00624847412109375\n",
      "did one more step, loss reduced by 0.005396842956542969\n",
      "did one more step, loss reduced by 0.005385398864746094\n",
      "did one more step, loss reduced by 0.005381584167480469\n",
      "did one more step, loss reduced by 0.0053691864013671875\n",
      "did one more step, loss reduced by 0.005366325378417969\n",
      "did one more step, loss reduced by 0.005354881286621094\n",
      "did one more step, loss reduced by 0.005351066589355469\n",
      "did one more step, loss reduced by 0.0053386688232421875\n",
      "did one more step, loss reduced by 0.005336761474609375\n",
      "did one more step, loss reduced by 0.0053234100341796875\n",
      "did one more step, loss reduced by 0.005320549011230469\n",
      "did one more step, loss reduced by 0.005309104919433594\n",
      "did one more step, loss reduced by 0.006244659423828125\n",
      "did one more step, loss reduced by 0.005362510681152344\n",
      "did one more step, loss reduced by 0.005358695983886719\n",
      "did one more step, loss reduced by 0.0053462982177734375\n",
      "did one more step, loss reduced by 0.005344390869140625\n",
      "did one more step, loss reduced by 0.0053310394287109375\n",
      "did one more step, loss reduced by 0.005328178405761719\n",
      "did one more step, loss reduced by 0.005316734313964844\n",
      "did one more step, loss reduced by 0.005312919616699219\n",
      "did one more step, loss reduced by 0.0053005218505859375\n",
      "did one more step, loss reduced by 0.005297660827636719\n",
      "did one more step, loss reduced by 0.005286216735839844\n",
      "did one more step, loss reduced by 0.005282402038574219\n",
      "did one more step, loss reduced by 0.00623321533203125\n",
      "did one more step, loss reduced by 0.005335807800292969\n",
      "did one more step, loss reduced by 0.005324363708496094\n",
      "did one more step, loss reduced by 0.005320549011230469\n",
      "did one more step, loss reduced by 0.0053081512451171875\n",
      "did one more step, loss reduced by 0.005305290222167969\n",
      "did one more step, loss reduced by 0.005293846130371094\n",
      "did one more step, loss reduced by 0.005290031433105469\n",
      "did one more step, loss reduced by 0.0052776336669921875\n",
      "did one more step, loss reduced by 0.005275726318359375\n",
      "did one more step, loss reduced by 0.0052623748779296875\n",
      "did one more step, loss reduced by 0.005259513854980469\n",
      "did one more step, loss reduced by 0.00616455078125\n",
      "did one more step, loss reduced by 0.005312919616699219\n",
      "did one more step, loss reduced by 0.005301475524902344\n",
      "did one more step, loss reduced by 0.005297660827636719\n",
      "did one more step, loss reduced by 0.0052852630615234375\n",
      "did one more step, loss reduced by 0.005283355712890625\n",
      "did one more step, loss reduced by 0.0052700042724609375\n",
      "did one more step, loss reduced by 0.005267143249511719\n",
      "did one more step, loss reduced by 0.005255699157714844\n",
      "did one more step, loss reduced by 0.005251884460449219\n",
      "did one more step, loss reduced by 0.0052394866943359375\n",
      "did one more step, loss reduced by 0.005236625671386719\n",
      "did one more step, loss reduced by 0.005225181579589844\n",
      "did one more step, loss reduced by 0.006161689758300781\n",
      "did one more step, loss reduced by 0.0052776336669921875\n",
      "did one more step, loss reduced by 0.005274772644042969\n",
      "did one more step, loss reduced by 0.005263328552246094\n",
      "did one more step, loss reduced by 0.005259513854980469\n",
      "did one more step, loss reduced by 0.0052471160888671875\n",
      "did one more step, loss reduced by 0.005244255065917969\n",
      "did one more step, loss reduced by 0.005232810974121094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.005228996276855469\n",
      "did one more step, loss reduced by 0.0052165985107421875\n",
      "did one more step, loss reduced by 0.005214691162109375\n",
      "did one more step, loss reduced by 0.0052013397216796875\n",
      "did one more step, loss reduced by 0.005198478698730469\n",
      "did one more step, loss reduced by 0.0061492919921875\n",
      "did one more step, loss reduced by 0.005251884460449219\n",
      "did one more step, loss reduced by 0.005240440368652344\n",
      "did one more step, loss reduced by 0.005236625671386719\n",
      "did one more step, loss reduced by 0.0052242279052734375\n",
      "did one more step, loss reduced by 0.005222320556640625\n",
      "did one more step, loss reduced by 0.0052089691162109375\n",
      "did one more step, loss reduced by 0.005206108093261719\n",
      "did one more step, loss reduced by 0.005194664001464844\n",
      "did one more step, loss reduced by 0.005190849304199219\n",
      "did one more step, loss reduced by 0.0051784515380859375\n",
      "did one more step, loss reduced by 0.005175590515136719\n",
      "did one more step, loss reduced by 0.00608062744140625\n",
      "did one more step, loss reduced by 0.005229949951171875\n",
      "did one more step, loss reduced by 0.0052165985107421875\n",
      "did one more step, loss reduced by 0.005213737487792969\n",
      "did one more step, loss reduced by 0.005202293395996094\n",
      "did one more step, loss reduced by 0.005198478698730469\n",
      "did one more step, loss reduced by 0.0051860809326171875\n",
      "did one more step, loss reduced by 0.005183219909667969\n",
      "did one more step, loss reduced by 0.005171775817871094\n",
      "did one more step, loss reduced by 0.005167961120605469\n",
      "did one more step, loss reduced by 0.0051555633544921875\n",
      "did one more step, loss reduced by 0.005153656005859375\n",
      "did one more step, loss reduced by 0.0051403045654296875\n",
      "did one more step, loss reduced by 0.006077766418457031\n",
      "did one more step, loss reduced by 0.0051937103271484375\n",
      "did one more step, loss reduced by 0.005190849304199219\n",
      "did one more step, loss reduced by 0.005179405212402344\n",
      "did one more step, loss reduced by 0.005175590515136719\n",
      "did one more step, loss reduced by 0.0051631927490234375\n",
      "did one more step, loss reduced by 0.005161285400390625\n",
      "did one more step, loss reduced by 0.0051479339599609375\n",
      "did one more step, loss reduced by 0.005145072937011719\n",
      "did one more step, loss reduced by 0.005133628845214844\n",
      "did one more step, loss reduced by 0.005129814147949219\n",
      "did one more step, loss reduced by 0.0051174163818359375\n",
      "did one more step, loss reduced by 0.005114555358886719\n",
      "did one more step, loss reduced by 0.00606536865234375\n",
      "did one more step, loss reduced by 0.005168914794921875\n",
      "did one more step, loss reduced by 0.0051555633544921875\n",
      "did one more step, loss reduced by 0.005152702331542969\n",
      "did one more step, loss reduced by 0.005141258239746094\n",
      "did one more step, loss reduced by 0.005137443542480469\n",
      "did one more step, loss reduced by 0.0051250457763671875\n",
      "did one more step, loss reduced by 0.005122184753417969\n",
      "did one more step, loss reduced by 0.005110740661621094\n",
      "did one more step, loss reduced by 0.005106925964355469\n",
      "did one more step, loss reduced by 0.0050945281982421875\n",
      "did one more step, loss reduced by 0.005092620849609375\n",
      "did one more step, loss reduced by 0.0059967041015625\n",
      "did one more step, loss reduced by 0.005145072937011719\n",
      "did one more step, loss reduced by 0.0051326751708984375\n",
      "did one more step, loss reduced by 0.005129814147949219\n",
      "did one more step, loss reduced by 0.005118370056152344\n",
      "did one more step, loss reduced by 0.005114555358886719\n",
      "did one more step, loss reduced by 0.0051021575927734375\n",
      "did one more step, loss reduced by 0.005100250244140625\n",
      "did one more step, loss reduced by 0.0050868988037109375\n",
      "did one more step, loss reduced by 0.005084037780761719\n",
      "did one more step, loss reduced by 0.005072593688964844\n",
      "did one more step, loss reduced by 0.005068778991699219\n",
      "did one more step, loss reduced by 0.0050563812255859375\n",
      "did one more step, loss reduced by 0.005993843078613281\n",
      "did one more step, loss reduced by 0.0051097869873046875\n",
      "did one more step, loss reduced by 0.005107879638671875\n",
      "did one more step, loss reduced by 0.0050945281982421875\n",
      "did one more step, loss reduced by 0.005091667175292969\n",
      "did one more step, loss reduced by 0.005080223083496094\n",
      "did one more step, loss reduced by 0.005076408386230469\n",
      "did one more step, loss reduced by 0.0050640106201171875\n",
      "did one more step, loss reduced by 0.005061149597167969\n",
      "did one more step, loss reduced by 0.005049705505371094\n",
      "did one more step, loss reduced by 0.005045890808105469\n",
      "did one more step, loss reduced by 0.0050334930419921875\n",
      "did one more step, loss reduced by 0.005031585693359375\n",
      "did one more step, loss reduced by 0.0059814453125\n",
      "did one more step, loss reduced by 0.005084037780761719\n",
      "did one more step, loss reduced by 0.0050716400146484375\n",
      "did one more step, loss reduced by 0.005068778991699219\n",
      "did one more step, loss reduced by 0.005057334899902344\n",
      "did one more step, loss reduced by 0.005053520202636719\n",
      "did one more step, loss reduced by 0.0050411224365234375\n",
      "did one more step, loss reduced by 0.005039215087890625\n",
      "did one more step, loss reduced by 0.0050258636474609375\n",
      "did one more step, loss reduced by 0.005023002624511719\n",
      "did one more step, loss reduced by 0.005011558532714844\n",
      "did one more step, loss reduced by 0.005007743835449219\n",
      "did one more step, loss reduced by 0.00591278076171875\n",
      "did one more step, loss reduced by 0.005061149597167969\n",
      "did one more step, loss reduced by 0.0050487518310546875\n",
      "did one more step, loss reduced by 0.005046844482421875\n",
      "did one more step, loss reduced by 0.0050334930419921875\n",
      "did one more step, loss reduced by 0.005030632019042969\n",
      "did one more step, loss reduced by 0.005019187927246094\n",
      "did one more step, loss reduced by 0.005015373229980469\n",
      "did one more step, loss reduced by 0.0050029754638671875\n",
      "did one more step, loss reduced by 0.005000114440917969\n",
      "did one more step, loss reduced by 0.004988670349121094\n",
      "did one more step, loss reduced by 0.004984855651855469\n",
      "did one more step, loss reduced by 0.0049724578857421875\n",
      "did one more step, loss reduced by 0.005909919738769531\n",
      "did one more step, loss reduced by 0.005026817321777344\n",
      "did one more step, loss reduced by 0.005023002624511719\n",
      "did one more step, loss reduced by 0.0050106048583984375\n",
      "did one more step, loss reduced by 0.005007743835449219\n",
      "did one more step, loss reduced by 0.004996299743652344\n",
      "did one more step, loss reduced by 0.004992485046386719\n",
      "did one more step, loss reduced by 0.0049800872802734375\n",
      "did one more step, loss reduced by 0.004978179931640625\n",
      "did one more step, loss reduced by 0.0049648284912109375\n",
      "did one more step, loss reduced by 0.004961967468261719\n",
      "did one more step, loss reduced by 0.004950523376464844\n",
      "did one more step, loss reduced by 0.004946708679199219\n",
      "did one more step, loss reduced by 0.00589752197265625\n",
      "did one more step, loss reduced by 0.005000114440917969\n",
      "did one more step, loss reduced by 0.0049877166748046875\n",
      "did one more step, loss reduced by 0.004985809326171875\n",
      "did one more step, loss reduced by 0.0049724578857421875\n",
      "did one more step, loss reduced by 0.004969596862792969\n",
      "did one more step, loss reduced by 0.004958152770996094\n",
      "did one more step, loss reduced by 0.004954338073730469\n",
      "did one more step, loss reduced by 0.0049419403076171875\n",
      "did one more step, loss reduced by 0.004939079284667969\n",
      "did one more step, loss reduced by 0.004927635192871094\n",
      "did one more step, loss reduced by 0.004923820495605469\n",
      "did one more step, loss reduced by 0.005828857421875\n",
      "did one more step, loss reduced by 0.004977226257324219\n",
      "did one more step, loss reduced by 0.004965782165527344\n",
      "did one more step, loss reduced by 0.004961967468261719\n",
      "did one more step, loss reduced by 0.0049495697021484375\n",
      "did one more step, loss reduced by 0.004946708679199219\n",
      "did one more step, loss reduced by 0.004935264587402344\n",
      "did one more step, loss reduced by 0.004931449890136719\n",
      "did one more step, loss reduced by 0.0049190521240234375\n",
      "did one more step, loss reduced by 0.004917144775390625\n",
      "did one more step, loss reduced by 0.0049037933349609375\n",
      "did one more step, loss reduced by 0.004900932312011719\n",
      "did one more step, loss reduced by 0.004889488220214844\n",
      "did one more step, loss reduced by 0.005825042724609375\n",
      "did one more step, loss reduced by 0.004942893981933594\n",
      "did one more step, loss reduced by 0.004939079284667969\n",
      "did one more step, loss reduced by 0.0049266815185546875\n",
      "did one more step, loss reduced by 0.004924774169921875\n",
      "did one more step, loss reduced by 0.0049114227294921875\n",
      "did one more step, loss reduced by 0.004908561706542969\n",
      "did one more step, loss reduced by 0.004897117614746094\n",
      "did one more step, loss reduced by 0.004893302917480469\n",
      "did one more step, loss reduced by 0.0048809051513671875\n",
      "did one more step, loss reduced by 0.004878044128417969\n",
      "did one more step, loss reduced by 0.004866600036621094\n",
      "did one more step, loss reduced by 0.004862785339355469\n",
      "did one more step, loss reduced by 0.0058135986328125\n",
      "did one more step, loss reduced by 0.004916191101074219\n",
      "did one more step, loss reduced by 0.004904747009277344\n",
      "did one more step, loss reduced by 0.004900932312011719\n",
      "did one more step, loss reduced by 0.0048885345458984375\n",
      "did one more step, loss reduced by 0.004885673522949219\n",
      "did one more step, loss reduced by 0.004874229431152344\n",
      "did one more step, loss reduced by 0.004870414733886719\n",
      "did one more step, loss reduced by 0.0048580169677734375\n",
      "did one more step, loss reduced by 0.004856109619140625\n",
      "did one more step, loss reduced by 0.0048427581787109375\n",
      "did one more step, loss reduced by 0.004839897155761719\n",
      "did one more step, loss reduced by 0.00574493408203125\n",
      "did one more step, loss reduced by 0.004893302917480469\n",
      "did one more step, loss reduced by 0.004881858825683594\n",
      "did one more step, loss reduced by 0.004878044128417969\n",
      "did one more step, loss reduced by 0.0048656463623046875\n",
      "did one more step, loss reduced by 0.004863739013671875\n",
      "did one more step, loss reduced by 0.0048503875732421875\n",
      "did one more step, loss reduced by 0.004847526550292969\n",
      "did one more step, loss reduced by 0.004836082458496094\n",
      "did one more step, loss reduced by 0.004832267761230469\n",
      "did one more step, loss reduced by 0.0048198699951171875\n",
      "did one more step, loss reduced by 0.004817008972167969\n",
      "did one more step, loss reduced by 0.004805564880371094\n",
      "did one more step, loss reduced by 0.005742073059082031\n",
      "did one more step, loss reduced by 0.0048580169677734375\n",
      "did one more step, loss reduced by 0.004855155944824219\n",
      "did one more step, loss reduced by 0.004843711853027344\n",
      "did one more step, loss reduced by 0.004839897155761719\n",
      "did one more step, loss reduced by 0.0048274993896484375\n",
      "did one more step, loss reduced by 0.004824638366699219\n",
      "did one more step, loss reduced by 0.004813194274902344\n",
      "did one more step, loss reduced by 0.004809379577636719\n",
      "did one more step, loss reduced by 0.0047969818115234375\n",
      "did one more step, loss reduced by 0.004795074462890625\n",
      "did one more step, loss reduced by 0.0047817230224609375\n",
      "did one more step, loss reduced by 0.004778861999511719\n",
      "did one more step, loss reduced by 0.00572967529296875\n",
      "did one more step, loss reduced by 0.004832267761230469\n",
      "did one more step, loss reduced by 0.004820823669433594\n",
      "did one more step, loss reduced by 0.004817008972167969\n",
      "did one more step, loss reduced by 0.0048046112060546875\n",
      "did one more step, loss reduced by 0.004802703857421875\n",
      "did one more step, loss reduced by 0.0047893524169921875\n",
      "did one more step, loss reduced by 0.004786491394042969\n",
      "did one more step, loss reduced by 0.004775047302246094\n",
      "did one more step, loss reduced by 0.004771232604980469\n",
      "did one more step, loss reduced by 0.0047588348388671875\n",
      "did one more step, loss reduced by 0.004755973815917969\n",
      "did one more step, loss reduced by 0.0056610107421875\n",
      "did one more step, loss reduced by 0.004810333251953125\n",
      "did one more step, loss reduced by 0.0047969818115234375\n",
      "did one more step, loss reduced by 0.004794120788574219\n",
      "did one more step, loss reduced by 0.004782676696777344\n",
      "did one more step, loss reduced by 0.004778861999511719\n",
      "did one more step, loss reduced by 0.0047664642333984375\n",
      "did one more step, loss reduced by 0.004763603210449219\n",
      "did one more step, loss reduced by 0.004752159118652344\n",
      "did one more step, loss reduced by 0.004748344421386719\n",
      "did one more step, loss reduced by 0.0047359466552734375\n",
      "did one more step, loss reduced by 0.004734039306640625\n",
      "did one more step, loss reduced by 0.0047206878662109375\n",
      "did one more step, loss reduced by 0.005658149719238281\n",
      "did one more step, loss reduced by 0.0047740936279296875\n",
      "did one more step, loss reduced by 0.004771232604980469\n",
      "did one more step, loss reduced by 0.004759788513183594\n",
      "did one more step, loss reduced by 0.004755973815917969\n",
      "did one more step, loss reduced by 0.0047435760498046875\n",
      "did one more step, loss reduced by 0.004741668701171875\n",
      "did one more step, loss reduced by 0.0047283172607421875\n",
      "did one more step, loss reduced by 0.004725456237792969\n",
      "did one more step, loss reduced by 0.004714012145996094\n",
      "did one more step, loss reduced by 0.004710197448730469\n",
      "did one more step, loss reduced by 0.0046977996826171875\n",
      "did one more step, loss reduced by 0.004694938659667969\n",
      "did one more step, loss reduced by 0.005645751953125\n",
      "did one more step, loss reduced by 0.004749298095703125\n",
      "did one more step, loss reduced by 0.0047359466552734375\n",
      "did one more step, loss reduced by 0.004733085632324219\n",
      "did one more step, loss reduced by 0.004721641540527344\n",
      "did one more step, loss reduced by 0.004717826843261719\n",
      "did one more step, loss reduced by 0.0047054290771484375\n",
      "did one more step, loss reduced by 0.004702568054199219\n",
      "did one more step, loss reduced by 0.004691123962402344\n",
      "did one more step, loss reduced by 0.004687309265136719\n",
      "did one more step, loss reduced by 0.0046749114990234375\n",
      "did one more step, loss reduced by 0.004673004150390625\n",
      "did one more step, loss reduced by 0.00557708740234375\n",
      "did one more step, loss reduced by 0.004725456237792969\n",
      "did one more step, loss reduced by 0.0047130584716796875\n",
      "did one more step, loss reduced by 0.004710197448730469\n",
      "did one more step, loss reduced by 0.004698753356933594\n",
      "did one more step, loss reduced by 0.004694938659667969\n",
      "did one more step, loss reduced by 0.0046825408935546875\n",
      "did one more step, loss reduced by 0.004680633544921875\n",
      "did one more step, loss reduced by 0.0046672821044921875\n",
      "did one more step, loss reduced by 0.004664421081542969\n",
      "did one more step, loss reduced by 0.004652976989746094\n",
      "did one more step, loss reduced by 0.004649162292480469\n",
      "did one more step, loss reduced by 0.0046367645263671875\n",
      "did one more step, loss reduced by 0.005574226379394531\n",
      "did one more step, loss reduced by 0.0046901702880859375\n",
      "did one more step, loss reduced by 0.004688262939453125\n",
      "did one more step, loss reduced by 0.0046749114990234375\n",
      "did one more step, loss reduced by 0.004672050476074219\n",
      "did one more step, loss reduced by 0.004660606384277344\n",
      "did one more step, loss reduced by 0.004656791687011719\n",
      "did one more step, loss reduced by 0.0046443939208984375\n",
      "did one more step, loss reduced by 0.004641532897949219\n",
      "did one more step, loss reduced by 0.004630088806152344\n",
      "did one more step, loss reduced by 0.004626274108886719\n",
      "did one more step, loss reduced by 0.0046138763427734375\n",
      "did one more step, loss reduced by 0.004611968994140625\n",
      "did one more step, loss reduced by 0.00556182861328125\n",
      "did one more step, loss reduced by 0.004664421081542969\n",
      "did one more step, loss reduced by 0.0046520233154296875\n",
      "did one more step, loss reduced by 0.004649162292480469\n",
      "did one more step, loss reduced by 0.004637718200683594\n",
      "did one more step, loss reduced by 0.004633903503417969\n",
      "did one more step, loss reduced by 0.0046215057373046875\n",
      "did one more step, loss reduced by 0.004619598388671875\n",
      "did one more step, loss reduced by 0.0046062469482421875\n",
      "did one more step, loss reduced by 0.004603385925292969\n",
      "did one more step, loss reduced by 0.004591941833496094\n",
      "did one more step, loss reduced by 0.004588127136230469\n",
      "did one more step, loss reduced by 0.0054931640625\n",
      "did one more step, loss reduced by 0.004641532897949219\n",
      "did one more step, loss reduced by 0.0046291351318359375\n",
      "did one more step, loss reduced by 0.004627227783203125\n",
      "did one more step, loss reduced by 0.0046138763427734375\n",
      "did one more step, loss reduced by 0.004611015319824219\n",
      "did one more step, loss reduced by 0.004599571228027344\n",
      "did one more step, loss reduced by 0.004595756530761719\n",
      "did one more step, loss reduced by 0.0045833587646484375\n",
      "did one more step, loss reduced by 0.004580497741699219\n",
      "did one more step, loss reduced by 0.004569053649902344\n",
      "did one more step, loss reduced by 0.004565238952636719\n",
      "did one more step, loss reduced by 0.0045528411865234375\n",
      "did one more step, loss reduced by 0.005490303039550781\n",
      "did one more step, loss reduced by 0.004607200622558594\n",
      "did one more step, loss reduced by 0.004603385925292969\n",
      "did one more step, loss reduced by 0.0045909881591796875\n",
      "did one more step, loss reduced by 0.004588127136230469\n",
      "did one more step, loss reduced by 0.004576683044433594\n",
      "did one more step, loss reduced by 0.004572868347167969\n",
      "did one more step, loss reduced by 0.0045604705810546875\n",
      "did one more step, loss reduced by 0.004558563232421875\n",
      "did one more step, loss reduced by 0.0045452117919921875\n",
      "did one more step, loss reduced by 0.004542350769042969\n",
      "did one more step, loss reduced by 0.004530906677246094\n",
      "did one more step, loss reduced by 0.004527091979980469\n",
      "did one more step, loss reduced by 0.0054779052734375\n",
      "did one more step, loss reduced by 0.004580497741699219\n",
      "did one more step, loss reduced by 0.0045680999755859375\n",
      "did one more step, loss reduced by 0.004566192626953125\n",
      "did one more step, loss reduced by 0.0045528411865234375\n",
      "did one more step, loss reduced by 0.004549980163574219\n",
      "did one more step, loss reduced by 0.004538536071777344\n",
      "did one more step, loss reduced by 0.004534721374511719\n",
      "did one more step, loss reduced by 0.0045223236083984375\n",
      "did one more step, loss reduced by 0.004519462585449219\n",
      "did one more step, loss reduced by 0.004508018493652344\n",
      "did one more step, loss reduced by 0.004504203796386719\n",
      "did one more step, loss reduced by 0.00540924072265625\n",
      "did one more step, loss reduced by 0.004557609558105469\n",
      "did one more step, loss reduced by 0.004546165466308594\n",
      "did one more step, loss reduced by 0.004542350769042969\n",
      "did one more step, loss reduced by 0.0045299530029296875\n",
      "did one more step, loss reduced by 0.004527091979980469\n",
      "did one more step, loss reduced by 0.004515647888183594\n",
      "did one more step, loss reduced by 0.004511833190917969\n",
      "did one more step, loss reduced by 0.0044994354248046875\n",
      "did one more step, loss reduced by 0.004497528076171875\n",
      "did one more step, loss reduced by 0.0044841766357421875\n",
      "did one more step, loss reduced by 0.004481315612792969\n",
      "did one more step, loss reduced by 0.004469871520996094\n",
      "did one more step, loss reduced by 0.005405426025390625\n",
      "did one more step, loss reduced by 0.004523277282714844\n",
      "did one more step, loss reduced by 0.004519462585449219\n",
      "did one more step, loss reduced by 0.0045070648193359375\n",
      "did one more step, loss reduced by 0.004505157470703125\n",
      "did one more step, loss reduced by 0.0044918060302734375\n",
      "did one more step, loss reduced by 0.004488945007324219\n",
      "did one more step, loss reduced by 0.004477500915527344\n",
      "did one more step, loss reduced by 0.004473686218261719\n",
      "did one more step, loss reduced by 0.0044612884521484375\n",
      "did one more step, loss reduced by 0.004458427429199219\n",
      "did one more step, loss reduced by 0.004446983337402344\n",
      "did one more step, loss reduced by 0.004443168640136719\n",
      "did one more step, loss reduced by 0.00539398193359375\n",
      "did one more step, loss reduced by 0.004496574401855469\n",
      "did one more step, loss reduced by 0.004485130310058594\n",
      "did one more step, loss reduced by 0.004481315612792969\n",
      "did one more step, loss reduced by 0.0044689178466796875\n",
      "did one more step, loss reduced by 0.004466056823730469\n",
      "did one more step, loss reduced by 0.004454612731933594\n",
      "did one more step, loss reduced by 0.004450798034667969\n",
      "did one more step, loss reduced by 0.0044384002685546875\n",
      "did one more step, loss reduced by 0.004436492919921875\n",
      "did one more step, loss reduced by 0.0044231414794921875\n",
      "did one more step, loss reduced by 0.004420280456542969\n",
      "did one more step, loss reduced by 0.0053253173828125\n",
      "did one more step, loss reduced by 0.004473686218261719\n",
      "did one more step, loss reduced by 0.004462242126464844\n",
      "did one more step, loss reduced by 0.004458427429199219\n",
      "did one more step, loss reduced by 0.0044460296630859375\n",
      "did one more step, loss reduced by 0.004444122314453125\n",
      "did one more step, loss reduced by 0.0044307708740234375\n",
      "did one more step, loss reduced by 0.004427909851074219\n",
      "did one more step, loss reduced by 0.004416465759277344\n",
      "did one more step, loss reduced by 0.004412651062011719\n",
      "did one more step, loss reduced by 0.0044002532958984375\n",
      "did one more step, loss reduced by 0.004397392272949219\n",
      "did one more step, loss reduced by 0.004385948181152344\n",
      "did one more step, loss reduced by 0.005322456359863281\n",
      "did one more step, loss reduced by 0.0044384002685546875\n",
      "did one more step, loss reduced by 0.004435539245605469\n",
      "did one more step, loss reduced by 0.004424095153808594\n",
      "did one more step, loss reduced by 0.004420280456542969\n",
      "did one more step, loss reduced by 0.0044078826904296875\n",
      "did one more step, loss reduced by 0.004405021667480469\n",
      "did one more step, loss reduced by 0.004393577575683594\n",
      "did one more step, loss reduced by 0.004389762878417969\n",
      "did one more step, loss reduced by 0.0043773651123046875\n",
      "did one more step, loss reduced by 0.004375457763671875\n",
      "did one more step, loss reduced by 0.0043621063232421875\n",
      "did one more step, loss reduced by 0.0043582916259765625\n",
      "did one more step, loss reduced by 0.005311012268066406\n",
      "did one more step, loss reduced by 0.0044116973876953125\n",
      "did one more step, loss reduced by 0.00440216064453125\n",
      "did one more step, loss reduced by 0.004395484924316406\n",
      "did one more step, loss reduced by 0.00438690185546875\n",
      "did one more step, loss reduced by 0.0043811798095703125\n",
      "did one more step, loss reduced by 0.00437164306640625\n",
      "did one more step, loss reduced by 0.0043659210205078125\n",
      "did one more step, loss reduced by 0.00435638427734375\n",
      "did one more step, loss reduced by 0.004349708557128906\n",
      "did one more step, loss reduced by 0.00434112548828125\n",
      "did one more step, loss reduced by 0.0043354034423828125\n",
      "did one more step, loss reduced by 0.005242347717285156\n",
      "did one more step, loss reduced by 0.0043888092041015625\n",
      "did one more step, loss reduced by 0.0043792724609375\n",
      "did one more step, loss reduced by 0.0043735504150390625\n",
      "did one more step, loss reduced by 0.004364013671875\n",
      "did one more step, loss reduced by 0.004357337951660156\n",
      "did one more step, loss reduced by 0.0043487548828125\n",
      "did one more step, loss reduced by 0.0043430328369140625\n",
      "did one more step, loss reduced by 0.00433349609375\n",
      "did one more step, loss reduced by 0.004326820373535156\n",
      "did one more step, loss reduced by 0.0043182373046875\n",
      "did one more step, loss reduced by 0.0043125152587890625\n",
      "did one more step, loss reduced by 0.004302978515625\n",
      "did one more step, loss reduced by 0.005236625671386719\n",
      "did one more step, loss reduced by 0.00435638427734375\n",
      "did one more step, loss reduced by 0.0043506622314453125\n",
      "did one more step, loss reduced by 0.00434112548828125\n",
      "did one more step, loss reduced by 0.004334449768066406\n",
      "did one more step, loss reduced by 0.00432586669921875\n",
      "did one more step, loss reduced by 0.0043201446533203125\n",
      "did one more step, loss reduced by 0.00431060791015625\n",
      "did one more step, loss reduced by 0.0043048858642578125\n",
      "did one more step, loss reduced by 0.00429534912109375\n",
      "did one more step, loss reduced by 0.004288673400878906\n",
      "did one more step, loss reduced by 0.00428009033203125\n",
      "did one more step, loss reduced by 0.0042743682861328125\n",
      "did one more step, loss reduced by 0.005227088928222656\n",
      "did one more step, loss reduced by 0.0043277740478515625\n",
      "did one more step, loss reduced by 0.0043182373046875\n",
      "did one more step, loss reduced by 0.0043125152587890625\n",
      "did one more step, loss reduced by 0.004302978515625\n",
      "did one more step, loss reduced by 0.004296302795410156\n",
      "did one more step, loss reduced by 0.0042877197265625\n",
      "did one more step, loss reduced by 0.0042819976806640625\n",
      "did one more step, loss reduced by 0.0042724609375\n",
      "did one more step, loss reduced by 0.004265785217285156\n",
      "did one more step, loss reduced by 0.0042572021484375\n",
      "did one more step, loss reduced by 0.0042514801025390625\n",
      "did one more step, loss reduced by 0.0051593780517578125\n",
      "did one more step, loss reduced by 0.004303932189941406\n",
      "did one more step, loss reduced by 0.00429534912109375\n",
      "did one more step, loss reduced by 0.0042896270751953125\n",
      "did one more step, loss reduced by 0.00428009033203125\n",
      "did one more step, loss reduced by 0.004273414611816406\n",
      "did one more step, loss reduced by 0.00426483154296875\n",
      "did one more step, loss reduced by 0.0042591094970703125\n",
      "did one more step, loss reduced by 0.00424957275390625\n",
      "did one more step, loss reduced by 0.0042438507080078125\n",
      "did one more step, loss reduced by 0.00423431396484375\n",
      "did one more step, loss reduced by 0.004227638244628906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.00421905517578125\n",
      "did one more step, loss reduced by 0.005152702331542969\n",
      "did one more step, loss reduced by 0.0042724609375\n",
      "did one more step, loss reduced by 0.0042667388916015625\n",
      "did one more step, loss reduced by 0.0042572021484375\n",
      "did one more step, loss reduced by 0.0042514801025390625\n",
      "did one more step, loss reduced by 0.004241943359375\n",
      "did one more step, loss reduced by 0.004235267639160156\n",
      "did one more step, loss reduced by 0.0042266845703125\n",
      "did one more step, loss reduced by 0.0042209625244140625\n",
      "did one more step, loss reduced by 0.00421142578125\n",
      "did one more step, loss reduced by 0.004204750061035156\n",
      "did one more step, loss reduced by 0.0041961669921875\n",
      "did one more step, loss reduced by 0.0041904449462890625\n",
      "did one more step, loss reduced by 0.0051441192626953125\n",
      "did one more step, loss reduced by 0.004242897033691406\n",
      "did one more step, loss reduced by 0.00423431396484375\n",
      "did one more step, loss reduced by 0.0042285919189453125\n",
      "did one more step, loss reduced by 0.00421905517578125\n",
      "did one more step, loss reduced by 0.004212379455566406\n",
      "did one more step, loss reduced by 0.00420379638671875\n",
      "did one more step, loss reduced by 0.0041980743408203125\n",
      "did one more step, loss reduced by 0.00418853759765625\n",
      "did one more step, loss reduced by 0.0041828155517578125\n",
      "did one more step, loss reduced by 0.00417327880859375\n",
      "did one more step, loss reduced by 0.004166603088378906\n",
      "did one more step, loss reduced by 0.0050754547119140625\n",
      "did one more step, loss reduced by 0.004220008850097656\n",
      "did one more step, loss reduced by 0.00421142578125\n",
      "did one more step, loss reduced by 0.0042057037353515625\n",
      "did one more step, loss reduced by 0.0041961669921875\n",
      "did one more step, loss reduced by 0.0041904449462890625\n",
      "did one more step, loss reduced by 0.004180908203125\n",
      "did one more step, loss reduced by 0.004174232482910156\n",
      "did one more step, loss reduced by 0.0041656494140625\n",
      "did one more step, loss reduced by 0.0041599273681640625\n",
      "did one more step, loss reduced by 0.004150390625\n",
      "did one more step, loss reduced by 0.004143714904785156\n",
      "did one more step, loss reduced by 0.0041351318359375\n",
      "did one more step, loss reduced by 0.0008716583251953125\n",
      "did one more step, loss reduced by 0.0\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#now stopping criteria is difference between errors  \n",
    "learning_rait = 0.0006 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "y_pred = predict(w, x)\n",
    "previous_error = mseerror(y, y_pred)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "error = mseerror(y, y_pred)\n",
    "y_pred = predict(w, x)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "previous_error = error\n",
    "error = mseerror(y, y_pred)\n",
    "while previous_error - error > 0.0000000005:\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    previous_error = error \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'did one more step, loss reduced by {previous_error - error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad64464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative. Slope = 5.60546875, intercept = 4.0, loss = 10.619758605957031')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688ca691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1131.9000244140625, weights = tensor([0.2133, 0.0363], dtype=torch.float16, requires_grad=True)\n",
      "step №1: loss = 1055.273193359375, weights = tensor([0.4189, 0.0715], dtype=torch.float16, requires_grad=True)\n",
      "step №2: loss = 983.9490356445312, weights = tensor([0.6177, 0.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №3: loss = 917.4542236328125, weights = tensor([0.8091, 0.1384], dtype=torch.float16, requires_grad=True)\n",
      "step №4: loss = 855.6471557617188, weights = tensor([0.9941, 0.1702], dtype=torch.float16, requires_grad=True)\n",
      "step №5: loss = 797.9888916015625, weights = tensor([1.1729, 0.2009], dtype=torch.float16, requires_grad=True)\n",
      "step №6: loss = 744.2576904296875, weights = tensor([1.3447, 0.2307], dtype=torch.float16, requires_grad=True)\n",
      "step №7: loss = 694.3846435546875, weights = tensor([1.5107, 0.2595], dtype=torch.float16, requires_grad=True)\n",
      "step №8: loss = 647.8980102539062, weights = tensor([1.6709, 0.2874], dtype=torch.float16, requires_grad=True)\n",
      "step №9: loss = 604.6220703125, weights = tensor([1.8252, 0.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №10: loss = 564.377197265625, weights = tensor([1.9746, 0.3406], dtype=torch.float16, requires_grad=True)\n",
      "step №11: loss = 526.7772827148438, weights = tensor([2.1191, 0.3660], dtype=torch.float16, requires_grad=True)\n",
      "step №12: loss = 491.68011474609375, weights = tensor([2.2578, 0.3904], dtype=torch.float16, requires_grad=True)\n",
      "step №13: loss = 459.18634033203125, weights = tensor([2.3926, 0.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №14: loss = 428.7193298339844, weights = tensor([2.5215, 0.4370], dtype=torch.float16, requires_grad=True)\n",
      "step №15: loss = 400.5894470214844, weights = tensor([2.6465, 0.4592], dtype=torch.float16, requires_grad=True)\n",
      "step №16: loss = 374.2697448730469, weights = tensor([2.7676, 0.4807], dtype=torch.float16, requires_grad=True)\n",
      "step №17: loss = 349.6717224121094, weights = tensor([2.8828, 0.5015], dtype=torch.float16, requires_grad=True)\n",
      "step №18: loss = 327.0741271972656, weights = tensor([2.9941, 0.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №19: loss = 306.0039978027344, weights = tensor([3.1016, 0.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №20: loss = 286.37493896484375, weights = tensor([3.2051, 0.5601], dtype=torch.float16, requires_grad=True)\n",
      "step №21: loss = 268.11163330078125, weights = tensor([3.3047, 0.5786], dtype=torch.float16, requires_grad=True)\n",
      "step №22: loss = 251.1414337158203, weights = tensor([3.4023, 0.5967], dtype=torch.float16, requires_grad=True)\n",
      "step №23: loss = 235.0896453857422, weights = tensor([3.4961, 0.6138], dtype=torch.float16, requires_grad=True)\n",
      "step №24: loss = 220.2288818359375, weights = tensor([3.5859, 0.6304], dtype=torch.float16, requires_grad=True)\n",
      "step №25: loss = 206.4789581298828, weights = tensor([3.6738, 0.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №26: loss = 193.5030059814453, weights = tensor([3.7578, 0.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №27: loss = 181.53366088867188, weights = tensor([3.8398, 0.6772], dtype=torch.float16, requires_grad=True)\n",
      "step №28: loss = 170.2571563720703, weights = tensor([3.9180, 0.6919], dtype=torch.float16, requires_grad=True)\n",
      "step №29: loss = 159.88986206054688, weights = tensor([3.9941, 0.7061], dtype=torch.float16, requires_grad=True)\n",
      "step №30: loss = 150.13980102539062, weights = tensor([4.0664, 0.7202], dtype=torch.float16, requires_grad=True)\n",
      "step №31: loss = 141.1973876953125, weights = tensor([4.1367, 0.7339], dtype=torch.float16, requires_grad=True)\n",
      "step №32: loss = 132.80276489257812, weights = tensor([4.2031, 0.7471], dtype=torch.float16, requires_grad=True)\n",
      "step №33: loss = 125.1436996459961, weights = tensor([4.2695, 0.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №34: loss = 117.76219177246094, weights = tensor([4.3320, 0.7720], dtype=torch.float16, requires_grad=True)\n",
      "step №35: loss = 111.053466796875, weights = tensor([4.3945, 0.7842], dtype=torch.float16, requires_grad=True)\n",
      "step №36: loss = 104.58145904541016, weights = tensor([4.4531, 0.7959], dtype=torch.float16, requires_grad=True)\n",
      "step №37: loss = 98.72368621826172, weights = tensor([4.5078, 0.8071], dtype=torch.float16, requires_grad=True)\n",
      "step №38: loss = 93.43900299072266, weights = tensor([4.5625, 0.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №39: loss = 88.33609771728516, weights = tensor([4.6172, 0.8291], dtype=torch.float16, requires_grad=True)\n",
      "step №40: loss = 83.42345428466797, weights = tensor([4.6680, 0.8394], dtype=torch.float16, requires_grad=True)\n",
      "step №41: loss = 79.01931762695312, weights = tensor([4.7188, 0.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №42: loss = 74.7717514038086, weights = tensor([4.7656, 0.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №43: loss = 70.9850845336914, weights = tensor([4.8125, 0.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №44: loss = 67.33209228515625, weights = tensor([4.8555, 0.8784], dtype=torch.float16, requires_grad=True)\n",
      "step №45: loss = 64.09601593017578, weights = tensor([4.8984, 0.8877], dtype=torch.float16, requires_grad=True)\n",
      "step №46: loss = 60.9725227355957, weights = tensor([4.9375, 0.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №47: loss = 58.2255859375, weights = tensor([4.9766, 0.9053], dtype=torch.float16, requires_grad=True)\n",
      "step №48: loss = 55.57195281982422, weights = tensor([5.0156, 0.9136], dtype=torch.float16, requires_grad=True)\n",
      "step №49: loss = 53.01829147338867, weights = tensor([5.0508, 0.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №50: loss = 50.788421630859375, weights = tensor([5.0859, 0.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №51: loss = 48.64072799682617, weights = tensor([5.1211, 0.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №52: loss = 46.56855010986328, weights = tensor([5.1523, 0.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №53: loss = 44.779296875, weights = tensor([5.1836, 0.9526], dtype=torch.float16, requires_grad=True)\n",
      "step №54: loss = 43.05611038208008, weights = tensor([5.2148, 0.9600], dtype=torch.float16, requires_grad=True)\n",
      "step №55: loss = 41.39280700683594, weights = tensor([5.2461, 0.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №56: loss = 39.79499053955078, weights = tensor([5.2734, 0.9736], dtype=torch.float16, requires_grad=True)\n",
      "step №57: loss = 38.43622589111328, weights = tensor([5.3008, 0.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №58: loss = 37.12354278564453, weights = tensor([5.3281, 0.9868], dtype=torch.float16, requires_grad=True)\n",
      "step №59: loss = 35.862144470214844, weights = tensor([5.3555, 0.9932], dtype=torch.float16, requires_grad=True)\n",
      "step №60: loss = 34.646568298339844, weights = tensor([5.3789, 0.9995], dtype=torch.float16, requires_grad=True)\n",
      "step №61: loss = 33.631874084472656, weights = tensor([5.4023, 1.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №62: loss = 32.651248931884766, weights = tensor([5.4258, 1.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №63: loss = 31.709453582763672, weights = tensor([5.4492, 1.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №64: loss = 30.801509857177734, weights = tensor([5.4688, 1.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №65: loss = 30.061634063720703, weights = tensor([5.4883, 1.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №66: loss = 29.345626831054688, weights = tensor([5.5078, 1.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №67: loss = 28.653493881225586, weights = tensor([5.5273, 1.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №68: loss = 27.985233306884766, weights = tensor([5.5469, 1.0459], dtype=torch.float16, requires_grad=True)\n",
      "step №69: loss = 27.349227905273438, weights = tensor([5.5664, 1.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №70: loss = 26.736730575561523, weights = tensor([5.5820, 1.0557], dtype=torch.float16, requires_grad=True)\n",
      "step №71: loss = 26.255603790283203, weights = tensor([5.5977, 1.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №72: loss = 25.789810180664062, weights = tensor([5.6133, 1.0654], dtype=torch.float16, requires_grad=True)\n",
      "step №73: loss = 25.339353561401367, weights = tensor([5.6289, 1.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №74: loss = 24.904237747192383, weights = tensor([5.6445, 1.0752], dtype=torch.float16, requires_grad=True)\n",
      "step №75: loss = 24.484455108642578, weights = tensor([5.6602, 1.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №76: loss = 24.08000946044922, weights = tensor([5.6758, 1.0850], dtype=torch.float16, requires_grad=True)\n",
      "step №77: loss = 23.690906524658203, weights = tensor([5.6875, 1.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №78: loss = 23.40030860900879, weights = tensor([5.6992, 1.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №79: loss = 23.125568389892578, weights = tensor([5.7109, 1.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №80: loss = 22.859512329101562, weights = tensor([5.7227, 1.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №81: loss = 22.60213851928711, weights = tensor([5.7344, 1.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №82: loss = 22.353445053100586, weights = tensor([5.7461, 1.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №83: loss = 22.113435745239258, weights = tensor([5.7578, 1.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №84: loss = 21.88210678100586, weights = tensor([5.7695, 1.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №85: loss = 21.659461975097656, weights = tensor([5.7812, 1.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №86: loss = 21.445499420166016, weights = tensor([5.7891, 1.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №87: loss = 21.299541473388672, weights = tensor([5.7969, 1.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №88: loss = 21.157644271850586, weights = tensor([5.8047, 1.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №89: loss = 21.019805908203125, weights = tensor([5.8125, 1.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №90: loss = 20.88602638244629, weights = tensor([5.8203, 1.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №91: loss = 20.756305694580078, weights = tensor([5.8281, 1.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №92: loss = 20.63064193725586, weights = tensor([5.8359, 1.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №93: loss = 20.5090389251709, weights = tensor([5.8438, 1.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №94: loss = 20.391494750976562, weights = tensor([5.8516, 1.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №95: loss = 20.27800941467285, weights = tensor([5.8594, 1.1592], dtype=torch.float16, requires_grad=True)\n",
      "step №96: loss = 20.173999786376953, weights = tensor([5.8672, 1.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №97: loss = 20.073894500732422, weights = tensor([5.8750, 1.1650], dtype=torch.float16, requires_grad=True)\n",
      "step №98: loss = 19.97770118713379, weights = tensor([5.8828, 1.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №99: loss = 19.88541603088379, weights = tensor([5.8867, 1.1709], dtype=torch.float16, requires_grad=True)\n",
      "step №100: loss = 19.833005905151367, weights = tensor([5.8906, 1.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №101: loss = 19.78168487548828, weights = tensor([5.8945, 1.1768], dtype=torch.float16, requires_grad=True)\n",
      "step №102: loss = 19.731462478637695, weights = tensor([5.8984, 1.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №103: loss = 19.682331085205078, weights = tensor([5.9023, 1.1826], dtype=torch.float16, requires_grad=True)\n",
      "step №104: loss = 19.63429069519043, weights = tensor([5.9062, 1.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №105: loss = 19.587343215942383, weights = tensor([5.9102, 1.1885], dtype=torch.float16, requires_grad=True)\n",
      "step №106: loss = 19.541492462158203, weights = tensor([5.9141, 1.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №107: loss = 19.49673080444336, weights = tensor([5.9180, 1.1943], dtype=torch.float16, requires_grad=True)\n",
      "step №108: loss = 19.453065872192383, weights = tensor([5.9219, 1.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №109: loss = 19.41048812866211, weights = tensor([5.9258, 1.2002], dtype=torch.float16, requires_grad=True)\n",
      "step №110: loss = 19.369007110595703, weights = tensor([5.9297, 1.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №111: loss = 19.3286190032959, weights = tensor([5.9336, 1.2061], dtype=torch.float16, requires_grad=True)\n",
      "step №112: loss = 19.289323806762695, weights = tensor([5.9375, 1.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №113: loss = 19.251117706298828, weights = tensor([5.9414, 1.2119], dtype=torch.float16, requires_grad=True)\n",
      "step №114: loss = 19.214012145996094, weights = tensor([5.9453, 1.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №115: loss = 19.177993774414062, weights = tensor([5.9492, 1.2178], dtype=torch.float16, requires_grad=True)\n",
      "step №116: loss = 19.143070220947266, weights = tensor([5.9531, 1.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №117: loss = 19.109235763549805, weights = tensor([5.9570, 1.2236], dtype=torch.float16, requires_grad=True)\n",
      "step №118: loss = 19.076501846313477, weights = tensor([5.9609, 1.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №119: loss = 19.04485511779785, weights = tensor([5.9648, 1.2295], dtype=torch.float16, requires_grad=True)\n",
      "step №120: loss = 19.01430320739746, weights = tensor([5.9688, 1.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №121: loss = 18.984840393066406, weights = tensor([5.9727, 1.2354], dtype=torch.float16, requires_grad=True)\n",
      "step №122: loss = 18.956478118896484, weights = tensor([5.9766, 1.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №123: loss = 18.929203033447266, weights = tensor([5.9805, 1.2412], dtype=torch.float16, requires_grad=True)\n",
      "step №124: loss = 18.90302276611328, weights = tensor([5.9844, 1.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №125: loss = 18.877931594848633, weights = tensor([5.9844, 1.2471], dtype=torch.float16, requires_grad=True)\n",
      "step №126: loss = 18.86548614501953, weights = tensor([5.9844, 1.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №127: loss = 18.853052139282227, weights = tensor([5.9844, 1.2529], dtype=torch.float16, requires_grad=True)\n",
      "step №128: loss = 18.840639114379883, weights = tensor([5.9844, 1.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №129: loss = 18.828235626220703, weights = tensor([5.9844, 1.2588], dtype=torch.float16, requires_grad=True)\n",
      "step №130: loss = 18.81585693359375, weights = tensor([5.9844, 1.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №131: loss = 18.80349349975586, weights = tensor([5.9844, 1.2646], dtype=torch.float16, requires_grad=True)\n",
      "step №132: loss = 18.791147232055664, weights = tensor([5.9844, 1.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №133: loss = 18.77881622314453, weights = tensor([5.9844, 1.2705], dtype=torch.float16, requires_grad=True)\n",
      "step №134: loss = 18.76650619506836, weights = tensor([5.9844, 1.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №135: loss = 18.75421142578125, weights = tensor([5.9844, 1.2764], dtype=torch.float16, requires_grad=True)\n",
      "step №136: loss = 18.741933822631836, weights = tensor([5.9844, 1.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №137: loss = 18.729671478271484, weights = tensor([5.9844, 1.2822], dtype=torch.float16, requires_grad=True)\n",
      "step №138: loss = 18.71742820739746, weights = tensor([5.9844, 1.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №139: loss = 18.705204010009766, weights = tensor([5.9844, 1.2881], dtype=torch.float16, requires_grad=True)\n",
      "step №140: loss = 18.6929931640625, weights = tensor([5.9844, 1.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №141: loss = 18.68079948425293, weights = tensor([5.9844, 1.2939], dtype=torch.float16, requires_grad=True)\n",
      "step №142: loss = 18.668628692626953, weights = tensor([5.9844, 1.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №143: loss = 18.656469345092773, weights = tensor([5.9844, 1.2998], dtype=torch.float16, requires_grad=True)\n",
      "step №144: loss = 18.644330978393555, weights = tensor([5.9844, 1.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №145: loss = 18.632204055786133, weights = tensor([5.9844, 1.3057], dtype=torch.float16, requires_grad=True)\n",
      "step №146: loss = 18.620098114013672, weights = tensor([5.9844, 1.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №147: loss = 18.60801124572754, weights = tensor([5.9844, 1.3115], dtype=torch.float16, requires_grad=True)\n",
      "step №148: loss = 18.595937728881836, weights = tensor([5.9844, 1.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №149: loss = 18.583881378173828, weights = tensor([5.9844, 1.3174], dtype=torch.float16, requires_grad=True)\n",
      "step №150: loss = 18.571847915649414, weights = tensor([5.9844, 1.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №151: loss = 18.559825897216797, weights = tensor([5.9844, 1.3232], dtype=torch.float16, requires_grad=True)\n",
      "step №152: loss = 18.54782485961914, weights = tensor([5.9844, 1.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №153: loss = 18.53583526611328, weights = tensor([5.9844, 1.3291], dtype=torch.float16, requires_grad=True)\n",
      "step №154: loss = 18.523868560791016, weights = tensor([5.9844, 1.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №155: loss = 18.511917114257812, weights = tensor([5.9844, 1.3350], dtype=torch.float16, requires_grad=True)\n",
      "step №156: loss = 18.499980926513672, weights = tensor([5.9844, 1.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №157: loss = 18.48806381225586, weights = tensor([5.9844, 1.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №158: loss = 18.48012924194336, weights = tensor([5.9844, 1.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №159: loss = 18.472200393676758, weights = tensor([5.9844, 1.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №160: loss = 18.464282989501953, weights = tensor([5.9844, 1.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №161: loss = 18.45636749267578, weights = tensor([5.9844, 1.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №162: loss = 18.44846534729004, weights = tensor([5.9844, 1.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №163: loss = 18.44056510925293, weights = tensor([5.9844, 1.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №164: loss = 18.43267822265625, weights = tensor([5.9844, 1.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №165: loss = 18.424793243408203, weights = tensor([5.9844, 1.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №166: loss = 18.416921615600586, weights = tensor([5.9844, 1.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №167: loss = 18.409053802490234, weights = tensor([5.9844, 1.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №168: loss = 18.401195526123047, weights = tensor([5.9844, 1.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №169: loss = 18.393342971801758, weights = tensor([5.9844, 1.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №170: loss = 18.385501861572266, weights = tensor([5.9844, 1.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №171: loss = 18.377662658691406, weights = tensor([5.9844, 1.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №172: loss = 18.369836807250977, weights = tensor([5.9844, 1.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №173: loss = 18.36201286315918, weights = tensor([5.9844, 1.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №174: loss = 18.354202270507812, weights = tensor([5.9844, 1.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №175: loss = 18.346393585205078, weights = tensor([5.9844, 1.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №176: loss = 18.338598251342773, weights = tensor([5.9844, 1.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №177: loss = 18.330806732177734, weights = tensor([5.9844, 1.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №178: loss = 18.32302474975586, weights = tensor([5.9844, 1.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №179: loss = 18.315248489379883, weights = tensor([5.9844, 1.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №180: loss = 18.307483673095703, weights = tensor([5.9844, 1.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №181: loss = 18.299720764160156, weights = tensor([5.9844, 1.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №182: loss = 18.29197120666504, weights = tensor([5.9844, 1.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №183: loss = 18.284223556518555, weights = tensor([5.9844, 1.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №184: loss = 18.2764892578125, weights = tensor([5.9844, 1.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №185: loss = 18.268756866455078, weights = tensor([5.9844, 1.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №186: loss = 18.261037826538086, weights = tensor([5.9844, 1.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №187: loss = 18.25332260131836, weights = tensor([5.9844, 1.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №188: loss = 18.245616912841797, weights = tensor([5.9844, 1.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №189: loss = 18.237916946411133, weights = tensor([5.9844, 1.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №190: loss = 18.230228424072266, weights = tensor([5.9844, 1.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №191: loss = 18.22254180908203, weights = tensor([5.9844, 1.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №192: loss = 18.214868545532227, weights = tensor([5.9844, 1.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №193: loss = 18.207197189331055, weights = tensor([5.9844, 1.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №194: loss = 18.199539184570312, weights = tensor([5.9844, 1.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №195: loss = 18.191883087158203, weights = tensor([5.9844, 1.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №196: loss = 18.184240341186523, weights = tensor([5.9844, 1.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №197: loss = 18.17660140991211, weights = tensor([5.9844, 1.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №198: loss = 18.16897201538086, weights = tensor([5.9844, 1.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №199: loss = 18.161348342895508, weights = tensor([5.9844, 1.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №200: loss = 18.153736114501953, weights = tensor([5.9844, 1.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №201: loss = 18.14612579345703, weights = tensor([5.9844, 1.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №202: loss = 18.13852882385254, weights = tensor([5.9844, 1.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №203: loss = 18.13093376159668, weights = tensor([5.9844, 1.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №204: loss = 18.12335205078125, weights = tensor([5.9844, 1.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №205: loss = 18.115772247314453, weights = tensor([5.9844, 1.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №206: loss = 18.108205795288086, weights = tensor([5.9844, 1.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №207: loss = 18.100643157958984, weights = tensor([5.9844, 1.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №208: loss = 18.093090057373047, weights = tensor([5.9844, 1.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №209: loss = 18.085542678833008, weights = tensor([5.9844, 1.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №210: loss = 18.078006744384766, weights = tensor([5.9844, 1.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №211: loss = 18.070472717285156, weights = tensor([5.9844, 1.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №212: loss = 18.062952041625977, weights = tensor([5.9844, 1.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №213: loss = 18.05543327331543, weights = tensor([5.9844, 1.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №214: loss = 18.047927856445312, weights = tensor([5.9844, 1.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №215: loss = 18.040424346923828, weights = tensor([5.9844, 1.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №216: loss = 18.032934188842773, weights = tensor([5.9844, 1.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №217: loss = 18.025447845458984, weights = tensor([5.9844, 1.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №218: loss = 18.01797103881836, weights = tensor([5.9844, 1.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №219: loss = 18.010499954223633, weights = tensor([5.9844, 1.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №220: loss = 18.003040313720703, weights = tensor([5.9844, 1.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №221: loss = 17.995582580566406, weights = tensor([5.9844, 1.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №222: loss = 17.98813819885254, weights = tensor([5.9844, 1.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №223: loss = 17.980695724487305, weights = tensor([5.9844, 1.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №224: loss = 17.9732666015625, weights = tensor([5.9844, 1.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №225: loss = 17.965839385986328, weights = tensor([5.9844, 1.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №226: loss = 17.958425521850586, weights = tensor([5.9844, 1.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №227: loss = 17.95101547241211, weights = tensor([5.9844, 1.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №228: loss = 17.943614959716797, weights = tensor([5.9844, 1.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №229: loss = 17.936220169067383, weights = tensor([5.9844, 1.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №230: loss = 17.928836822509766, weights = tensor([5.9844, 1.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №231: loss = 17.92145538330078, weights = tensor([5.9844, 1.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №232: loss = 17.914087295532227, weights = tensor([5.9844, 1.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №233: loss = 17.906721115112305, weights = tensor([5.9844, 1.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №234: loss = 17.899368286132812, weights = tensor([5.9844, 1.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №235: loss = 17.892017364501953, weights = tensor([5.9844, 1.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №236: loss = 17.884679794311523, weights = tensor([5.9844, 1.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №237: loss = 17.87734603881836, weights = tensor([5.9844, 1.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №238: loss = 17.87002182006836, weights = tensor([5.9844, 1.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №239: loss = 17.862703323364258, weights = tensor([5.9844, 1.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №240: loss = 17.855396270751953, weights = tensor([5.9844, 1.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №241: loss = 17.84809112548828, weights = tensor([5.9844, 1.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №242: loss = 17.84079933166504, weights = tensor([5.9844, 1.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №243: loss = 17.83350944519043, weights = tensor([5.9844, 1.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №244: loss = 17.82623291015625, weights = tensor([5.9844, 1.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №245: loss = 17.818958282470703, weights = tensor([5.9844, 1.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №246: loss = 17.811697006225586, weights = tensor([5.9844, 1.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №247: loss = 17.804439544677734, weights = tensor([5.9844, 1.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №248: loss = 17.797191619873047, weights = tensor([5.9844, 1.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №249: loss = 17.789949417114258, weights = tensor([5.9844, 1.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №250: loss = 17.782718658447266, weights = tensor([5.9844, 1.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №251: loss = 17.775489807128906, weights = tensor([5.9844, 1.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №252: loss = 17.768274307250977, weights = tensor([5.9844, 1.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №253: loss = 17.76106071472168, weights = tensor([5.9844, 1.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №254: loss = 17.753860473632812, weights = tensor([5.9844, 1.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №255: loss = 17.746662139892578, weights = tensor([5.9844, 1.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №256: loss = 17.739477157592773, weights = tensor([5.9844, 1.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №257: loss = 17.732295989990234, weights = tensor([5.9844, 1.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №258: loss = 17.72512435913086, weights = tensor([5.9844, 1.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №259: loss = 17.717958450317383, weights = tensor([5.9844, 1.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №260: loss = 17.710803985595703, weights = tensor([5.9844, 1.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №261: loss = 17.703651428222656, weights = tensor([5.9844, 1.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №262: loss = 17.69651222229004, weights = tensor([5.9844, 1.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №263: loss = 17.689374923706055, weights = tensor([5.9844, 1.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №264: loss = 17.6822509765625, weights = tensor([5.9844, 1.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №265: loss = 17.675128936767578, weights = tensor([5.9844, 1.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №266: loss = 17.668020248413086, weights = tensor([5.9844, 1.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №267: loss = 17.66091537475586, weights = tensor([5.9844, 1.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №268: loss = 17.653820037841797, weights = tensor([5.9844, 1.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №269: loss = 17.646730422973633, weights = tensor([5.9844, 1.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №270: loss = 17.639652252197266, weights = tensor([5.9844, 1.5605], dtype=torch.float16, requires_grad=True)\n",
      "step №271: loss = 17.63257598876953, weights = tensor([5.9844, 1.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №272: loss = 17.625513076782227, weights = tensor([5.9844, 1.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №273: loss = 17.618452072143555, weights = tensor([5.9844, 1.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №274: loss = 17.611404418945312, weights = tensor([5.9844, 1.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №275: loss = 17.604358673095703, weights = tensor([5.9844, 1.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №276: loss = 17.597326278686523, weights = tensor([5.9844, 1.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №277: loss = 17.59029769897461, weights = tensor([5.9844, 1.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №278: loss = 17.58327865600586, weights = tensor([5.9844, 1.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №279: loss = 17.576265335083008, weights = tensor([5.9844, 1.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №280: loss = 17.569263458251953, weights = tensor([5.9844, 1.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №281: loss = 17.56226348876953, weights = tensor([5.9844, 1.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №282: loss = 17.55527687072754, weights = tensor([5.9844, 1.5840], dtype=torch.float16, requires_grad=True)\n",
      "step №283: loss = 17.54829216003418, weights = tensor([5.9844, 1.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №284: loss = 17.54132080078125, weights = tensor([5.9844, 1.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №285: loss = 17.534351348876953, weights = tensor([5.9844, 1.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №286: loss = 17.527395248413086, weights = tensor([5.9844, 1.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №287: loss = 17.520442962646484, weights = tensor([5.9844, 1.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №288: loss = 17.513500213623047, weights = tensor([5.9844, 1.5957], dtype=torch.float16, requires_grad=True)\n",
      "step №289: loss = 17.506563186645508, weights = tensor([5.9844, 1.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №290: loss = 17.499637603759766, weights = tensor([5.9844, 1.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №291: loss = 17.492713928222656, weights = tensor([5.9844, 1.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №292: loss = 17.485803604125977, weights = tensor([5.9844, 1.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №293: loss = 17.47889518737793, weights = tensor([5.9844, 1.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №294: loss = 17.472000122070312, weights = tensor([5.9844, 1.6074], dtype=torch.float16, requires_grad=True)\n",
      "step №295: loss = 17.465106964111328, weights = tensor([5.9844, 1.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №296: loss = 17.458227157592773, weights = tensor([5.9844, 1.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №297: loss = 17.451351165771484, weights = tensor([5.9844, 1.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №298: loss = 17.44448471069336, weights = tensor([5.9844, 1.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №299: loss = 17.437623977661133, weights = tensor([5.9844, 1.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №300: loss = 17.430774688720703, weights = tensor([5.9844, 1.6191], dtype=torch.float16, requires_grad=True)\n",
      "step №301: loss = 17.423927307128906, weights = tensor([5.9844, 1.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №302: loss = 17.41709327697754, weights = tensor([5.9844, 1.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №303: loss = 17.410261154174805, weights = tensor([5.9844, 1.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №304: loss = 17.4034423828125, weights = tensor([5.9844, 1.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №305: loss = 17.396625518798828, weights = tensor([5.9844, 1.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №306: loss = 17.389822006225586, weights = tensor([5.9844, 1.6309], dtype=torch.float16, requires_grad=True)\n",
      "step №307: loss = 17.38302230834961, weights = tensor([5.9844, 1.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №308: loss = 17.376232147216797, weights = tensor([5.9844, 1.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №309: loss = 17.369447708129883, weights = tensor([5.9844, 1.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №310: loss = 17.362674713134766, weights = tensor([5.9844, 1.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №311: loss = 17.35590362548828, weights = tensor([5.9844, 1.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №312: loss = 17.349145889282227, weights = tensor([5.9844, 1.6426], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №313: loss = 17.342390060424805, weights = tensor([5.9844, 1.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №314: loss = 17.335647583007812, weights = tensor([5.9844, 1.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №315: loss = 17.328907012939453, weights = tensor([5.9844, 1.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №316: loss = 17.322179794311523, weights = tensor([5.9844, 1.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №317: loss = 17.31545639038086, weights = tensor([5.9844, 1.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №318: loss = 17.30874252319336, weights = tensor([5.9844, 1.6543], dtype=torch.float16, requires_grad=True)\n",
      "step №319: loss = 17.302034378051758, weights = tensor([5.9844, 1.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №320: loss = 17.295337677001953, weights = tensor([5.9844, 1.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №321: loss = 17.28864288330078, weights = tensor([5.9844, 1.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №322: loss = 17.28196144104004, weights = tensor([5.9844, 1.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №323: loss = 17.27528190612793, weights = tensor([5.9844, 1.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №324: loss = 17.26861572265625, weights = tensor([5.9844, 1.6660], dtype=torch.float16, requires_grad=True)\n",
      "step №325: loss = 17.261951446533203, weights = tensor([5.9844, 1.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №326: loss = 17.255300521850586, weights = tensor([5.9844, 1.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №327: loss = 17.248653411865234, weights = tensor([5.9844, 1.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №328: loss = 17.242015838623047, weights = tensor([5.9844, 1.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №329: loss = 17.235383987426758, weights = tensor([5.9844, 1.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №330: loss = 17.228763580322266, weights = tensor([5.9844, 1.6777], dtype=torch.float16, requires_grad=True)\n",
      "step №331: loss = 17.222145080566406, weights = tensor([5.9844, 1.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №332: loss = 17.215539932250977, weights = tensor([5.9844, 1.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №333: loss = 17.20893669128418, weights = tensor([5.9844, 1.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №334: loss = 17.202346801757812, weights = tensor([5.9844, 1.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №335: loss = 17.195758819580078, weights = tensor([5.9844, 1.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №336: loss = 17.189184188842773, weights = tensor([5.9844, 1.6895], dtype=torch.float16, requires_grad=True)\n",
      "step №337: loss = 17.182613372802734, weights = tensor([5.9844, 1.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №338: loss = 17.17605209350586, weights = tensor([5.9844, 1.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №339: loss = 17.169496536254883, weights = tensor([5.9844, 1.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №340: loss = 17.162952423095703, weights = tensor([5.9844, 1.6973], dtype=torch.float16, requires_grad=True)\n",
      "step №341: loss = 17.156410217285156, weights = tensor([5.9844, 1.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №342: loss = 17.14988136291504, weights = tensor([5.9844, 1.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №343: loss = 17.143354415893555, weights = tensor([5.9844, 1.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №344: loss = 17.1368408203125, weights = tensor([5.9844, 1.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №345: loss = 17.130329132080078, weights = tensor([5.9844, 1.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №346: loss = 17.123830795288086, weights = tensor([5.9844, 1.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №347: loss = 17.11733627319336, weights = tensor([5.9844, 1.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №348: loss = 17.110851287841797, weights = tensor([5.9844, 1.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №349: loss = 17.104372024536133, weights = tensor([5.9844, 1.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №350: loss = 17.097904205322266, weights = tensor([5.9844, 1.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №351: loss = 17.09143829345703, weights = tensor([5.9844, 1.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №352: loss = 17.084985733032227, weights = tensor([5.9844, 1.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №353: loss = 17.078535079956055, weights = tensor([5.9844, 1.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №354: loss = 17.072097778320312, weights = tensor([5.9844, 1.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №355: loss = 17.065662384033203, weights = tensor([5.9844, 1.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №356: loss = 17.059240341186523, weights = tensor([5.9844, 1.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №357: loss = 17.05282211303711, weights = tensor([5.9844, 1.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №358: loss = 17.04641342163086, weights = tensor([5.9844, 1.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №359: loss = 17.040010452270508, weights = tensor([5.9844, 1.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №360: loss = 17.033618927001953, weights = tensor([5.9844, 1.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №361: loss = 17.02722930908203, weights = tensor([5.9844, 1.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №362: loss = 17.02085304260254, weights = tensor([5.9844, 1.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №363: loss = 17.01447868347168, weights = tensor([5.9844, 1.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №364: loss = 17.00811767578125, weights = tensor([5.9844, 1.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №365: loss = 17.001758575439453, weights = tensor([5.9844, 1.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №366: loss = 16.995412826538086, weights = tensor([5.9844, 1.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №367: loss = 16.989070892333984, weights = tensor([5.9844, 1.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №368: loss = 16.982738494873047, weights = tensor([5.9844, 1.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №369: loss = 16.976411819458008, weights = tensor([5.9844, 1.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №370: loss = 16.970096588134766, weights = tensor([5.9844, 1.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №371: loss = 16.963783264160156, weights = tensor([5.9844, 1.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №372: loss = 16.957483291625977, weights = tensor([5.9844, 1.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №373: loss = 16.95118522644043, weights = tensor([5.9844, 1.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №374: loss = 16.944900512695312, weights = tensor([5.9844, 1.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №375: loss = 16.938617706298828, weights = tensor([5.9844, 1.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №376: loss = 16.932348251342773, weights = tensor([5.9844, 1.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №377: loss = 16.926082611083984, weights = tensor([5.9844, 1.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №378: loss = 16.91982650756836, weights = tensor([5.9844, 1.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №379: loss = 16.913576126098633, weights = tensor([5.9844, 1.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №380: loss = 16.907337188720703, weights = tensor([5.9844, 1.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №381: loss = 16.901100158691406, weights = tensor([5.9844, 1.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №382: loss = 16.89487648010254, weights = tensor([5.9844, 1.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №383: loss = 16.888654708862305, weights = tensor([5.9844, 1.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №384: loss = 16.8824462890625, weights = tensor([5.9844, 1.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №385: loss = 16.876239776611328, weights = tensor([5.9844, 1.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №386: loss = 16.870046615600586, weights = tensor([5.9844, 1.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №387: loss = 16.86385726928711, weights = tensor([5.9844, 1.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №388: loss = 16.857677459716797, weights = tensor([5.9844, 1.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №389: loss = 16.851503372192383, weights = tensor([5.9844, 1.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №390: loss = 16.845340728759766, weights = tensor([5.9844, 1.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №391: loss = 16.83917999267578, weights = tensor([5.9844, 1.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №392: loss = 16.833032608032227, weights = tensor([5.9844, 1.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №393: loss = 16.826887130737305, weights = tensor([5.9844, 1.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №394: loss = 16.820755004882812, weights = tensor([5.9844, 1.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №395: loss = 16.814624786376953, weights = tensor([5.9844, 1.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №396: loss = 16.808507919311523, weights = tensor([5.9844, 1.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №397: loss = 16.80239486694336, weights = tensor([5.9844, 1.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №398: loss = 16.79629135131836, weights = tensor([5.9844, 1.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №399: loss = 16.790193557739258, weights = tensor([5.9844, 1.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №400: loss = 16.784107208251953, weights = tensor([5.9844, 1.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №401: loss = 16.77802276611328, weights = tensor([5.9844, 1.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №402: loss = 16.77195167541504, weights = tensor([5.9844, 1.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №403: loss = 16.76588249206543, weights = tensor([5.9844, 1.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №404: loss = 16.75982666015625, weights = tensor([5.9844, 1.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №405: loss = 16.753772735595703, weights = tensor([5.9844, 1.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №406: loss = 16.747732162475586, weights = tensor([5.9844, 1.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №407: loss = 16.741695404052734, weights = tensor([5.9844, 1.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №408: loss = 16.735668182373047, weights = tensor([5.9844, 1.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №409: loss = 16.729646682739258, weights = tensor([5.9844, 1.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №410: loss = 16.723636627197266, weights = tensor([5.9844, 1.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №411: loss = 16.717628479003906, weights = tensor([5.9844, 1.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №412: loss = 16.711633682250977, weights = tensor([5.9844, 1.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №413: loss = 16.70564079284668, weights = tensor([5.9844, 1.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №414: loss = 16.699661254882812, weights = tensor([5.9844, 1.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №415: loss = 16.693683624267578, weights = tensor([5.9844, 1.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №416: loss = 16.687719345092773, weights = tensor([5.9844, 1.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №417: loss = 16.681758880615234, weights = tensor([5.9844, 1.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №418: loss = 16.67580795288086, weights = tensor([5.9844, 1.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №419: loss = 16.669862747192383, weights = tensor([5.9844, 1.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №420: loss = 16.663928985595703, weights = tensor([5.9844, 1.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №421: loss = 16.657997131347656, weights = tensor([5.9844, 1.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №422: loss = 16.65207862854004, weights = tensor([5.9844, 1.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №423: loss = 16.646162033081055, weights = tensor([5.9844, 1.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №424: loss = 16.6402587890625, weights = tensor([5.9844, 1.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №425: loss = 16.634357452392578, weights = tensor([5.9844, 1.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №426: loss = 16.628469467163086, weights = tensor([5.9844, 1.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №427: loss = 16.62258529663086, weights = tensor([5.9844, 1.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №428: loss = 16.616710662841797, weights = tensor([5.9844, 1.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №429: loss = 16.610841751098633, weights = tensor([5.9844, 1.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №430: loss = 16.604984283447266, weights = tensor([5.9844, 1.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №431: loss = 16.59912872314453, weights = tensor([5.9844, 1.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №432: loss = 16.593286514282227, weights = tensor([5.9844, 1.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №433: loss = 16.587446212768555, weights = tensor([5.9844, 1.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №434: loss = 16.581619262695312, weights = tensor([5.9844, 1.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №435: loss = 16.575794219970703, weights = tensor([5.9844, 1.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №436: loss = 16.569982528686523, weights = tensor([5.9844, 1.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №437: loss = 16.56417465209961, weights = tensor([5.9844, 1.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №438: loss = 16.55837631225586, weights = tensor([5.9844, 1.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №439: loss = 16.552583694458008, weights = tensor([5.9844, 1.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №440: loss = 16.546802520751953, weights = tensor([5.9844, 1.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №441: loss = 16.54102325439453, weights = tensor([5.9844, 1.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №442: loss = 16.53525733947754, weights = tensor([5.9844, 1.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №443: loss = 16.52949333190918, weights = tensor([5.9844, 1.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №444: loss = 16.52374267578125, weights = tensor([5.9844, 1.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №445: loss = 16.517993927001953, weights = tensor([5.9844, 1.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №446: loss = 16.512258529663086, weights = tensor([5.9844, 1.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №447: loss = 16.506526947021484, weights = tensor([5.9844, 1.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №448: loss = 16.500804901123047, weights = tensor([5.9844, 1.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №449: loss = 16.495088577270508, weights = tensor([5.9844, 1.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №450: loss = 16.489383697509766, weights = tensor([5.9844, 1.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №451: loss = 16.483680725097656, weights = tensor([5.9844, 1.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №452: loss = 16.477991104125977, weights = tensor([5.9844, 1.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №453: loss = 16.47230339050293, weights = tensor([5.9844, 1.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №454: loss = 16.466629028320312, weights = tensor([5.9844, 1.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №455: loss = 16.460956573486328, weights = tensor([5.9844, 1.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №456: loss = 16.455297470092773, weights = tensor([5.9844, 1.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №457: loss = 16.449642181396484, weights = tensor([5.9844, 1.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №458: loss = 16.44399642944336, weights = tensor([5.9844, 1.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №459: loss = 16.438356399536133, weights = tensor([5.9844, 1.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №460: loss = 16.432727813720703, weights = tensor([5.9844, 1.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №461: loss = 16.427101135253906, weights = tensor([5.9844, 1.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №462: loss = 16.42148780822754, weights = tensor([5.9844, 1.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №463: loss = 16.415876388549805, weights = tensor([5.9844, 1.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №464: loss = 16.4102783203125, weights = tensor([5.9844, 1.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №465: loss = 16.404682159423828, weights = tensor([5.9844, 1.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №466: loss = 16.399099349975586, weights = tensor([5.9844, 1.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №467: loss = 16.39352035522461, weights = tensor([5.9844, 1.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №468: loss = 16.387950897216797, weights = tensor([5.9844, 1.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №469: loss = 16.382387161254883, weights = tensor([5.9844, 1.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №470: loss = 16.376834869384766, weights = tensor([5.9844, 1.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №471: loss = 16.37128448486328, weights = tensor([5.9805, 1.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №472: loss = 16.35334014892578, weights = tensor([5.9805, 1.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №473: loss = 16.347736358642578, weights = tensor([5.9805, 1.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №474: loss = 16.342145919799805, weights = tensor([5.9805, 1.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №475: loss = 16.336559295654297, weights = tensor([5.9805, 1.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №476: loss = 16.330982208251953, weights = tensor([5.9805, 1.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №477: loss = 16.325410842895508, weights = tensor([5.9805, 1.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №478: loss = 16.31985092163086, weights = tensor([5.9805, 1.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №479: loss = 16.314292907714844, weights = tensor([5.9805, 1.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №480: loss = 16.308748245239258, weights = tensor([5.9805, 1.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №481: loss = 16.303205490112305, weights = tensor([5.9805, 1.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №482: loss = 16.29767608642578, weights = tensor([5.9805, 1.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №483: loss = 16.29214859008789, weights = tensor([5.9766, 1.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №484: loss = 16.274272918701172, weights = tensor([5.9766, 1.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №485: loss = 16.268693923950195, weights = tensor([5.9766, 1.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №486: loss = 16.263126373291016, weights = tensor([5.9766, 1.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №487: loss = 16.25756072998047, weights = tensor([5.9766, 1.9844], dtype=torch.float16, requires_grad=True)\n",
      "step №488: loss = 16.25200843811035, weights = tensor([5.9766, 1.9863], dtype=torch.float16, requires_grad=True)\n",
      "step №489: loss = 16.246458053588867, weights = tensor([5.9766, 1.9883], dtype=torch.float16, requires_grad=True)\n",
      "step №490: loss = 16.240921020507812, weights = tensor([5.9766, 1.9902], dtype=torch.float16, requires_grad=True)\n",
      "step №491: loss = 16.23538589477539, weights = tensor([5.9766, 1.9922], dtype=torch.float16, requires_grad=True)\n",
      "step №492: loss = 16.2298641204834, weights = tensor([5.9766, 1.9941], dtype=torch.float16, requires_grad=True)\n",
      "step №493: loss = 16.224346160888672, weights = tensor([5.9766, 1.9961], dtype=torch.float16, requires_grad=True)\n",
      "step №494: loss = 16.21883773803711, weights = tensor([5.9766, 1.9980], dtype=torch.float16, requires_grad=True)\n",
      "step №495: loss = 16.213335037231445, weights = tensor([5.9766, 2.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №496: loss = 16.207843780517578, weights = tensor([5.9727, 2.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №497: loss = 16.189970016479492, weights = tensor([5.9727, 2.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №498: loss = 16.184425354003906, weights = tensor([5.9727, 2.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №499: loss = 16.178882598876953, weights = tensor([5.9727, 2.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №500: loss = 16.17335319519043, weights = tensor([5.9727, 2.0098], dtype=torch.float16, requires_grad=True)\n",
      "step №501: loss = 16.167827606201172, weights = tensor([5.9727, 2.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №502: loss = 16.162311553955078, weights = tensor([5.9727, 2.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №503: loss = 16.156801223754883, weights = tensor([5.9727, 2.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №504: loss = 16.151302337646484, weights = tensor([5.9727, 2.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №505: loss = 16.14580535888672, weights = tensor([5.9727, 2.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №506: loss = 16.140321731567383, weights = tensor([5.9727, 2.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №507: loss = 16.13484001159668, weights = tensor([5.9727, 2.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №508: loss = 16.129371643066406, weights = tensor([5.9727, 2.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №509: loss = 16.123905181884766, weights = tensor([5.9688, 2.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №510: loss = 16.10604476928711, weights = tensor([5.9688, 2.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №511: loss = 16.100526809692383, weights = tensor([5.9688, 2.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №512: loss = 16.095020294189453, weights = tensor([5.9688, 2.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №513: loss = 16.089515686035156, weights = tensor([5.9688, 2.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №514: loss = 16.08402442932129, weights = tensor([5.9688, 2.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №515: loss = 16.078535079956055, weights = tensor([5.9688, 2.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №516: loss = 16.07305908203125, weights = tensor([5.9688, 2.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №517: loss = 16.067584991455078, weights = tensor([5.9688, 2.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №518: loss = 16.062124252319336, weights = tensor([5.9688, 2.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №519: loss = 16.05666732788086, weights = tensor([5.9688, 2.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №520: loss = 16.051219940185547, weights = tensor([5.9688, 2.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №521: loss = 16.045778274536133, weights = tensor([5.9648, 2.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №522: loss = 16.027986526489258, weights = tensor([5.9648, 2.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №523: loss = 16.022489547729492, weights = tensor([5.9648, 2.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №524: loss = 16.017005920410156, weights = tensor([5.9648, 2.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №525: loss = 16.011524200439453, weights = tensor([5.9648, 2.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №526: loss = 16.00605583190918, weights = tensor([5.9648, 2.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №527: loss = 16.000591278076172, weights = tensor([5.9648, 2.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №528: loss = 15.995137214660645, weights = tensor([5.9648, 2.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №529: loss = 15.989686965942383, weights = tensor([5.9648, 2.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №530: loss = 15.984248161315918, weights = tensor([5.9648, 2.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №531: loss = 15.978813171386719, weights = tensor([5.9648, 2.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №532: loss = 15.973390579223633, weights = tensor([5.9648, 2.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №533: loss = 15.96796989440918, weights = tensor([5.9648, 2.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №534: loss = 15.962562561035156, weights = tensor([5.9609, 2.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №535: loss = 15.94477367401123, weights = tensor([5.9609, 2.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №536: loss = 15.939312934875488, weights = tensor([5.9609, 2.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №537: loss = 15.933855056762695, weights = tensor([5.9609, 2.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №538: loss = 15.9284086227417, weights = tensor([5.9609, 2.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №539: loss = 15.922966003417969, weights = tensor([5.9609, 2.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №540: loss = 15.917535781860352, weights = tensor([5.9609, 2.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №541: loss = 15.912107467651367, weights = tensor([5.9609, 2.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №542: loss = 15.906692504882812, weights = tensor([5.9609, 2.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №543: loss = 15.901280403137207, weights = tensor([5.9609, 2.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №544: loss = 15.895879745483398, weights = tensor([5.9609, 2.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №545: loss = 15.890482902526855, weights = tensor([5.9609, 2.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №546: loss = 15.885098457336426, weights = tensor([5.9609, 2.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №547: loss = 15.879716873168945, weights = tensor([5.9570, 2.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №548: loss = 15.861940383911133, weights = tensor([5.9570, 2.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №549: loss = 15.856504440307617, weights = tensor([5.9570, 2.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №550: loss = 15.851081848144531, weights = tensor([5.9570, 2.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №551: loss = 15.845662117004395, weights = tensor([5.9570, 2.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №552: loss = 15.840253829956055, weights = tensor([5.9570, 2.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №553: loss = 15.83484935760498, weights = tensor([5.9570, 2.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №554: loss = 15.82945728302002, weights = tensor([5.9570, 2.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №555: loss = 15.824068069458008, weights = tensor([5.9570, 2.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №556: loss = 15.818690299987793, weights = tensor([5.9570, 2.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №557: loss = 15.813316345214844, weights = tensor([5.9570, 2.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №558: loss = 15.807954788208008, weights = tensor([5.9570, 2.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №559: loss = 15.802595138549805, weights = tensor([5.9531, 2.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №560: loss = 15.784887313842773, weights = tensor([5.9531, 2.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №561: loss = 15.779475212097168, weights = tensor([5.9531, 2.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №562: loss = 15.774075508117676, weights = tensor([5.9531, 2.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №563: loss = 15.768678665161133, weights = tensor([5.9531, 2.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №564: loss = 15.763293266296387, weights = tensor([5.9531, 2.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №565: loss = 15.757911682128906, weights = tensor([5.9531, 2.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №566: loss = 15.752542495727539, weights = tensor([5.9531, 2.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №567: loss = 15.747175216674805, weights = tensor([5.9531, 2.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №568: loss = 15.7418212890625, weights = tensor([5.9531, 2.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №569: loss = 15.736470222473145, weights = tensor([5.9531, 2.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №570: loss = 15.731130599975586, weights = tensor([5.9531, 2.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №571: loss = 15.725794792175293, weights = tensor([5.9531, 2.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №572: loss = 15.720471382141113, weights = tensor([5.9492, 2.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №573: loss = 15.702766418457031, weights = tensor([5.9492, 2.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №574: loss = 15.697389602661133, weights = tensor([5.9492, 2.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №575: loss = 15.692014694213867, weights = tensor([5.9492, 2.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №576: loss = 15.686653137207031, weights = tensor([5.9492, 2.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №577: loss = 15.681294441223145, weights = tensor([5.9492, 2.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №578: loss = 15.675947189331055, weights = tensor([5.9492, 2.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №579: loss = 15.67060375213623, weights = tensor([5.9492, 2.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №580: loss = 15.66527271270752, weights = tensor([5.9492, 2.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №581: loss = 15.659944534301758, weights = tensor([5.9492, 2.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №582: loss = 15.654627799987793, weights = tensor([5.9492, 2.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №583: loss = 15.649314880371094, weights = tensor([5.9492, 2.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №584: loss = 15.644014358520508, weights = tensor([5.9492, 2.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №585: loss = 15.638715744018555, weights = tensor([5.9453, 2.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №586: loss = 15.621023178100586, weights = tensor([5.9453, 2.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №587: loss = 15.61567211151123, weights = tensor([5.9453, 2.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №588: loss = 15.610333442687988, weights = tensor([5.9453, 2.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №589: loss = 15.604997634887695, weights = tensor([5.9453, 2.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №590: loss = 15.5996732711792, weights = tensor([5.9453, 2.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №591: loss = 15.594352722167969, weights = tensor([5.9453, 2.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №592: loss = 15.589044570922852, weights = tensor([5.9453, 2.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №593: loss = 15.583738327026367, weights = tensor([5.9453, 2.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №594: loss = 15.578445434570312, weights = tensor([5.9453, 2.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №595: loss = 15.573155403137207, weights = tensor([5.9453, 2.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №596: loss = 15.567876815795898, weights = tensor([5.9453, 2.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №597: loss = 15.562602043151855, weights = tensor([5.9414, 2.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №598: loss = 15.544978141784668, weights = tensor([5.9414, 2.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №599: loss = 15.539649963378906, weights = tensor([5.9414, 2.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №600: loss = 15.534334182739258, weights = tensor([5.9414, 2.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №601: loss = 15.529020309448242, weights = tensor([5.9414, 2.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №602: loss = 15.523719787597656, weights = tensor([5.9414, 2.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №603: loss = 15.51842212677002, weights = tensor([5.9414, 2.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №604: loss = 15.51313591003418, weights = tensor([5.9414, 2.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №605: loss = 15.507853507995605, weights = tensor([5.9414, 2.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №606: loss = 15.502583503723145, weights = tensor([5.9414, 2.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №607: loss = 15.497316360473633, weights = tensor([5.9414, 2.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №608: loss = 15.492060661315918, weights = tensor([5.9414, 2.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №609: loss = 15.486808776855469, weights = tensor([5.9414, 2.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №610: loss = 15.481569290161133, weights = tensor([5.9375, 2.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №611: loss = 15.463948249816895, weights = tensor([5.9375, 2.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №612: loss = 15.458654403686523, weights = tensor([5.9375, 2.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №613: loss = 15.453364372253418, weights = tensor([5.9375, 2.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №614: loss = 15.448086738586426, weights = tensor([5.9375, 2.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №615: loss = 15.442811965942383, weights = tensor([5.9375, 2.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №616: loss = 15.437548637390137, weights = tensor([5.9375, 2.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №617: loss = 15.432289123535156, weights = tensor([5.9375, 2.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №618: loss = 15.427042007446289, weights = tensor([5.9375, 2.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №619: loss = 15.421796798706055, weights = tensor([5.9375, 2.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №620: loss = 15.41656494140625, weights = tensor([5.9375, 2.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №621: loss = 15.411335945129395, weights = tensor([5.9375, 2.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №622: loss = 15.406118392944336, weights = tensor([5.9375, 2.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №623: loss = 15.400904655456543, weights = tensor([5.9336, 2.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №624: loss = 15.383296012878418, weights = tensor([5.9336, 2.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №625: loss = 15.378028869628906, weights = tensor([5.9336, 2.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №626: loss = 15.372774124145508, weights = tensor([5.9336, 2.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №627: loss = 15.367521286010742, weights = tensor([5.9336, 2.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №628: loss = 15.362281799316406, weights = tensor([5.9336, 2.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №629: loss = 15.35704517364502, weights = tensor([5.9336, 2.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №630: loss = 15.35181999206543, weights = tensor([5.9336, 2.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №631: loss = 15.346598625183105, weights = tensor([5.9336, 2.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №632: loss = 15.341389656066895, weights = tensor([5.9336, 2.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №633: loss = 15.336183547973633, weights = tensor([5.9336, 2.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №634: loss = 15.330988883972168, weights = tensor([5.9336, 2.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №635: loss = 15.325798034667969, weights = tensor([5.9297, 2.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №636: loss = 15.308258056640625, weights = tensor([5.9297, 2.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №637: loss = 15.303013801574707, weights = tensor([5.9297, 2.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №638: loss = 15.297780990600586, weights = tensor([5.9297, 2.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №639: loss = 15.29255199432373, weights = tensor([5.9297, 2.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №640: loss = 15.287335395812988, weights = tensor([5.9297, 2.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №641: loss = 15.282121658325195, weights = tensor([5.9297, 2.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №642: loss = 15.2769193649292, weights = tensor([5.9297, 2.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №643: loss = 15.271720886230469, weights = tensor([5.9297, 2.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №644: loss = 15.266534805297852, weights = tensor([5.9297, 2.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №645: loss = 15.261350631713867, weights = tensor([5.9297, 2.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №646: loss = 15.256179809570312, weights = tensor([5.9297, 2.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №647: loss = 15.251011848449707, weights = tensor([5.9297, 2.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №648: loss = 15.245855331420898, weights = tensor([5.9258, 2.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №649: loss = 15.22831916809082, weights = tensor([5.9258, 2.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №650: loss = 15.223109245300293, weights = tensor([5.9258, 2.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №651: loss = 15.217903137207031, weights = tensor([5.9258, 2.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №652: loss = 15.212709426879883, weights = tensor([5.9258, 2.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №653: loss = 15.207517623901367, weights = tensor([5.9258, 2.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №654: loss = 15.202339172363281, weights = tensor([5.9258, 2.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №655: loss = 15.197163581848145, weights = tensor([5.9258, 2.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №656: loss = 15.191999435424805, weights = tensor([5.9258, 2.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №657: loss = 15.18683910369873, weights = tensor([5.9258, 2.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №658: loss = 15.18169116973877, weights = tensor([5.9258, 2.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №659: loss = 15.176546096801758, weights = tensor([5.9258, 2.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №660: loss = 15.171412467956543, weights = tensor([5.9258, 2.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №661: loss = 15.166282653808594, weights = tensor([5.9219, 2.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №662: loss = 15.148757934570312, weights = tensor([5.9219, 2.3262], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №663: loss = 15.143574714660645, weights = tensor([5.9219, 2.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №664: loss = 15.138402938842773, weights = tensor([5.9219, 2.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №665: loss = 15.133234977722168, weights = tensor([5.9219, 2.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №666: loss = 15.128079414367676, weights = tensor([5.9219, 2.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №667: loss = 15.122926712036133, weights = tensor([5.9219, 2.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №668: loss = 15.117785453796387, weights = tensor([5.9219, 2.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №669: loss = 15.112648010253906, weights = tensor([5.9219, 2.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №670: loss = 15.107522964477539, weights = tensor([5.9219, 2.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №671: loss = 15.102399826049805, weights = tensor([5.9219, 2.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №672: loss = 15.0972900390625, weights = tensor([5.9219, 2.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №673: loss = 15.092183113098145, weights = tensor([5.9180, 2.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №674: loss = 15.074727058410645, weights = tensor([5.9180, 2.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №675: loss = 15.06956672668457, weights = tensor([5.9180, 2.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №676: loss = 15.064417839050293, weights = tensor([5.9180, 2.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №677: loss = 15.059272766113281, weights = tensor([5.9180, 2.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №678: loss = 15.054140090942383, weights = tensor([5.9180, 2.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №679: loss = 15.049009323120117, weights = tensor([5.9180, 2.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №680: loss = 15.043891906738281, weights = tensor([5.9180, 2.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №681: loss = 15.038777351379395, weights = tensor([5.9180, 2.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №682: loss = 15.033674240112305, weights = tensor([5.9180, 2.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №683: loss = 15.02857494354248, weights = tensor([5.9180, 2.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №684: loss = 15.02348804473877, weights = tensor([5.9180, 2.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №685: loss = 15.018404006958008, weights = tensor([5.9180, 2.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №686: loss = 15.013331413269043, weights = tensor([5.9141, 2.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №687: loss = 14.995878219604492, weights = tensor([5.9141, 2.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №688: loss = 14.990753173828125, weights = tensor([5.9141, 2.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №689: loss = 14.985630989074707, weights = tensor([5.9141, 2.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №690: loss = 14.980520248413086, weights = tensor([5.9141, 2.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №691: loss = 14.97541332244873, weights = tensor([5.9141, 2.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №692: loss = 14.970318794250488, weights = tensor([5.9141, 2.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №693: loss = 14.965227127075195, weights = tensor([5.9141, 2.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №694: loss = 14.9601469039917, weights = tensor([5.9141, 2.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №695: loss = 14.955070495605469, weights = tensor([5.9141, 2.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №696: loss = 14.950006484985352, weights = tensor([5.9141, 2.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №697: loss = 14.944944381713867, weights = tensor([5.9141, 2.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №698: loss = 14.939895629882812, weights = tensor([5.9141, 2.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №699: loss = 14.934849739074707, weights = tensor([5.9102, 2.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №700: loss = 14.91740894317627, weights = tensor([5.9102, 2.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №701: loss = 14.912309646606445, weights = tensor([5.9102, 2.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №702: loss = 14.907221794128418, weights = tensor([5.9102, 2.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №703: loss = 14.902137756347656, weights = tensor([5.9102, 2.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №704: loss = 14.897066116333008, weights = tensor([5.9102, 2.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №705: loss = 14.891996383666992, weights = tensor([5.9102, 2.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №706: loss = 14.886940002441406, weights = tensor([5.9102, 2.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №707: loss = 14.88188648223877, weights = tensor([5.9102, 2.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №708: loss = 14.87684440612793, weights = tensor([5.9102, 2.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №709: loss = 14.871806144714355, weights = tensor([5.9102, 2.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №710: loss = 14.866780281066895, weights = tensor([5.9102, 2.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №711: loss = 14.861757278442383, weights = tensor([5.9062, 2.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №712: loss = 14.844385147094727, weights = tensor([5.9062, 2.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №713: loss = 14.83930778503418, weights = tensor([5.9062, 2.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №714: loss = 14.834243774414062, weights = tensor([5.9062, 2.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №715: loss = 14.829182624816895, weights = tensor([5.9062, 2.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №716: loss = 14.824132919311523, weights = tensor([5.9062, 2.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №717: loss = 14.819087028503418, weights = tensor([5.9062, 2.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №718: loss = 14.814053535461426, weights = tensor([5.9062, 2.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №719: loss = 14.809022903442383, weights = tensor([5.9062, 2.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №720: loss = 14.804003715515137, weights = tensor([5.9062, 2.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №721: loss = 14.798988342285156, weights = tensor([5.9062, 2.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №722: loss = 14.793985366821289, weights = tensor([5.9062, 2.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №723: loss = 14.788984298706055, weights = tensor([5.9062, 2.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №724: loss = 14.78399658203125, weights = tensor([5.9023, 2.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №725: loss = 14.766627311706543, weights = tensor([5.9023, 2.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №726: loss = 14.76158618927002, weights = tensor([5.9023, 2.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №727: loss = 14.756547927856445, weights = tensor([5.9023, 2.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №728: loss = 14.751521110534668, weights = tensor([5.9023, 2.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №729: loss = 14.746498107910156, weights = tensor([5.9023, 2.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №730: loss = 14.741487503051758, weights = tensor([5.9023, 2.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №731: loss = 14.736478805541992, weights = tensor([5.9023, 2.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №732: loss = 14.731483459472656, weights = tensor([5.9023, 2.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №733: loss = 14.72649097442627, weights = tensor([5.9023, 2.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №734: loss = 14.72150993347168, weights = tensor([5.9023, 2.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №735: loss = 14.716532707214355, weights = tensor([5.9023, 2.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №736: loss = 14.711567878723145, weights = tensor([5.9023, 2.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №737: loss = 14.706605911254883, weights = tensor([5.8984, 2.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №738: loss = 14.689249038696289, weights = tensor([5.8984, 2.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №739: loss = 14.684232711791992, weights = tensor([5.8984, 2.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №740: loss = 14.679229736328125, weights = tensor([5.8984, 2.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №741: loss = 14.674229621887207, weights = tensor([5.8984, 2.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №742: loss = 14.669240951538086, weights = tensor([5.8984, 2.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №743: loss = 14.66425609588623, weights = tensor([5.8984, 2.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №744: loss = 14.659283638000488, weights = tensor([5.8984, 2.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №745: loss = 14.654314041137695, weights = tensor([5.8984, 2.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №746: loss = 14.6493558883667, weights = tensor([5.8984, 2.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №747: loss = 14.644401550292969, weights = tensor([5.8984, 2.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №748: loss = 14.639459609985352, weights = tensor([5.8984, 2.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №749: loss = 14.634519577026367, weights = tensor([5.8945, 2.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №750: loss = 14.617231369018555, weights = tensor([5.8945, 2.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №751: loss = 14.612238883972168, weights = tensor([5.8945, 2.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №752: loss = 14.607258796691895, weights = tensor([5.8945, 2.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №753: loss = 14.60228157043457, weights = tensor([5.8945, 2.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №754: loss = 14.597315788269043, weights = tensor([5.8945, 2.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №755: loss = 14.592353820800781, weights = tensor([5.8945, 2.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №756: loss = 14.587404251098633, weights = tensor([5.8945, 2.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №757: loss = 14.582456588745117, weights = tensor([5.8945, 2.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №758: loss = 14.577522277832031, weights = tensor([5.8945, 2.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №759: loss = 14.572590827941895, weights = tensor([5.8945, 2.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №760: loss = 14.567670822143555, weights = tensor([5.8945, 2.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №761: loss = 14.56275463104248, weights = tensor([5.8945, 2.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №762: loss = 14.55785083770752, weights = tensor([5.8906, 2.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №763: loss = 14.540565490722656, weights = tensor([5.8906, 2.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №764: loss = 14.535608291625977, weights = tensor([5.8906, 2.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №765: loss = 14.53065299987793, weights = tensor([5.8906, 2.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №766: loss = 14.525711059570312, weights = tensor([5.8906, 2.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №767: loss = 14.520771980285645, weights = tensor([5.8906, 2.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №768: loss = 14.515844345092773, weights = tensor([5.8906, 2.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №769: loss = 14.510920524597168, weights = tensor([5.8906, 2.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №770: loss = 14.506009101867676, weights = tensor([5.8906, 2.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №771: loss = 14.501100540161133, weights = tensor([5.8906, 2.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №772: loss = 14.496203422546387, weights = tensor([5.8906, 2.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №773: loss = 14.491310119628906, weights = tensor([5.8906, 2.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №774: loss = 14.486429214477539, weights = tensor([5.8906, 2.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №775: loss = 14.481550216674805, weights = tensor([5.8867, 2.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №776: loss = 14.464277267456055, weights = tensor([5.8867, 2.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №777: loss = 14.459345817565918, weights = tensor([5.8867, 2.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №778: loss = 14.454426765441895, weights = tensor([5.8867, 2.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №779: loss = 14.44951057434082, weights = tensor([5.8867, 2.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №780: loss = 14.444605827331543, weights = tensor([5.8867, 2.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №781: loss = 14.439704895019531, weights = tensor([5.8867, 2.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №782: loss = 14.434816360473633, weights = tensor([5.8867, 2.5605], dtype=torch.float16, requires_grad=True)\n",
      "step №783: loss = 14.429929733276367, weights = tensor([5.8867, 2.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №784: loss = 14.425056457519531, weights = tensor([5.8867, 2.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №785: loss = 14.420186042785645, weights = tensor([5.8867, 2.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №786: loss = 14.415327072143555, weights = tensor([5.8867, 2.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №787: loss = 14.41047191619873, weights = tensor([5.8828, 2.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №788: loss = 14.393267631530762, weights = tensor([5.8828, 2.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №789: loss = 14.388359069824219, weights = tensor([5.8828, 2.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №790: loss = 14.383462905883789, weights = tensor([5.8828, 2.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №791: loss = 14.378568649291992, weights = tensor([5.8828, 2.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №792: loss = 14.373687744140625, weights = tensor([5.8828, 2.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №793: loss = 14.368809700012207, weights = tensor([5.8828, 2.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №794: loss = 14.363943099975586, weights = tensor([5.8828, 2.5840], dtype=torch.float16, requires_grad=True)\n",
      "step №795: loss = 14.35908031463623, weights = tensor([5.8828, 2.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №796: loss = 14.354229927062988, weights = tensor([5.8828, 2.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №797: loss = 14.349382400512695, weights = tensor([5.8828, 2.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №798: loss = 14.3445463180542, weights = tensor([5.8828, 2.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №799: loss = 14.339714050292969, weights = tensor([5.8828, 2.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №800: loss = 14.334894180297852, weights = tensor([5.8789, 2.5957], dtype=torch.float16, requires_grad=True)\n",
      "step №801: loss = 14.317692756652832, weights = tensor([5.8789, 2.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №802: loss = 14.31281852722168, weights = tensor([5.8789, 2.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №803: loss = 14.307948112487793, weights = tensor([5.8789, 2.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №804: loss = 14.30309009552002, weights = tensor([5.8789, 2.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №805: loss = 14.298234939575195, weights = tensor([5.8789, 2.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №806: loss = 14.293391227722168, weights = tensor([5.8789, 2.6074], dtype=torch.float16, requires_grad=True)\n",
      "step №807: loss = 14.288551330566406, weights = tensor([5.8789, 2.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №808: loss = 14.283723831176758, weights = tensor([5.8789, 2.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №809: loss = 14.278898239135742, weights = tensor([5.8789, 2.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №810: loss = 14.274085998535156, weights = tensor([5.8789, 2.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №811: loss = 14.26927661895752, weights = tensor([5.8789, 2.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №812: loss = 14.26447868347168, weights = tensor([5.8789, 2.6191], dtype=torch.float16, requires_grad=True)\n",
      "step №813: loss = 14.259684562683105, weights = tensor([5.8750, 2.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №814: loss = 14.2424955368042, weights = tensor([5.8750, 2.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №815: loss = 14.237648010253906, weights = tensor([5.8750, 2.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №816: loss = 14.232812881469727, weights = tensor([5.8750, 2.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №817: loss = 14.22797966003418, weights = tensor([5.8750, 2.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №818: loss = 14.223159790039062, weights = tensor([5.8750, 2.6309], dtype=torch.float16, requires_grad=True)\n",
      "step №819: loss = 14.218342781066895, weights = tensor([5.8750, 2.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №820: loss = 14.213537216186523, weights = tensor([5.8750, 2.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №821: loss = 14.208735466003418, weights = tensor([5.8750, 2.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №822: loss = 14.203946113586426, weights = tensor([5.8750, 2.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №823: loss = 14.199159622192383, weights = tensor([5.8750, 2.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №824: loss = 14.194384574890137, weights = tensor([5.8750, 2.6426], dtype=torch.float16, requires_grad=True)\n",
      "step №825: loss = 14.189613342285156, weights = tensor([5.8711, 2.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №826: loss = 14.172492980957031, weights = tensor([5.8711, 2.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №827: loss = 14.167668342590332, weights = tensor([5.8711, 2.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №828: loss = 14.16285514831543, weights = tensor([5.8711, 2.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №829: loss = 14.158045768737793, weights = tensor([5.8711, 2.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №830: loss = 14.15324878692627, weights = tensor([5.8711, 2.6543], dtype=torch.float16, requires_grad=True)\n",
      "step №831: loss = 14.148454666137695, weights = tensor([5.8711, 2.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №832: loss = 14.143671989440918, weights = tensor([5.8711, 2.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №833: loss = 14.138893127441406, weights = tensor([5.8711, 2.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №834: loss = 14.134126663208008, weights = tensor([5.8711, 2.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №835: loss = 14.129362106323242, weights = tensor([5.8711, 2.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №836: loss = 14.124610900878906, weights = tensor([5.8711, 2.6660], dtype=torch.float16, requires_grad=True)\n",
      "step №837: loss = 14.11986255645752, weights = tensor([5.8711, 2.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №838: loss = 14.11512565612793, weights = tensor([5.8672, 2.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №839: loss = 14.09800910949707, weights = tensor([5.8672, 2.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №840: loss = 14.093218803405762, weights = tensor([5.8672, 2.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №841: loss = 14.088432312011719, weights = tensor([5.8672, 2.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №842: loss = 14.083658218383789, weights = tensor([5.8672, 2.6777], dtype=torch.float16, requires_grad=True)\n",
      "step №843: loss = 14.078886032104492, weights = tensor([5.8672, 2.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №844: loss = 14.074127197265625, weights = tensor([5.8672, 2.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №845: loss = 14.069371223449707, weights = tensor([5.8672, 2.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №846: loss = 14.064626693725586, weights = tensor([5.8672, 2.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №847: loss = 14.05988597869873, weights = tensor([5.8672, 2.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №848: loss = 14.055157661437988, weights = tensor([5.8672, 2.6895], dtype=torch.float16, requires_grad=True)\n",
      "step №849: loss = 14.050432205200195, weights = tensor([5.8672, 2.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №850: loss = 14.0457181930542, weights = tensor([5.8672, 2.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №851: loss = 14.041007995605469, weights = tensor([5.8633, 2.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №852: loss = 14.023902893066406, weights = tensor([5.8633, 2.6973], dtype=torch.float16, requires_grad=True)\n",
      "step №853: loss = 14.019139289855957, weights = tensor([5.8633, 2.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №854: loss = 14.014387130737305, weights = tensor([5.8633, 2.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №855: loss = 14.009638786315918, weights = tensor([5.8633, 2.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №856: loss = 14.004902839660645, weights = tensor([5.8633, 2.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №857: loss = 14.00016975402832, weights = tensor([5.8633, 2.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №858: loss = 13.995448112487793, weights = tensor([5.8633, 2.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №859: loss = 13.990730285644531, weights = tensor([5.8633, 2.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №860: loss = 13.986024856567383, weights = tensor([5.8633, 2.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №861: loss = 13.981321334838867, weights = tensor([5.8633, 2.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №862: loss = 13.976631164550781, weights = tensor([5.8633, 2.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №863: loss = 13.971943855285645, weights = tensor([5.8594, 2.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №864: loss = 13.954907417297363, weights = tensor([5.8594, 2.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №865: loss = 13.950166702270508, weights = tensor([5.8594, 2.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №866: loss = 13.94543743133545, weights = tensor([5.8594, 2.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №867: loss = 13.940711975097656, weights = tensor([5.8594, 2.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №868: loss = 13.935998916625977, weights = tensor([5.8594, 2.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №869: loss = 13.93128776550293, weights = tensor([5.8594, 2.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №870: loss = 13.926589965820312, weights = tensor([5.8594, 2.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №871: loss = 13.921895027160645, weights = tensor([5.8594, 2.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №872: loss = 13.917211532592773, weights = tensor([5.8594, 2.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №873: loss = 13.912531852722168, weights = tensor([5.8594, 2.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №874: loss = 13.907864570617676, weights = tensor([5.8594, 2.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №875: loss = 13.903200149536133, weights = tensor([5.8594, 2.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №876: loss = 13.898547172546387, weights = tensor([5.8555, 2.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №877: loss = 13.881513595581055, weights = tensor([5.8555, 2.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №878: loss = 13.876808166503906, weights = tensor([5.8555, 2.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №879: loss = 13.872105598449707, weights = tensor([5.8555, 2.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №880: loss = 13.867414474487305, weights = tensor([5.8555, 2.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №881: loss = 13.862727165222168, weights = tensor([5.8555, 2.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №882: loss = 13.858052253723145, weights = tensor([5.8555, 2.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №883: loss = 13.85338020324707, weights = tensor([5.8555, 2.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №884: loss = 13.848719596862793, weights = tensor([5.8555, 2.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №885: loss = 13.844062805175781, weights = tensor([5.8555, 2.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №886: loss = 13.839418411254883, weights = tensor([5.8555, 2.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №887: loss = 13.834775924682617, weights = tensor([5.8555, 2.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №888: loss = 13.830146789550781, weights = tensor([5.8555, 2.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №889: loss = 13.825520515441895, weights = tensor([5.8516, 2.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №890: loss = 13.808499336242676, weights = tensor([5.8516, 2.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №891: loss = 13.80381965637207, weights = tensor([5.8516, 2.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №892: loss = 13.799151420593262, weights = tensor([5.8516, 2.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №893: loss = 13.794486999511719, weights = tensor([5.8516, 2.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №894: loss = 13.789834976196289, weights = tensor([5.8516, 2.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №895: loss = 13.785184860229492, weights = tensor([5.8516, 2.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №896: loss = 13.780548095703125, weights = tensor([5.8516, 2.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №897: loss = 13.775914192199707, weights = tensor([5.8516, 2.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №898: loss = 13.771291732788086, weights = tensor([5.8516, 2.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №899: loss = 13.76667308807373, weights = tensor([5.8516, 2.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №900: loss = 13.762066841125488, weights = tensor([5.8516, 2.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №901: loss = 13.757463455200195, weights = tensor([5.8477, 2.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №902: loss = 13.740510940551758, weights = tensor([5.8477, 2.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №903: loss = 13.73585319519043, weights = tensor([5.8477, 2.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №904: loss = 13.731208801269531, weights = tensor([5.8477, 2.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №905: loss = 13.726567268371582, weights = tensor([5.8477, 2.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №906: loss = 13.72193717956543, weights = tensor([5.8477, 2.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №907: loss = 13.717310905456543, weights = tensor([5.8477, 2.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №908: loss = 13.71269702911377, weights = tensor([5.8477, 2.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №909: loss = 13.708086013793945, weights = tensor([5.8477, 2.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №910: loss = 13.703486442565918, weights = tensor([5.8477, 2.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №911: loss = 13.698890686035156, weights = tensor([5.8477, 2.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №912: loss = 13.694307327270508, weights = tensor([5.8477, 2.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №913: loss = 13.689725875854492, weights = tensor([5.8477, 2.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №914: loss = 13.685157775878906, weights = tensor([5.8438, 2.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №915: loss = 13.668208122253418, weights = tensor([5.8438, 2.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №916: loss = 13.663586616516113, weights = tensor([5.8438, 2.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №917: loss = 13.658967971801758, weights = tensor([5.8438, 2.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №918: loss = 13.6543607711792, weights = tensor([5.8438, 2.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №919: loss = 13.649757385253906, weights = tensor([5.8438, 2.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №920: loss = 13.645166397094727, weights = tensor([5.8438, 2.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №921: loss = 13.64057731628418, weights = tensor([5.8438, 2.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №922: loss = 13.636001586914062, weights = tensor([5.8438, 2.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №923: loss = 13.631428718566895, weights = tensor([5.8438, 2.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №924: loss = 13.626867294311523, weights = tensor([5.8438, 2.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №925: loss = 13.622309684753418, weights = tensor([5.8438, 2.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №926: loss = 13.617764472961426, weights = tensor([5.8438, 2.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №927: loss = 13.613222122192383, weights = tensor([5.8398, 2.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №928: loss = 13.596284866333008, weights = tensor([5.8398, 2.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №929: loss = 13.59168815612793, weights = tensor([5.8398, 2.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №930: loss = 13.587104797363281, weights = tensor([5.8398, 2.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №931: loss = 13.582524299621582, weights = tensor([5.8398, 2.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №932: loss = 13.57795524597168, weights = tensor([5.8398, 2.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №933: loss = 13.573390007019043, weights = tensor([5.8398, 2.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №934: loss = 13.56883716583252, weights = tensor([5.8398, 2.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №935: loss = 13.564287185668945, weights = tensor([5.8398, 2.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №936: loss = 13.559748649597168, weights = tensor([5.8398, 2.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №937: loss = 13.555213928222656, weights = tensor([5.8398, 2.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №938: loss = 13.550691604614258, weights = tensor([5.8398, 2.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №939: loss = 13.546171188354492, weights = tensor([5.8359, 2.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №940: loss = 13.529302597045898, weights = tensor([5.8359, 2.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №941: loss = 13.52472972869873, weights = tensor([5.8359, 2.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №942: loss = 13.520169258117676, weights = tensor([5.8359, 2.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №943: loss = 13.51561164855957, weights = tensor([5.8359, 2.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №944: loss = 13.511065483093262, weights = tensor([5.8359, 2.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №945: loss = 13.506523132324219, weights = tensor([5.8359, 2.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №946: loss = 13.501993179321289, weights = tensor([5.8359, 2.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №947: loss = 13.497465133666992, weights = tensor([5.8359, 2.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №948: loss = 13.492950439453125, weights = tensor([5.8359, 2.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №949: loss = 13.488438606262207, weights = tensor([5.8359, 2.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №950: loss = 13.483938217163086, weights = tensor([5.8359, 2.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №951: loss = 13.47944164276123, weights = tensor([5.8359, 2.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №952: loss = 13.474957466125488, weights = tensor([5.8320, 2.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №953: loss = 13.458091735839844, weights = tensor([5.8320, 2.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №954: loss = 13.453554153442383, weights = tensor([5.8320, 2.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №955: loss = 13.449018478393555, weights = tensor([5.8320, 2.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №956: loss = 13.444496154785156, weights = tensor([5.8320, 2.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №957: loss = 13.439976692199707, weights = tensor([5.8320, 2.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №958: loss = 13.435468673706055, weights = tensor([5.8320, 2.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №959: loss = 13.430964469909668, weights = tensor([5.8320, 2.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №960: loss = 13.426472663879395, weights = tensor([5.8320, 2.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №961: loss = 13.42198371887207, weights = tensor([5.8320, 2.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №962: loss = 13.417506217956543, weights = tensor([5.8320, 2.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №963: loss = 13.413032531738281, weights = tensor([5.8320, 2.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №964: loss = 13.408571243286133, weights = tensor([5.8320, 2.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №965: loss = 13.404111862182617, weights = tensor([5.8281, 2.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №966: loss = 13.387258529663086, weights = tensor([5.8281, 2.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №967: loss = 13.382746696472168, weights = tensor([5.8281, 2.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №968: loss = 13.378247261047363, weights = tensor([5.8281, 2.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №969: loss = 13.373750686645508, weights = tensor([5.8281, 2.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №970: loss = 13.36926555633545, weights = tensor([5.8281, 2.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №971: loss = 13.364784240722656, weights = tensor([5.8281, 2.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №972: loss = 13.360315322875977, weights = tensor([5.8281, 2.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №973: loss = 13.35584831237793, weights = tensor([5.8281, 2.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №974: loss = 13.351394653320312, weights = tensor([5.8281, 2.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №975: loss = 13.346943855285645, weights = tensor([5.8281, 2.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №976: loss = 13.342504501342773, weights = tensor([5.8281, 2.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №977: loss = 13.338068962097168, weights = tensor([5.8242, 2.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №978: loss = 13.321284294128418, weights = tensor([5.8242, 2.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №979: loss = 13.316795349121094, weights = tensor([5.8242, 2.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №980: loss = 13.312318801879883, weights = tensor([5.8242, 2.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №981: loss = 13.307844161987305, weights = tensor([5.8242, 2.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №982: loss = 13.303382873535156, weights = tensor([5.8242, 2.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №983: loss = 13.298924446105957, weights = tensor([5.8242, 2.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №984: loss = 13.294477462768555, weights = tensor([5.8242, 2.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №985: loss = 13.290034294128418, weights = tensor([5.8242, 2.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №986: loss = 13.285603523254395, weights = tensor([5.8242, 2.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №987: loss = 13.28117561340332, weights = tensor([5.8242, 2.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №988: loss = 13.276759147644043, weights = tensor([5.8242, 2.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №989: loss = 13.272346496582031, weights = tensor([5.8242, 2.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №990: loss = 13.267946243286133, weights = tensor([5.8203, 2.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №991: loss = 13.251164436340332, weights = tensor([5.8203, 2.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №992: loss = 13.246709823608398, weights = tensor([5.8203, 2.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №993: loss = 13.24225902557373, weights = tensor([5.8203, 2.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №994: loss = 13.237820625305176, weights = tensor([5.8203, 2.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №995: loss = 13.23338508605957, weights = tensor([5.8203, 2.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №996: loss = 13.228960990905762, weights = tensor([5.8203, 2.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №997: loss = 13.224540710449219, weights = tensor([5.8203, 2.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №998: loss = 13.220132827758789, weights = tensor([5.8203, 2.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №999: loss = 13.215726852416992, weights = tensor([5.8203, 2.9844], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#now let's use PyTorch gradient calculation. We don't need our gradient function. \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16, requires_grad = True) #it should be mentioned that in the future \n",
    "                                                            #you will need a derivative with respect to these values\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = mseerror(y, y_pred)\n",
    "    error.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rait*w.grad\n",
    "    w.grad.zero_()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193b97bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Derivative is calculated by Pytorch. Slope = 5.8203125, intercept = 2.984375, loss = 13.215726852416992')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAEJCAYAAAB40EgyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABREklEQVR4nO3dd1xV9f8H8Ndd3IuCqAgOxC0uHLgRFVegIgJq5sLVsDIt65epOcoyR30brrJylA0tDTX3VhS3JqKiOQBRlux15/n8/iBvkjJVLuP1fDx6JPee8b6fe8+5r/M559yPTAghQERERFQByS1dABEREZGlMAgRERFRhcUgRERERBUWgxARERFVWAxCREREVGExCBEREVGFlW8Qio6ORosWLeDn5wc/Pz/4+vpixIgR2LlzZ7FW9vLLL+PGjRvFmjc0NBRz584FAFy6dAlTp04t1nL+69dff8W33377VJaVlz59+uDSpUvFmnfZsmWYP39+gdNNnDgRSUlJRVr2pUuX0KdPn0cej46OhpubW5GW9V/NmjWDr68v/Pz84O/vj0GDBmHVqlUFzlec15GXU6dOYdCgQUWeLyMjA7Nnz4avry8GDx4Mf39//P777+bnn+T9fFaGDBmCgQMHmrfV77///rHTLVu2DAMGDMCgQYPw3nvvQafTAQAiIyMxYcIE+Pn5YeDAgVizZo15nsOHD8PX1xfe3t6YOnUqMjIyAADp6emYOnUqBg0ahIEDBz52O9q0aRNeffXVXI+tWbMGPj4+GDx4MMaPH4+oqCjz8lq3bm1+DX5+fjh58mS+r7uw+4Lly5dj//79BU73LN25cwdTpkx54uXk1X4PE0Lgiy++gJeXF/z8/PDBBx+Y3+vMzEy8+eab5vdt9erVj8wfGhoKV1dX87ao1+sxd+5cDBw4EAMHDsSiRYtgMpmQlpaW6/3y8/NDixYtsHbtWgDAokWL0KtXL/Nzb731VoGvLzAwELt3736CFnr6wsPD0b1791yP7dq1C4MHD4avry/Gjh2LiIiIx84bHByMIUOGwM/PDwEBATh27Fiu59PS0uDr65trn3Lt2jW4ubnlatdbt25hy5YtuR7r06cPWrVqhfv370Or1WLmzJkYNGgQfHx8MHPmTGi1WgBASkoK3nnnHfj7+6N///7YsmWLeV0bNmyAj48PfH198dprr5nf8/y2x4JeEwAsWLAAkyZNeuTxY8eOwc/PL9dj165dQ2BgIPz9/TFkyBCEhYWZ637rrbfg7e2NgIAArF+/3jzPyZMnERAQAF9fXwQGBiI8PNz8XGG2kVxEPu7cuSPatWuX67Ho6GjRr18/sXv37vxmfeo2b94sXnnllRJd59PSu3dvERoaWqx5ly5dKj788MMCp3NxcRGJiYlFWnZoaKjo3bv3I48/7n0vqv/Wk56eLvr06SMOHjxYpPmexMmTJ4WPj0+R5/vggw/EggULhCRJQgghYmNjhaenpwgODhZCPNn7+SxkZmaKDh06CL1en+90J0+eFN7e3iI7O1tIkiRef/118d133wkhhBgxYoT47bffhBBCpKWlCS8vLxESEiISExNF165dxe3bt4UQQixZskTMmzdPCCHERx99JD7++GNzDb179xbnz58XQgiRnJws5syZI9q1a5druz1+/LgYMGCASE9PF0II8dNPP4lRo0YJIYQ4evSomDBhwtNplP8YM2aM2LVr1zNZdmEV9/P4sPza72GbNm0SgwcPFqmpqUIIIZYvXy4WLVokhMjZp0yfPl0IkbNd9uzZU1y8eNE8b2JioggICMi1La5Zs0ZMnjxZmEwmYTAYxPDhw8Wff/75yHp//PFHMXr0aPNncfjw4eLcuXNFeo2l4b16wGAwiLVr14pu3brl2ifGx8eLTp06iZiYGCGEEOvXrxcTJ058ZP60tDTRuXNncf36dSGEEFevXhUdOnQwv3+HDx8WXl5eolWrVrn2Kb/++quYPXt2vrXp9XoxfPhw8euvvwohhPj888/Fu+++K0wmkzAajWLatGniyy+/FEIIMWnSJLFkyRIhhBAxMTGiY8eOIiYmRkRFRYnOnTuLpKQkIUTONv3BBx8IIfLeHgt6TUIIsWPHDtGlS5dc2352drb4/PPPRceOHXNtB1lZWcLDw0McPnxYCCHEvn37hLe3txBCiOnTp4uZM2cKo9EodDqdeOmll8TBgwdFWlqa6NixowgJCRFCCHHjxg3h5eUldDpdobeRhynzj0mPcnJywtSpU7F69Wp4e3tDr9fjs88+w5kzZ2AymdCyZUvMnj0bNjY26NOnD9q0aYNr167h7bffxsKFC/HVV19h3bp1aNWqFSZOnAgA+OWXX3D69Gl8/vnn+OSTT3Dx4kVkZmZCCIGPP/4YderUwdKlS5Geno6ZM2fC398fH330EX799Vd4enpiz549cHBwAAA8//zzeOONN+Du7p5nXQ9btmwZkpOTMXfuXPzyyy/YsGEDVCoV1Go15s+fjyZNmuSa3mg04tNPP8Xhw4ehUCjg5uaGefPmIS0tDXPnzkViYiISEhLg5OSEL7/8Evb29rnm37RpE9auXQu5XI5q1aph8eLFiIqKwkcffYTt27cDyOnJePjvBw4dOoRVq1ZBr9cjKSkJ/v7+eOuttzBz5kwAwLhx4/Dtt99CLpdj/vz5iImJgcFggI+Pj/mo/JdffsEPP/wAGxsbuLi45Pk+S5KE999/H5cvX4ZSqcTs2bPRtm1b9O/fH3PnzoWHhwcA4P3334eLiwvGjRuX7+fGxsYGrq6uuHXrFg4cOAB7e3tMmzYNALB161bs3bsXVapUyfU6MjIyMH/+fKSkpEAmk2HixInw9/fHqVOnsGDBAlSqVAmZmZnYvHkztm3b9ki7AkBWVhamTZuGW7duQafT4eOPP0bHjh3zrTUhIQH29vYwGAywsrJCzZo1sWzZMlStWvWRaTdu3Ij169dDLpejRo0amDNnDho2bIgZM2ZArVYjPDwciYmJ8PDwwOzZs6FSqXDz5k0sWLAAKSkpMJlMCAwMxLBhwx5Z9ogRI5CdnZ3rsfbt22PevHm5HgsNDUWlSpXw0ksvISkpCe7u7nj77beh0WhyTSdJEvR6PbRaLeRyOXQ6HdRqNQBg2LBhGDhwIADA1tYW9erVw71795CQkIDWrVujQYMGAICRI0fCz88P8+bNw/vvvw+TyWRuM71eD1tbWwA5R8uOjo547733cOjQIXMNNWrUwAcffGDeDlu3bm3uvbpw4QJSUlIwfPhw6PV6DB8+HKNGjcr3vXp4W5kxYwZsbGxw7do1xMbGolmzZli8eDG2bNmCsLAwLFmyBAqFAp6enoXeZzVp0gRz585FUlIS5HI5XnvtNQwcOBBxcXGP3caio6MRGBiIHj164OLFixBCYO7cuXBzc8Ps2bMRFxeHF1988ZFemKlTpyIyMjLXY3Xr1sWKFStyPZZf+z3s8uXL6Nevn3mb8vLywqRJk/Dee+/BZDIhMzMTRqMROp0OkiTBysrK/Bl59913MW3aNLz00kvm5U2YMAFjxoyBXC5HUlIS0tLSYGdnl2udkZGR+Prrr7Fp0yaoVCro9XpcuXIF33//Pe7cuYMGDRpg5syZqFOnTr7v6cP279+P5cuXQ5IkVK5cGTNnzkSbNm1w8+ZNvP/++9Dr9RBCYNiwYRg9enSejz/sxo0beOeddx5Z19ixYzF06NBcj125cgXXrl3D8uXLzd9XAODg4IDjx49DpVLBaDTi7t27j90/GAwGzJs3D02bNgUANGnSBEIIJCcnw8bGBj/++CM+/fTTR3rKLly4gDt37iAgIAAKhQKvvPIKvLy8ck3z3XffoXr16hgxYgQAoFOnTnBycoJcnnOip0WLFrhx4wZSUlIQEhKCL774AgBQq1Yt/Pbbb7Czs0N8fDyMRiMyMzNhZ2cHrVZr/mzltT0W9Jpu3ryJ77//HpMnT87VU3Ts2DFkZ2dj0aJF5loA4Pjx43B2doanpycAoG/fvqhbty6AnM/xnDlzoFAooFAo0KtXL+zZswc1atSAra0t3N3dAQCNGzeGjY0NLly4UOhtJJf8UlJePQPXr18Xbdu2FUIIsWzZMrFo0SLz0fP//vc/8xFj7969xfLly83zPTiSPnHihBg0aJD58WHDhonjx4+L8+fPiylTpgiTySSEEGLVqlVi0qRJQojcPUIPH1lNnz5dfP/99+ZU2KtXL2EymfKt62EPelyMRqNo1aqViIuLE0IIERQUJDZs2PDI9D/88IMYPXq0yM7OFiaTSbz55psiKChIrFu3TqxatUoIIYQkSeKll14Sq1evzvW6r169Krp06SLu3bsnhBBi7dq1Ys6cOY8cKT7894P6JEkSY8aMMR+Zx8bGihYtWpiP2B4+egsMDBQHDhwQQgih1WpFYGCg2LFjh7hy5Ypwd3cX8fHxQggh5syZk2ePkIuLi9ixY4cQQojg4GDRs2dPodPpxNq1a8XUqVOFEDlHk127djUfdT7svz07N2/eFO7u7uLixYviypUrwsPDQxgMBiGEEKNGjRJHjx7NNZ/BYBB9+/YVe/bsMb/eHj16iPPnz4uTJ0+K5s2bi+joaCGEyLddW7RoIf766y/z42PHjn2k1v+6evWq8PLyEm5ubmLixIli+fLl4tatW+bnH7yfISEhol+/fubXuXnzZjFgwAAhSZJ47733hL+/v8jIyBA6nU6MHj1arF+/XhgMBjFw4EARFhYmhMg5uhowYIC4cOFCgXXlZf/+/eL//u//RHJystBqteKNN94w99T81/vvvy/atWsnOnXqJIYPHy50Ot0j0xw5ckR06NBBxMXFiVWrVok5c+aYnzMYDMLFxSXX0d8777wjXF1dxbRp04TRaMy1rPx6cnU6nQgMDDT3VCxfvlwsW7ZM6HQ6ERsbK7y8vMS+ffvyfe0PbyvvvfeeeOGFF4ROpxN6vV74+/uLTZs2CSFy9zIUZZ/l7+8vfvrpJyGEEPfu3RN9+/YV6enpeW5jD7adbdu2CSFyjvg9PDyEXq9/Kj1CD/tv+z0sKChI+Pv7i8TERGEymcSSJUtEq1athBA5262/v7/o2rWrcHV1FQsXLjTP9/nnn5t7ER7XO/vpp5+Kdu3aiTFjxoisrKxcz7355ptixYoV5r+joqLESy+9JK5duyYkSRLfffed8PPzM7d7Xh68Vzdu3BDdunUTUVFRQgghQkJChIeHh0hPTxczZ84072/j4+PFW2+9JUwmU56PP6m8vgtDQ0NFt27dRPv27c29ofn53//+J4YMGfLI4//tZZ43b5746aefhNFoFDdu3BBdu3bN9XxiYqLo2LGjuW3+Kzo6Wnh4eIiDBw+Kixcvij59+ogVK1aIF154QQQEBIjt27ebp12xYoVo1aqVcHd3F15eXubeocJujw+/poyMDBEQECCuXbuW57b/3+3g22+/FVOmTBEzZ84UAQEBYty4ceb948yZM8XMmTOFXq8XGRkZIjAwUEycOFGkp6eLLl26mHvpL168KNq0afNIL2V+28jDitwjBAAymcx8tHn48GGkp6cjJCQEQE4CfrgX5HFH3126dIFOp8OlS5dgbW1tPoqVyWSws7PDhg0bcOfOHZw6dQqVK1fOt5bnn38eH374IV588UVs3rwZQ4cOhVwuL7Cu/1IoFOjfvz9GjBiBXr16oXv37uaE+rCQkBD4+fmZX/+XX35pfu7s2bNYu3YtIiIi8Pfff6Nt27a55j1x4gS6d++O2rVrAwDGjx8PIOeotiAymQzffPMNDh8+jO3bt+PmzZsQQjzSY5CVlYUzZ84gNTUVX331lfmx8PBwxMbGwsPDw9x79sILLzz23C4AVKlSxdxD8ODc+K1btzBkyBCsWLECSUlJ2L17N3r16mU+6vyvcePGQS6XQ5IkWFtbY/r06WjTpg2AnKPdw4cPo2HDhoiPj3/k/HtERAR0Op35KKhmzZrw8vJCcHAwunTpgtq1a8PJyanAdnV2dja/D82bN8fmzZsLbOvmzZtj9+7duHz5Ms6cOYPjx4/jm2++wVdffZXrmqrg4GAMHDgQ1atXB5Bznc6CBQsQHR0NAAgICDB/fv38/HDgwAF07doVUVFRmDVrlnk5Wq0WV65cQbt27XLVUdgeob59+6Jv377mvydNmoQpU6bg/fffzzXdpk2bEB0djeDgYFhZWWHmzJlYvHgx5syZY55my5YtWLhwIZYuXQpHR0dIkgSZTPZIGz046gSAzz77DB9++CGmTp2KFStWFOqanaSkJEydOhU2NjbmnsHJkyebn69ZsyZeeOEF7Nu3D/369StweQ/06NHD3Lvh4uKC1NTUR6Yp7D4rJSUF4eHheP755wEAtWvXxv79+/Pdxtq0aQM7Ozv4+voCADw9PaFQKHDt2rV86y5sj9ADj2u/h/n7+yMuLg7jxo1DpUqVMHz4cKhUKgDA/Pnz4eHhgbfffhv379/HhAkT4ObmBrVajdDQ0MdeM/TA//3f/+HNN9/EnDlz8MEHH5h7XmNiYnDs2DF8/PHH5mmdnZ3x3Xffmf9+8cUXsXLlSkRHR8PZ2Tnf9gByrgHp2rWreVp3d3dUr14dYWFheO655/Dee+8hNDQU7u7umD17NuRyeZ6PP6woPUIFad26NY4fP46jR49i0qRJ2L9//2P3h0ajEYsWLcLRo0exbt26Apf7wQcfmP/duHFjDBw4EIcOHULr1q0BAL/99hv69u372HYMCwvDG2+8gTFjxqB37944d+4coqOjYWNjgw0bNiAyMhKjR49G/fr1kZKSgr179+LIkSOoVq0aPv30U8ycORPffPNNgdvj417T+++/j8DAQLi4uJiv8ymI0WjEkSNH8OOPP6Jt27bYv38/XnnlFRw6dAgzZszA4sWLERAQgBo1asDDwwMXLlyAjY0NVqxYgS+//BJLlixBp06d0LVrV/NnHCh4G3lYsYLQpUuXzKdVJEnCrFmzzKEhMzPTfFEeAFSqVOmR+WUyGYYNG4atW7dCpVJh2LBhkMlkOHz4MBYsWIAJEyagb9++aNSoEbZt25ZvLR07doTRaERoaCi2b9+OjRs3Fqqux/nss89w/fp1hISE4Ntvv8XWrVvNO7oHlMrcTXb//n1IkoQffvgBoaGhGDp0KLp06QKj0Qjxn2HcFApFri8VrVaLu3fvQiaT5ZrWYDA8UltWVhYCAgLQr18/dOzYEUOHDsX+/fsfWYckSRBCYMOGDbC2tgaQ84FQq9XYuHFjrukVCkWebfHfnYckSVCpVKhSpQr69++Pbdu24c8//3zkS/lhP/zwgzkk/Nfo0aOxefNmNGjQAMOHD3/ky9ZkMj3ymBACRqMRQO7PVV7tCiDXhvHfdn4co9GI+fPn4+2334arqytcXV0xYcIErFy5Ehs3bswVhCRJemT+h2t8uH2FEJDL5TCZTLC1tcXWrVvNz92/f998SulhGzZsyLfWBw4ePAhbW1t06tTJvK7/fk4BYN++ffD19TV3GQ8fPhwfffSReZ7Fixdjz549WLduHVq0aAEg58v/4sWL5mXExcXBzs4OlSpVQnBwMFxcXFCzZk1UrlwZPj4+2Lt3b4H1hoeH4/XXX0e/fv3w3nvvmdtp/fr16Nu3r/nUSV6vIz8Pnw7M6/0u7D7rwbof/mzdunULDg4OeW5jycnJj2xXkiTlu60BwNKlSwv9GvNqv4elpKRg0KBB5otVz58/j/r16wPI+Rxs27YNcrkcjo6O6N+/P06dOoWEhATExsYiICDAvJxx48bhk08+gV6vR/Xq1dGwYUOoVCoEBATkCj179uzBc889l+vSg/DwcISHh8Pf39/8mBAi1zaZn8eF8AfbV+/evbFnzx6EhITgxIkTWLFiBf744488H69Vq5Z5GU2aNMm1/RVHXFwcrl+/jh49egAAevbsCRsbG0RFRcHV1TXXtKmpqZg6dSqEENi4cSOqVauW77JNJhO+/fZbBAYGmtvzv9vCzp07MXv27Efm3bFjBz788EPMmTPHHMYdHR0B5ByoAUD9+vXRvn17hIaG4saNG+jTp4/5QGD06NHm+fLbHh/3mmJjY3H27Fncvn0b69atQ2pqKtLT0/Hyyy/nCsT/5ejoiMaNG5sPWPv164fZs2fjzp07sLa2xrvvvms+7fjNN9+gXr165lOlD1887e3tbf6MF2YbeViRb5+/ffs2Vq5caT5f2r17d/z888/Q6/WQJAlz5szB559/XuByAgICcPDgQezZs8f8Bh0/fhy9e/fGqFGj4Orqiv3795uvQVAoFOYvmP96/vnn8dFHH6FZs2bmXoGi1pWUlARPT09UrVoV48ePx1tvvfXYO4Pc3d2xfft283I/+OAD7NixA8eOHcO4cePg7+8Pe3t7hISEmGt/oEuXLjhx4gTi4+MB5HzRffrpp6hevTru3buHxMRECCGwY8eOR9YbGRmJjIwMvPXWW+jTpw9OnTplruHh9rGxsUG7du3Md22kpaVh5MiROHDgADw8PHD8+HHExsYCAIKCgvJsj5SUFPO1HQcPHoRGozF/yEaPHo0ff/wRQghzD09ReXt74+rVq9izZ0+uo7AHr6NRo0ZQKpXmL9a4uDjs2bMH3bp1e2RZebVrcSiVSvNn/EEgNRqNuHnzJlq2bJlr2h49emDnzp3muyw2b96MqlWrmttp165d0Ov10Ol0CAoKQu/evdGwYUNoNBrzjjgmJgaDBg0q9NHT48TGxmLx4sXQarUwmUxYt26duTfvYS1btsS+ffvMIX3fvn3mnc+SJUtw5swZbN682RyCgJzt6OLFi+Y7YjZs2GDufdq1axdWrFgBIQT0ej127dqFrl27FljruHHj8Prrr2PWrFm5dlDnzp0z90akpKRg06ZNj30dxfHw/qOw+wYbGxu0atXKfIdNTEwMRo4cCa1Wm+c2BuTsS44ePQogZ9tRqVRwcXGBQqF47EFOUeTXfg970CtgMBhgNBrx7bffmr/gWrZsiV27dgHIOcAKDg5G27ZtsWzZMuzatQtbt241fz5/+OEHtG7dGidPnsTChQthNBohSRL+/PNPdOnSxby+06dPP/Ley+VyLFiwAHfu3AGQc31is2bNcoWS/Li7u+PYsWPm+U+cOIGYmBi0bdsW77zzDnbu3AkfHx/MmzfPHELyevxp0+v1ePvtt829eCdPnoTRaETjxo1zTWcymfDKK6+gbt26WLNmTYEhCMj5rB48eBC//fYbAODu3bvYu3cvvL29AeSEkKioqEfu7D148CA+/vhjrF692vxeAzk9cw9/ju/fv48LFy7A1dUVLVu2xOHDh5GZmQkA2Lt3r3mfkNf2mNdrqlWrFo4dO2b+/EydOhUdO3bMNwQBOSEyOjravA88c+YMZDIZ6tatiw0bNpgPEu7fv4/ff/8dgwYNgkwmw8svv2z+jt65cyesrKzQrFmzQm8jDyvwcEur1ZpvdZPL5VCr1Xj77bfRq1cvAMDrr79u7roymUxo0aIFZsyYUeCKHRwc0LJlSxiNRtSsWRNAzqmAd955B76+vjAajfDw8MDevXshSRLatWuHFStW4I033kBgYGCuZfn7++Pzzz/PtTMral3Vq1fHa6+9hvHjx0Oj0UChUOQ64nlgxIgRuHv3LoYMGQIhBDp37ozAwEA4OTlhyZIl+Oqrr6BSqdC+fftHNsBmzZrh3XffNV+E6ODggE8++QQ1a9bEiBEjMHToUDg4OKBXr16PhLBmzZqhV69eGDBgAKysrODi4oImTZogMjIS9erVQ//+/REYGIhly5bhs88+w0cffQRfX1/o9XoMGjQIgwcPBgC8++67GDduHCpXrpxviLG3t8fevXvx5ZdfwtraGsuWLTMfDTRv3hx2dnbmi/SKw8rKCt7e3rh//36uXqOHX8fKlSvx8ccfY9myZTCZTJg8eTK6du36yKnEvNo1r9tZgZxezdmzZz/2yPCrr77Cp59+Cm9vb1hbW0OSJDz33HO5uooBwMPDA+PHj8e4ceMgSRKqV6+OVatWmXvTNBoNRo0ahbS0NHh7e5tP265cuRILFizA999/D6PRiDfffBMdOnQoblNixIgR5gsrTSYTunTpYq71119/RVhYGBYsWIBXX30VCxcuhI+Pj3mnMW/ePMTGxmLdunWoXbs2JkyYYF7ug1MFCxcuxNSpU2EwGFCvXj3z6ZAZM2Zg3rx55p1uv379MHbs2HxrXblyJbKzs7F+/Xrz0ZyVlRV+//13zJ07F3PnzoWPjw+MRiNGjx6d66J8V1dXjBw5slht1KdPH3z++ecwGAxF2jf873//w4cffoj169dDJpNhwYIFcHBwyHMbi46OhlqtxtatW/HZZ59Bo9FgxYoVUCgUaNKkCdRqNYYNG4bff//9saccC5Jf+x04cAAbNmzAd999h+7du+PMmTMYPHgwJElCv379zKeMFy9ejPnz52PLli2Qy+UYMGDAI7cz/9fLL7+MTz75BH5+fpDL5Wjfvn2u00uRkZHmU9UPuLi4YPbs2XjttddgMplQq1Yt8z46v+3vgSZNmmDevHl44403YDKZoNFo8M0338DW1havv/463n//fWzcuBEKhQL9+vVDp06dYG9v/9jHnzZnZ2d8/PHHmDJlCmQyGapUqYJvvvkG1tbWiIuLwyuvvIJvv/0WZ86cwV9//YWsrKxcB3xLlixBs2bN8lz+Z599hnnz5iEoKAgmkwmzZs0yh6zIyEg4ODg80rO2ePFiCCFy9RQ9OJW+fPlyzJ8/H7/++iskScLkyZPRpk0btG7d2vx9ZmVlBScnJyxatAgA8twet2/fXqzXlBcHBwesWLECH374IbKzs2FlZYVly5ZBrVbjlVdewfTp0zFo0CAIITB16lTz99b//vc/zJkzBwaDAQ4ODli5ciVkMlm+20heZKKgcwVE/xEVFWX+rY8HpwaKKisrC2PGjMHcuXMfuTamPJgxYwaaNm2KF1980dKllAvHjx9HVFRUsYNQSYmOjoavry8uXLhg6VJKvSlTpmDZsmWWLoOIvyxNRfPVV19h5MiRmDNnTrFDUHBwMHr16oUePXqUyxBET19KSkqu7n4q2+Li4op8YTLRs8IeISIiIqqw2CNEREREFRaDEBEREVVYDEJERERUYTEIERERUYVVrF+WprIpOTkTklT0a+Pt7W2QmJjxDCoqm9geubE9/sW2yK2st4dcLkO1avkP80RlH4NQBSJJolhB6MG89C+2R25sj3+xLXJje1Bpx1NjREREVGExCBEREVGFxSBEREREFRaDEBEREVVYDEJERERUYTEIERERUYXFIEREROWOEBL0YfuR8fPbMCXctnQ5VIrxd4SIiKhckVJioD2yBqa4v6Go6wq5XS1Ll0SlGIMQERGVC0IyQn9xN/TntwBKNTS9XoayaTfIZDJLl0alGIMQERGVeab7kdAeWQMpMRLKhh2h9giEvJKdpcuiMoBBiIiIyixh1EN/fhv0F3dCprGF5rk3oGrY0dJlURnCIERERGWSMfY6tEfWQKTGQunSAxr3EZCpOUgqFQ2DEBERlSlCnw3d6U0wXDkAmW0NWA/8Pyjrulq6LCqjGISIiKjMMN65BG3wOoiMJKhcn4O601DIVBpLl0VlGIMQERGVekKbAe2JX2H8+zjkVWvDevAsKGo1LXg+IXjXGOWLQYiIiEo1w60z0B1fD6HNhJWbL6zcfCFTWuU7jxACR/66h40Hb+D/RrRDYyfeQUaPxyBERESlkpSVAt2x9TBGnIO8Rn1YD3gHihr1C5xPbzBh/d5rOH4pFq4Nq6Oug00JVEtlFYMQERGVKkIIGK8fg/bEr4DJAKvOw2HVxhsyuaLAeRNSsrEi6BKi4jIw2KMBBns0hFzOU2OUNwYhIiIqNaS0BGiD18F09zIUtVyg6TkR8qqFGyIj9GYivvvzMoQApg5rg3ZNajzjaqk8YBAiIiKLE5IEw5UD0J3+HZDJoe4+FqoWvSCTFTw2uCQE/jwegW3HbqOuow0mB7jCsVqlEqiaygMGISIisihT8j1oj66BFHcDCuc20PQYB7mNfaHmzcg24PvtVxB6MxHdXGsh0LsZ1KqCT6ERPcAgREREFiEkI/R/7YT+/DZApYam9ytQNnEv9O3ukbHpWBF0CcnpOgR6uaCXmxNvlaciYxAiIqISZ0qIgPbIakhJd6Bs1BlqjzGQW1cp9PzHL8Xgxz3XYGOtwozR7Xl7PBUbg1ApFRgYiKSkJCiVOW/R/PnzkZmZiYULF0Kn02HAgAGYNm2ahaskIioaYdRDf24L9KG7IbOuAo3XFKgadCj0/AajhF8P/I3DF+6ieb2qeNXPFVUq5/+bQkT5YRAqhYQQiIiIwKFDh8xBSKvVon///li/fj1q166NSZMm4ciRI/D09LRwtUREhWOMuQbt0TUQqXFQNe8JdZcXijRIalKaFiuCwnA7Jg0DutbDkJ6NoJAXfDE1UX4YhEqhW7duAQAmTpyIlJQUDB8+HC4uLqhfvz6cnZ0BAL6+vti9ezeDEBGVejmDpP4Ow5WDkNk6wNpnOpROLYu0jCsRSfhm62UYTRImB7iiQzPHZ1QtVTQMQqVQWloa3N3dMWfOHBgMBowdOxYvvfQSHBwczNM4OjoiLi7OglUSERXMGHUR2uAfIDKToWrtDXXHIZCp1IWeXwiBXaeisPnITdS2r4zJAa6obV/4XiSigjAIlUJubm5wc3Mz/z1s2DAsXboUHTr8ex69OAMJ2tsX/2fmHRxsiz1vecT2yI3t8S+2RQ5TVhrit36F7LCjUNWoC4fn34XGyaVIy8jMNuCrjRdw4lIMerRzwpTh7WCt5tcWPV38RJVCZ8+ehcFggLu7O4Cc0OPk5ISEhATzNAkJCXB0LFrXcGJiBiRJFLkeBwdbJCSkF3m+8ortkRvb419si3+Gx3gwSKo+C1bt/WDlNgjpChXSi9A2dxMysDwoDAnJ2RjRtyme61gXGWnZyHiGtf+XXC57ogNIKht4lVkplJ6ejiVLlkCn0yEjIwNBQUF4++23cfv2bURGRsJkMmH79u3o2bOnpUslIjKTMpOh3bsU2gMrIbOtAaeJS6DuGACZQlWk5Zy6EoePfjwLrc6I6aPc4NXJmb8PRM8Me4RKod69e+PixYvw9/eHJEkYNWoU3NzcsGjRIkyZMgU6nQ6enp7o37+/pUslIoIQAoZrR6E7uQEwGaHu8gJUrb2grlkVKEIvkNEk4fdDN7Hv7B00qWuH1/1dUdWm8NcTERWHTAhR9HMlVCbx1NjTwfbIje3xr4rYFlJaPLRH18J07yoUtZvlDJJqVxNA0dojJUOHr7eE4e/oVPTrWBfDezeBUmHZkxY8NVYxsEeIiIiKTEgSDGH7oDu7OWeQ1B7joWres1CDpP7X9Tsp+HpLGLL1RrwyuCW6tizcaPNETwODEBERFYkp6S60R1dDir8FRb220HQfB7lN9SIvRwiB/Wej8duhG6hhp8E7I9qhrgN7YKhkMQgREVGhCJMR+r92QH9hG2RWlaDp8yqUjbsU60Jmrd6IdbvCcfpqPNya1sCLPi1RScOvJCp5/NQREVGBTPG3oD26BlJSNJSNu0LdbVSRBkl9WExiJlYEhSEmMRPDejXGgC71eFcYWQyDEBER5UkYddCdDYLh0h7IKlWFtfebUNZ3K3jGPJy7loDVO65AqZDjnRfaoWWDop9SI3qaGISIiOixjPeuQnt0HURaHFQtekHdZThkVpWKtSyTJOGPo7ew62QUGtaugskBrqheRfOUKyYqOgYhIiLKReizoDv1GwxXD0NWxRHWg96Dsk6LYi8vLVOPVdsu42pkMnq5OWFk36ZQKfl7vlQ6MAgREZGZMfIvaI/9AJGVAlWb/jm/DK0s/o8a3ryXipVBYcjINmDiwBbo3qb2U6yW6MkxCBEREaTsNOhCfoHx5knIq9WF9XNToHBsVOzlCSGwK+Q2VgVdQjVbNWaN6YD6tTggLZU+DEJEROXIsdAYhN5KLPwMQqCBLhyd0g9CJXS4VLkbLiu7QArJAhBW7DoysvQIj0pB60b2eNm3JWysizbeGFFJYRAiIionzobHY83Oq6heRQ21SlHg9LYiA94IRhNE4i4csQu+uJ9VHcjKfuJaZDIZxvRvjl5ta0POW+OpFGMQIiIqB+7EZ2D1jqtoVKcK3hvlBpUy7yAkhARD+FHoTm4GJBPUnUaimetzaC5/uhcwV8Sx16jsYRAiIirj0rP0WLY5FBq1ApMDWucbgqTUuJxBUmPCoajTApqeEyCv4liC1RKVLgxCRERlmNEk4estYUjJ0GPG6PaoZvv4O7yEZILh0l7ozv4ByJVQ95wAVbOe/EVnqvAYhIiIyrCNB24gPCoFL/q0QKM6jx/ywpR0B9ojayAl3IayvhvU3cdCXrlaCVdKVDoxCBERlVFHL97DgfPR8OrkDI/Wj/4+jzAZoL+wHfoL2yFTV4Km72tQNurMXiCihzAIERGVQX9Hp2D9nmto1bA6nu/d+JHnTfE3c3qBku9C2cQ9Z5BUDX/Hh+i/GISIiMqYpDQtVgSFwd5Og1f9WkHx0N1ewqCD7uwfMFzaC1nlarDuPw3Kem0tWC1R6cYgRERUhugNJiz74xL0BhPeHemGypp/f6jQePcKtEfXQqQnQNWyD9Sdn4fMytqC1RKVfgxCRERlhBAC63aFIyo2HVOGtoFTjco5j+syoTu1EYbwo5DZ1YT1oBlQ1mlu4WqJygYGISKiMmL3qSicvBKHIT0boV3TGgAAQ8R56I79CJGdCqu2A2HVwR8ypZWFKyUqOxiEiIjKgNCb97Hp8E10buEIH/f6OYOkHv8JxlunIa/uDGvvN6FwaGjpMonKHAYhIqJSLiYxE6u2XYGzow3G928O440T0Ib8DBh0sOo4BFbtBkIm5+6cqDi45RARlWJZWgOWbr4EpUKGKf3rQjq4FIY7oZDXbAJNz4lQVKtj6RKJyjQGISKiUkqSBFZtu4LElCzM7ZYFqz3zYRIS1N1GQ9WyL2RPeZBUooqIQYiIqJTafOQmYiNu44O6f8H2SgQUTq2g6TEe8ioOli6NqNxgECIiKoVOht2F7uJOzKwWCoXeChrPF6F06c7hMYieMgYhIqJSJir8KuyCv4dfpUTI67WHdY+xkFeqaumyiMolBiEiolJCGPVIPxWEKmG7oZJrIHpOQuXm7pYui6hc45V2pdjixYsxY8YMAEBISAh8fX3h5eWFL774wsKVEdHTZor9G5mb50F2eRcuGBvBOHAeqjAEET1zDEKl1IkTJxAUFAQA0Gq1mDVrFlauXImdO3ciLCwMR44csXCFRPQ0CIMW2pCfkbXtE2RkZOLr9L6w6TsJzs61LF0aUYXAIFQKpaSk4IsvvsCrr74KAAgNDUX9+vXh7OwMpVIJX19f7N6928JVEtGTMkaHIXPTbBjC9iPOoQs+TPBBk07d0Km5o6VLI6oweI1QKTR37lxMmzYNMTExAID4+Hg4OPx7u6yjoyPi4uIsVR4RPSGhy4T2xAYYrwdDblcLcZ3fwKI9aWjTxB7+PRpZujyiCoVBqJT5/fffUbt2bbi7u+OPP/4AAEiSlOuWWSFEsW6htbe3KXZdDg62xZ63PGJ75Mb2+FdBbZEZfhL3d38HU1YaqnYbAl3LgVi6/CScHG0wc0JnVNKoSqjSksHPBpV2DEKlzM6dO5GQkAA/Pz+kpqYiKysLd+/ehUKhME+TkJAAR8eid50nJmZAkkSR53NwsEVCQnqR5yuv2B65sT3+lV9bSFkpOYOk3j4LuX09VPKehswqTvhk7TlIkoTX/VshM12LzHRtCVf97JT1z4ZcLnuiA0gqGxiESpm1a9ea//3HH3/g9OnT+PDDD+Hl5YXIyEjUrVsX27dvx9ChQy1YJREVlhACxr+PQ3viV8Cog1WnYbBq2x9CpsD3QWG4ez8Tbw9vh5rVKlm6VKIKiUGoDFCr1Vi0aBGmTJkCnU4HT09P9O/f39JlEVEBpPT70Aavgyk6DIqaTaH2nABF1ZxBUrcdu43z1xMwok8TtGpY3cKVElVcMiFE0c+VUJnEU2NPB9sjN7bHvx60hRASDJcPQnf6d0Amg7rzMKha9oFMlnOj7rlrCVgRdAkerrUw0adFuR02o6x/NnhqrGJgjxAR0VNkSrkH3ZG1MMX9DUVd15xBUm1rmJ+Pjs/A99uvoFGdKhjbv1m5DUFEZQWDEBHRUyAkI5KPb0bW0d8AlRqaXi9D2bRbrqCTkW3A0s2h0KgVmBzQGiqlIp8lElFJYBAiInpCpvuR0B5ZDSkxCsqGHaH2CIS8kl2uaYwmCV9vCUNKhh4zRrdHNVu1haoloocxCBERFZMw6qE/vxX6i7sg09ii5tB3kWXf6rHT/nbwBq5GJuNFnxZoVKdKCVdKRHlhECIiKgZj7HVoj6yBSI2FqlkPqLuOQOW6tZD1mIuDgy/ew/5z0fDq5AyP1rUtUC0R5YVBiIioCIQ+G7rTm2C4cgAy2xqwHvgulHUf3wsEADeiU/Hjnmto1aAanu/duAQrJaLCYBAiIiok451QaIN/gMhIgsr1Oag7DYVMpclz+qQ0LZYHXYK9nQav+rtCIec410SlDYMQEVEBhDYD2hO/wvj3ccir1oG13/tQ1GyS7zx6gwnL/rgEvcGEd0e6oXI5G0OMqLxgECIiyoMQAsbbZ6E7vh5CmwkrN19YtR8MmSL/UCOEwLrd4YiKTceUoW3gVKNyCVVMREXFIERE9BhSVgp0x9bDGHEO8hoNYD3w/6Cwr1eoeXefjsLJy3EY0rMR2jWtUfAMRGQxDEJERA8RQsB4LRjakxsAkwFWnYfDqo03ZPLC/fjhpVuJ2HToJjo1d4SPe/1nXC0RPSkGISKif0hpCTmDpN69DEXtZtD0mAB51VqFnj86Ph3fbL0MZ0cbTBxYfscQIypPGISIqMITkgTD5f3QndkEyORQdx8LVYte5kFSCyNLa8DCny9AqZDhjaGtobbi8BlEZQGDEBFVaKbku9AeXQsp7gYUzm2g6TEOchv7Qs1rMEq4fDsJZ8LjcOHv+zAYJbw70g017KyfcdVE9LQwCBFRhSQkI/R/7YT+/DbIVBpoer8CZRP3Ak9nGYwSLkck4czVePx1IwHZOhMqa5To1NwRgz2bwL4yb5MnKksYhIiowjElROQMkpp0B8pGnaH2GAO5dd7jfxlND3p+4nHh7/vI1hlRWaNEh2aO6NzcEc3rV4NSIYeDgy0SHjPEBhGVXgxCRFRhCKMe+nNboA/dBZm1HTReU6Fq0P6x0xpNEq780/Nz/p/wU0mtRAcXB3Rq4YgW/4QfIirbGISIqEIw3guH9uhaiLQ4qJr3hLrLC5Cpc//QYU74Sc655uf6fWTpjLBWK9HepQY6Na+Jlg0YfojKGwYhIirXhD4bulO/wXD1EGS2DrD2mQ6lU0vz80aThKuRyTk9P9cT/g0/TWugY3NHtGpYneGHqBxjECKicssYdTFnkNSsZKhae0PdcQhkKjWMJgnhkck4HR6PC9cTkKk1wlqtgFtTh5zw06A6VEqGH6KKgEGIiModSZsOXcgvMN44AXm1OrB+bjaEfUNcjkrGmau3cP6h8NOuiQM6/dPzw/BDVPEwCBFRuSGEgPHWaeiO/wShy4LSbTBuVe+OM+cScf76cWRkG6CxUsDtn9Nerg2rQ6XkDx8SVWQMQkRULkiZydAd+xHGyAvQVXHGUbvncThEhozsMKitFHBrUgOdmjvCtRHDDxH9i0GIiMo0IQT0Vw9De3IjhMmIPYbO2BfhApWVQLsm9jnhp2F1WKkYfojoUQxCRFQscclZWL3jKnQGE4xGySI1VBWpGIAjqI97uGmoiT903VG3cUO81rwmWjdi+CGigjEIEVGRGYwmfB0UhvupWrg1d4ROZyzR9cuEhObZ59Eu4xgkmRzn7Lxh26Y35jZ2YPghoiJhECKiIttw8Aai4jMwdVgbPOfesESHlTAlRUN7ZA2kjFtQ1GsLTfdx6GVTvcTWT0TlC4MQERXJ2fB4HDp/F96dndGuSY0SW68wGaH/azv0F/6EzKoSNH1ehbJxlwIHSSUiyg+DEBEVWnxyFtbuuopGdapgqGfjEluvKf5WTi9QcjSUTbpC7T4q30FSiYgKi0GolPrqq6+wZ88eyGQyDBs2DBMmTEBISAgWLlwInU6HAQMGYNq0aZYukyoQg1HC11suQwYZXh3cqkSGnRBGHXRng2C4tAeySlVh7f0mlPXdnvl6iajiYBAqhU6fPo2TJ09i27ZtMBqNGDhwINzd3TFr1iysX78etWvXxqRJk3DkyBF4enpaulyqIH47dAORcemYMqQ1alS1fubrM967Cu2RNRDpCVC16AV1l+GQWVV65uslooqFvydfCnXu3Bk//vgjlEolEhMTYTKZkJaWhvr168PZ2RlKpRK+vr7YvXu3pUulCuLctXgcOBeN5zo6w83F4ZmuS+izoD26DtnbFwMyGawHvQdNj/EMQUT0TLBHqJRSqVRYunQp1qxZg/79+yM+Ph4ODv9+ATk6OiIuLq5Iy7S3tyl2PQ4OtsWetzyqSO0Rm5iJdbvC0dS5Kl57vt1jx+N6Wu2Ref0M7u/6FqbMFNh1HYxqPUdArlI/lWWXlIr02SgMtgeVdgxCpdjUqVPx8ssv49VXX0VERESuu2OEEEW+WyYxMQOSJIpch4ODbYneHl3aVaT2MJokLPzpHCQBvOTTAinJmY9M8zTaQ8pOgy7kZxhvnoK8el1U6vcGJMdGSEzRA9A/0bJLUkX6bBRGWW8PuVz2RAeQVDYwCJVCN2/ehF6vR4sWLWBtbQ0vLy/s3r0bCsW/PxSXkJAAR0dHC1ZJFcGmwzdxOyYdkwNc4fAMrgsSQsB48yR0x3+GMGTDqkMArNr5QKbgromISgavESqFoqOjMXv2bOj1euj1ehw4cAAjRozA7du3ERkZCZPJhO3bt6Nnz56WLpXKsQvXE7D3zB30bV8XHZo9/dAtZSQie8+X0B5cBZmdIyoNmQ91Bz+GICIqUdzjlEKenp4IDQ2Fv78/FAoFvLy84OPjg+rVq2PKlCnQ6XTw9PRE//79LV0qlVP3U7OxesdV1K9pi+F9mjzVZQshwXD1CHSnNgKSBHXXkVC5PgeZnMdlRFTyZEKIol80QmUSrxF6Osp7exhNEhb/fB73EjMxb3wnOFbL/26torSHlBoL7dG1MMVcg6JOC2h6ToC8Svk5xVvePxtFVdbbg9cIVQzsESKiXP44cgs376XhNX/XAkNQYQnJBMOlPdCdDQIUSqh7ToCqWU8Oj0FEFscgRERmf924j92no9DbzQmdmj+dnhpT4h1oj66BlHAbyvpuUHcfC3nlak9l2URET4pBiIgAAElpWqzefgX1HG0wou+TXxckTAboL/wJ/YUdkKkrQdP3dSgbdWIvEBGVKgxCRASjScI3Wy/DKAm85u8KlVJR8Ez5MMXdyOkFSr4HZRN3aLqNhkzDay2IqPRhECIiBAXfwo27qZg0uBVqVi/+dUHCoIPuzGYYwvZBVrkarPtPg7Je26dYKRHR08UgRFTBhd5MxK6TUfBsVwddWtYs9nKM0ZehDV6XM0hqyz5Qd34eMqtnPzgrEdGTYBAiqsCS03X4fvsV1HWwwci+TYu1DJM2E9ojq2G4FgyZXU1Y+86Esnazp1wpEdGzwSBEVEGZJAmrtobBYJTwmn8rWKmKfl2QIeIcokN+gikzFVZtB8Kqgz9kSqtnUC0R0bPBIERUQW0Jvo3r0al4eVBL1LavXKR5paxU6EJ+gvHWGVg5NoD6uTehcGjwbAolInqGGISIKqCw24nYeSISPdrUhrtrrULPJ4SA8e8QaE/8Ahh0sOo4BE79XsD9pOxnWC0R0bPDIERUwSSn6/Ddn1dQx6EyRj3nUuj5pIxEaIPXwXTnEuQ1m0DTcyIU1epwkFQiKtO4ByOqQCRJ4Ls/L0NnMOE1P1eoC3FdkBASDFcOQnd6EyAkqLuNhqplXw6SSkTlAoMQUQWy7fhthEel4EWfFqhTo+DrgqSUmJxBUmOvQ+HUCpoe4yGv4lAClRIRlQwGIaIK4nJEEv48HgEP11rwaF0732mFZII+dBf057YACitoPF+E0qU7h8cgonKHQYioAkjNyLkuqJZ9JYzxyv83fkz3I3OGx7gfCWWDDlB3D4S8UtWSKZSIqIQxCBGVc5Ik8O2fV6DVGfHuiHZQWz3+uiBh1EN/fhv0F3dCprGBpt9kqBp1KuFqiYhKFoMQUTm3PSQCVyOTMWFAczg5PH7gU2Ps39AdXQMpJQZKFw9ouo7kIKlEVCEwCBGVY+GRydh6/DbcW9VE9zaPXhckDFroTm+C4fIByGyqw3rAO1A6t7ZApURElsEgRFROpWbqsWrbZdSsVgmB3s0eudDZGB0G7dG1EBmJULXqC3WnYRwklYgqHAYhonJIEgLf/3kZWToj3n6hHTRW/27qQpsB7ckNMF4/BrldLWgGz4KyVuF/WJGIqDxhECIqh3aciMTliGSM698Mzo7/XutjuHUGuuPrIbQZsGo3CFbtB3OQVCKq0BiEiMqZa1HJ2BJ8C11a1kTPtnUAAFJWCnTHf4Lx9lnI7evBesA7UNSob+FKiYgsj0GIqBxJy8q5LsixqjXGeuf8XpDhWjC0J34FTHpYdRoGq7b9IZNz0yciAhiEiMoNSQh8v/0KMrKNeOv5tlDrk5G9fx1Mdy9DUbMp1J4ToKhax9JlEhGVKgxCROXErpORCLuVhMDnmqBWwklknt4EAFB3GwNVqz6QyThIKhHRfzEIEZUD1++kIOjobfRpokDn6B+hi7sBRV3XnEFSbWtYujwiolKLQYiojMvINuC7bZcw2O4KeqdcgKRSQ9PrJSibenCQVCKiAjAIERVBQko27iZnQy5JqG6ryXPcrpIiCYGgLYfwonwXnOTJUNbvCLXHGA6SSkRUSAxCRIUUHZ+BBevPQWcwmR+rrFGimq0a1atocv5vq0Y1Ww2qV/n3MbXq2YQlYdTj2p8/wjftOEzqytD0fgOqhh2fybqIiMorBqFSavny5di1axcAwNPTE9OnT0dISAgWLlwInU6HAQMGYNq0aRausuLIyDZg6eZQaNQKzBzfCXdj05CUpkVyug5JaTokpWtxOyYN6VmGR+bNCUv/hCNbNapV0aD6P6HpQViyKmJYMsZcQ/rB1aibGY8bmlZo+8JrkHOQVCKiImMQKoVCQkJw7NgxBAUFQSaT4aWXXsL27dvx2WefYf369ahduzYmTZqEI0eOwNPT09LllntGk4Svt4QhJUOPGaPbo0PzmqhnX+mx0xqMJnM4Sk7PCUjmf6dpceteGjKyHw1LNtYqc4+SuXepyr+9S9VscsKS0GdDd/p3GK4cRLqwwQ5pIMa9EAC5RvWsm4GIqFxiECqFHBwcMGPGDFhZ5Qx90LhxY0RERKB+/fpwdnYGAPj6+mL37t0MQiVg48EbuBqZjBd9WqBRnSr5TqtSKuBYrRIcqz0+KAGA3vBPWPonHD34d3KaFknpOty4m4pMrfGR+dpXjkWAOgS2yMAFeRv8luSK/wvsikoMQURExcYgVAo1bdrU/O+IiAjs2rULY8aMgYODg/lxR0dHxMXFFWm59vbFP3Xi4GBb7HnLsr2nInHgXDT8PRvDv8+/A5M+aXs4FfC8Vm9EYqoW91OykRR/H5XDNqPG/QtIVVTHRvlQXMu2x4tDm6Nzm4KWVDIq6ufjcdgWubE9qLRjECrF/v77b0yaNAnTp0+HQqFARESE+TkhRJFvjU5MzIAkiSLX4eBgi4SE9CLPV9bdiE7Fyk0X0aphdfh0cTa3QUm1h0oIOCSFosqpnyC0mbBy84WTmy9efmiQ1NLwvlTUz8fjsC1yK+vtIZfLnugAksoGBqFS6ty5c5g6dSpmzZoFHx8fnD59GgkJCebnExIS4OjoaMEKy7ekNC2WB12CvZ0Gr/q1gkJesr/KLGUmQ3d8PYwR5yGv0QDWA/8PCvt6JVoDEVFFwCBUCsXExGDy5Mn44osv4O7uDgBo27Ytbt++jcjISNStWxfbt2/H0KFDLVxp+aQ3mLDsj0vQG0x4d6QbKpfgNThCCBiuHYXu5AbAZIRV5+GwauMNmdyyv1dERFReMQiVQqtXr4ZOp8OiRYvMj40YMQKLFi3ClClToNPp4Onpif79+1uwyvJJCIF1u8IRFZuOKUPbwKlG5RJbt5QWD23wOpjuXoGilgs0PSdCXrVWia2fiKgikgkhin7RCJVJvEaoYLtOReL3QzcxpGcjDOrW4LHTPO32EJIEw+V90J3ZDMjkUHcZDlWLXmVmkNSK9PkoCNsit7LeHrxGqGJgjxDRP0JvJmLToZvo3MIRPu71S2SdpuS70B5ZAyn+JhTObaDpMQ5yG/sSWTcRETEIEQEAYhIzsWrbZTg72mDCgBbPfLBSYTJCf3EH9Of/hEylgab3K1A2cecgqUREJYxBiCq8LK0RyzZfglIhw5ShbZ75QKqmhNs5vUBJd6Bs1DlnkFTr/H+okYiIng0GIarQJElg1bbLSEjJxrsj3WBvp3lm6xJGHXRnt8BwaTdk1nbQeE2FqkH7Z7Y+IiIqGIMQVWibj97EpVuJGOvdDC7OVZ/Zeoz3wqE9uhYiLQ6q5p5QdxkOmbrk7kgjIqLHYxCiCuvk5VjsOhmFXm5O6OX2bIaqEPps6E79BsPVQ5DZOsDaZzqUTi2fybqIiKjoGISoQoqITcPaXeFwca6KUf2aFjxDMRij/oI2+EeIrGSoWntD3XEIZCr1M1kXEREVD4MQVTipGTos23wJVSqp8Lq/K5SKp/t7PZI2HbqQX2C8cQLyak6wfm4yFI6Nn+o6iIjo6WAQogrFYJSwIigMmdkGzArsgCqVrQqeqZCEEDDePAVdyM8Q+ixYtfeDlZsvZApuZkREpRX30FRhCCHw095ruHE3Fa/6tUK9mrZPbdlSZjJ0x36EMfIC5A4NYe05EYrqzk9t+URE9GwwCFGFcfD8XQSHxmBQt/ro3KLmU1mmEAKG8CPQndwISCaou74Alas3ZCU8Wj0RERUPgxBVCFcjk/Hr/r/RrkkN+Pdo9FSWKaXFQ3t0LUz3rkJRuzk0PSdAbvd0AhYREZUMBiEq9xJSsvH1ljDUsq+El31bQv6Ew1gIyQR96G7ozvwByBVQ9xgPVXNPDo9BRFQGMQhRuabVG7FscygkSWDK0NawVj/ZR96UFI1723+A7t7fUNRrlzNIauVqT6laIiIqaQxCVG5JQuD77Vdx934mpg1vi5rVKhV7WcJkhP6v7dBf+BNyTWVo+rwKZeMu7AUiIirjGISo3PrzeATOX0/AiD5N4NrQvtjLMcXfyhkkNTkayiZd4TToFSRl8WJoIqLygEGIyqVz1xKw9dhtdHOthec6Fe829pxBUoNguLQHskpVYe39FpT120FR2RbISn/KFRMRkSUwCFG5Ex2fge+3X0HD2lUwrn+zYp2+Mt67Cu2RNRDpCVC16J0zSKqV9TOoloiILIlBiMqV9Cw9lm4OhUatwBtDWkOlVBRpfqHPgu7kbzCEH4asSk1YD5oBZZ3mz6haIiKyNAYhKjeMJglfbwlDSoYO741uj2q2RRvg1Bh5AdrgHyCyU6FqMwDqjv6QKTlIKhFRecYgROXGxoM3EB6Vghd9WqBxHbtCzydlp0EX8jOMN09BXr0urL3fhMKh4TOslIiISgsGISoXjl68hwPnouHVyRkerWsXap6cQVJPQnf8ZwhDNqw6BsCqrQ8HSSUiqkC4x6cy70Z0KtbvuYZWDarh+d6NCzWPlJEI7bEfYYq6CLljY1j3nAhFdadnXCkREZU2DEJUpiWlabE86BLsq2gwyc8VigIGOxVCguHqEehObQSEBLX7KKha9eMgqUREFRSDEJVZeoMJy/64BJ3BhHdHusHGWpXv9FJqbM4gqTHXoHBqCU2P8ZBXcSyhaomIqDRiEKIySQiBdbvCERWbjjeGtoZTjcp5TyuZYLi0F7qzfwAKJTQ9J0LZrAeHxyAiIgYhKpt2n47CyStxCOjZCG5NHfKczpR4B9qjayAl3IayvhvU3cdykFQiIjJjEKIyJ/RmIjYduomOzR0xyL3+Y6cRJgP0F/6E/sIOyDSVoen3OpQNO7EXiIiIcuEVoqVURkYGBg0ahOjoaABASEgIfH194eXlhS+++MLC1VlOTGImVm27jLqONnhxYIvHBhtT3A1k/TEP+vPboGzSBZWf/wSqRp0ZgoiI6BEMQqXQxYsXMXLkSERERAAAtFotZs2ahZUrV2Lnzp0ICwvDkSNHLFukBWRpDVi6+RIUchmmDG0NtVXu4TOEQQdtyC/I2roAQq+Fdf+3Yd37Fcg0NhaqmIiISjsGoVLot99+w7x58+DomHNHU2hoKOrXrw9nZ2colUr4+vpi9+7dFq6yZEmSwKptV3A/JRuTA1xRwy73AKjGu1eQuWk2DGF7oWrZB5WfXwBlvTYWqpaIiMoKXiNUCi1YsCDX3/Hx8XBw+PeCYEdHR8TFxZV0WSUuW2fE7Zg03LybissRybh+JwWBXi5oVu/fi52FLhO6kxthuHYUMruasPadCWXtZhasmoiIyhIGoTJAkqRc17cIIYp1vYu9ffFPETk42BZ73sIQQiDmfiauRiQhPDIZ4RFJiIpNgyRynneuaYOxA1vg+b4u5nkyr53G/d3fwpSZiqrdAlC1+/OQq0pmkNRn3R5lDdvjX2yL3NgeVNoxCJUBtWrVQkJCgvnvhIQE82mzokhMzID0IFkUgYODLRIS0os8X360eiNux6Tj5t3UnP/upSEj2wAAsFYr0Kh2FQzq1gCNnezQqE4VVNbk/FhiQkI6pKzUnEFSb52G3N4ZlZ57EyaHBkhM0QPQP9U6H+dZtEdZxvb4F9sit7LeHnK57IkOIKlsYBAqA9q2bYvbt28jMjISdevWxfbt2zF06FBLl1VoQggkpGTj5t003LiXE3yi4zMhiZxQVqt6JbRtYo8mTnZo7GSHOvaVIZc/2uMlhIDx7xBoT/wCGHSw6jQUVm0HQCbnx5iIiIqH3yBlgFqtxqJFizBlyhTodDp4enqif//+li4rTzqDCRExabhxNxU376bh1r1UpGXl9PaorXJ6ewa610cTpypoVMeuwKExgH8GSQ3+AaY7oZDXbAJNz4lQVKvzrF8KERGVcwxCpdjBgwfN/3Z3d8e2bdssWM3jCSFwP1X7zymunB6f6PgMmP45BVezmjVcG9mjsZMdGtepgroONo/t7cl7+RIMVw5Bd/p3QAiou42GqlVfyGS84ZGIiJ4cgxAVid5gQkRsOm7e+yf43E1FWmbOdTlqlQINa9uif5d65uBjW8mq2OuSUmKhPboGptjrUDi1gqbneMht8x5Og4iIqKgYhChfJklC8F93cf5KLG7eS0VU3L+9PY5VrdGqQbV/Qo8d6jpWhkL+5D01QjJBH7ob+nNBgMIKGs8XoXTpzl+GJiKip45BiPJ15mo8vv3zCqyUcjSoXQXeneuhsVMVNK5jhyqVi9/bkxdTYhS0R1ZDuh8JZYMOUHcPhLxS1ae+HiIiIoBBiArQuUVNuLWsBYUkQal4dtflCKM+Z5DUv3b+M0jqZKgadXpm6yMiIgIYhKgAcrkMdZ/xb4GYYv+G9ugaSCkxULp4QNN1JMcHIyKiEsEgRBYjDFroTm+C4fIByGyqw3rAO1A6t7Z0WUREVIEwCJFFGKPDoD26FiIjCapWfaHuPAwylcbSZRERUQXDIEQlSugyoT3xK4zXj0FuVwuawbOgrNXU0mUREVEFxSBEJcZw+yx0x9ZDaNNh1W4QrNoPhkz59O88IyIiKiwGIXrmpKwU6I7/BOPts5Db14f1gLehqFHf0mURERExCNGzI4SA8foxaE9uAIw6WHUeBqs2/TlIKhERlRr8RqJnQkpPyBkkNToMilou0PScAHnV2pYui4iIKBcGIXqqhJBguHwAutObAJkMao9AqFr25iCpRERUKjEI0VNjSrkH3ZG1MMX9DYVza2i6j4PctoalyyIiIsoTgxA9MSEZob+4C/pzWwGVGppeL0PZtBsHSSUiolKPQYieiOl+BLRH1kBKjIKyUSeou42BvJKdpcsiIiIqFAYhKhZh1EN/fiv0F3dBprGF5rkpUDXsYOmyiIiIioRBiIrMGHsd2iNrIFJjoWrWA+quIyBTV7Z0WUREREXGIESFJvTZOYOkXjkAmW0NWA98F8q6rSxdFhERUbExCFGhGO+EQhv8Q84gqa5eUHcaCplKbemyiIiIngiDEOVL6LMRv20dsi8dhrxqHVj7vQ9FzSaWLouIiOipYBCifBnvXIL2cjCs2g+GlZsvZAqVpUsiIiJ6ahiEKF/Khh3RoH03JKYaLV0KERHRU8dxDyhfMrkccitrS5dBRET0TDAIERERUYXFIEREREQVFoMQERERVVgMQkRERFRhMQgRERFRhcUgRERERBUWf0eoApHLZRaZtzxie+TG9vgX2yK3stweZbl2KjyZEEJYuggiIiIiS+CpMSIiIqqwGISIiIiowmIQIiIiogqLQYiIiIgqLAYhIiIiqrAYhIiIiKjCYhAiIiKiCotBiIiIiCosBiEiIiKqsBiEKF9//vknBg4cCC8vL/z888+WLseili9fDh8fH/j4+GDJkiWWLqfUWLx4MWbMmGHpMizq4MGDGDJkCAYMGICPP/7Y0uVY3NatW83byuLFiy1dDlG+GIQoT3Fxcfjiiy/wyy+/YMuWLdi4cSNu3Lhh6bIsIiQkBMeOHUNQUBC2bNmCy5cvY9++fZYuy+JOnDiBoKAgS5dhUXfu3MG8efOwcuVKbNu2DVeuXMGRI0csXZbFZGdnY8GCBVi/fj22bt2Ks2fPIiQkxNJlEeWJQYjyFBISgq5du6Jq1aqoVKkSvL29sXv3bkuXZREODg6YMWMGrKysoFKp0LhxY9y7d8/SZVlUSkoKvvjiC7z66quWLsWi9u3bh4EDB6JWrVpQqVT44osv0LZtW0uXZTEmkwmSJCE7OxtGoxFGoxFqtdrSZRHliUGI8hQfHw8HBwfz346OjoiLi7NgRZbTtGlTtGvXDgAQERGBXbt2wdPT07JFWdjcuXMxbdo0VKlSxdKlWFRkZCRMJhNeffVV+Pn54ZdffoGdnZ2ly7IYGxsbvPnmmxgwYAA8PT3h5OSE9u3bW7osojwxCFGeJEmCTCYz/y2EyPV3RfT3339j4sSJmD59Oho0aGDpcizm999/R+3ateHu7m7pUizOZDLhxIkT+OSTT7Bx40aEhoZW6NOF4eHh2Lx5Mw4dOoTg4GDI5XKsXr3a0mUR5YlBiPJUq1YtJCQkmP9OSEiAo6OjBSuyrHPnzmH8+PF45513EBAQYOlyLGrnzp04fvw4/Pz8sHTpUhw8eBCffPKJpcuyiBo1asDd3R3Vq1eHRqNBv379EBoaaumyLObYsWNwd3eHvb09rKysMGTIEJw+fdrSZRHliUGI8tStWzecOHECSUlJyM7Oxt69e9GzZ09Ll2URMTExmDx5Mj777DP4+PhYuhyLW7t2LbZv346tW7di6tSp6NOnD2bNmmXpsiyid+/eOHbsGNLS0mAymRAcHIxWrVpZuiyLad68OUJCQpCVlQUhBA4ePIjWrVtbuiyiPCktXQCVXjVr1sS0adMwduxYGAwGDBs2DG3atLF0WRaxevVq6HQ6LFq0yPzYiBEjMHLkSAtWRaVB27Zt8dJLL2HUqFEwGAzw8PDA0KFDLV2WxXTv3h1XrlzBkCFDoFKp0Lp1a7zyyiuWLosoTzIhhLB0EURERESWwFNjREREVGExCBEREVGFxSBEREREFRaDEBEREVVYDEJERERUYTEIERERUYXFIEREREQVFoMQERERVVj/Dys8SsqaWZrZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Derivative is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7eecf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1131.9000244140625, weights = tensor([0.2132, 0.0364], requires_grad=True)\n",
      "step №1: loss = 1055.278076171875, weights = tensor([0.4190, 0.0715], requires_grad=True)\n",
      "step №2: loss = 983.9343872070312, weights = tensor([0.6175, 0.1055], requires_grad=True)\n",
      "step №3: loss = 917.5050048828125, weights = tensor([0.8091, 0.1384], requires_grad=True)\n",
      "step №4: loss = 855.6515502929688, weights = tensor([0.9939, 0.1703], requires_grad=True)\n",
      "step №5: loss = 798.0588989257812, weights = tensor([1.1722, 0.2010], requires_grad=True)\n",
      "step №6: loss = 744.4332885742188, weights = tensor([1.3443, 0.2308], requires_grad=True)\n",
      "step №7: loss = 694.50146484375, weights = tensor([1.5103, 0.2597], requires_grad=True)\n",
      "step №8: loss = 648.0089721679688, weights = tensor([1.6705, 0.2876], requires_grad=True)\n",
      "step №9: loss = 604.7190551757812, weights = tensor([1.8250, 0.3145], requires_grad=True)\n",
      "step №10: loss = 564.4107055664062, weights = tensor([1.9742, 0.3407], requires_grad=True)\n",
      "step №11: loss = 526.8787841796875, weights = tensor([2.1181, 0.3660], requires_grad=True)\n",
      "step №12: loss = 491.931884765625, weights = tensor([2.2569, 0.3904], requires_grad=True)\n",
      "step №13: loss = 459.39202880859375, weights = tensor([2.3908, 0.4142], requires_grad=True)\n",
      "step №14: loss = 429.09332275390625, weights = tensor([2.5201, 0.4371], requires_grad=True)\n",
      "step №15: loss = 400.88134765625, weights = tensor([2.6448, 0.4593], requires_grad=True)\n",
      "step №16: loss = 374.6124267578125, weights = tensor([2.7651, 0.4809], requires_grad=True)\n",
      "step №17: loss = 350.1526184082031, weights = tensor([2.8811, 0.5017], requires_grad=True)\n",
      "step №18: loss = 327.3773498535156, weights = tensor([2.9931, 0.5219], requires_grad=True)\n",
      "step №19: loss = 306.17047119140625, weights = tensor([3.1012, 0.5415], requires_grad=True)\n",
      "step №20: loss = 286.42401123046875, weights = tensor([3.2055, 0.5604], requires_grad=True)\n",
      "step №21: loss = 268.03729248046875, weights = tensor([3.3060, 0.5788], requires_grad=True)\n",
      "step №22: loss = 250.91677856445312, weights = tensor([3.4031, 0.5966], requires_grad=True)\n",
      "step №23: loss = 234.97509765625, weights = tensor([3.4967, 0.6139], requires_grad=True)\n",
      "step №24: loss = 220.13107299804688, weights = tensor([3.5871, 0.6306], requires_grad=True)\n",
      "step №25: loss = 206.3091278076172, weights = tensor([3.6742, 0.6469], requires_grad=True)\n",
      "step №26: loss = 193.43875122070312, weights = tensor([3.7583, 0.6626], requires_grad=True)\n",
      "step №27: loss = 181.4545135498047, weights = tensor([3.8394, 0.6779], requires_grad=True)\n",
      "step №28: loss = 170.2952880859375, weights = tensor([3.9177, 0.6927], requires_grad=True)\n",
      "step №29: loss = 159.90423583984375, weights = tensor([3.9932, 0.7071], requires_grad=True)\n",
      "step №30: loss = 150.22845458984375, weights = tensor([4.0661, 0.7210], requires_grad=True)\n",
      "step №31: loss = 141.21865844726562, weights = tensor([4.1364, 0.7346], requires_grad=True)\n",
      "step №32: loss = 132.8289794921875, weights = tensor([4.2042, 0.7477], requires_grad=True)\n",
      "step №33: loss = 125.0167236328125, weights = tensor([4.2696, 0.7605], requires_grad=True)\n",
      "step №34: loss = 117.74202728271484, weights = tensor([4.3327, 0.7729], requires_grad=True)\n",
      "step №35: loss = 110.96793365478516, weights = tensor([4.3936, 0.7849], requires_grad=True)\n",
      "step №36: loss = 104.659912109375, weights = tensor([4.4523, 0.7966], requires_grad=True)\n",
      "step №37: loss = 98.78585052490234, weights = tensor([4.5090, 0.8079], requires_grad=True)\n",
      "step №38: loss = 93.31593322753906, weights = tensor([4.5637, 0.8190], requires_grad=True)\n",
      "step №39: loss = 88.22219848632812, weights = tensor([4.6164, 0.8297], requires_grad=True)\n",
      "step №40: loss = 83.47883605957031, weights = tensor([4.6673, 0.8402], requires_grad=True)\n",
      "step №41: loss = 79.06163024902344, weights = tensor([4.7164, 0.8503], requires_grad=True)\n",
      "step №42: loss = 74.94816589355469, weights = tensor([4.7637, 0.8602], requires_grad=True)\n",
      "step №43: loss = 71.11747741699219, weights = tensor([4.8094, 0.8698], requires_grad=True)\n",
      "step №44: loss = 67.55010223388672, weights = tensor([4.8535, 0.8791], requires_grad=True)\n",
      "step №45: loss = 64.22789764404297, weights = tensor([4.8960, 0.8882], requires_grad=True)\n",
      "step №46: loss = 61.13397216796875, weights = tensor([4.9370, 0.8971], requires_grad=True)\n",
      "step №47: loss = 58.252601623535156, weights = tensor([4.9765, 0.9057], requires_grad=True)\n",
      "step №48: loss = 55.56914520263672, weights = tensor([5.0147, 0.9141], requires_grad=True)\n",
      "step №49: loss = 53.06995391845703, weights = tensor([5.0515, 0.9223], requires_grad=True)\n",
      "step №50: loss = 50.74234390258789, weights = tensor([5.0870, 0.9303], requires_grad=True)\n",
      "step №51: loss = 48.57451629638672, weights = tensor([5.1212, 0.9380], requires_grad=True)\n",
      "step №52: loss = 46.555442810058594, weights = tensor([5.1542, 0.9456], requires_grad=True)\n",
      "step №53: loss = 44.67487335205078, weights = tensor([5.1861, 0.9530], requires_grad=True)\n",
      "step №54: loss = 42.923255920410156, weights = tensor([5.2168, 0.9602], requires_grad=True)\n",
      "step №55: loss = 41.29172134399414, weights = tensor([5.2465, 0.9673], requires_grad=True)\n",
      "step №56: loss = 39.77201461791992, weights = tensor([5.2751, 0.9741], requires_grad=True)\n",
      "step №57: loss = 38.35638427734375, weights = tensor([5.3026, 0.9808], requires_grad=True)\n",
      "step №58: loss = 37.03770065307617, weights = tensor([5.3292, 0.9874], requires_grad=True)\n",
      "step №59: loss = 35.809288024902344, weights = tensor([5.3549, 0.9938], requires_grad=True)\n",
      "step №60: loss = 34.66489028930664, weights = tensor([5.3796, 1.0000], requires_grad=True)\n",
      "step №61: loss = 33.598777770996094, weights = tensor([5.4035, 1.0061], requires_grad=True)\n",
      "step №62: loss = 32.605506896972656, weights = tensor([5.4265, 1.0121], requires_grad=True)\n",
      "step №63: loss = 31.680078506469727, weights = tensor([5.4487, 1.0180], requires_grad=True)\n",
      "step №64: loss = 30.81781578063965, weights = tensor([5.4701, 1.0237], requires_grad=True)\n",
      "step №65: loss = 30.014358520507812, weights = tensor([5.4907, 1.0293], requires_grad=True)\n",
      "step №66: loss = 29.26568031311035, weights = tensor([5.5106, 1.0347], requires_grad=True)\n",
      "step №67: loss = 28.567981719970703, weights = tensor([5.5298, 1.0401], requires_grad=True)\n",
      "step №68: loss = 27.917766571044922, weights = tensor([5.5483, 1.0454], requires_grad=True)\n",
      "step №69: loss = 27.3117733001709, weights = tensor([5.5661, 1.0505], requires_grad=True)\n",
      "step №70: loss = 26.746923446655273, weights = tensor([5.5833, 1.0555], requires_grad=True)\n",
      "step №71: loss = 26.22039794921875, weights = tensor([5.5999, 1.0605], requires_grad=True)\n",
      "step №72: loss = 25.72957992553711, weights = tensor([5.6159, 1.0653], requires_grad=True)\n",
      "step №73: loss = 25.271976470947266, weights = tensor([5.6313, 1.0701], requires_grad=True)\n",
      "step №74: loss = 24.845325469970703, weights = tensor([5.6462, 1.0748], requires_grad=True)\n",
      "step №75: loss = 24.44746971130371, weights = tensor([5.6606, 1.0793], requires_grad=True)\n",
      "step №76: loss = 24.07645034790039, weights = tensor([5.6744, 1.0838], requires_grad=True)\n",
      "step №77: loss = 23.730405807495117, weights = tensor([5.6877, 1.0883], requires_grad=True)\n",
      "step №78: loss = 23.407615661621094, weights = tensor([5.7005, 1.0926], requires_grad=True)\n",
      "step №79: loss = 23.10648536682129, weights = tensor([5.7129, 1.0969], requires_grad=True)\n",
      "step №80: loss = 22.82550811767578, weights = tensor([5.7249, 1.1011], requires_grad=True)\n",
      "step №81: loss = 22.563310623168945, weights = tensor([5.7364, 1.1052], requires_grad=True)\n",
      "step №82: loss = 22.31859588623047, weights = tensor([5.7475, 1.1092], requires_grad=True)\n",
      "step №83: loss = 22.09016227722168, weights = tensor([5.7581, 1.1132], requires_grad=True)\n",
      "step №84: loss = 21.87688446044922, weights = tensor([5.7684, 1.1172], requires_grad=True)\n",
      "step №85: loss = 21.677715301513672, weights = tensor([5.7784, 1.1210], requires_grad=True)\n",
      "step №86: loss = 21.4916934967041, weights = tensor([5.7879, 1.1248], requires_grad=True)\n",
      "step №87: loss = 21.317899703979492, weights = tensor([5.7971, 1.1286], requires_grad=True)\n",
      "step №88: loss = 21.155506134033203, weights = tensor([5.8060, 1.1323], requires_grad=True)\n",
      "step №89: loss = 21.003726959228516, weights = tensor([5.8146, 1.1359], requires_grad=True)\n",
      "step №90: loss = 20.86181640625, weights = tensor([5.8228, 1.1395], requires_grad=True)\n",
      "step №91: loss = 20.729101181030273, weights = tensor([5.8308, 1.1431], requires_grad=True)\n",
      "step №92: loss = 20.604955673217773, weights = tensor([5.8384, 1.1466], requires_grad=True)\n",
      "step №93: loss = 20.488786697387695, weights = tensor([5.8458, 1.1500], requires_grad=True)\n",
      "step №94: loss = 20.380043029785156, weights = tensor([5.8529, 1.1535], requires_grad=True)\n",
      "step №95: loss = 20.278217315673828, weights = tensor([5.8598, 1.1568], requires_grad=True)\n",
      "step №96: loss = 20.182825088500977, weights = tensor([5.8663, 1.1602], requires_grad=True)\n",
      "step №97: loss = 20.093435287475586, weights = tensor([5.8727, 1.1635], requires_grad=True)\n",
      "step №98: loss = 20.009620666503906, weights = tensor([5.8788, 1.1667], requires_grad=True)\n",
      "step №99: loss = 19.931011199951172, weights = tensor([5.8847, 1.1699], requires_grad=True)\n",
      "step №100: loss = 19.857242584228516, weights = tensor([5.8904, 1.1731], requires_grad=True)\n",
      "step №101: loss = 19.787982940673828, weights = tensor([5.8958, 1.1762], requires_grad=True)\n",
      "step №102: loss = 19.722915649414062, weights = tensor([5.9011, 1.1794], requires_grad=True)\n",
      "step №103: loss = 19.661766052246094, weights = tensor([5.9061, 1.1824], requires_grad=True)\n",
      "step №104: loss = 19.604251861572266, weights = tensor([5.9110, 1.1855], requires_grad=True)\n",
      "step №105: loss = 19.550125122070312, weights = tensor([5.9157, 1.1885], requires_grad=True)\n",
      "step №106: loss = 19.4991512298584, weights = tensor([5.9202, 1.1915], requires_grad=True)\n",
      "step №107: loss = 19.451126098632812, weights = tensor([5.9245, 1.1944], requires_grad=True)\n",
      "step №108: loss = 19.405832290649414, weights = tensor([5.9287, 1.1974], requires_grad=True)\n",
      "step №109: loss = 19.36309051513672, weights = tensor([5.9327, 1.2003], requires_grad=True)\n",
      "step №110: loss = 19.322721481323242, weights = tensor([5.9366, 1.2032], requires_grad=True)\n",
      "step №111: loss = 19.28456687927246, weights = tensor([5.9403, 1.2060], requires_grad=True)\n",
      "step №112: loss = 19.248470306396484, weights = tensor([5.9438, 1.2089], requires_grad=True)\n",
      "step №113: loss = 19.214284896850586, weights = tensor([5.9473, 1.2117], requires_grad=True)\n",
      "step №114: loss = 19.1818904876709, weights = tensor([5.9506, 1.2145], requires_grad=True)\n",
      "step №115: loss = 19.15116310119629, weights = tensor([5.9537, 1.2172], requires_grad=True)\n",
      "step №116: loss = 19.121973037719727, weights = tensor([5.9568, 1.2200], requires_grad=True)\n",
      "step №117: loss = 19.094228744506836, weights = tensor([5.9597, 1.2227], requires_grad=True)\n",
      "step №118: loss = 19.067834854125977, weights = tensor([5.9625, 1.2254], requires_grad=True)\n",
      "step №119: loss = 19.042688369750977, weights = tensor([5.9652, 1.2281], requires_grad=True)\n",
      "step №120: loss = 19.018709182739258, weights = tensor([5.9678, 1.2308], requires_grad=True)\n",
      "step №121: loss = 18.995811462402344, weights = tensor([5.9703, 1.2334], requires_grad=True)\n",
      "step №122: loss = 18.973926544189453, weights = tensor([5.9727, 1.2361], requires_grad=True)\n",
      "step №123: loss = 18.952983856201172, weights = tensor([5.9750, 1.2387], requires_grad=True)\n",
      "step №124: loss = 18.932918548583984, weights = tensor([5.9772, 1.2413], requires_grad=True)\n",
      "step №125: loss = 18.913671493530273, weights = tensor([5.9793, 1.2439], requires_grad=True)\n",
      "step №126: loss = 18.895183563232422, weights = tensor([5.9814, 1.2465], requires_grad=True)\n",
      "step №127: loss = 18.877408981323242, weights = tensor([5.9833, 1.2491], requires_grad=True)\n",
      "step №128: loss = 18.860292434692383, weights = tensor([5.9852, 1.2516], requires_grad=True)\n",
      "step №129: loss = 18.843793869018555, weights = tensor([5.9870, 1.2541], requires_grad=True)\n",
      "step №130: loss = 18.82786750793457, weights = tensor([5.9887, 1.2567], requires_grad=True)\n",
      "step №131: loss = 18.81247329711914, weights = tensor([5.9903, 1.2592], requires_grad=True)\n",
      "step №132: loss = 18.79758071899414, weights = tensor([5.9919, 1.2617], requires_grad=True)\n",
      "step №133: loss = 18.783153533935547, weights = tensor([5.9934, 1.2642], requires_grad=True)\n",
      "step №134: loss = 18.769161224365234, weights = tensor([5.9948, 1.2667], requires_grad=True)\n",
      "step №135: loss = 18.755565643310547, weights = tensor([5.9962, 1.2691], requires_grad=True)\n",
      "step №136: loss = 18.74234390258789, weights = tensor([5.9975, 1.2716], requires_grad=True)\n",
      "step №137: loss = 18.729475021362305, weights = tensor([5.9988, 1.2740], requires_grad=True)\n",
      "step №138: loss = 18.716938018798828, weights = tensor([6.0000, 1.2765], requires_grad=True)\n",
      "step №139: loss = 18.704700469970703, weights = tensor([6.0011, 1.2789], requires_grad=True)\n",
      "step №140: loss = 18.692752838134766, weights = tensor([6.0022, 1.2813], requires_grad=True)\n",
      "step №141: loss = 18.68105697631836, weights = tensor([6.0033, 1.2837], requires_grad=True)\n",
      "step №142: loss = 18.66962242126465, weights = tensor([6.0043, 1.2861], requires_grad=True)\n",
      "step №143: loss = 18.658405303955078, weights = tensor([6.0052, 1.2885], requires_grad=True)\n",
      "step №144: loss = 18.64740562438965, weights = tensor([6.0061, 1.2909], requires_grad=True)\n",
      "step №145: loss = 18.636611938476562, weights = tensor([6.0070, 1.2933], requires_grad=True)\n",
      "step №146: loss = 18.626005172729492, weights = tensor([6.0078, 1.2957], requires_grad=True)\n",
      "step №147: loss = 18.61556625366211, weights = tensor([6.0086, 1.2980], requires_grad=True)\n",
      "step №148: loss = 18.60529136657715, weights = tensor([6.0093, 1.3004], requires_grad=True)\n",
      "step №149: loss = 18.59517478942871, weights = tensor([6.0100, 1.3027], requires_grad=True)\n",
      "step №150: loss = 18.585195541381836, weights = tensor([6.0107, 1.3051], requires_grad=True)\n",
      "step №151: loss = 18.57534408569336, weights = tensor([6.0113, 1.3074], requires_grad=True)\n",
      "step №152: loss = 18.565624237060547, weights = tensor([6.0119, 1.3097], requires_grad=True)\n",
      "step №153: loss = 18.556018829345703, weights = tensor([6.0125, 1.3121], requires_grad=True)\n",
      "step №154: loss = 18.54651641845703, weights = tensor([6.0130, 1.3144], requires_grad=True)\n",
      "step №155: loss = 18.537120819091797, weights = tensor([6.0135, 1.3167], requires_grad=True)\n",
      "step №156: loss = 18.527812957763672, weights = tensor([6.0140, 1.3190], requires_grad=True)\n",
      "step №157: loss = 18.518598556518555, weights = tensor([6.0144, 1.3213], requires_grad=True)\n",
      "step №158: loss = 18.50946617126465, weights = tensor([6.0148, 1.3236], requires_grad=True)\n",
      "step №159: loss = 18.500410079956055, weights = tensor([6.0152, 1.3259], requires_grad=True)\n",
      "step №160: loss = 18.491424560546875, weights = tensor([6.0156, 1.3282], requires_grad=True)\n",
      "step №161: loss = 18.482505798339844, weights = tensor([6.0159, 1.3305], requires_grad=True)\n",
      "step №162: loss = 18.47365379333496, weights = tensor([6.0162, 1.3327], requires_grad=True)\n",
      "step №163: loss = 18.464862823486328, weights = tensor([6.0165, 1.3350], requires_grad=True)\n",
      "step №164: loss = 18.456125259399414, weights = tensor([6.0168, 1.3373], requires_grad=True)\n",
      "step №165: loss = 18.447437286376953, weights = tensor([6.0170, 1.3395], requires_grad=True)\n",
      "step №166: loss = 18.43880271911621, weights = tensor([6.0172, 1.3418], requires_grad=True)\n",
      "step №167: loss = 18.430206298828125, weights = tensor([6.0174, 1.3441], requires_grad=True)\n",
      "step №168: loss = 18.421663284301758, weights = tensor([6.0176, 1.3463], requires_grad=True)\n",
      "step №169: loss = 18.41314697265625, weights = tensor([6.0178, 1.3486], requires_grad=True)\n",
      "step №170: loss = 18.404682159423828, weights = tensor([6.0179, 1.3508], requires_grad=True)\n",
      "step №171: loss = 18.396249771118164, weights = tensor([6.0181, 1.3530], requires_grad=True)\n",
      "step №172: loss = 18.387847900390625, weights = tensor([6.0182, 1.3553], requires_grad=True)\n",
      "step №173: loss = 18.379478454589844, weights = tensor([6.0183, 1.3575], requires_grad=True)\n",
      "step №174: loss = 18.37114143371582, weights = tensor([6.0184, 1.3598], requires_grad=True)\n",
      "step №175: loss = 18.362834930419922, weights = tensor([6.0184, 1.3620], requires_grad=True)\n",
      "step №176: loss = 18.354551315307617, weights = tensor([6.0185, 1.3642], requires_grad=True)\n",
      "step №177: loss = 18.346290588378906, weights = tensor([6.0185, 1.3664], requires_grad=True)\n",
      "step №178: loss = 18.33806037902832, weights = tensor([6.0186, 1.3686], requires_grad=True)\n",
      "step №179: loss = 18.329845428466797, weights = tensor([6.0186, 1.3709], requires_grad=True)\n",
      "step №180: loss = 18.321659088134766, weights = tensor([6.0186, 1.3731], requires_grad=True)\n",
      "step №181: loss = 18.31348991394043, weights = tensor([6.0186, 1.3753], requires_grad=True)\n",
      "step №182: loss = 18.30533790588379, weights = tensor([6.0185, 1.3775], requires_grad=True)\n",
      "step №183: loss = 18.29720687866211, weights = tensor([6.0185, 1.3797], requires_grad=True)\n",
      "step №184: loss = 18.289094924926758, weights = tensor([6.0185, 1.3819], requires_grad=True)\n",
      "step №185: loss = 18.280990600585938, weights = tensor([6.0184, 1.3841], requires_grad=True)\n",
      "step №186: loss = 18.27291488647461, weights = tensor([6.0184, 1.3863], requires_grad=True)\n",
      "step №187: loss = 18.264842987060547, weights = tensor([6.0183, 1.3885], requires_grad=True)\n",
      "step №188: loss = 18.256792068481445, weights = tensor([6.0182, 1.3907], requires_grad=True)\n",
      "step №189: loss = 18.24875259399414, weights = tensor([6.0181, 1.3929], requires_grad=True)\n",
      "step №190: loss = 18.240734100341797, weights = tensor([6.0180, 1.3951], requires_grad=True)\n",
      "step №191: loss = 18.232715606689453, weights = tensor([6.0179, 1.3973], requires_grad=True)\n",
      "step №192: loss = 18.224720001220703, weights = tensor([6.0178, 1.3995], requires_grad=True)\n",
      "step №193: loss = 18.216726303100586, weights = tensor([6.0177, 1.4017], requires_grad=True)\n",
      "step №194: loss = 18.208751678466797, weights = tensor([6.0175, 1.4038], requires_grad=True)\n",
      "step №195: loss = 18.200782775878906, weights = tensor([6.0174, 1.4060], requires_grad=True)\n",
      "step №196: loss = 18.192821502685547, weights = tensor([6.0172, 1.4082], requires_grad=True)\n",
      "step №197: loss = 18.184879302978516, weights = tensor([6.0171, 1.4104], requires_grad=True)\n",
      "step №198: loss = 18.176939010620117, weights = tensor([6.0169, 1.4125], requires_grad=True)\n",
      "step №199: loss = 18.169017791748047, weights = tensor([6.0168, 1.4147], requires_grad=True)\n",
      "step №200: loss = 18.16109275817871, weights = tensor([6.0166, 1.4169], requires_grad=True)\n",
      "step №201: loss = 18.153188705444336, weights = tensor([6.0164, 1.4191], requires_grad=True)\n",
      "step №202: loss = 18.145288467407227, weights = tensor([6.0162, 1.4212], requires_grad=True)\n",
      "step №203: loss = 18.13739585876465, weights = tensor([6.0160, 1.4234], requires_grad=True)\n",
      "step №204: loss = 18.129512786865234, weights = tensor([6.0158, 1.4256], requires_grad=True)\n",
      "step №205: loss = 18.121633529663086, weights = tensor([6.0156, 1.4277], requires_grad=True)\n",
      "step №206: loss = 18.113765716552734, weights = tensor([6.0154, 1.4299], requires_grad=True)\n",
      "step №207: loss = 18.105907440185547, weights = tensor([6.0152, 1.4320], requires_grad=True)\n",
      "step №208: loss = 18.098052978515625, weights = tensor([6.0150, 1.4342], requires_grad=True)\n",
      "step №209: loss = 18.090206146240234, weights = tensor([6.0148, 1.4364], requires_grad=True)\n",
      "step №210: loss = 18.08237075805664, weights = tensor([6.0146, 1.4385], requires_grad=True)\n",
      "step №211: loss = 18.074535369873047, weights = tensor([6.0143, 1.4407], requires_grad=True)\n",
      "step №212: loss = 18.066715240478516, weights = tensor([6.0141, 1.4428], requires_grad=True)\n",
      "step №213: loss = 18.058897018432617, weights = tensor([6.0139, 1.4450], requires_grad=True)\n",
      "step №214: loss = 18.051088333129883, weights = tensor([6.0136, 1.4471], requires_grad=True)\n",
      "step №215: loss = 18.04327964782715, weights = tensor([6.0134, 1.4493], requires_grad=True)\n",
      "step №216: loss = 18.03548812866211, weights = tensor([6.0132, 1.4514], requires_grad=True)\n",
      "step №217: loss = 18.027690887451172, weights = tensor([6.0129, 1.4536], requires_grad=True)\n",
      "step №218: loss = 18.019906997680664, weights = tensor([6.0127, 1.4557], requires_grad=True)\n",
      "step №219: loss = 18.012130737304688, weights = tensor([6.0124, 1.4579], requires_grad=True)\n",
      "step №220: loss = 18.004352569580078, weights = tensor([6.0121, 1.4600], requires_grad=True)\n",
      "step №221: loss = 17.9965877532959, weights = tensor([6.0119, 1.4622], requires_grad=True)\n",
      "step №222: loss = 17.988828659057617, weights = tensor([6.0116, 1.4643], requires_grad=True)\n",
      "step №223: loss = 17.9810733795166, weights = tensor([6.0114, 1.4664], requires_grad=True)\n",
      "step №224: loss = 17.973323822021484, weights = tensor([6.0111, 1.4686], requires_grad=True)\n",
      "step №225: loss = 17.96558380126953, weights = tensor([6.0108, 1.4707], requires_grad=True)\n",
      "step №226: loss = 17.957843780517578, weights = tensor([6.0106, 1.4729], requires_grad=True)\n",
      "step №227: loss = 17.950119018554688, weights = tensor([6.0103, 1.4750], requires_grad=True)\n",
      "step №228: loss = 17.942386627197266, weights = tensor([6.0100, 1.4771], requires_grad=True)\n",
      "step №229: loss = 17.934667587280273, weights = tensor([6.0097, 1.4793], requires_grad=True)\n",
      "step №230: loss = 17.926952362060547, weights = tensor([6.0094, 1.4814], requires_grad=True)\n",
      "step №231: loss = 17.919246673583984, weights = tensor([6.0092, 1.4835], requires_grad=True)\n",
      "step №232: loss = 17.91154670715332, weights = tensor([6.0089, 1.4857], requires_grad=True)\n",
      "step №233: loss = 17.90384292602539, weights = tensor([6.0086, 1.4878], requires_grad=True)\n",
      "step №234: loss = 17.896156311035156, weights = tensor([6.0083, 1.4899], requires_grad=True)\n",
      "step №235: loss = 17.888473510742188, weights = tensor([6.0080, 1.4920], requires_grad=True)\n",
      "step №236: loss = 17.88079071044922, weights = tensor([6.0077, 1.4942], requires_grad=True)\n",
      "step №237: loss = 17.873119354248047, weights = tensor([6.0074, 1.4963], requires_grad=True)\n",
      "step №238: loss = 17.865449905395508, weights = tensor([6.0071, 1.4984], requires_grad=True)\n",
      "step №239: loss = 17.857784271240234, weights = tensor([6.0068, 1.5005], requires_grad=True)\n",
      "step №240: loss = 17.85012435913086, weights = tensor([6.0065, 1.5027], requires_grad=True)\n",
      "step №241: loss = 17.84247398376465, weights = tensor([6.0062, 1.5048], requires_grad=True)\n",
      "step №242: loss = 17.834827423095703, weights = tensor([6.0059, 1.5069], requires_grad=True)\n",
      "step №243: loss = 17.82718276977539, weights = tensor([6.0056, 1.5090], requires_grad=True)\n",
      "step №244: loss = 17.819543838500977, weights = tensor([6.0053, 1.5111], requires_grad=True)\n",
      "step №245: loss = 17.811912536621094, weights = tensor([6.0050, 1.5133], requires_grad=True)\n",
      "step №246: loss = 17.80428695678711, weights = tensor([6.0047, 1.5154], requires_grad=True)\n",
      "step №247: loss = 17.79666519165039, weights = tensor([6.0044, 1.5175], requires_grad=True)\n",
      "step №248: loss = 17.789043426513672, weights = tensor([6.0041, 1.5196], requires_grad=True)\n",
      "step №249: loss = 17.781435012817383, weights = tensor([6.0038, 1.5217], requires_grad=True)\n",
      "step №250: loss = 17.773834228515625, weights = tensor([6.0035, 1.5238], requires_grad=True)\n",
      "step №251: loss = 17.766231536865234, weights = tensor([6.0032, 1.5259], requires_grad=True)\n",
      "step №252: loss = 17.758636474609375, weights = tensor([6.0029, 1.5281], requires_grad=True)\n",
      "step №253: loss = 17.751041412353516, weights = tensor([6.0026, 1.5302], requires_grad=True)\n",
      "step №254: loss = 17.743457794189453, weights = tensor([6.0023, 1.5323], requires_grad=True)\n",
      "step №255: loss = 17.735883712768555, weights = tensor([6.0020, 1.5344], requires_grad=True)\n",
      "step №256: loss = 17.72831153869629, weights = tensor([6.0016, 1.5365], requires_grad=True)\n",
      "step №257: loss = 17.72073745727539, weights = tensor([6.0013, 1.5386], requires_grad=True)\n",
      "step №258: loss = 17.71317481994629, weights = tensor([6.0010, 1.5407], requires_grad=True)\n",
      "step №259: loss = 17.705615997314453, weights = tensor([6.0007, 1.5428], requires_grad=True)\n",
      "step №260: loss = 17.698062896728516, weights = tensor([6.0004, 1.5449], requires_grad=True)\n",
      "step №261: loss = 17.69051742553711, weights = tensor([6.0001, 1.5470], requires_grad=True)\n",
      "step №262: loss = 17.682971954345703, weights = tensor([5.9998, 1.5491], requires_grad=True)\n",
      "step №263: loss = 17.675434112548828, weights = tensor([5.9994, 1.5512], requires_grad=True)\n",
      "step №264: loss = 17.667903900146484, weights = tensor([5.9991, 1.5533], requires_grad=True)\n",
      "step №265: loss = 17.660377502441406, weights = tensor([5.9988, 1.5554], requires_grad=True)\n",
      "step №266: loss = 17.652849197387695, weights = tensor([5.9985, 1.5575], requires_grad=True)\n",
      "step №267: loss = 17.64533233642578, weights = tensor([5.9982, 1.5596], requires_grad=True)\n",
      "step №268: loss = 17.637821197509766, weights = tensor([5.9978, 1.5617], requires_grad=True)\n",
      "step №269: loss = 17.63031005859375, weights = tensor([5.9975, 1.5638], requires_grad=True)\n",
      "step №270: loss = 17.622814178466797, weights = tensor([5.9972, 1.5659], requires_grad=True)\n",
      "step №271: loss = 17.61531639099121, weights = tensor([5.9969, 1.5680], requires_grad=True)\n",
      "step №272: loss = 17.607826232910156, weights = tensor([5.9966, 1.5701], requires_grad=True)\n",
      "step №273: loss = 17.600337982177734, weights = tensor([5.9962, 1.5722], requires_grad=True)\n",
      "step №274: loss = 17.59286117553711, weights = tensor([5.9959, 1.5743], requires_grad=True)\n",
      "step №275: loss = 17.585376739501953, weights = tensor([5.9956, 1.5764], requires_grad=True)\n",
      "step №276: loss = 17.577909469604492, weights = tensor([5.9953, 1.5785], requires_grad=True)\n",
      "step №277: loss = 17.570436477661133, weights = tensor([5.9950, 1.5806], requires_grad=True)\n",
      "step №278: loss = 17.5629825592041, weights = tensor([5.9946, 1.5827], requires_grad=True)\n",
      "step №279: loss = 17.555524826049805, weights = tensor([5.9943, 1.5848], requires_grad=True)\n",
      "step №280: loss = 17.54806900024414, weights = tensor([5.9940, 1.5868], requires_grad=True)\n",
      "step №281: loss = 17.54062843322754, weights = tensor([5.9937, 1.5889], requires_grad=True)\n",
      "step №282: loss = 17.533187866210938, weights = tensor([5.9933, 1.5910], requires_grad=True)\n",
      "step №283: loss = 17.52574920654297, weights = tensor([5.9930, 1.5931], requires_grad=True)\n",
      "step №284: loss = 17.518321990966797, weights = tensor([5.9927, 1.5952], requires_grad=True)\n",
      "step №285: loss = 17.510896682739258, weights = tensor([5.9924, 1.5973], requires_grad=True)\n",
      "step №286: loss = 17.50347328186035, weights = tensor([5.9920, 1.5994], requires_grad=True)\n",
      "step №287: loss = 17.496061325073242, weights = tensor([5.9917, 1.6014], requires_grad=True)\n",
      "step №288: loss = 17.4886474609375, weights = tensor([5.9914, 1.6035], requires_grad=True)\n",
      "step №289: loss = 17.48123550415039, weights = tensor([5.9911, 1.6056], requires_grad=True)\n",
      "step №290: loss = 17.473840713500977, weights = tensor([5.9907, 1.6077], requires_grad=True)\n",
      "step №291: loss = 17.466442108154297, weights = tensor([5.9904, 1.6098], requires_grad=True)\n",
      "step №292: loss = 17.459054946899414, weights = tensor([5.9901, 1.6119], requires_grad=True)\n",
      "step №293: loss = 17.451669692993164, weights = tensor([5.9898, 1.6139], requires_grad=True)\n",
      "step №294: loss = 17.44428253173828, weights = tensor([5.9894, 1.6160], requires_grad=True)\n",
      "step №295: loss = 17.43691062927246, weights = tensor([5.9891, 1.6181], requires_grad=True)\n",
      "step №296: loss = 17.429540634155273, weights = tensor([5.9888, 1.6202], requires_grad=True)\n",
      "step №297: loss = 17.422168731689453, weights = tensor([5.9885, 1.6222], requires_grad=True)\n",
      "step №298: loss = 17.41481590270996, weights = tensor([5.9881, 1.6243], requires_grad=True)\n",
      "step №299: loss = 17.407461166381836, weights = tensor([5.9878, 1.6264], requires_grad=True)\n",
      "step №300: loss = 17.400110244750977, weights = tensor([5.9875, 1.6285], requires_grad=True)\n",
      "step №301: loss = 17.392759323120117, weights = tensor([5.9872, 1.6305], requires_grad=True)\n",
      "step №302: loss = 17.385417938232422, weights = tensor([5.9868, 1.6326], requires_grad=True)\n",
      "step №303: loss = 17.378084182739258, weights = tensor([5.9865, 1.6347], requires_grad=True)\n",
      "step №304: loss = 17.370752334594727, weights = tensor([5.9862, 1.6368], requires_grad=True)\n",
      "step №305: loss = 17.363426208496094, weights = tensor([5.9859, 1.6388], requires_grad=True)\n",
      "step №306: loss = 17.35610580444336, weights = tensor([5.9855, 1.6409], requires_grad=True)\n",
      "step №307: loss = 17.348793029785156, weights = tensor([5.9852, 1.6430], requires_grad=True)\n",
      "step №308: loss = 17.341480255126953, weights = tensor([5.9849, 1.6450], requires_grad=True)\n",
      "step №309: loss = 17.334177017211914, weights = tensor([5.9846, 1.6471], requires_grad=True)\n",
      "step №310: loss = 17.32686996459961, weights = tensor([5.9842, 1.6492], requires_grad=True)\n",
      "step №311: loss = 17.319580078125, weights = tensor([5.9839, 1.6512], requires_grad=True)\n",
      "step №312: loss = 17.312286376953125, weights = tensor([5.9836, 1.6533], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №313: loss = 17.305004119873047, weights = tensor([5.9833, 1.6554], requires_grad=True)\n",
      "step №314: loss = 17.29771614074707, weights = tensor([5.9829, 1.6574], requires_grad=True)\n",
      "step №315: loss = 17.290443420410156, weights = tensor([5.9826, 1.6595], requires_grad=True)\n",
      "step №316: loss = 17.28316879272461, weights = tensor([5.9823, 1.6616], requires_grad=True)\n",
      "step №317: loss = 17.275907516479492, weights = tensor([5.9819, 1.6636], requires_grad=True)\n",
      "step №318: loss = 17.268638610839844, weights = tensor([5.9816, 1.6657], requires_grad=True)\n",
      "step №319: loss = 17.261390686035156, weights = tensor([5.9813, 1.6677], requires_grad=True)\n",
      "step №320: loss = 17.254135131835938, weights = tensor([5.9810, 1.6698], requires_grad=True)\n",
      "step №321: loss = 17.246891021728516, weights = tensor([5.9806, 1.6719], requires_grad=True)\n",
      "step №322: loss = 17.239648818969727, weights = tensor([5.9803, 1.6739], requires_grad=True)\n",
      "step №323: loss = 17.232412338256836, weights = tensor([5.9800, 1.6760], requires_grad=True)\n",
      "step №324: loss = 17.225177764892578, weights = tensor([5.9797, 1.6780], requires_grad=True)\n",
      "step №325: loss = 17.217954635620117, weights = tensor([5.9793, 1.6801], requires_grad=True)\n",
      "step №326: loss = 17.210735321044922, weights = tensor([5.9790, 1.6821], requires_grad=True)\n",
      "step №327: loss = 17.20351791381836, weights = tensor([5.9787, 1.6842], requires_grad=True)\n",
      "step №328: loss = 17.196300506591797, weights = tensor([5.9784, 1.6863], requires_grad=True)\n",
      "step №329: loss = 17.189098358154297, weights = tensor([5.9780, 1.6883], requires_grad=True)\n",
      "step №330: loss = 17.181888580322266, weights = tensor([5.9777, 1.6904], requires_grad=True)\n",
      "step №331: loss = 17.174694061279297, weights = tensor([5.9774, 1.6924], requires_grad=True)\n",
      "step №332: loss = 17.167495727539062, weights = tensor([5.9771, 1.6945], requires_grad=True)\n",
      "step №333: loss = 17.160314559936523, weights = tensor([5.9767, 1.6965], requires_grad=True)\n",
      "step №334: loss = 17.15312957763672, weights = tensor([5.9764, 1.6986], requires_grad=True)\n",
      "step №335: loss = 17.145952224731445, weights = tensor([5.9761, 1.7006], requires_grad=True)\n",
      "step №336: loss = 17.138776779174805, weights = tensor([5.9758, 1.7027], requires_grad=True)\n",
      "step №337: loss = 17.131610870361328, weights = tensor([5.9754, 1.7047], requires_grad=True)\n",
      "step №338: loss = 17.12444496154785, weights = tensor([5.9751, 1.7068], requires_grad=True)\n",
      "step №339: loss = 17.11728286743164, weights = tensor([5.9748, 1.7088], requires_grad=True)\n",
      "step №340: loss = 17.11013412475586, weights = tensor([5.9745, 1.7108], requires_grad=True)\n",
      "step №341: loss = 17.102983474731445, weights = tensor([5.9741, 1.7129], requires_grad=True)\n",
      "step №342: loss = 17.095840454101562, weights = tensor([5.9738, 1.7149], requires_grad=True)\n",
      "step №343: loss = 17.088703155517578, weights = tensor([5.9735, 1.7170], requires_grad=True)\n",
      "step №344: loss = 17.081560134887695, weights = tensor([5.9732, 1.7190], requires_grad=True)\n",
      "step №345: loss = 17.07443618774414, weights = tensor([5.9728, 1.7211], requires_grad=True)\n",
      "step №346: loss = 17.06730842590332, weights = tensor([5.9725, 1.7231], requires_grad=True)\n",
      "step №347: loss = 17.0601863861084, weights = tensor([5.9722, 1.7251], requires_grad=True)\n",
      "step №348: loss = 17.053071975708008, weights = tensor([5.9719, 1.7272], requires_grad=True)\n",
      "step №349: loss = 17.045963287353516, weights = tensor([5.9715, 1.7292], requires_grad=True)\n",
      "step №350: loss = 17.038860321044922, weights = tensor([5.9712, 1.7313], requires_grad=True)\n",
      "step №351: loss = 17.031755447387695, weights = tensor([5.9709, 1.7333], requires_grad=True)\n",
      "step №352: loss = 17.024658203125, weights = tensor([5.9706, 1.7353], requires_grad=True)\n",
      "step №353: loss = 17.017566680908203, weights = tensor([5.9702, 1.7374], requires_grad=True)\n",
      "step №354: loss = 17.010478973388672, weights = tensor([5.9699, 1.7394], requires_grad=True)\n",
      "step №355: loss = 17.003402709960938, weights = tensor([5.9696, 1.7414], requires_grad=True)\n",
      "step №356: loss = 16.996328353881836, weights = tensor([5.9693, 1.7435], requires_grad=True)\n",
      "step №357: loss = 16.989253997802734, weights = tensor([5.9689, 1.7455], requires_grad=True)\n",
      "step №358: loss = 16.98218536376953, weights = tensor([5.9686, 1.7475], requires_grad=True)\n",
      "step №359: loss = 16.97512435913086, weights = tensor([5.9683, 1.7496], requires_grad=True)\n",
      "step №360: loss = 16.96806526184082, weights = tensor([5.9680, 1.7516], requires_grad=True)\n",
      "step №361: loss = 16.961009979248047, weights = tensor([5.9676, 1.7536], requires_grad=True)\n",
      "step №362: loss = 16.953964233398438, weights = tensor([5.9673, 1.7557], requires_grad=True)\n",
      "step №363: loss = 16.946918487548828, weights = tensor([5.9670, 1.7577], requires_grad=True)\n",
      "step №364: loss = 16.939884185791016, weights = tensor([5.9667, 1.7597], requires_grad=True)\n",
      "step №365: loss = 16.932849884033203, weights = tensor([5.9664, 1.7618], requires_grad=True)\n",
      "step №366: loss = 16.92582130432129, weights = tensor([5.9660, 1.7638], requires_grad=True)\n",
      "step №367: loss = 16.918794631958008, weights = tensor([5.9657, 1.7658], requires_grad=True)\n",
      "step №368: loss = 16.911775588989258, weights = tensor([5.9654, 1.7678], requires_grad=True)\n",
      "step №369: loss = 16.904766082763672, weights = tensor([5.9651, 1.7699], requires_grad=True)\n",
      "step №370: loss = 16.897754669189453, weights = tensor([5.9647, 1.7719], requires_grad=True)\n",
      "step №371: loss = 16.890743255615234, weights = tensor([5.9644, 1.7739], requires_grad=True)\n",
      "step №372: loss = 16.883747100830078, weights = tensor([5.9641, 1.7759], requires_grad=True)\n",
      "step №373: loss = 16.876745223999023, weights = tensor([5.9638, 1.7780], requires_grad=True)\n",
      "step №374: loss = 16.869760513305664, weights = tensor([5.9635, 1.7800], requires_grad=True)\n",
      "step №375: loss = 16.862775802612305, weights = tensor([5.9631, 1.7820], requires_grad=True)\n",
      "step №376: loss = 16.855792999267578, weights = tensor([5.9628, 1.7840], requires_grad=True)\n",
      "step №377: loss = 16.848812103271484, weights = tensor([5.9625, 1.7860], requires_grad=True)\n",
      "step №378: loss = 16.841838836669922, weights = tensor([5.9622, 1.7881], requires_grad=True)\n",
      "step №379: loss = 16.834875106811523, weights = tensor([5.9618, 1.7901], requires_grad=True)\n",
      "step №380: loss = 16.82790756225586, weights = tensor([5.9615, 1.7921], requires_grad=True)\n",
      "step №381: loss = 16.820953369140625, weights = tensor([5.9612, 1.7941], requires_grad=True)\n",
      "step №382: loss = 16.814001083374023, weights = tensor([5.9609, 1.7961], requires_grad=True)\n",
      "step №383: loss = 16.807052612304688, weights = tensor([5.9606, 1.7982], requires_grad=True)\n",
      "step №384: loss = 16.800106048583984, weights = tensor([5.9602, 1.8002], requires_grad=True)\n",
      "step №385: loss = 16.79317283630371, weights = tensor([5.9599, 1.8022], requires_grad=True)\n",
      "step №386: loss = 16.786235809326172, weights = tensor([5.9596, 1.8042], requires_grad=True)\n",
      "step №387: loss = 16.779306411743164, weights = tensor([5.9593, 1.8062], requires_grad=True)\n",
      "step №388: loss = 16.77237892150879, weights = tensor([5.9590, 1.8082], requires_grad=True)\n",
      "step №389: loss = 16.765460968017578, weights = tensor([5.9586, 1.8102], requires_grad=True)\n",
      "step №390: loss = 16.758541107177734, weights = tensor([5.9583, 1.8122], requires_grad=True)\n",
      "step №391: loss = 16.751630783081055, weights = tensor([5.9580, 1.8143], requires_grad=True)\n",
      "step №392: loss = 16.74472427368164, weights = tensor([5.9577, 1.8163], requires_grad=True)\n",
      "step №393: loss = 16.737823486328125, weights = tensor([5.9574, 1.8183], requires_grad=True)\n",
      "step №394: loss = 16.73093032836914, weights = tensor([5.9570, 1.8203], requires_grad=True)\n",
      "step №395: loss = 16.72403335571289, weights = tensor([5.9567, 1.8223], requires_grad=True)\n",
      "step №396: loss = 16.717147827148438, weights = tensor([5.9564, 1.8243], requires_grad=True)\n",
      "step №397: loss = 16.710262298583984, weights = tensor([5.9561, 1.8263], requires_grad=True)\n",
      "step №398: loss = 16.703388214111328, weights = tensor([5.9558, 1.8283], requires_grad=True)\n",
      "step №399: loss = 16.696516036987305, weights = tensor([5.9554, 1.8303], requires_grad=True)\n",
      "step №400: loss = 16.68964385986328, weights = tensor([5.9551, 1.8323], requires_grad=True)\n",
      "step №401: loss = 16.682781219482422, weights = tensor([5.9548, 1.8343], requires_grad=True)\n",
      "step №402: loss = 16.67592430114746, weights = tensor([5.9545, 1.8363], requires_grad=True)\n",
      "step №403: loss = 16.669069290161133, weights = tensor([5.9542, 1.8383], requires_grad=True)\n",
      "step №404: loss = 16.662216186523438, weights = tensor([5.9538, 1.8403], requires_grad=True)\n",
      "step №405: loss = 16.65536880493164, weights = tensor([5.9535, 1.8423], requires_grad=True)\n",
      "step №406: loss = 16.648530960083008, weights = tensor([5.9532, 1.8443], requires_grad=True)\n",
      "step №407: loss = 16.641691207885742, weights = tensor([5.9529, 1.8463], requires_grad=True)\n",
      "step №408: loss = 16.63486099243164, weights = tensor([5.9526, 1.8483], requires_grad=True)\n",
      "step №409: loss = 16.628032684326172, weights = tensor([5.9522, 1.8503], requires_grad=True)\n",
      "step №410: loss = 16.6212100982666, weights = tensor([5.9519, 1.8523], requires_grad=True)\n",
      "step №411: loss = 16.61439323425293, weights = tensor([5.9516, 1.8543], requires_grad=True)\n",
      "step №412: loss = 16.60757827758789, weights = tensor([5.9513, 1.8563], requires_grad=True)\n",
      "step №413: loss = 16.600772857666016, weights = tensor([5.9510, 1.8583], requires_grad=True)\n",
      "step №414: loss = 16.59396743774414, weights = tensor([5.9507, 1.8603], requires_grad=True)\n",
      "step №415: loss = 16.587173461914062, weights = tensor([5.9503, 1.8623], requires_grad=True)\n",
      "step №416: loss = 16.580371856689453, weights = tensor([5.9500, 1.8643], requires_grad=True)\n",
      "step №417: loss = 16.573583602905273, weights = tensor([5.9497, 1.8663], requires_grad=True)\n",
      "step №418: loss = 16.56679916381836, weights = tensor([5.9494, 1.8683], requires_grad=True)\n",
      "step №419: loss = 16.560016632080078, weights = tensor([5.9491, 1.8703], requires_grad=True)\n",
      "step №420: loss = 16.55324363708496, weights = tensor([5.9487, 1.8723], requires_grad=True)\n",
      "step №421: loss = 16.546472549438477, weights = tensor([5.9484, 1.8742], requires_grad=True)\n",
      "step №422: loss = 16.53969955444336, weights = tensor([5.9481, 1.8762], requires_grad=True)\n",
      "step №423: loss = 16.532941818237305, weights = tensor([5.9478, 1.8782], requires_grad=True)\n",
      "step №424: loss = 16.526180267333984, weights = tensor([5.9475, 1.8802], requires_grad=True)\n",
      "step №425: loss = 16.51942253112793, weights = tensor([5.9472, 1.8822], requires_grad=True)\n",
      "step №426: loss = 16.512676239013672, weights = tensor([5.9468, 1.8842], requires_grad=True)\n",
      "step №427: loss = 16.505931854248047, weights = tensor([5.9465, 1.8862], requires_grad=True)\n",
      "step №428: loss = 16.499197006225586, weights = tensor([5.9462, 1.8882], requires_grad=True)\n",
      "step №429: loss = 16.492460250854492, weights = tensor([5.9459, 1.8901], requires_grad=True)\n",
      "step №430: loss = 16.485727310180664, weights = tensor([5.9456, 1.8921], requires_grad=True)\n",
      "step №431: loss = 16.47899627685547, weights = tensor([5.9453, 1.8941], requires_grad=True)\n",
      "step №432: loss = 16.472278594970703, weights = tensor([5.9449, 1.8961], requires_grad=True)\n",
      "step №433: loss = 16.465564727783203, weights = tensor([5.9446, 1.8981], requires_grad=True)\n",
      "step №434: loss = 16.458852767944336, weights = tensor([5.9443, 1.9001], requires_grad=True)\n",
      "step №435: loss = 16.452144622802734, weights = tensor([5.9440, 1.9020], requires_grad=True)\n",
      "step №436: loss = 16.445438385009766, weights = tensor([5.9437, 1.9040], requires_grad=True)\n",
      "step №437: loss = 16.438739776611328, weights = tensor([5.9434, 1.9060], requires_grad=True)\n",
      "step №438: loss = 16.432048797607422, weights = tensor([5.9430, 1.9080], requires_grad=True)\n",
      "step №439: loss = 16.425355911254883, weights = tensor([5.9427, 1.9100], requires_grad=True)\n",
      "step №440: loss = 16.418670654296875, weights = tensor([5.9424, 1.9119], requires_grad=True)\n",
      "step №441: loss = 16.41199493408203, weights = tensor([5.9421, 1.9139], requires_grad=True)\n",
      "step №442: loss = 16.405315399169922, weights = tensor([5.9418, 1.9159], requires_grad=True)\n",
      "step №443: loss = 16.398639678955078, weights = tensor([5.9415, 1.9179], requires_grad=True)\n",
      "step №444: loss = 16.391979217529297, weights = tensor([5.9412, 1.9198], requires_grad=True)\n",
      "step №445: loss = 16.38531494140625, weights = tensor([5.9408, 1.9218], requires_grad=True)\n",
      "step №446: loss = 16.378658294677734, weights = tensor([5.9405, 1.9238], requires_grad=True)\n",
      "step №447: loss = 16.37200355529785, weights = tensor([5.9402, 1.9258], requires_grad=True)\n",
      "step №448: loss = 16.365352630615234, weights = tensor([5.9399, 1.9277], requires_grad=True)\n",
      "step №449: loss = 16.358707427978516, weights = tensor([5.9396, 1.9297], requires_grad=True)\n",
      "step №450: loss = 16.352069854736328, weights = tensor([5.9393, 1.9317], requires_grad=True)\n",
      "step №451: loss = 16.345434188842773, weights = tensor([5.9390, 1.9336], requires_grad=True)\n",
      "step №452: loss = 16.338802337646484, weights = tensor([5.9386, 1.9356], requires_grad=True)\n",
      "step №453: loss = 16.332176208496094, weights = tensor([5.9383, 1.9376], requires_grad=True)\n",
      "step №454: loss = 16.325557708740234, weights = tensor([5.9380, 1.9395], requires_grad=True)\n",
      "step №455: loss = 16.318933486938477, weights = tensor([5.9377, 1.9415], requires_grad=True)\n",
      "step №456: loss = 16.31232452392578, weights = tensor([5.9374, 1.9435], requires_grad=True)\n",
      "step №457: loss = 16.305713653564453, weights = tensor([5.9371, 1.9454], requires_grad=True)\n",
      "step №458: loss = 16.299114227294922, weights = tensor([5.9368, 1.9474], requires_grad=True)\n",
      "step №459: loss = 16.29251480102539, weights = tensor([5.9364, 1.9494], requires_grad=True)\n",
      "step №460: loss = 16.28591537475586, weights = tensor([5.9361, 1.9513], requires_grad=True)\n",
      "step №461: loss = 16.279325485229492, weights = tensor([5.9358, 1.9533], requires_grad=True)\n",
      "step №462: loss = 16.272737503051758, weights = tensor([5.9355, 1.9553], requires_grad=True)\n",
      "step №463: loss = 16.266159057617188, weights = tensor([5.9352, 1.9572], requires_grad=True)\n",
      "step №464: loss = 16.259580612182617, weights = tensor([5.9349, 1.9592], requires_grad=True)\n",
      "step №465: loss = 16.253009796142578, weights = tensor([5.9346, 1.9611], requires_grad=True)\n",
      "step №466: loss = 16.246437072753906, weights = tensor([5.9343, 1.9631], requires_grad=True)\n",
      "step №467: loss = 16.2398738861084, weights = tensor([5.9339, 1.9651], requires_grad=True)\n",
      "step №468: loss = 16.233312606811523, weights = tensor([5.9336, 1.9670], requires_grad=True)\n",
      "step №469: loss = 16.226762771606445, weights = tensor([5.9333, 1.9690], requires_grad=True)\n",
      "step №470: loss = 16.2202091217041, weights = tensor([5.9330, 1.9709], requires_grad=True)\n",
      "step №471: loss = 16.21366310119629, weights = tensor([5.9327, 1.9729], requires_grad=True)\n",
      "step №472: loss = 16.20711898803711, weights = tensor([5.9324, 1.9749], requires_grad=True)\n",
      "step №473: loss = 16.200580596923828, weights = tensor([5.9321, 1.9768], requires_grad=True)\n",
      "step №474: loss = 16.194053649902344, weights = tensor([5.9318, 1.9788], requires_grad=True)\n",
      "step №475: loss = 16.187522888183594, weights = tensor([5.9315, 1.9807], requires_grad=True)\n",
      "step №476: loss = 16.181001663208008, weights = tensor([5.9311, 1.9827], requires_grad=True)\n",
      "step №477: loss = 16.174480438232422, weights = tensor([5.9308, 1.9846], requires_grad=True)\n",
      "step №478: loss = 16.1679630279541, weights = tensor([5.9305, 1.9866], requires_grad=True)\n",
      "step №479: loss = 16.16145133972168, weights = tensor([5.9302, 1.9885], requires_grad=True)\n",
      "step №480: loss = 16.154943466186523, weights = tensor([5.9299, 1.9905], requires_grad=True)\n",
      "step №481: loss = 16.148447036743164, weights = tensor([5.9296, 1.9924], requires_grad=True)\n",
      "step №482: loss = 16.14194679260254, weights = tensor([5.9293, 1.9944], requires_grad=True)\n",
      "step №483: loss = 16.135456085205078, weights = tensor([5.9290, 1.9963], requires_grad=True)\n",
      "step №484: loss = 16.12896728515625, weights = tensor([5.9287, 1.9983], requires_grad=True)\n",
      "step №485: loss = 16.122478485107422, weights = tensor([5.9283, 2.0002], requires_grad=True)\n",
      "step №486: loss = 16.11600112915039, weights = tensor([5.9280, 2.0022], requires_grad=True)\n",
      "step №487: loss = 16.109525680541992, weights = tensor([5.9277, 2.0041], requires_grad=True)\n",
      "step №488: loss = 16.10305404663086, weights = tensor([5.9274, 2.0061], requires_grad=True)\n",
      "step №489: loss = 16.09658432006836, weights = tensor([5.9271, 2.0080], requires_grad=True)\n",
      "step №490: loss = 16.09012794494629, weights = tensor([5.9268, 2.0099], requires_grad=True)\n",
      "step №491: loss = 16.083667755126953, weights = tensor([5.9265, 2.0119], requires_grad=True)\n",
      "step №492: loss = 16.077213287353516, weights = tensor([5.9262, 2.0138], requires_grad=True)\n",
      "step №493: loss = 16.070762634277344, weights = tensor([5.9259, 2.0158], requires_grad=True)\n",
      "step №494: loss = 16.06432342529297, weights = tensor([5.9256, 2.0177], requires_grad=True)\n",
      "step №495: loss = 16.057878494262695, weights = tensor([5.9252, 2.0197], requires_grad=True)\n",
      "step №496: loss = 16.05143928527832, weights = tensor([5.9249, 2.0216], requires_grad=True)\n",
      "step №497: loss = 16.045013427734375, weights = tensor([5.9246, 2.0235], requires_grad=True)\n",
      "step №498: loss = 16.038583755493164, weights = tensor([5.9243, 2.0255], requires_grad=True)\n",
      "step №499: loss = 16.032155990600586, weights = tensor([5.9240, 2.0274], requires_grad=True)\n",
      "step №500: loss = 16.025737762451172, weights = tensor([5.9237, 2.0294], requires_grad=True)\n",
      "step №501: loss = 16.019325256347656, weights = tensor([5.9234, 2.0313], requires_grad=True)\n",
      "step №502: loss = 16.01291847229004, weights = tensor([5.9231, 2.0332], requires_grad=True)\n",
      "step №503: loss = 16.006511688232422, weights = tensor([5.9228, 2.0352], requires_grad=True)\n",
      "step №504: loss = 16.00010871887207, weights = tensor([5.9225, 2.0371], requires_grad=True)\n",
      "step №505: loss = 15.9937162399292, weights = tensor([5.9222, 2.0390], requires_grad=True)\n",
      "step №506: loss = 15.987319946289062, weights = tensor([5.9218, 2.0410], requires_grad=True)\n",
      "step №507: loss = 15.980931282043457, weights = tensor([5.9215, 2.0429], requires_grad=True)\n",
      "step №508: loss = 15.974546432495117, weights = tensor([5.9212, 2.0448], requires_grad=True)\n",
      "step №509: loss = 15.968165397644043, weights = tensor([5.9209, 2.0468], requires_grad=True)\n",
      "step №510: loss = 15.961793899536133, weights = tensor([5.9206, 2.0487], requires_grad=True)\n",
      "step №511: loss = 15.955419540405273, weights = tensor([5.9203, 2.0506], requires_grad=True)\n",
      "step №512: loss = 15.949053764343262, weights = tensor([5.9200, 2.0526], requires_grad=True)\n",
      "step №513: loss = 15.942692756652832, weights = tensor([5.9197, 2.0545], requires_grad=True)\n",
      "step №514: loss = 15.936332702636719, weights = tensor([5.9194, 2.0564], requires_grad=True)\n",
      "step №515: loss = 15.92997932434082, weights = tensor([5.9191, 2.0583], requires_grad=True)\n",
      "step №516: loss = 15.923632621765137, weights = tensor([5.9188, 2.0603], requires_grad=True)\n",
      "step №517: loss = 15.917284965515137, weights = tensor([5.9185, 2.0622], requires_grad=True)\n",
      "step №518: loss = 15.91093921661377, weights = tensor([5.9182, 2.0641], requires_grad=True)\n",
      "step №519: loss = 15.904606819152832, weights = tensor([5.9178, 2.0660], requires_grad=True)\n",
      "step №520: loss = 15.898277282714844, weights = tensor([5.9175, 2.0680], requires_grad=True)\n",
      "step №521: loss = 15.891949653625488, weights = tensor([5.9172, 2.0699], requires_grad=True)\n",
      "step №522: loss = 15.885627746582031, weights = tensor([5.9169, 2.0718], requires_grad=True)\n",
      "step №523: loss = 15.879307746887207, weights = tensor([5.9166, 2.0737], requires_grad=True)\n",
      "step №524: loss = 15.872990608215332, weights = tensor([5.9163, 2.0757], requires_grad=True)\n",
      "step №525: loss = 15.866676330566406, weights = tensor([5.9160, 2.0776], requires_grad=True)\n",
      "step №526: loss = 15.860371589660645, weights = tensor([5.9157, 2.0795], requires_grad=True)\n",
      "step №527: loss = 15.8540678024292, weights = tensor([5.9154, 2.0814], requires_grad=True)\n",
      "step №528: loss = 15.847773551940918, weights = tensor([5.9151, 2.0833], requires_grad=True)\n",
      "step №529: loss = 15.841476440429688, weights = tensor([5.9148, 2.0853], requires_grad=True)\n",
      "step №530: loss = 15.835189819335938, weights = tensor([5.9145, 2.0872], requires_grad=True)\n",
      "step №531: loss = 15.828906059265137, weights = tensor([5.9142, 2.0891], requires_grad=True)\n",
      "step №532: loss = 15.82262134552002, weights = tensor([5.9139, 2.0910], requires_grad=True)\n",
      "step №533: loss = 15.8163480758667, weights = tensor([5.9136, 2.0929], requires_grad=True)\n",
      "step №534: loss = 15.810073852539062, weights = tensor([5.9133, 2.0948], requires_grad=True)\n",
      "step №535: loss = 15.803805351257324, weights = tensor([5.9129, 2.0968], requires_grad=True)\n",
      "step №536: loss = 15.7975435256958, weights = tensor([5.9126, 2.0987], requires_grad=True)\n",
      "step №537: loss = 15.791285514831543, weights = tensor([5.9123, 2.1006], requires_grad=True)\n",
      "step №538: loss = 15.785028457641602, weights = tensor([5.9120, 2.1025], requires_grad=True)\n",
      "step №539: loss = 15.778775215148926, weights = tensor([5.9117, 2.1044], requires_grad=True)\n",
      "step №540: loss = 15.772529602050781, weights = tensor([5.9114, 2.1063], requires_grad=True)\n",
      "step №541: loss = 15.76628303527832, weights = tensor([5.9111, 2.1082], requires_grad=True)\n",
      "step №542: loss = 15.760045051574707, weights = tensor([5.9108, 2.1101], requires_grad=True)\n",
      "step №543: loss = 15.753805160522461, weights = tensor([5.9105, 2.1120], requires_grad=True)\n",
      "step №544: loss = 15.747578620910645, weights = tensor([5.9102, 2.1140], requires_grad=True)\n",
      "step №545: loss = 15.741351127624512, weights = tensor([5.9099, 2.1159], requires_grad=True)\n",
      "step №546: loss = 15.735136032104492, weights = tensor([5.9096, 2.1178], requires_grad=True)\n",
      "step №547: loss = 15.728917121887207, weights = tensor([5.9093, 2.1197], requires_grad=True)\n",
      "step №548: loss = 15.72270393371582, weights = tensor([5.9090, 2.1216], requires_grad=True)\n",
      "step №549: loss = 15.716489791870117, weights = tensor([5.9087, 2.1235], requires_grad=True)\n",
      "step №550: loss = 15.710287094116211, weights = tensor([5.9084, 2.1254], requires_grad=True)\n",
      "step №551: loss = 15.704086303710938, weights = tensor([5.9081, 2.1273], requires_grad=True)\n",
      "step №552: loss = 15.697885513305664, weights = tensor([5.9078, 2.1292], requires_grad=True)\n",
      "step №553: loss = 15.691694259643555, weights = tensor([5.9075, 2.1311], requires_grad=True)\n",
      "step №554: loss = 15.685506820678711, weights = tensor([5.9072, 2.1330], requires_grad=True)\n",
      "step №555: loss = 15.679326057434082, weights = tensor([5.9069, 2.1349], requires_grad=True)\n",
      "step №556: loss = 15.673144340515137, weights = tensor([5.9066, 2.1368], requires_grad=True)\n",
      "step №557: loss = 15.666972160339355, weights = tensor([5.9063, 2.1387], requires_grad=True)\n",
      "step №558: loss = 15.660795211791992, weights = tensor([5.9060, 2.1406], requires_grad=True)\n",
      "step №559: loss = 15.654632568359375, weights = tensor([5.9056, 2.1425], requires_grad=True)\n",
      "step №560: loss = 15.648470878601074, weights = tensor([5.9053, 2.1444], requires_grad=True)\n",
      "step №561: loss = 15.642313003540039, weights = tensor([5.9050, 2.1463], requires_grad=True)\n",
      "step №562: loss = 15.63615608215332, weights = tensor([5.9047, 2.1482], requires_grad=True)\n",
      "step №563: loss = 15.630002975463867, weights = tensor([5.9044, 2.1501], requires_grad=True)\n",
      "step №564: loss = 15.623861312866211, weights = tensor([5.9041, 2.1520], requires_grad=True)\n",
      "step №565: loss = 15.617716789245605, weights = tensor([5.9038, 2.1539], requires_grad=True)\n",
      "step №566: loss = 15.611577033996582, weights = tensor([5.9035, 2.1558], requires_grad=True)\n",
      "step №567: loss = 15.605445861816406, weights = tensor([5.9032, 2.1577], requires_grad=True)\n",
      "step №568: loss = 15.59931468963623, weights = tensor([5.9029, 2.1596], requires_grad=True)\n",
      "step №569: loss = 15.593191146850586, weights = tensor([5.9026, 2.1615], requires_grad=True)\n",
      "step №570: loss = 15.587068557739258, weights = tensor([5.9023, 2.1634], requires_grad=True)\n",
      "step №571: loss = 15.580950736999512, weights = tensor([5.9020, 2.1652], requires_grad=True)\n",
      "step №572: loss = 15.574838638305664, weights = tensor([5.9017, 2.1671], requires_grad=True)\n",
      "step №573: loss = 15.56872844696045, weights = tensor([5.9014, 2.1690], requires_grad=True)\n",
      "step №574: loss = 15.5626220703125, weights = tensor([5.9011, 2.1709], requires_grad=True)\n",
      "step №575: loss = 15.55652141571045, weights = tensor([5.9008, 2.1728], requires_grad=True)\n",
      "step №576: loss = 15.550424575805664, weights = tensor([5.9005, 2.1747], requires_grad=True)\n",
      "step №577: loss = 15.544332504272461, weights = tensor([5.9002, 2.1766], requires_grad=True)\n",
      "step №578: loss = 15.538243293762207, weights = tensor([5.8999, 2.1785], requires_grad=True)\n",
      "step №579: loss = 15.5321626663208, weights = tensor([5.8996, 2.1804], requires_grad=True)\n",
      "step №580: loss = 15.526079177856445, weights = tensor([5.8993, 2.1822], requires_grad=True)\n",
      "step №581: loss = 15.520009994506836, weights = tensor([5.8990, 2.1841], requires_grad=True)\n",
      "step №582: loss = 15.513936042785645, weights = tensor([5.8987, 2.1860], requires_grad=True)\n",
      "step №583: loss = 15.507867813110352, weights = tensor([5.8984, 2.1879], requires_grad=True)\n",
      "step №584: loss = 15.501806259155273, weights = tensor([5.8981, 2.1898], requires_grad=True)\n",
      "step №585: loss = 15.495742797851562, weights = tensor([5.8978, 2.1917], requires_grad=True)\n",
      "step №586: loss = 15.4896879196167, weights = tensor([5.8975, 2.1935], requires_grad=True)\n",
      "step №587: loss = 15.483637809753418, weights = tensor([5.8972, 2.1954], requires_grad=True)\n",
      "step №588: loss = 15.477587699890137, weights = tensor([5.8969, 2.1973], requires_grad=True)\n",
      "step №589: loss = 15.471548080444336, weights = tensor([5.8966, 2.1992], requires_grad=True)\n",
      "step №590: loss = 15.465507507324219, weights = tensor([5.8963, 2.2011], requires_grad=True)\n",
      "step №591: loss = 15.459477424621582, weights = tensor([5.8960, 2.2029], requires_grad=True)\n",
      "step №592: loss = 15.45344352722168, weights = tensor([5.8957, 2.2048], requires_grad=True)\n",
      "step №593: loss = 15.447415351867676, weights = tensor([5.8954, 2.2067], requires_grad=True)\n",
      "step №594: loss = 15.441396713256836, weights = tensor([5.8951, 2.2086], requires_grad=True)\n",
      "step №595: loss = 15.435380935668945, weights = tensor([5.8948, 2.2105], requires_grad=True)\n",
      "step №596: loss = 15.429359436035156, weights = tensor([5.8945, 2.2123], requires_grad=True)\n",
      "step №597: loss = 15.423352241516113, weights = tensor([5.8942, 2.2142], requires_grad=True)\n",
      "step №598: loss = 15.417346000671387, weights = tensor([5.8939, 2.2161], requires_grad=True)\n",
      "step №599: loss = 15.411343574523926, weights = tensor([5.8936, 2.2179], requires_grad=True)\n",
      "step №600: loss = 15.405342102050781, weights = tensor([5.8933, 2.2198], requires_grad=True)\n",
      "step №601: loss = 15.399348258972168, weights = tensor([5.8930, 2.2217], requires_grad=True)\n",
      "step №602: loss = 15.393359184265137, weights = tensor([5.8927, 2.2236], requires_grad=True)\n",
      "step №603: loss = 15.387370109558105, weights = tensor([5.8924, 2.2254], requires_grad=True)\n",
      "step №604: loss = 15.381391525268555, weights = tensor([5.8921, 2.2273], requires_grad=True)\n",
      "step №605: loss = 15.37541389465332, weights = tensor([5.8918, 2.2292], requires_grad=True)\n",
      "step №606: loss = 15.369436264038086, weights = tensor([5.8915, 2.2310], requires_grad=True)\n",
      "step №607: loss = 15.363469123840332, weights = tensor([5.8912, 2.2329], requires_grad=True)\n",
      "step №608: loss = 15.357503890991211, weights = tensor([5.8909, 2.2348], requires_grad=True)\n",
      "step №609: loss = 15.351541519165039, weights = tensor([5.8906, 2.2366], requires_grad=True)\n",
      "step №610: loss = 15.34558391571045, weights = tensor([5.8903, 2.2385], requires_grad=True)\n",
      "step №611: loss = 15.339632987976074, weights = tensor([5.8900, 2.2404], requires_grad=True)\n",
      "step №612: loss = 15.333683967590332, weights = tensor([5.8897, 2.2422], requires_grad=True)\n",
      "step №613: loss = 15.327737808227539, weights = tensor([5.8894, 2.2441], requires_grad=True)\n",
      "step №614: loss = 15.321794509887695, weights = tensor([5.8891, 2.2460], requires_grad=True)\n",
      "step №615: loss = 15.31585693359375, weights = tensor([5.8889, 2.2478], requires_grad=True)\n",
      "step №616: loss = 15.309926986694336, weights = tensor([5.8886, 2.2497], requires_grad=True)\n",
      "step №617: loss = 15.303993225097656, weights = tensor([5.8883, 2.2516], requires_grad=True)\n",
      "step №618: loss = 15.298074722290039, weights = tensor([5.8880, 2.2534], requires_grad=True)\n",
      "step №619: loss = 15.292150497436523, weights = tensor([5.8877, 2.2553], requires_grad=True)\n",
      "step №620: loss = 15.286231994628906, weights = tensor([5.8874, 2.2571], requires_grad=True)\n",
      "step №621: loss = 15.280322074890137, weights = tensor([5.8871, 2.2590], requires_grad=True)\n",
      "step №622: loss = 15.274406433105469, weights = tensor([5.8868, 2.2609], requires_grad=True)\n",
      "step №623: loss = 15.268503189086914, weights = tensor([5.8865, 2.2627], requires_grad=True)\n",
      "step №624: loss = 15.262603759765625, weights = tensor([5.8862, 2.2646], requires_grad=True)\n",
      "step №625: loss = 15.256708145141602, weights = tensor([5.8859, 2.2664], requires_grad=True)\n",
      "step №626: loss = 15.250813484191895, weights = tensor([5.8856, 2.2683], requires_grad=True)\n",
      "step №627: loss = 15.24492359161377, weights = tensor([5.8853, 2.2702], requires_grad=True)\n",
      "step №628: loss = 15.239036560058594, weights = tensor([5.8850, 2.2720], requires_grad=True)\n",
      "step №629: loss = 15.233156204223633, weights = tensor([5.8847, 2.2739], requires_grad=True)\n",
      "step №630: loss = 15.227276802062988, weights = tensor([5.8844, 2.2757], requires_grad=True)\n",
      "step №631: loss = 15.221405029296875, weights = tensor([5.8841, 2.2776], requires_grad=True)\n",
      "step №632: loss = 15.215535163879395, weights = tensor([5.8838, 2.2794], requires_grad=True)\n",
      "step №633: loss = 15.209672927856445, weights = tensor([5.8835, 2.2813], requires_grad=True)\n",
      "step №634: loss = 15.203811645507812, weights = tensor([5.8832, 2.2831], requires_grad=True)\n",
      "step №635: loss = 15.197949409484863, weights = tensor([5.8829, 2.2850], requires_grad=True)\n",
      "step №636: loss = 15.192098617553711, weights = tensor([5.8826, 2.2868], requires_grad=True)\n",
      "step №637: loss = 15.186245918273926, weights = tensor([5.8823, 2.2887], requires_grad=True)\n",
      "step №638: loss = 15.180401802062988, weights = tensor([5.8820, 2.2905], requires_grad=True)\n",
      "step №639: loss = 15.174560546875, weights = tensor([5.8818, 2.2924], requires_grad=True)\n",
      "step №640: loss = 15.168726921081543, weights = tensor([5.8815, 2.2942], requires_grad=True)\n",
      "step №641: loss = 15.16289234161377, weights = tensor([5.8812, 2.2961], requires_grad=True)\n",
      "step №642: loss = 15.157058715820312, weights = tensor([5.8809, 2.2979], requires_grad=True)\n",
      "step №643: loss = 15.15123176574707, weights = tensor([5.8806, 2.2998], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №644: loss = 15.145413398742676, weights = tensor([5.8803, 2.3016], requires_grad=True)\n",
      "step №645: loss = 15.139595031738281, weights = tensor([5.8800, 2.3034], requires_grad=True)\n",
      "step №646: loss = 15.13377571105957, weights = tensor([5.8797, 2.3053], requires_grad=True)\n",
      "step №647: loss = 15.127970695495605, weights = tensor([5.8794, 2.3071], requires_grad=True)\n",
      "step №648: loss = 15.122163772583008, weights = tensor([5.8791, 2.3090], requires_grad=True)\n",
      "step №649: loss = 15.116361618041992, weights = tensor([5.8788, 2.3108], requires_grad=True)\n",
      "step №650: loss = 15.110562324523926, weights = tensor([5.8785, 2.3127], requires_grad=True)\n",
      "step №651: loss = 15.104769706726074, weights = tensor([5.8782, 2.3145], requires_grad=True)\n",
      "step №652: loss = 15.098978996276855, weights = tensor([5.8779, 2.3163], requires_grad=True)\n",
      "step №653: loss = 15.09319019317627, weights = tensor([5.8776, 2.3182], requires_grad=True)\n",
      "step №654: loss = 15.087408065795898, weights = tensor([5.8773, 2.3200], requires_grad=True)\n",
      "step №655: loss = 15.081632614135742, weights = tensor([5.8770, 2.3219], requires_grad=True)\n",
      "step №656: loss = 15.07585620880127, weights = tensor([5.8768, 2.3237], requires_grad=True)\n",
      "step №657: loss = 15.070086479187012, weights = tensor([5.8765, 2.3255], requires_grad=True)\n",
      "step №658: loss = 15.06431770324707, weights = tensor([5.8762, 2.3274], requires_grad=True)\n",
      "step №659: loss = 15.058555603027344, weights = tensor([5.8759, 2.3292], requires_grad=True)\n",
      "step №660: loss = 15.052797317504883, weights = tensor([5.8756, 2.3310], requires_grad=True)\n",
      "step №661: loss = 15.047039985656738, weights = tensor([5.8753, 2.3329], requires_grad=True)\n",
      "step №662: loss = 15.041292190551758, weights = tensor([5.8750, 2.3347], requires_grad=True)\n",
      "step №663: loss = 15.035548210144043, weights = tensor([5.8747, 2.3365], requires_grad=True)\n",
      "step №664: loss = 15.029797554016113, weights = tensor([5.8744, 2.3384], requires_grad=True)\n",
      "step №665: loss = 15.02406120300293, weights = tensor([5.8741, 2.3402], requires_grad=True)\n",
      "step №666: loss = 15.01832389831543, weights = tensor([5.8738, 2.3420], requires_grad=True)\n",
      "step №667: loss = 15.012593269348145, weights = tensor([5.8735, 2.3439], requires_grad=True)\n",
      "step №668: loss = 15.006861686706543, weights = tensor([5.8732, 2.3457], requires_grad=True)\n",
      "step №669: loss = 15.001139640808105, weights = tensor([5.8730, 2.3475], requires_grad=True)\n",
      "step №670: loss = 14.9954195022583, weights = tensor([5.8727, 2.3494], requires_grad=True)\n",
      "step №671: loss = 14.989703178405762, weights = tensor([5.8724, 2.3512], requires_grad=True)\n",
      "step №672: loss = 14.98399543762207, weights = tensor([5.8721, 2.3530], requires_grad=True)\n",
      "step №673: loss = 14.97828197479248, weights = tensor([5.8718, 2.3548], requires_grad=True)\n",
      "step №674: loss = 14.972577095031738, weights = tensor([5.8715, 2.3567], requires_grad=True)\n",
      "step №675: loss = 14.966880798339844, weights = tensor([5.8712, 2.3585], requires_grad=True)\n",
      "step №676: loss = 14.961183547973633, weights = tensor([5.8709, 2.3603], requires_grad=True)\n",
      "step №677: loss = 14.955485343933105, weights = tensor([5.8706, 2.3621], requires_grad=True)\n",
      "step №678: loss = 14.949801445007324, weights = tensor([5.8703, 2.3640], requires_grad=True)\n",
      "step №679: loss = 14.944112777709961, weights = tensor([5.8700, 2.3658], requires_grad=True)\n",
      "step №680: loss = 14.938432693481445, weights = tensor([5.8698, 2.3676], requires_grad=True)\n",
      "step №681: loss = 14.932756423950195, weights = tensor([5.8695, 2.3694], requires_grad=True)\n",
      "step №682: loss = 14.927080154418945, weights = tensor([5.8692, 2.3713], requires_grad=True)\n",
      "step №683: loss = 14.921414375305176, weights = tensor([5.8689, 2.3731], requires_grad=True)\n",
      "step №684: loss = 14.915745735168457, weights = tensor([5.8686, 2.3749], requires_grad=True)\n",
      "step №685: loss = 14.910077095031738, weights = tensor([5.8683, 2.3767], requires_grad=True)\n",
      "step №686: loss = 14.904420852661133, weights = tensor([5.8680, 2.3785], requires_grad=True)\n",
      "step №687: loss = 14.898767471313477, weights = tensor([5.8677, 2.3804], requires_grad=True)\n",
      "step №688: loss = 14.893117904663086, weights = tensor([5.8674, 2.3822], requires_grad=True)\n",
      "step №689: loss = 14.887474060058594, weights = tensor([5.8671, 2.3840], requires_grad=True)\n",
      "step №690: loss = 14.881828308105469, weights = tensor([5.8668, 2.3858], requires_grad=True)\n",
      "step №691: loss = 14.876187324523926, weights = tensor([5.8666, 2.3876], requires_grad=True)\n",
      "step №692: loss = 14.8705472946167, weights = tensor([5.8663, 2.3894], requires_grad=True)\n",
      "step №693: loss = 14.864921569824219, weights = tensor([5.8660, 2.3913], requires_grad=True)\n",
      "step №694: loss = 14.859295845031738, weights = tensor([5.8657, 2.3931], requires_grad=True)\n",
      "step №695: loss = 14.853666305541992, weights = tensor([5.8654, 2.3949], requires_grad=True)\n",
      "step №696: loss = 14.848048210144043, weights = tensor([5.8651, 2.3967], requires_grad=True)\n",
      "step №697: loss = 14.842432975769043, weights = tensor([5.8648, 2.3985], requires_grad=True)\n",
      "step №698: loss = 14.836816787719727, weights = tensor([5.8645, 2.4003], requires_grad=True)\n",
      "step №699: loss = 14.831212997436523, weights = tensor([5.8642, 2.4021], requires_grad=True)\n",
      "step №700: loss = 14.825604438781738, weights = tensor([5.8640, 2.4039], requires_grad=True)\n",
      "step №701: loss = 14.820001602172852, weights = tensor([5.8637, 2.4058], requires_grad=True)\n",
      "step №702: loss = 14.814409255981445, weights = tensor([5.8634, 2.4076], requires_grad=True)\n",
      "step №703: loss = 14.808810234069824, weights = tensor([5.8631, 2.4094], requires_grad=True)\n",
      "step №704: loss = 14.803220748901367, weights = tensor([5.8628, 2.4112], requires_grad=True)\n",
      "step №705: loss = 14.797636032104492, weights = tensor([5.8625, 2.4130], requires_grad=True)\n",
      "step №706: loss = 14.792055130004883, weights = tensor([5.8622, 2.4148], requires_grad=True)\n",
      "step №707: loss = 14.786478996276855, weights = tensor([5.8619, 2.4166], requires_grad=True)\n",
      "step №708: loss = 14.780900955200195, weights = tensor([5.8617, 2.4184], requires_grad=True)\n",
      "step №709: loss = 14.775334358215332, weights = tensor([5.8614, 2.4202], requires_grad=True)\n",
      "step №710: loss = 14.769762992858887, weights = tensor([5.8611, 2.4220], requires_grad=True)\n",
      "step №711: loss = 14.764201164245605, weights = tensor([5.8608, 2.4238], requires_grad=True)\n",
      "step №712: loss = 14.758639335632324, weights = tensor([5.8605, 2.4256], requires_grad=True)\n",
      "step №713: loss = 14.753082275390625, weights = tensor([5.8602, 2.4274], requires_grad=True)\n",
      "step №714: loss = 14.747533798217773, weights = tensor([5.8599, 2.4292], requires_grad=True)\n",
      "step №715: loss = 14.741984367370605, weights = tensor([5.8596, 2.4310], requires_grad=True)\n",
      "step №716: loss = 14.73643684387207, weights = tensor([5.8594, 2.4328], requires_grad=True)\n",
      "step №717: loss = 14.730900764465332, weights = tensor([5.8591, 2.4346], requires_grad=True)\n",
      "step №718: loss = 14.72535514831543, weights = tensor([5.8588, 2.4364], requires_grad=True)\n",
      "step №719: loss = 14.719830513000488, weights = tensor([5.8585, 2.4382], requires_grad=True)\n",
      "step №720: loss = 14.714302062988281, weights = tensor([5.8582, 2.4400], requires_grad=True)\n",
      "step №721: loss = 14.708772659301758, weights = tensor([5.8579, 2.4418], requires_grad=True)\n",
      "step №722: loss = 14.703248977661133, weights = tensor([5.8576, 2.4436], requires_grad=True)\n",
      "step №723: loss = 14.697731018066406, weights = tensor([5.8573, 2.4454], requires_grad=True)\n",
      "step №724: loss = 14.692217826843262, weights = tensor([5.8571, 2.4472], requires_grad=True)\n",
      "step №725: loss = 14.68670654296875, weights = tensor([5.8568, 2.4490], requires_grad=True)\n",
      "step №726: loss = 14.681200981140137, weights = tensor([5.8565, 2.4508], requires_grad=True)\n",
      "step №727: loss = 14.675695419311523, weights = tensor([5.8562, 2.4526], requires_grad=True)\n",
      "step №728: loss = 14.670194625854492, weights = tensor([5.8559, 2.4544], requires_grad=True)\n",
      "step №729: loss = 14.664701461791992, weights = tensor([5.8556, 2.4562], requires_grad=True)\n",
      "step №730: loss = 14.659207344055176, weights = tensor([5.8553, 2.4580], requires_grad=True)\n",
      "step №731: loss = 14.653714179992676, weights = tensor([5.8551, 2.4598], requires_grad=True)\n",
      "step №732: loss = 14.648239135742188, weights = tensor([5.8548, 2.4616], requires_grad=True)\n",
      "step №733: loss = 14.6427583694458, weights = tensor([5.8545, 2.4633], requires_grad=True)\n",
      "step №734: loss = 14.63727855682373, weights = tensor([5.8542, 2.4651], requires_grad=True)\n",
      "step №735: loss = 14.631802558898926, weights = tensor([5.8539, 2.4669], requires_grad=True)\n",
      "step №736: loss = 14.626337051391602, weights = tensor([5.8536, 2.4687], requires_grad=True)\n",
      "step №737: loss = 14.620867729187012, weights = tensor([5.8533, 2.4705], requires_grad=True)\n",
      "step №738: loss = 14.61540699005127, weights = tensor([5.8531, 2.4723], requires_grad=True)\n",
      "step №739: loss = 14.609944343566895, weights = tensor([5.8528, 2.4741], requires_grad=True)\n",
      "step №740: loss = 14.60449504852295, weights = tensor([5.8525, 2.4759], requires_grad=True)\n",
      "step №741: loss = 14.599040031433105, weights = tensor([5.8522, 2.4776], requires_grad=True)\n",
      "step №742: loss = 14.593589782714844, weights = tensor([5.8519, 2.4794], requires_grad=True)\n",
      "step №743: loss = 14.588144302368164, weights = tensor([5.8516, 2.4812], requires_grad=True)\n",
      "step №744: loss = 14.582708358764648, weights = tensor([5.8513, 2.4830], requires_grad=True)\n",
      "step №745: loss = 14.5772705078125, weights = tensor([5.8511, 2.4848], requires_grad=True)\n",
      "step №746: loss = 14.571840286254883, weights = tensor([5.8508, 2.4866], requires_grad=True)\n",
      "step №747: loss = 14.566408157348633, weights = tensor([5.8505, 2.4883], requires_grad=True)\n",
      "step №748: loss = 14.560983657836914, weights = tensor([5.8502, 2.4901], requires_grad=True)\n",
      "step №749: loss = 14.555562019348145, weights = tensor([5.8499, 2.4919], requires_grad=True)\n",
      "step №750: loss = 14.550145149230957, weights = tensor([5.8496, 2.4937], requires_grad=True)\n",
      "step №751: loss = 14.544726371765137, weights = tensor([5.8494, 2.4955], requires_grad=True)\n",
      "step №752: loss = 14.53931713104248, weights = tensor([5.8491, 2.4973], requires_grad=True)\n",
      "step №753: loss = 14.533907890319824, weights = tensor([5.8488, 2.4990], requires_grad=True)\n",
      "step №754: loss = 14.52850341796875, weights = tensor([5.8485, 2.5008], requires_grad=True)\n",
      "step №755: loss = 14.523104667663574, weights = tensor([5.8482, 2.5026], requires_grad=True)\n",
      "step №756: loss = 14.51771068572998, weights = tensor([5.8479, 2.5044], requires_grad=True)\n",
      "step №757: loss = 14.512316703796387, weights = tensor([5.8477, 2.5061], requires_grad=True)\n",
      "step №758: loss = 14.506922721862793, weights = tensor([5.8474, 2.5079], requires_grad=True)\n",
      "step №759: loss = 14.501541137695312, weights = tensor([5.8471, 2.5097], requires_grad=True)\n",
      "step №760: loss = 14.4961576461792, weights = tensor([5.8468, 2.5115], requires_grad=True)\n",
      "step №761: loss = 14.490778923034668, weights = tensor([5.8465, 2.5132], requires_grad=True)\n",
      "step №762: loss = 14.485407829284668, weights = tensor([5.8462, 2.5150], requires_grad=True)\n",
      "step №763: loss = 14.480039596557617, weights = tensor([5.8460, 2.5168], requires_grad=True)\n",
      "step №764: loss = 14.474668502807617, weights = tensor([5.8457, 2.5185], requires_grad=True)\n",
      "step №765: loss = 14.469305038452148, weights = tensor([5.8454, 2.5203], requires_grad=True)\n",
      "step №766: loss = 14.463948249816895, weights = tensor([5.8451, 2.5221], requires_grad=True)\n",
      "step №767: loss = 14.458590507507324, weights = tensor([5.8448, 2.5239], requires_grad=True)\n",
      "step №768: loss = 14.453239440917969, weights = tensor([5.8446, 2.5256], requires_grad=True)\n",
      "step №769: loss = 14.44788932800293, weights = tensor([5.8443, 2.5274], requires_grad=True)\n",
      "step №770: loss = 14.442540168762207, weights = tensor([5.8440, 2.5292], requires_grad=True)\n",
      "step №771: loss = 14.437202453613281, weights = tensor([5.8437, 2.5309], requires_grad=True)\n",
      "step №772: loss = 14.431863784790039, weights = tensor([5.8434, 2.5327], requires_grad=True)\n",
      "step №773: loss = 14.42652702331543, weights = tensor([5.8431, 2.5345], requires_grad=True)\n",
      "step №774: loss = 14.421198844909668, weights = tensor([5.8429, 2.5362], requires_grad=True)\n",
      "step №775: loss = 14.415868759155273, weights = tensor([5.8426, 2.5380], requires_grad=True)\n",
      "step №776: loss = 14.410548210144043, weights = tensor([5.8423, 2.5398], requires_grad=True)\n",
      "step №777: loss = 14.405230522155762, weights = tensor([5.8420, 2.5415], requires_grad=True)\n",
      "step №778: loss = 14.39991283416748, weights = tensor([5.8417, 2.5433], requires_grad=True)\n",
      "step №779: loss = 14.394599914550781, weights = tensor([5.8415, 2.5451], requires_grad=True)\n",
      "step №780: loss = 14.389289855957031, weights = tensor([5.8412, 2.5468], requires_grad=True)\n",
      "step №781: loss = 14.38398265838623, weights = tensor([5.8409, 2.5486], requires_grad=True)\n",
      "step №782: loss = 14.378682136535645, weights = tensor([5.8406, 2.5503], requires_grad=True)\n",
      "step №783: loss = 14.373387336730957, weights = tensor([5.8403, 2.5521], requires_grad=True)\n",
      "step №784: loss = 14.368090629577637, weights = tensor([5.8400, 2.5539], requires_grad=True)\n",
      "step №785: loss = 14.36279582977295, weights = tensor([5.8398, 2.5556], requires_grad=True)\n",
      "step №786: loss = 14.357507705688477, weights = tensor([5.8395, 2.5574], requires_grad=True)\n",
      "step №787: loss = 14.35222339630127, weights = tensor([5.8392, 2.5591], requires_grad=True)\n",
      "step №788: loss = 14.346943855285645, weights = tensor([5.8389, 2.5609], requires_grad=True)\n",
      "step №789: loss = 14.341667175292969, weights = tensor([5.8386, 2.5626], requires_grad=True)\n",
      "step №790: loss = 14.336392402648926, weights = tensor([5.8384, 2.5644], requires_grad=True)\n",
      "step №791: loss = 14.33112621307373, weights = tensor([5.8381, 2.5662], requires_grad=True)\n",
      "step №792: loss = 14.325860977172852, weights = tensor([5.8378, 2.5679], requires_grad=True)\n",
      "step №793: loss = 14.320592880249023, weights = tensor([5.8375, 2.5697], requires_grad=True)\n",
      "step №794: loss = 14.315338134765625, weights = tensor([5.8372, 2.5714], requires_grad=True)\n",
      "step №795: loss = 14.310087203979492, weights = tensor([5.8370, 2.5732], requires_grad=True)\n",
      "step №796: loss = 14.304826736450195, weights = tensor([5.8367, 2.5749], requires_grad=True)\n",
      "step №797: loss = 14.299585342407227, weights = tensor([5.8364, 2.5767], requires_grad=True)\n",
      "step №798: loss = 14.294336318969727, weights = tensor([5.8361, 2.5784], requires_grad=True)\n",
      "step №799: loss = 14.289095878601074, weights = tensor([5.8359, 2.5802], requires_grad=True)\n",
      "step №800: loss = 14.283856391906738, weights = tensor([5.8356, 2.5819], requires_grad=True)\n",
      "step №801: loss = 14.2786226272583, weights = tensor([5.8353, 2.5837], requires_grad=True)\n",
      "step №802: loss = 14.273394584655762, weights = tensor([5.8350, 2.5854], requires_grad=True)\n",
      "step №803: loss = 14.268165588378906, weights = tensor([5.8347, 2.5872], requires_grad=True)\n",
      "step №804: loss = 14.262939453125, weights = tensor([5.8345, 2.5889], requires_grad=True)\n",
      "step №805: loss = 14.257720947265625, weights = tensor([5.8342, 2.5907], requires_grad=True)\n",
      "step №806: loss = 14.252504348754883, weights = tensor([5.8339, 2.5924], requires_grad=True)\n",
      "step №807: loss = 14.247291564941406, weights = tensor([5.8336, 2.5942], requires_grad=True)\n",
      "step №808: loss = 14.242085456848145, weights = tensor([5.8333, 2.5959], requires_grad=True)\n",
      "step №809: loss = 14.236879348754883, weights = tensor([5.8331, 2.5977], requires_grad=True)\n",
      "step №810: loss = 14.23167610168457, weights = tensor([5.8328, 2.5994], requires_grad=True)\n",
      "step №811: loss = 14.226480484008789, weights = tensor([5.8325, 2.6011], requires_grad=True)\n",
      "step №812: loss = 14.221282958984375, weights = tensor([5.8322, 2.6029], requires_grad=True)\n",
      "step №813: loss = 14.216092109680176, weights = tensor([5.8320, 2.6046], requires_grad=True)\n",
      "step №814: loss = 14.210901260375977, weights = tensor([5.8317, 2.6064], requires_grad=True)\n",
      "step №815: loss = 14.205716133117676, weights = tensor([5.8314, 2.6081], requires_grad=True)\n",
      "step №816: loss = 14.200535774230957, weights = tensor([5.8311, 2.6099], requires_grad=True)\n",
      "step №817: loss = 14.19536018371582, weights = tensor([5.8308, 2.6116], requires_grad=True)\n",
      "step №818: loss = 14.190185546875, weights = tensor([5.8306, 2.6133], requires_grad=True)\n",
      "step №819: loss = 14.185017585754395, weights = tensor([5.8303, 2.6151], requires_grad=True)\n",
      "step №820: loss = 14.179845809936523, weights = tensor([5.8300, 2.6168], requires_grad=True)\n",
      "step №821: loss = 14.174676895141602, weights = tensor([5.8297, 2.6185], requires_grad=True)\n",
      "step №822: loss = 14.169523239135742, weights = tensor([5.8295, 2.6203], requires_grad=True)\n",
      "step №823: loss = 14.1643648147583, weights = tensor([5.8292, 2.6220], requires_grad=True)\n",
      "step №824: loss = 14.159208297729492, weights = tensor([5.8289, 2.6238], requires_grad=True)\n",
      "step №825: loss = 14.15406322479248, weights = tensor([5.8286, 2.6255], requires_grad=True)\n",
      "step №826: loss = 14.148913383483887, weights = tensor([5.8283, 2.6272], requires_grad=True)\n",
      "step №827: loss = 14.143775939941406, weights = tensor([5.8281, 2.6290], requires_grad=True)\n",
      "step №828: loss = 14.138635635375977, weights = tensor([5.8278, 2.6307], requires_grad=True)\n",
      "step №829: loss = 14.13349723815918, weights = tensor([5.8275, 2.6324], requires_grad=True)\n",
      "step №830: loss = 14.12836742401123, weights = tensor([5.8272, 2.6342], requires_grad=True)\n",
      "step №831: loss = 14.123235702514648, weights = tensor([5.8270, 2.6359], requires_grad=True)\n",
      "step №832: loss = 14.118112564086914, weights = tensor([5.8267, 2.6376], requires_grad=True)\n",
      "step №833: loss = 14.112991333007812, weights = tensor([5.8264, 2.6394], requires_grad=True)\n",
      "step №834: loss = 14.107872009277344, weights = tensor([5.8261, 2.6411], requires_grad=True)\n",
      "step №835: loss = 14.102755546569824, weights = tensor([5.8259, 2.6428], requires_grad=True)\n",
      "step №836: loss = 14.097643852233887, weights = tensor([5.8256, 2.6445], requires_grad=True)\n",
      "step №837: loss = 14.092538833618164, weights = tensor([5.8253, 2.6463], requires_grad=True)\n",
      "step №838: loss = 14.087432861328125, weights = tensor([5.8250, 2.6480], requires_grad=True)\n",
      "step №839: loss = 14.0823335647583, weights = tensor([5.8248, 2.6497], requires_grad=True)\n",
      "step №840: loss = 14.077234268188477, weights = tensor([5.8245, 2.6515], requires_grad=True)\n",
      "step №841: loss = 14.07214069366455, weights = tensor([5.8242, 2.6532], requires_grad=True)\n",
      "step №842: loss = 14.067045211791992, weights = tensor([5.8239, 2.6549], requires_grad=True)\n",
      "step №843: loss = 14.06196403503418, weights = tensor([5.8237, 2.6566], requires_grad=True)\n",
      "step №844: loss = 14.056879043579102, weights = tensor([5.8234, 2.6584], requires_grad=True)\n",
      "step №845: loss = 14.051795959472656, weights = tensor([5.8231, 2.6601], requires_grad=True)\n",
      "step №846: loss = 14.046720504760742, weights = tensor([5.8228, 2.6618], requires_grad=True)\n",
      "step №847: loss = 14.041643142700195, weights = tensor([5.8226, 2.6635], requires_grad=True)\n",
      "step №848: loss = 14.03657341003418, weights = tensor([5.8223, 2.6652], requires_grad=True)\n",
      "step №849: loss = 14.031506538391113, weights = tensor([5.8220, 2.6670], requires_grad=True)\n",
      "step №850: loss = 14.026446342468262, weights = tensor([5.8217, 2.6687], requires_grad=True)\n",
      "step №851: loss = 14.021387100219727, weights = tensor([5.8215, 2.6704], requires_grad=True)\n",
      "step №852: loss = 14.016334533691406, weights = tensor([5.8212, 2.6721], requires_grad=True)\n",
      "step №853: loss = 14.01127815246582, weights = tensor([5.8209, 2.6738], requires_grad=True)\n",
      "step №854: loss = 14.00622844696045, weights = tensor([5.8206, 2.6756], requires_grad=True)\n",
      "step №855: loss = 14.001184463500977, weights = tensor([5.8204, 2.6773], requires_grad=True)\n",
      "step №856: loss = 13.996137619018555, weights = tensor([5.8201, 2.6790], requires_grad=True)\n",
      "step №857: loss = 13.991101264953613, weights = tensor([5.8198, 2.6807], requires_grad=True)\n",
      "step №858: loss = 13.986063003540039, weights = tensor([5.8195, 2.6824], requires_grad=True)\n",
      "step №859: loss = 13.98103141784668, weights = tensor([5.8193, 2.6841], requires_grad=True)\n",
      "step №860: loss = 13.976007461547852, weights = tensor([5.8190, 2.6859], requires_grad=True)\n",
      "step №861: loss = 13.970977783203125, weights = tensor([5.8187, 2.6876], requires_grad=True)\n",
      "step №862: loss = 13.965959548950195, weights = tensor([5.8185, 2.6893], requires_grad=True)\n",
      "step №863: loss = 13.96093463897705, weights = tensor([5.8182, 2.6910], requires_grad=True)\n",
      "step №864: loss = 13.955917358398438, weights = tensor([5.8179, 2.6927], requires_grad=True)\n",
      "step №865: loss = 13.950907707214355, weights = tensor([5.8176, 2.6944], requires_grad=True)\n",
      "step №866: loss = 13.945901870727539, weights = tensor([5.8174, 2.6961], requires_grad=True)\n",
      "step №867: loss = 13.940895080566406, weights = tensor([5.8171, 2.6979], requires_grad=True)\n",
      "step №868: loss = 13.935893058776855, weights = tensor([5.8168, 2.6996], requires_grad=True)\n",
      "step №869: loss = 13.93089485168457, weights = tensor([5.8165, 2.7013], requires_grad=True)\n",
      "step №870: loss = 13.925897598266602, weights = tensor([5.8163, 2.7030], requires_grad=True)\n",
      "step №871: loss = 13.920910835266113, weights = tensor([5.8160, 2.7047], requires_grad=True)\n",
      "step №872: loss = 13.915916442871094, weights = tensor([5.8157, 2.7064], requires_grad=True)\n",
      "step №873: loss = 13.910934448242188, weights = tensor([5.8155, 2.7081], requires_grad=True)\n",
      "step №874: loss = 13.905957221984863, weights = tensor([5.8152, 2.7098], requires_grad=True)\n",
      "step №875: loss = 13.900975227355957, weights = tensor([5.8149, 2.7115], requires_grad=True)\n",
      "step №876: loss = 13.89599609375, weights = tensor([5.8146, 2.7132], requires_grad=True)\n",
      "step №877: loss = 13.891032218933105, weights = tensor([5.8144, 2.7149], requires_grad=True)\n",
      "step №878: loss = 13.886064529418945, weights = tensor([5.8141, 2.7166], requires_grad=True)\n",
      "step №879: loss = 13.881093978881836, weights = tensor([5.8138, 2.7183], requires_grad=True)\n",
      "step №880: loss = 13.876134872436523, weights = tensor([5.8135, 2.7200], requires_grad=True)\n",
      "step №881: loss = 13.871179580688477, weights = tensor([5.8133, 2.7217], requires_grad=True)\n",
      "step №882: loss = 13.86622142791748, weights = tensor([5.8130, 2.7234], requires_grad=True)\n",
      "step №883: loss = 13.861271858215332, weights = tensor([5.8127, 2.7251], requires_grad=True)\n",
      "step №884: loss = 13.856321334838867, weights = tensor([5.8125, 2.7268], requires_grad=True)\n",
      "step №885: loss = 13.85137939453125, weights = tensor([5.8122, 2.7285], requires_grad=True)\n",
      "step №886: loss = 13.84643840789795, weights = tensor([5.8119, 2.7302], requires_grad=True)\n",
      "step №887: loss = 13.84150218963623, weights = tensor([5.8116, 2.7319], requires_grad=True)\n",
      "step №888: loss = 13.836565971374512, weights = tensor([5.8114, 2.7336], requires_grad=True)\n",
      "step №889: loss = 13.831631660461426, weights = tensor([5.8111, 2.7353], requires_grad=True)\n",
      "step №890: loss = 13.826708793640137, weights = tensor([5.8108, 2.7370], requires_grad=True)\n",
      "step №891: loss = 13.821786880493164, weights = tensor([5.8106, 2.7387], requires_grad=True)\n",
      "step №892: loss = 13.816858291625977, weights = tensor([5.8103, 2.7404], requires_grad=True)\n",
      "step №893: loss = 13.811943054199219, weights = tensor([5.8100, 2.7421], requires_grad=True)\n",
      "step №894: loss = 13.807031631469727, weights = tensor([5.8098, 2.7438], requires_grad=True)\n",
      "step №895: loss = 13.802118301391602, weights = tensor([5.8095, 2.7455], requires_grad=True)\n",
      "step №896: loss = 13.797208786010742, weights = tensor([5.8092, 2.7472], requires_grad=True)\n",
      "step №897: loss = 13.792303085327148, weights = tensor([5.8089, 2.7489], requires_grad=True)\n",
      "step №898: loss = 13.787402153015137, weights = tensor([5.8087, 2.7506], requires_grad=True)\n",
      "step №899: loss = 13.782504081726074, weights = tensor([5.8084, 2.7523], requires_grad=True)\n",
      "step №900: loss = 13.777610778808594, weights = tensor([5.8081, 2.7540], requires_grad=True)\n",
      "step №901: loss = 13.772717475891113, weights = tensor([5.8079, 2.7557], requires_grad=True)\n",
      "step №902: loss = 13.767834663391113, weights = tensor([5.8076, 2.7574], requires_grad=True)\n",
      "step №903: loss = 13.762948989868164, weights = tensor([5.8073, 2.7591], requires_grad=True)\n",
      "step №904: loss = 13.75806713104248, weights = tensor([5.8071, 2.7607], requires_grad=True)\n",
      "step №905: loss = 13.753189086914062, weights = tensor([5.8068, 2.7624], requires_grad=True)\n",
      "step №906: loss = 13.748316764831543, weights = tensor([5.8065, 2.7641], requires_grad=True)\n",
      "step №907: loss = 13.743443489074707, weights = tensor([5.8062, 2.7658], requires_grad=True)\n",
      "step №908: loss = 13.73857593536377, weights = tensor([5.8060, 2.7675], requires_grad=True)\n",
      "step №909: loss = 13.733709335327148, weights = tensor([5.8057, 2.7692], requires_grad=True)\n",
      "step №910: loss = 13.728848457336426, weights = tensor([5.8054, 2.7709], requires_grad=True)\n",
      "step №911: loss = 13.723991394042969, weights = tensor([5.8052, 2.7726], requires_grad=True)\n",
      "step №912: loss = 13.719136238098145, weights = tensor([5.8049, 2.7742], requires_grad=True)\n",
      "step №913: loss = 13.71428394317627, weights = tensor([5.8046, 2.7759], requires_grad=True)\n",
      "step №914: loss = 13.709432601928711, weights = tensor([5.8044, 2.7776], requires_grad=True)\n",
      "step №915: loss = 13.704591751098633, weights = tensor([5.8041, 2.7793], requires_grad=True)\n",
      "step №916: loss = 13.699747085571289, weights = tensor([5.8038, 2.7810], requires_grad=True)\n",
      "step №917: loss = 13.694911003112793, weights = tensor([5.8036, 2.7827], requires_grad=True)\n",
      "step №918: loss = 13.690078735351562, weights = tensor([5.8033, 2.7843], requires_grad=True)\n",
      "step №919: loss = 13.685243606567383, weights = tensor([5.8030, 2.7860], requires_grad=True)\n",
      "step №920: loss = 13.680418014526367, weights = tensor([5.8028, 2.7877], requires_grad=True)\n",
      "step №921: loss = 13.675590515136719, weights = tensor([5.8025, 2.7894], requires_grad=True)\n",
      "step №922: loss = 13.67077350616455, weights = tensor([5.8022, 2.7911], requires_grad=True)\n",
      "step №923: loss = 13.665949821472168, weights = tensor([5.8020, 2.7927], requires_grad=True)\n",
      "step №924: loss = 13.661134719848633, weights = tensor([5.8017, 2.7944], requires_grad=True)\n",
      "step №925: loss = 13.65632438659668, weights = tensor([5.8014, 2.7961], requires_grad=True)\n",
      "step №926: loss = 13.651510238647461, weights = tensor([5.8012, 2.7978], requires_grad=True)\n",
      "step №927: loss = 13.646707534790039, weights = tensor([5.8009, 2.7995], requires_grad=True)\n",
      "step №928: loss = 13.641908645629883, weights = tensor([5.8006, 2.8011], requires_grad=True)\n",
      "step №929: loss = 13.637104988098145, weights = tensor([5.8003, 2.8028], requires_grad=True)\n",
      "step №930: loss = 13.63231086730957, weights = tensor([5.8001, 2.8045], requires_grad=True)\n",
      "step №931: loss = 13.62751579284668, weights = tensor([5.7998, 2.8062], requires_grad=True)\n",
      "step №932: loss = 13.622726440429688, weights = tensor([5.7995, 2.8078], requires_grad=True)\n",
      "step №933: loss = 13.617940902709961, weights = tensor([5.7993, 2.8095], requires_grad=True)\n",
      "step №934: loss = 13.613157272338867, weights = tensor([5.7990, 2.8112], requires_grad=True)\n",
      "step №935: loss = 13.608380317687988, weights = tensor([5.7987, 2.8128], requires_grad=True)\n",
      "step №936: loss = 13.603602409362793, weights = tensor([5.7985, 2.8145], requires_grad=True)\n",
      "step №937: loss = 13.598828315734863, weights = tensor([5.7982, 2.8162], requires_grad=True)\n",
      "step №938: loss = 13.5940580368042, weights = tensor([5.7979, 2.8179], requires_grad=True)\n",
      "step №939: loss = 13.589289665222168, weights = tensor([5.7977, 2.8195], requires_grad=True)\n",
      "step №940: loss = 13.584531784057617, weights = tensor([5.7974, 2.8212], requires_grad=True)\n",
      "step №941: loss = 13.579765319824219, weights = tensor([5.7972, 2.8229], requires_grad=True)\n",
      "step №942: loss = 13.57501220703125, weights = tensor([5.7969, 2.8245], requires_grad=True)\n",
      "step №943: loss = 13.570256233215332, weights = tensor([5.7966, 2.8262], requires_grad=True)\n",
      "step №944: loss = 13.565507888793945, weights = tensor([5.7964, 2.8279], requires_grad=True)\n",
      "step №945: loss = 13.560758590698242, weights = tensor([5.7961, 2.8295], requires_grad=True)\n",
      "step №946: loss = 13.556015014648438, weights = tensor([5.7958, 2.8312], requires_grad=True)\n",
      "step №947: loss = 13.551271438598633, weights = tensor([5.7956, 2.8329], requires_grad=True)\n",
      "step №948: loss = 13.546536445617676, weights = tensor([5.7953, 2.8345], requires_grad=True)\n",
      "step №949: loss = 13.541801452636719, weights = tensor([5.7950, 2.8362], requires_grad=True)\n",
      "step №950: loss = 13.537066459655762, weights = tensor([5.7948, 2.8379], requires_grad=True)\n",
      "step №951: loss = 13.532339096069336, weights = tensor([5.7945, 2.8395], requires_grad=True)\n",
      "step №952: loss = 13.527613639831543, weights = tensor([5.7942, 2.8412], requires_grad=True)\n",
      "step №953: loss = 13.522892951965332, weights = tensor([5.7940, 2.8428], requires_grad=True)\n",
      "step №954: loss = 13.518176078796387, weights = tensor([5.7937, 2.8445], requires_grad=True)\n",
      "step №955: loss = 13.513456344604492, weights = tensor([5.7934, 2.8462], requires_grad=True)\n",
      "step №956: loss = 13.508750915527344, weights = tensor([5.7932, 2.8478], requires_grad=True)\n",
      "step №957: loss = 13.50404167175293, weights = tensor([5.7929, 2.8495], requires_grad=True)\n",
      "step №958: loss = 13.499334335327148, weights = tensor([5.7926, 2.8511], requires_grad=True)\n",
      "step №959: loss = 13.49463176727295, weights = tensor([5.7924, 2.8528], requires_grad=True)\n",
      "step №960: loss = 13.4899263381958, weights = tensor([5.7921, 2.8545], requires_grad=True)\n",
      "step №961: loss = 13.485235214233398, weights = tensor([5.7918, 2.8561], requires_grad=True)\n",
      "step №962: loss = 13.48054027557373, weights = tensor([5.7916, 2.8578], requires_grad=True)\n",
      "step №963: loss = 13.475851058959961, weights = tensor([5.7913, 2.8594], requires_grad=True)\n",
      "step №964: loss = 13.471166610717773, weights = tensor([5.7911, 2.8611], requires_grad=True)\n",
      "step №965: loss = 13.46648120880127, weights = tensor([5.7908, 2.8627], requires_grad=True)\n",
      "step №966: loss = 13.461804389953613, weights = tensor([5.7905, 2.8644], requires_grad=True)\n",
      "step №967: loss = 13.457122802734375, weights = tensor([5.7903, 2.8660], requires_grad=True)\n",
      "step №968: loss = 13.4524507522583, weights = tensor([5.7900, 2.8677], requires_grad=True)\n",
      "step №969: loss = 13.447776794433594, weights = tensor([5.7897, 2.8694], requires_grad=True)\n",
      "step №970: loss = 13.443115234375, weights = tensor([5.7895, 2.8710], requires_grad=True)\n",
      "step №971: loss = 13.438448905944824, weights = tensor([5.7892, 2.8727], requires_grad=True)\n",
      "step №972: loss = 13.433784484863281, weights = tensor([5.7889, 2.8743], requires_grad=True)\n",
      "step №973: loss = 13.429132461547852, weights = tensor([5.7887, 2.8760], requires_grad=True)\n",
      "step №974: loss = 13.424471855163574, weights = tensor([5.7884, 2.8776], requires_grad=True)\n",
      "step №975: loss = 13.419822692871094, weights = tensor([5.7882, 2.8793], requires_grad=True)\n",
      "step №976: loss = 13.415170669555664, weights = tensor([5.7879, 2.8809], requires_grad=True)\n",
      "step №977: loss = 13.410527229309082, weights = tensor([5.7876, 2.8826], requires_grad=True)\n",
      "step №978: loss = 13.40588092803955, weights = tensor([5.7874, 2.8842], requires_grad=True)\n",
      "step №979: loss = 13.401243209838867, weights = tensor([5.7871, 2.8858], requires_grad=True)\n",
      "step №980: loss = 13.396608352661133, weights = tensor([5.7868, 2.8875], requires_grad=True)\n",
      "step №981: loss = 13.391977310180664, weights = tensor([5.7866, 2.8891], requires_grad=True)\n",
      "step №982: loss = 13.387346267700195, weights = tensor([5.7863, 2.8908], requires_grad=True)\n",
      "step №983: loss = 13.382719039916992, weights = tensor([5.7861, 2.8924], requires_grad=True)\n",
      "step №984: loss = 13.378095626831055, weights = tensor([5.7858, 2.8941], requires_grad=True)\n",
      "step №985: loss = 13.37347412109375, weights = tensor([5.7855, 2.8957], requires_grad=True)\n",
      "step №986: loss = 13.368858337402344, weights = tensor([5.7853, 2.8974], requires_grad=True)\n",
      "step №987: loss = 13.364248275756836, weights = tensor([5.7850, 2.8990], requires_grad=True)\n",
      "step №988: loss = 13.359631538391113, weights = tensor([5.7847, 2.9006], requires_grad=True)\n",
      "step №989: loss = 13.355024337768555, weights = tensor([5.7845, 2.9023], requires_grad=True)\n",
      "step №990: loss = 13.350423812866211, weights = tensor([5.7842, 2.9039], requires_grad=True)\n",
      "step №991: loss = 13.345817565917969, weights = tensor([5.7840, 2.9056], requires_grad=True)\n",
      "step №992: loss = 13.341217041015625, weights = tensor([5.7837, 2.9072], requires_grad=True)\n",
      "step №993: loss = 13.336624145507812, weights = tensor([5.7834, 2.9089], requires_grad=True)\n",
      "step №994: loss = 13.33203125, weights = tensor([5.7832, 2.9105], requires_grad=True)\n",
      "step №995: loss = 13.327444076538086, weights = tensor([5.7829, 2.9121], requires_grad=True)\n",
      "step №996: loss = 13.322854995727539, weights = tensor([5.7827, 2.9138], requires_grad=True)\n",
      "step №997: loss = 13.318272590637207, weights = tensor([5.7824, 2.9154], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №998: loss = 13.313690185546875, weights = tensor([5.7821, 2.9170], requires_grad=True)\n",
      "step №999: loss = 13.309114456176758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#let's also use mse error and optimizer which are provided by Pytorch library  \n",
    "import torch.nn as nn \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float32, requires_grad = True)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "487ca024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Everything is calculated by Pytorch. Slope = 5.781870365142822, intercept = 2.9186758995056152, loss = 13.309114456176758')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Everything is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8db81eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fa3d90854a0>\n",
      "step №0: loss = 1173.342529296875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №1: loss = 1093.9241943359375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №2: loss = 1019.9765625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №3: loss = 951.1226806640625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №4: loss = 887.0119018554688, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №5: loss = 827.3171997070312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №6: loss = 771.7342529296875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №7: loss = 719.9800415039062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №8: loss = 671.7907104492188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №9: loss = 626.9205932617188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №10: loss = 585.1409912109375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №11: loss = 546.2391357421875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №12: loss = 510.0167541503906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №13: loss = 476.2890625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №14: loss = 444.8844299316406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №15: loss = 415.6426696777344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №16: loss = 388.4148254394531, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №17: loss = 363.0621337890625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №18: loss = 339.4554443359375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №19: loss = 317.4744567871094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №20: loss = 297.007080078125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №21: loss = 277.94921875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №22: loss = 260.20361328125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №23: loss = 243.67990112304688, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №24: loss = 228.2938995361328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №25: loss = 213.96728515625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №26: loss = 200.62704467773438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №27: loss = 188.20516967773438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №28: loss = 176.63845825195312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №29: loss = 165.86795043945312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №30: loss = 155.83883666992188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №31: loss = 146.5, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №32: loss = 137.80393981933594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №33: loss = 129.706298828125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №34: loss = 122.16590881347656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №35: loss = 115.14436340332031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №36: loss = 108.60588073730469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №37: loss = 102.5172348022461, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №38: loss = 96.8473892211914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №39: loss = 91.56753540039062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №40: loss = 86.65080261230469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №41: loss = 82.0721435546875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №42: loss = 77.80831909179688, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №43: loss = 73.83760070800781, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №44: loss = 70.13981628417969, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №45: loss = 66.69611358642578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №46: loss = 63.489051818847656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №47: loss = 60.50226974487305, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №48: loss = 57.720611572265625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №49: loss = 55.12998580932617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №50: loss = 52.71721267700195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №51: loss = 50.47000503540039, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №52: loss = 48.376976013183594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №53: loss = 46.427528381347656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №54: loss = 44.61174774169922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №55: loss = 42.920433044433594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №56: loss = 41.34501266479492, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №57: loss = 39.87748336791992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №58: loss = 38.51043701171875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №59: loss = 37.23694610595703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №60: loss = 36.050559997558594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №61: loss = 34.945289611816406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №62: loss = 33.91551971435547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №63: loss = 32.95607376098633, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №64: loss = 32.06211471557617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №65: loss = 31.22909164428711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №66: loss = 30.45284652709961, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №67: loss = 29.729446411132812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №68: loss = 29.05526351928711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №69: loss = 28.426895141601562, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №70: loss = 27.841201782226562, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №71: loss = 27.295223236083984, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №72: loss = 26.7862491607666, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №73: loss = 26.311710357666016, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №74: loss = 25.86923599243164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №75: loss = 25.45663070678711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №76: loss = 25.071819305419922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №77: loss = 24.712902069091797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №78: loss = 24.378093719482422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №79: loss = 24.06572723388672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №80: loss = 23.774263381958008, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №81: loss = 23.50225830078125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №82: loss = 23.24837875366211, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №83: loss = 23.01136589050293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №84: loss = 22.79006576538086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №85: loss = 22.583383560180664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №86: loss = 22.39033317565918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №87: loss = 22.209957122802734, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №88: loss = 22.041400909423828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №89: loss = 21.88383674621582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №90: loss = 21.73651123046875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №91: loss = 21.598722457885742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №92: loss = 21.46980857849121, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №93: loss = 21.349159240722656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №94: loss = 21.236209869384766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №95: loss = 21.13042640686035, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №96: loss = 21.03131866455078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №97: loss = 20.93842315673828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №98: loss = 20.85132598876953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №99: loss = 20.76959800720215, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №100: loss = 20.692901611328125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №101: loss = 20.620880126953125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №102: loss = 20.55320167541504, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №103: loss = 20.489578247070312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №104: loss = 20.42972755432129, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №105: loss = 20.373394012451172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №106: loss = 20.320327758789062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №107: loss = 20.27031135559082, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №108: loss = 20.223127365112305, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №109: loss = 20.1785945892334, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №110: loss = 20.136507034301758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №111: loss = 20.096721649169922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №112: loss = 20.05907440185547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №113: loss = 20.023406982421875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №114: loss = 19.989591598510742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №115: loss = 19.957504272460938, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №116: loss = 19.927021026611328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №117: loss = 19.898035049438477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №118: loss = 19.870431900024414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №119: loss = 19.844133377075195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №120: loss = 19.819049835205078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №121: loss = 19.79508399963379, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №122: loss = 19.772165298461914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №123: loss = 19.750228881835938, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №124: loss = 19.729196548461914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №125: loss = 19.709012985229492, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №126: loss = 19.689619064331055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №127: loss = 19.67095947265625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №128: loss = 19.65298843383789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №129: loss = 19.63564682006836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №130: loss = 19.618911743164062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №131: loss = 19.602725982666016, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №132: loss = 19.587055206298828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №133: loss = 19.571866989135742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №134: loss = 19.557125091552734, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №135: loss = 19.542804718017578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №136: loss = 19.528871536254883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №137: loss = 19.515300750732422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №138: loss = 19.502071380615234, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №139: loss = 19.489160537719727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №140: loss = 19.476537704467773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №141: loss = 19.464191436767578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №142: loss = 19.452098846435547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №143: loss = 19.440250396728516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №144: loss = 19.428621292114258, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №145: loss = 19.417198181152344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №146: loss = 19.405969619750977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №147: loss = 19.394926071166992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №148: loss = 19.384052276611328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №149: loss = 19.373329162597656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №150: loss = 19.362754821777344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №151: loss = 19.35232162475586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №152: loss = 19.342012405395508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №153: loss = 19.33182716369629, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №154: loss = 19.32175064086914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №155: loss = 19.311779022216797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №156: loss = 19.30190086364746, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №157: loss = 19.292125701904297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №158: loss = 19.282428741455078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №159: loss = 19.272815704345703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №160: loss = 19.26327133178711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №161: loss = 19.253807067871094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №162: loss = 19.244401931762695, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №163: loss = 19.23505210876465, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №164: loss = 19.225772857666016, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №165: loss = 19.216543197631836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №166: loss = 19.20735740661621, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №167: loss = 19.198232650756836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №168: loss = 19.189144134521484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №169: loss = 19.180099487304688, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №170: loss = 19.171092987060547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №171: loss = 19.162126541137695, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №172: loss = 19.153194427490234, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №173: loss = 19.14429473876953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №174: loss = 19.135425567626953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №175: loss = 19.126590728759766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №176: loss = 19.11777114868164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №177: loss = 19.10898780822754, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №178: loss = 19.100231170654297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №179: loss = 19.091501235961914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №180: loss = 19.082780838012695, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №181: loss = 19.074094772338867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №182: loss = 19.065418243408203, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №183: loss = 19.0567684173584, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №184: loss = 19.04813003540039, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №185: loss = 19.03951644897461, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №186: loss = 19.030912399291992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №187: loss = 19.022323608398438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №188: loss = 19.013757705688477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №189: loss = 19.005207061767578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №190: loss = 18.996662139892578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №191: loss = 18.98813247680664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №192: loss = 18.979612350463867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №193: loss = 18.971107482910156, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №194: loss = 18.962617874145508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №195: loss = 18.95413589477539, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №196: loss = 18.945669174194336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №197: loss = 18.937206268310547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №198: loss = 18.928760528564453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №199: loss = 18.920326232910156, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №200: loss = 18.91189193725586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №201: loss = 18.903470993041992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №202: loss = 18.89506721496582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №203: loss = 18.886659622192383, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №204: loss = 18.87826919555664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №205: loss = 18.86988639831543, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №206: loss = 18.861509323120117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №207: loss = 18.853137969970703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №208: loss = 18.844778060913086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №209: loss = 18.836429595947266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №210: loss = 18.82808494567871, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №211: loss = 18.819744110107422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №212: loss = 18.811412811279297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №213: loss = 18.8030948638916, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №214: loss = 18.794776916503906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №215: loss = 18.786474227905273, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №216: loss = 18.77816390991211, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №217: loss = 18.769874572753906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №218: loss = 18.76158332824707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №219: loss = 18.753299713134766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №220: loss = 18.745023727416992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №221: loss = 18.73675537109375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №222: loss = 18.728496551513672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №223: loss = 18.72024154663086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №224: loss = 18.711990356445312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №225: loss = 18.703746795654297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №226: loss = 18.695505142211914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №227: loss = 18.687274932861328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №228: loss = 18.67905044555664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №229: loss = 18.670827865600586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №230: loss = 18.662616729736328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №231: loss = 18.6544132232666, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №232: loss = 18.646215438842773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №233: loss = 18.638019561767578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №234: loss = 18.62982749938965, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №235: loss = 18.62164306640625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №236: loss = 18.61347198486328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №237: loss = 18.60529899597168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №238: loss = 18.597129821777344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №239: loss = 18.588970184326172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №240: loss = 18.580820083618164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №241: loss = 18.57266616821289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №242: loss = 18.56452751159668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №243: loss = 18.5563907623291, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №244: loss = 18.548254013061523, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №245: loss = 18.54012680053711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №246: loss = 18.532005310058594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №247: loss = 18.523895263671875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №248: loss = 18.51578712463379, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №249: loss = 18.507680892944336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №250: loss = 18.499584197998047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №251: loss = 18.491497039794922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №252: loss = 18.48340606689453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №253: loss = 18.475322723388672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №254: loss = 18.467252731323242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №255: loss = 18.45918083190918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №256: loss = 18.45111656188965, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №257: loss = 18.44306182861328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №258: loss = 18.435009002685547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №259: loss = 18.426956176757812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №260: loss = 18.418916702270508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №261: loss = 18.410877227783203, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №262: loss = 18.402847290039062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №263: loss = 18.394824981689453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №264: loss = 18.386802673339844, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №265: loss = 18.378786087036133, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №266: loss = 18.370784759521484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №267: loss = 18.36277961730957, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №268: loss = 18.354778289794922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №269: loss = 18.346786499023438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №270: loss = 18.33879852294922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №271: loss = 18.330814361572266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №272: loss = 18.322834014892578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №273: loss = 18.31486701965332, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №274: loss = 18.306907653808594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №275: loss = 18.2989444732666, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №276: loss = 18.290992736816406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №277: loss = 18.283037185668945, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №278: loss = 18.275096893310547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №279: loss = 18.267154693603516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №280: loss = 18.259218215942383, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №281: loss = 18.251291275024414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №282: loss = 18.243366241455078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №283: loss = 18.235454559326172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №284: loss = 18.227542877197266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №285: loss = 18.21963882446289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №286: loss = 18.21173858642578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №287: loss = 18.20383644104004, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №288: loss = 18.19594955444336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №289: loss = 18.188066482543945, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №290: loss = 18.180187225341797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №291: loss = 18.172313690185547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №292: loss = 18.164447784423828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №293: loss = 18.156579971313477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №294: loss = 18.148725509643555, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №295: loss = 18.140867233276367, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №296: loss = 18.133020401000977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №297: loss = 18.125179290771484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №298: loss = 18.117338180541992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №299: loss = 18.109508514404297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №300: loss = 18.101682662963867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №301: loss = 18.093860626220703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №302: loss = 18.086048126220703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №303: loss = 18.078231811523438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №304: loss = 18.0704288482666, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №305: loss = 18.062631607055664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №306: loss = 18.05483627319336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №307: loss = 18.047048568725586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №308: loss = 18.039264678955078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №309: loss = 18.031482696533203, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №310: loss = 18.023706436157227, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №311: loss = 18.01594352722168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №312: loss = 18.008176803588867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №313: loss = 18.000423431396484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №314: loss = 17.992671966552734, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №315: loss = 17.984922409057617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №316: loss = 17.97718048095703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №317: loss = 17.969440460205078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №318: loss = 17.961715698242188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №319: loss = 17.953983306884766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №320: loss = 17.94626235961914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №321: loss = 17.93855094909668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №322: loss = 17.930843353271484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №323: loss = 17.923131942749023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №324: loss = 17.915433883666992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №325: loss = 17.907733917236328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №326: loss = 17.900047302246094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №327: loss = 17.89236068725586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №328: loss = 17.88468360900879, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №329: loss = 17.877012252807617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №330: loss = 17.869342803955078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №331: loss = 17.861677169799805, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №332: loss = 17.854019165039062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №333: loss = 17.846363067626953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №334: loss = 17.83871841430664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №335: loss = 17.831069946289062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №336: loss = 17.82343864440918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №337: loss = 17.815805435180664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №338: loss = 17.808177947998047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №339: loss = 17.800552368164062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №340: loss = 17.792938232421875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №341: loss = 17.785327911376953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №342: loss = 17.777719497680664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №343: loss = 17.770116806030273, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №344: loss = 17.762523651123047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №345: loss = 17.75493621826172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №346: loss = 17.747346878051758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №347: loss = 17.739768981933594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №348: loss = 17.732189178466797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №349: loss = 17.724620819091797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №350: loss = 17.71705436706543, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №351: loss = 17.70949363708496, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №352: loss = 17.701936721801758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №353: loss = 17.694387435913086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №354: loss = 17.686843872070312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №355: loss = 17.679304122924805, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №356: loss = 17.67176628112793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №357: loss = 17.664241790771484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №358: loss = 17.656713485717773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №359: loss = 17.64919662475586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №360: loss = 17.64168357849121, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №361: loss = 17.634174346923828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №362: loss = 17.626670837402344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №363: loss = 17.619173049926758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №364: loss = 17.611675262451172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №365: loss = 17.604188919067383, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №366: loss = 17.596702575683594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №367: loss = 17.58922576904297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №368: loss = 17.581748962402344, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №369: loss = 17.57427978515625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №370: loss = 17.566814422607422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №371: loss = 17.559356689453125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №372: loss = 17.551904678344727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №373: loss = 17.54445457458496, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №374: loss = 17.53701400756836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №375: loss = 17.529571533203125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №376: loss = 17.52214241027832, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №377: loss = 17.51471519470215, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №378: loss = 17.50728988647461, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №379: loss = 17.499868392944336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №380: loss = 17.492456436157227, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №381: loss = 17.485050201416016, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №382: loss = 17.47764778137207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №383: loss = 17.470247268676758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №384: loss = 17.462854385375977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №385: loss = 17.455467224121094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №386: loss = 17.448087692260742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №387: loss = 17.440706253051758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №388: loss = 17.43333625793457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №389: loss = 17.425962448120117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №390: loss = 17.418601989746094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №391: loss = 17.41124153137207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №392: loss = 17.40389060974121, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №393: loss = 17.39654541015625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №394: loss = 17.389196395874023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №395: loss = 17.38186264038086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №396: loss = 17.374530792236328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №397: loss = 17.36719512939453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №398: loss = 17.35987663269043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №399: loss = 17.352558135986328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №400: loss = 17.345245361328125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №401: loss = 17.337932586669922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №402: loss = 17.330631256103516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №403: loss = 17.323331832885742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №404: loss = 17.316038131713867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №405: loss = 17.308752059936523, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №406: loss = 17.301467895507812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №407: loss = 17.294193267822266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №408: loss = 17.28691291809082, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №409: loss = 17.27964973449707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №410: loss = 17.272380828857422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №411: loss = 17.265127182006836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №412: loss = 17.257869720458984, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №413: loss = 17.250625610351562, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №414: loss = 17.243375778198242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №415: loss = 17.23613739013672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №416: loss = 17.228900909423828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №417: loss = 17.221675872802734, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №418: loss = 17.21445083618164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №419: loss = 17.207225799560547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №420: loss = 17.20001220703125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №421: loss = 17.192806243896484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №422: loss = 17.185596466064453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №423: loss = 17.178394317626953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №424: loss = 17.171201705932617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №425: loss = 17.164012908935547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №426: loss = 17.15682601928711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №427: loss = 17.149648666381836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №428: loss = 17.14246940612793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №429: loss = 17.135297775268555, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №430: loss = 17.12813377380371, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №431: loss = 17.120967864990234, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №432: loss = 17.11381721496582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №433: loss = 17.10666275024414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №434: loss = 17.099512100219727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №435: loss = 17.09237289428711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №436: loss = 17.085237503051758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №437: loss = 17.078102111816406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №438: loss = 17.07097625732422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №439: loss = 17.063854217529297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №440: loss = 17.05673599243164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №441: loss = 17.04962158203125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №442: loss = 17.042518615722656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №443: loss = 17.035409927368164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №444: loss = 17.028310775756836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №445: loss = 17.021215438842773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №446: loss = 17.01412582397461, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №447: loss = 17.00704574584961, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №448: loss = 16.999971389770508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №449: loss = 16.99289321899414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №450: loss = 16.985820770263672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №451: loss = 16.978755950927734, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №452: loss = 16.971696853637695, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №453: loss = 16.964643478393555, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №454: loss = 16.957590103149414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №455: loss = 16.95054817199707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №456: loss = 16.943504333496094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №457: loss = 16.93646812438965, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №458: loss = 16.9294376373291, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №459: loss = 16.92241096496582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №460: loss = 16.91539192199707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №461: loss = 16.908369064331055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №462: loss = 16.901357650756836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №463: loss = 16.894351959228516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №464: loss = 16.887346267700195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №465: loss = 16.880352020263672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №466: loss = 16.873355865478516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №467: loss = 16.86636734008789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №468: loss = 16.859378814697266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №469: loss = 16.852401733398438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №470: loss = 16.845428466796875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №471: loss = 16.83846092224121, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №472: loss = 16.831491470336914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №473: loss = 16.82453727722168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №474: loss = 16.81757164001465, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №475: loss = 16.810626983642578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №476: loss = 16.803682327270508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №477: loss = 16.796741485595703, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №478: loss = 16.78980255126953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №479: loss = 16.78287124633789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №480: loss = 16.775943756103516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №481: loss = 16.769023895263672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №482: loss = 16.762104034423828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №483: loss = 16.755189895629883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №484: loss = 16.74828338623047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №485: loss = 16.741374969482422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №486: loss = 16.734477996826172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №487: loss = 16.72758674621582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №488: loss = 16.720693588256836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №489: loss = 16.71380615234375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №490: loss = 16.706926345825195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №491: loss = 16.700050354003906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №492: loss = 16.693180084228516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №493: loss = 16.68630599975586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №494: loss = 16.679454803466797, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №495: loss = 16.67259407043457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №496: loss = 16.665739059448242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №497: loss = 16.658893585205078, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №498: loss = 16.652050018310547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №499: loss = 16.645214080810547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №500: loss = 16.63838005065918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №501: loss = 16.631546020507812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №502: loss = 16.624719619750977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №503: loss = 16.61790657043457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №504: loss = 16.611082077026367, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №505: loss = 16.604278564453125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №506: loss = 16.597469329833984, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №507: loss = 16.59067153930664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №508: loss = 16.583873748779297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №509: loss = 16.577075958251953, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №510: loss = 16.57029151916504, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №511: loss = 16.563507080078125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №512: loss = 16.556726455688477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №513: loss = 16.54995346069336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №514: loss = 16.54318618774414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №515: loss = 16.53641700744629, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №516: loss = 16.529659271240234, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №517: loss = 16.522903442382812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №518: loss = 16.51615333557129, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №519: loss = 16.509403228759766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №520: loss = 16.502662658691406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №521: loss = 16.495922088623047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №522: loss = 16.489192962646484, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №523: loss = 16.482463836669922, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №524: loss = 16.47574234008789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №525: loss = 16.469024658203125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №526: loss = 16.462305068969727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №527: loss = 16.455598831176758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №528: loss = 16.448890686035156, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №529: loss = 16.442188262939453, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №530: loss = 16.435489654541016, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №531: loss = 16.42879867553711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №532: loss = 16.42211151123047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №533: loss = 16.41542625427246, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №534: loss = 16.408748626708984, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №535: loss = 16.402074813842773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №536: loss = 16.395404815673828, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №537: loss = 16.388744354248047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №538: loss = 16.382081985473633, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №539: loss = 16.375429153442383, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №540: loss = 16.368772506713867, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №541: loss = 16.36212921142578, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №542: loss = 16.355487823486328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №543: loss = 16.34885025024414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №544: loss = 16.342214584350586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №545: loss = 16.335582733154297, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №546: loss = 16.32895851135254, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №547: loss = 16.322338104248047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №548: loss = 16.315725326538086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №549: loss = 16.309118270874023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №550: loss = 16.302509307861328, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №551: loss = 16.29591178894043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №552: loss = 16.2893123626709, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №553: loss = 16.282718658447266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №554: loss = 16.276126861572266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №555: loss = 16.26954460144043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №556: loss = 16.262962341308594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №557: loss = 16.25638771057129, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №558: loss = 16.249820709228516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №559: loss = 16.243249893188477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №560: loss = 16.236690521240234, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №561: loss = 16.23013687133789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №562: loss = 16.223581314086914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №563: loss = 16.217031478881836, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №564: loss = 16.210485458374023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №565: loss = 16.203950881958008, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №566: loss = 16.19741439819336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №567: loss = 16.19088363647461, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №568: loss = 16.18435287475586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №569: loss = 16.177837371826172, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №570: loss = 16.17131996154785, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №571: loss = 16.164804458618164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №572: loss = 16.15829849243164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №573: loss = 16.15178871154785, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №574: loss = 16.145292282104492, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №575: loss = 16.138797760009766, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №576: loss = 16.132307052612305, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №577: loss = 16.125816345214844, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №578: loss = 16.119335174560547, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №579: loss = 16.112855911254883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №580: loss = 16.10638427734375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №581: loss = 16.09991455078125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №582: loss = 16.09345054626465, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №583: loss = 16.08698844909668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №584: loss = 16.080535888671875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №585: loss = 16.074081420898438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №586: loss = 16.067630767822266, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №587: loss = 16.06119155883789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №588: loss = 16.054752349853516, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №589: loss = 16.048320770263672, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №590: loss = 16.041893005371094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №591: loss = 16.03546714782715, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №592: loss = 16.02904510498047, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №593: loss = 16.022624969482422, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №594: loss = 16.01621437072754, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №595: loss = 16.009801864624023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №596: loss = 16.003402709960938, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №597: loss = 15.997004508972168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №598: loss = 15.990608215332031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №599: loss = 15.984217643737793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №600: loss = 15.977828979492188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №601: loss = 15.971453666687012, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №602: loss = 15.965070724487305, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №603: loss = 15.958699226379395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №604: loss = 15.952329635620117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №605: loss = 15.945966720581055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №606: loss = 15.939602851867676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №607: loss = 15.933252334594727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №608: loss = 15.92689323425293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №609: loss = 15.920552253723145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №610: loss = 15.914209365844727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №611: loss = 15.907864570617676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №612: loss = 15.901533126831055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №613: loss = 15.89520263671875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №614: loss = 15.888875007629395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №615: loss = 15.88255786895752, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №616: loss = 15.876235961914062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №617: loss = 15.869924545288086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №618: loss = 15.863612174987793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №619: loss = 15.85731315612793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №620: loss = 15.8510103225708, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №621: loss = 15.84471607208252, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №622: loss = 15.838421821594238, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №623: loss = 15.832135200500488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №624: loss = 15.825854301452637, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №625: loss = 15.819572448730469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №626: loss = 15.813295364379883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №627: loss = 15.807031631469727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №628: loss = 15.800765991210938, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №629: loss = 15.794499397277832, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №630: loss = 15.788241386413574, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №631: loss = 15.78199291229248, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №632: loss = 15.775741577148438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №633: loss = 15.769495964050293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №634: loss = 15.76325511932373, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №635: loss = 15.7570161819458, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №636: loss = 15.750788688659668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №637: loss = 15.744555473327637, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №638: loss = 15.738332748413086, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №639: loss = 15.732110977172852, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №640: loss = 15.725896835327148, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №641: loss = 15.719683647155762, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №642: loss = 15.713479995727539, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №643: loss = 15.707273483276367, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №644: loss = 15.701080322265625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №645: loss = 15.694883346557617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №646: loss = 15.688694953918457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №647: loss = 15.682504653930664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №648: loss = 15.676322937011719, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №649: loss = 15.670145988464355, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №650: loss = 15.663975715637207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №651: loss = 15.657804489135742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №652: loss = 15.651641845703125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №653: loss = 15.645475387573242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №654: loss = 15.639322280883789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №655: loss = 15.633166313171387, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №656: loss = 15.627021789550781, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №657: loss = 15.620877265930176, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №658: loss = 15.614738464355469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №659: loss = 15.608598709106445, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №660: loss = 15.602468490600586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №661: loss = 15.596343994140625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №662: loss = 15.59022045135498, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №663: loss = 15.584096908569336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №664: loss = 15.577985763549805, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №665: loss = 15.571873664855957, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №666: loss = 15.565768241882324, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №667: loss = 15.559663772583008, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №668: loss = 15.553565979003906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №669: loss = 15.547472953796387, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №670: loss = 15.5413818359375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №671: loss = 15.535290718078613, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №672: loss = 15.529208183288574, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №673: loss = 15.523134231567383, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №674: loss = 15.517062187194824, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №675: loss = 15.510991096496582, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №676: loss = 15.504925727844238, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №677: loss = 15.498863220214844, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №678: loss = 15.492805480957031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №679: loss = 15.486753463745117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №680: loss = 15.480700492858887, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №681: loss = 15.474655151367188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №682: loss = 15.46861457824707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №683: loss = 15.462579727172852, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №684: loss = 15.456550598144531, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №685: loss = 15.450520515441895, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №686: loss = 15.444494247436523, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №687: loss = 15.438470840454102, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №688: loss = 15.432456970214844, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №689: loss = 15.426445007324219, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №690: loss = 15.420437812805176, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №691: loss = 15.41443157196045, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №692: loss = 15.408430099487305, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №693: loss = 15.402433395385742, weights = tensor([5.7819, 2.9187], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №694: loss = 15.396444320678711, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №695: loss = 15.39045238494873, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №696: loss = 15.384466171264648, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №697: loss = 15.378491401672363, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №698: loss = 15.372512817382812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №699: loss = 15.366543769836426, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №700: loss = 15.360575675964355, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №701: loss = 15.35461139678955, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №702: loss = 15.348650932312012, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №703: loss = 15.342697143554688, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №704: loss = 15.336743354797363, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №705: loss = 15.330798149108887, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №706: loss = 15.324854850769043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №707: loss = 15.318918228149414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №708: loss = 15.31297779083252, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №709: loss = 15.30705451965332, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №710: loss = 15.301121711730957, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №711: loss = 15.295194625854492, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №712: loss = 15.289278030395508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №713: loss = 15.283365249633789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №714: loss = 15.277448654174805, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №715: loss = 15.271547317504883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №716: loss = 15.26563835144043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №717: loss = 15.259737968444824, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №718: loss = 15.253843307495117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №719: loss = 15.247955322265625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №720: loss = 15.2420654296875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №721: loss = 15.236186027526855, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №722: loss = 15.230302810668945, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №723: loss = 15.2244291305542, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №724: loss = 15.218554496765137, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №725: loss = 15.212692260742188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №726: loss = 15.206822395324707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №727: loss = 15.200967788696289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №728: loss = 15.195112228393555, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №729: loss = 15.189257621765137, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №730: loss = 15.18341064453125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №731: loss = 15.177569389343262, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №732: loss = 15.171722412109375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №733: loss = 15.165895462036133, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №734: loss = 15.160059928894043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №735: loss = 15.15423583984375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №736: loss = 15.148408889770508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №737: loss = 15.142585754394531, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №738: loss = 15.136775016784668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №739: loss = 15.130960464477539, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №740: loss = 15.125149726867676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №741: loss = 15.119343757629395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №742: loss = 15.113543510437012, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №743: loss = 15.107752799987793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №744: loss = 15.101957321166992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №745: loss = 15.096166610717773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №746: loss = 15.090387344360352, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №747: loss = 15.084607124328613, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №748: loss = 15.078828811645508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №749: loss = 15.073060989379883, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №750: loss = 15.067288398742676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №751: loss = 15.061521530151367, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №752: loss = 15.055761337280273, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №753: loss = 15.050000190734863, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №754: loss = 15.04425048828125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №755: loss = 15.03849983215332, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №756: loss = 15.032754898071289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №757: loss = 15.027010917663574, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №758: loss = 15.021276473999023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №759: loss = 15.015538215637207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №760: loss = 15.009811401367188, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №761: loss = 15.004087448120117, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №762: loss = 14.99836540222168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №763: loss = 14.992647171020508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №764: loss = 14.986933708190918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №765: loss = 14.981225967407227, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №766: loss = 14.97551441192627, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №767: loss = 14.969813346862793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №768: loss = 14.964106559753418, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №769: loss = 14.958416938781738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №770: loss = 14.952723503112793, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №771: loss = 14.947042465209961, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №772: loss = 14.941354751586914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №773: loss = 14.935676574707031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №774: loss = 14.9299955368042, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №775: loss = 14.924328804016113, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №776: loss = 14.918660163879395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №777: loss = 14.912991523742676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №778: loss = 14.907333374023438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №779: loss = 14.901678085327148, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №780: loss = 14.896028518676758, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №781: loss = 14.890376091003418, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №782: loss = 14.884730339050293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №783: loss = 14.87908935546875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №784: loss = 14.873448371887207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №785: loss = 14.867815017700195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №786: loss = 14.862190246582031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №787: loss = 14.856562614440918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №788: loss = 14.85094165802002, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №789: loss = 14.845318794250488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №790: loss = 14.83970832824707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №791: loss = 14.834101676940918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №792: loss = 14.828493118286133, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №793: loss = 14.822883605957031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №794: loss = 14.817286491394043, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №795: loss = 14.811689376831055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №796: loss = 14.806097030639648, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №797: loss = 14.800511360168457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №798: loss = 14.79492473602295, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №799: loss = 14.789346694946289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №800: loss = 14.783772468566895, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №801: loss = 14.77820110321045, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №802: loss = 14.772626876831055, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №803: loss = 14.767057418823242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №804: loss = 14.761500358581543, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №805: loss = 14.755943298339844, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №806: loss = 14.750391006469727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №807: loss = 14.744840621948242, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №808: loss = 14.739291191101074, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №809: loss = 14.733752250671387, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №810: loss = 14.728215217590332, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №811: loss = 14.722673416137695, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №812: loss = 14.717144966125488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №813: loss = 14.7116117477417, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №814: loss = 14.706092834472656, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №815: loss = 14.700573921203613, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №816: loss = 14.69505500793457, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №817: loss = 14.689541816711426, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №818: loss = 14.684036254882812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №819: loss = 14.678525924682617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №820: loss = 14.673022270202637, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №821: loss = 14.66753101348877, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №822: loss = 14.66203784942627, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №823: loss = 14.656549453735352, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №824: loss = 14.651056289672852, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №825: loss = 14.645574569702148, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №826: loss = 14.640090942382812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №827: loss = 14.634625434875488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №828: loss = 14.62914752960205, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №829: loss = 14.623680114746094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №830: loss = 14.618212699890137, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №831: loss = 14.612751960754395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №832: loss = 14.6072998046875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №833: loss = 14.601846694946289, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №834: loss = 14.596392631530762, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №835: loss = 14.590947151184082, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №836: loss = 14.5855073928833, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №837: loss = 14.58006763458252, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №838: loss = 14.574633598327637, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №839: loss = 14.569201469421387, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №840: loss = 14.563776016235352, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №841: loss = 14.55835247039795, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №842: loss = 14.552929878234863, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №843: loss = 14.547513008117676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №844: loss = 14.542098999023438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №845: loss = 14.536691665649414, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №846: loss = 14.531285285949707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №847: loss = 14.525888442993164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №848: loss = 14.520483016967773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №849: loss = 14.515090942382812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №850: loss = 14.50970458984375, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №851: loss = 14.504315376281738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №852: loss = 14.498933792114258, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №853: loss = 14.493553161621094, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №854: loss = 14.488174438476562, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №855: loss = 14.482806205749512, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №856: loss = 14.477429389953613, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №857: loss = 14.472065925598145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №858: loss = 14.466705322265625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №859: loss = 14.461343765258789, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №860: loss = 14.455988883972168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №861: loss = 14.450639724731445, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №862: loss = 14.445294380187988, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №863: loss = 14.439950942993164, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №864: loss = 14.434613227844238, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №865: loss = 14.42927360534668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №866: loss = 14.42393970489502, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №867: loss = 14.418614387512207, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №868: loss = 14.413289070129395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №869: loss = 14.40796947479248, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №870: loss = 14.4026460647583, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №871: loss = 14.397329330444336, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №872: loss = 14.392024040222168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №873: loss = 14.386713981628418, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №874: loss = 14.381406784057617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №875: loss = 14.37610912322998, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №876: loss = 14.370816230773926, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №877: loss = 14.365521430969238, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №878: loss = 14.3602294921875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №879: loss = 14.354945182800293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №880: loss = 14.349665641784668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №881: loss = 14.344385147094727, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №882: loss = 14.339103698730469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №883: loss = 14.333839416503906, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №884: loss = 14.328572273254395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №885: loss = 14.323301315307617, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №886: loss = 14.318046569824219, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №887: loss = 14.312788009643555, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №888: loss = 14.307533264160156, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №889: loss = 14.302282333374023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №890: loss = 14.297039985656738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №891: loss = 14.29179573059082, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №892: loss = 14.286550521850586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №893: loss = 14.281316757202148, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №894: loss = 14.276086807250977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №895: loss = 14.270855903625488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №896: loss = 14.265626907348633, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №897: loss = 14.260408401489258, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №898: loss = 14.2551908493042, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №899: loss = 14.249977111816406, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №900: loss = 14.244768142700195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №901: loss = 14.239555358886719, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №902: loss = 14.234354972839355, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №903: loss = 14.229153633117676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №904: loss = 14.223953247070312, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №905: loss = 14.218757629394531, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №906: loss = 14.213571548461914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №907: loss = 14.208386421203613, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №908: loss = 14.20319652557373, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №909: loss = 14.198025703430176, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №910: loss = 14.192846298217773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №911: loss = 14.18767261505127, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №912: loss = 14.182502746582031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №913: loss = 14.177343368530273, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №914: loss = 14.172174453735352, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №915: loss = 14.167015075683594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №916: loss = 14.161860466003418, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №917: loss = 14.156709671020508, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №918: loss = 14.151562690734863, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №919: loss = 14.146415710449219, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №920: loss = 14.141278266906738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №921: loss = 14.136137008666992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №922: loss = 14.131009101867676, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №923: loss = 14.125874519348145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №924: loss = 14.120747566223145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №925: loss = 14.115620613098145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №926: loss = 14.110504150390625, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №927: loss = 14.105389595031738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №928: loss = 14.100275039672852, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №929: loss = 14.095163345336914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №930: loss = 14.090060234069824, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №931: loss = 14.084956169128418, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №932: loss = 14.079852104187012, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №933: loss = 14.074760437011719, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №934: loss = 14.069668769836426, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №935: loss = 14.064579963684082, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №936: loss = 14.059491157531738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №937: loss = 14.054408073425293, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №938: loss = 14.04932689666748, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №939: loss = 14.0442533493042, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №940: loss = 14.0391845703125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №941: loss = 14.034113883972168, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №942: loss = 14.02904987335205, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №943: loss = 14.0239896774292, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №944: loss = 14.018930435180664, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №945: loss = 14.013879776000977, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №946: loss = 14.008825302124023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №947: loss = 14.003778457641602, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №948: loss = 13.998733520507812, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №949: loss = 13.993688583374023, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №950: loss = 13.98865795135498, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №951: loss = 13.983617782592773, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №952: loss = 13.978589057922363, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №953: loss = 13.973562240600586, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №954: loss = 13.968536376953125, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №955: loss = 13.963518142700195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №956: loss = 13.958498001098633, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №957: loss = 13.953486442565918, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №958: loss = 13.948478698730469, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №959: loss = 13.943473815917969, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №960: loss = 13.93846607208252, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №961: loss = 13.93346881866455, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №962: loss = 13.928468704223633, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №963: loss = 13.923479080200195, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №964: loss = 13.918485641479492, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №965: loss = 13.91349983215332, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №966: loss = 13.908513069152832, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №967: loss = 13.903536796569824, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №968: loss = 13.898561477661133, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №969: loss = 13.893589973449707, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №970: loss = 13.888620376586914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №971: loss = 13.883651733398438, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №972: loss = 13.878684997558594, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №973: loss = 13.873727798461914, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №974: loss = 13.868768692016602, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №975: loss = 13.86382007598877, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №976: loss = 13.858868598937988, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №977: loss = 13.853924751281738, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №978: loss = 13.848981857299805, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №979: loss = 13.844035148620605, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №980: loss = 13.83910083770752, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №981: loss = 13.83416748046875, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №982: loss = 13.829243659973145, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №983: loss = 13.824316024780273, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №984: loss = 13.819392204284668, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №985: loss = 13.814468383789062, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №986: loss = 13.809557914733887, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №987: loss = 13.804646492004395, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №988: loss = 13.799736022949219, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №989: loss = 13.794828414916992, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №990: loss = 13.789924621582031, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №991: loss = 13.78502368927002, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №992: loss = 13.780133247375488, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №993: loss = 13.775235176086426, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №994: loss = 13.770349502563477, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №995: loss = 13.765459060668945, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №996: loss = 13.760577201843262, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №997: loss = 13.755697250366211, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №998: loss = 13.750821113586426, weights = tensor([5.7819, 2.9187], requires_grad=True)\n",
      "step №999: loss = 13.745951652526855, weights = tensor([5.7819, 2.9187], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#it's time to replace prediction calculations too\n",
    "x = x.tolist()\n",
    "y = y.tolist()\n",
    "x = torch.tensor([[item] for item in x], dtype = torch.float32)\n",
    "y = torch.tensor([[item] for item in y], dtype = torch.float32)\n",
    "model = nn.Linear(1, 1)\n",
    "print(model.parameters())\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = model(x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4acf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for name, param in model.named_parameters():\n",
    "    weights.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d34b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Now absolutely verything is calculated by Pytorch. Slope = 5.8063859939575195, intercept = 2.764946937561035, loss = 13.745951652526855')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y])\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y_pred])\n",
    "plt.title(f'Now absolutely verything is calculated by Pytorch. Slope = {weights[0].item()}, intercept = {weights[1].item()}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a11e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
