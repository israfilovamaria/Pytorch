{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a412bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18610bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6352e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eef8bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([ 5., 18., 22., 27., 34., 35., 35., 44., 47., 50.])\n"
     ]
    }
   ],
   "source": [
    "# creating some data to train Linear Regression \n",
    "x = torch.tensor(np.arange(10), dtype = torch.float32)\n",
    "y = x*5 + 10 + torch.randint(low = -5, high = 5, size = (10,), dtype = torch.float32)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2d386c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cbe7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# initializing weights\n",
    "# we have only one feature, so we need to find two coefficients: slope and intercept \n",
    "# let's initialize them as zeros \n",
    "\n",
    "w = torch.zeros(2, dtype = torch.float16, )\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d8f3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def predict(w, x):\n",
    "    y_pred = w[0]*x + w[1]\n",
    "    return y_pred \n",
    "\n",
    "def mseerror(y, y_pred):\n",
    "    loss_val = ((y_pred - y)**2).mean()\n",
    "    return loss_val \n",
    "\n",
    "# derivative from mse with respect to slope: 2x*(y - y_pred)\n",
    "def beta1gradient(x, y, y_pred):\n",
    "    direction = np.dot(2*x, y_pred - y).mean()\n",
    "    return direction \n",
    "# derivative from mse with respect to intercept: 2x*(y - y_pred)\n",
    "def beta0gradient(x, y, y_pred):\n",
    "    direction = np.dot(2, y_pred - y).mean()\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e2992c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1181.3000)\n"
     ]
    }
   ],
   "source": [
    "# let's check our predictions before training \n",
    "y_pred = predict(w, x)\n",
    "error = mseerror(y, y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b345c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174bbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1181.300048828125, weights = tensor([2.1582, 0.0380], dtype=torch.float16)\n",
      "step №1: loss = 536.2879028320312, weights = tensor([3.5762, 0.0644], dtype=torch.float16)\n",
      "step №2: loss = 257.7894592285156, weights = tensor([4.5078, 0.0831], dtype=torch.float16)\n",
      "step №3: loss = 137.53172302246094, weights = tensor([5.1211, 0.0967], dtype=torch.float16)\n",
      "step №4: loss = 85.52013397216797, weights = tensor([5.5234, 0.1069], dtype=torch.float16)\n",
      "step №5: loss = 63.1077995300293, weights = tensor([5.7852, 0.1150], dtype=torch.float16)\n",
      "step №6: loss = 53.5067138671875, weights = tensor([5.9570, 0.1216], dtype=torch.float16)\n",
      "step №7: loss = 49.333187103271484, weights = tensor([6.0703, 0.1273], dtype=torch.float16)\n",
      "step №8: loss = 47.503379821777344, weights = tensor([6.1445, 0.1324], dtype=torch.float16)\n",
      "step №9: loss = 46.69642639160156, weights = tensor([6.1953, 0.1372], dtype=torch.float16)\n",
      "step №10: loss = 46.31995391845703, weights = tensor([6.2266, 0.1416], dtype=torch.float16)\n",
      "step №11: loss = 46.15315628051758, weights = tensor([6.2461, 0.1459], dtype=torch.float16)\n",
      "step №12: loss = 46.06792068481445, weights = tensor([6.2578, 0.1500], dtype=torch.float16)\n",
      "step №13: loss = 46.0172004699707, weights = tensor([6.2656, 0.1541], dtype=torch.float16)\n",
      "step №14: loss = 45.97978210449219, weights = tensor([6.2734, 0.1581], dtype=torch.float16)\n",
      "step №15: loss = 45.94644546508789, weights = tensor([6.2773, 0.1621], dtype=torch.float16)\n",
      "step №16: loss = 45.91804504394531, weights = tensor([6.2812, 0.1660], dtype=torch.float16)\n",
      "step №17: loss = 45.89163589477539, weights = tensor([6.2812, 0.1699], dtype=torch.float16)\n",
      "step №18: loss = 45.866111755371094, weights = tensor([6.2812, 0.1738], dtype=torch.float16)\n",
      "step №19: loss = 45.84062576293945, weights = tensor([6.2812, 0.1777], dtype=torch.float16)\n",
      "step №20: loss = 45.81517028808594, weights = tensor([6.2812, 0.1816], dtype=torch.float16)\n",
      "step №21: loss = 45.78974151611328, weights = tensor([6.2812, 0.1855], dtype=torch.float16)\n",
      "step №22: loss = 45.76434326171875, weights = tensor([6.2812, 0.1895], dtype=torch.float16)\n",
      "step №23: loss = 45.738975524902344, weights = tensor([6.2812, 0.1934], dtype=torch.float16)\n",
      "step №24: loss = 45.713645935058594, weights = tensor([6.2812, 0.1973], dtype=torch.float16)\n",
      "step №25: loss = 45.68833923339844, weights = tensor([6.2812, 0.2012], dtype=torch.float16)\n",
      "step №26: loss = 45.663063049316406, weights = tensor([6.2812, 0.2051], dtype=torch.float16)\n",
      "step №27: loss = 45.6378173828125, weights = tensor([6.2812, 0.2090], dtype=torch.float16)\n",
      "step №28: loss = 45.612606048583984, weights = tensor([6.2812, 0.2129], dtype=torch.float16)\n",
      "step №29: loss = 45.587425231933594, weights = tensor([6.2773, 0.2168], dtype=torch.float16)\n",
      "step №30: loss = 45.56121063232422, weights = tensor([6.2773, 0.2207], dtype=torch.float16)\n",
      "step №31: loss = 45.535953521728516, weights = tensor([6.2773, 0.2246], dtype=torch.float16)\n",
      "step №32: loss = 45.510719299316406, weights = tensor([6.2773, 0.2285], dtype=torch.float16)\n",
      "step №33: loss = 45.48552322387695, weights = tensor([6.2773, 0.2324], dtype=torch.float16)\n",
      "step №34: loss = 45.460357666015625, weights = tensor([6.2773, 0.2363], dtype=torch.float16)\n",
      "step №35: loss = 45.435218811035156, weights = tensor([6.2734, 0.2402], dtype=torch.float16)\n",
      "step №36: loss = 45.40909957885742, weights = tensor([6.2734, 0.2441], dtype=torch.float16)\n",
      "step №37: loss = 45.383888244628906, weights = tensor([6.2734, 0.2480], dtype=torch.float16)\n",
      "step №38: loss = 45.358699798583984, weights = tensor([6.2734, 0.2520], dtype=torch.float16)\n",
      "step №39: loss = 45.33354949951172, weights = tensor([6.2734, 0.2559], dtype=torch.float16)\n",
      "step №40: loss = 45.30842971801758, weights = tensor([6.2734, 0.2598], dtype=torch.float16)\n",
      "step №41: loss = 45.2833366394043, weights = tensor([6.2734, 0.2637], dtype=torch.float16)\n",
      "step №42: loss = 45.25827407836914, weights = tensor([6.2695, 0.2676], dtype=torch.float16)\n",
      "step №43: loss = 45.232139587402344, weights = tensor([6.2695, 0.2715], dtype=torch.float16)\n",
      "step №44: loss = 45.207000732421875, weights = tensor([6.2695, 0.2754], dtype=torch.float16)\n",
      "step №45: loss = 45.18189239501953, weights = tensor([6.2695, 0.2793], dtype=torch.float16)\n",
      "step №46: loss = 45.156822204589844, weights = tensor([6.2695, 0.2832], dtype=torch.float16)\n",
      "step №47: loss = 45.13177490234375, weights = tensor([6.2695, 0.2871], dtype=torch.float16)\n",
      "step №48: loss = 45.10675811767578, weights = tensor([6.2656, 0.2910], dtype=torch.float16)\n",
      "step №49: loss = 45.08071517944336, weights = tensor([6.2656, 0.2949], dtype=torch.float16)\n",
      "step №50: loss = 45.05562210083008, weights = tensor([6.2656, 0.2988], dtype=torch.float16)\n",
      "step №51: loss = 45.03055953979492, weights = tensor([6.2656, 0.3027], dtype=torch.float16)\n",
      "step №52: loss = 45.005531311035156, weights = tensor([6.2656, 0.3066], dtype=torch.float16)\n",
      "step №53: loss = 44.980533599853516, weights = tensor([6.2656, 0.3105], dtype=torch.float16)\n",
      "step №54: loss = 44.95555877685547, weights = tensor([6.2617, 0.3145], dtype=torch.float16)\n",
      "step №55: loss = 44.92961120605469, weights = tensor([6.2617, 0.3184], dtype=torch.float16)\n",
      "step №56: loss = 44.904563903808594, weights = tensor([6.2617, 0.3223], dtype=torch.float16)\n",
      "step №57: loss = 44.879547119140625, weights = tensor([6.2617, 0.3262], dtype=torch.float16)\n",
      "step №58: loss = 44.85456466674805, weights = tensor([6.2617, 0.3301], dtype=torch.float16)\n",
      "step №59: loss = 44.829612731933594, weights = tensor([6.2617, 0.3340], dtype=torch.float16)\n",
      "step №60: loss = 44.804683685302734, weights = tensor([6.2617, 0.3379], dtype=torch.float16)\n",
      "step №61: loss = 44.77979278564453, weights = tensor([6.2578, 0.3418], dtype=torch.float16)\n",
      "step №62: loss = 44.753822326660156, weights = tensor([6.2578, 0.3457], dtype=torch.float16)\n",
      "step №63: loss = 44.72885513305664, weights = tensor([6.2578, 0.3496], dtype=torch.float16)\n",
      "step №64: loss = 44.70391845703125, weights = tensor([6.2578, 0.3535], dtype=torch.float16)\n",
      "step №65: loss = 44.67900848388672, weights = tensor([6.2578, 0.3574], dtype=torch.float16)\n",
      "step №66: loss = 44.65412902832031, weights = tensor([6.2578, 0.3613], dtype=torch.float16)\n",
      "step №67: loss = 44.62928009033203, weights = tensor([6.2539, 0.3652], dtype=torch.float16)\n",
      "step №68: loss = 44.6034049987793, weights = tensor([6.2539, 0.3691], dtype=torch.float16)\n",
      "step №69: loss = 44.57848358154297, weights = tensor([6.2539, 0.3730], dtype=torch.float16)\n",
      "step №70: loss = 44.553592681884766, weights = tensor([6.2539, 0.3770], dtype=torch.float16)\n",
      "step №71: loss = 44.52872848510742, weights = tensor([6.2539, 0.3809], dtype=torch.float16)\n",
      "step №72: loss = 44.5038948059082, weights = tensor([6.2539, 0.3848], dtype=torch.float16)\n",
      "step №73: loss = 44.47909164428711, weights = tensor([6.2500, 0.3887], dtype=torch.float16)\n",
      "step №74: loss = 44.45330810546875, weights = tensor([6.2500, 0.3926], dtype=torch.float16)\n",
      "step №75: loss = 44.428428649902344, weights = tensor([6.2500, 0.3965], dtype=torch.float16)\n",
      "step №76: loss = 44.403587341308594, weights = tensor([6.2500, 0.4004], dtype=torch.float16)\n",
      "step №77: loss = 44.37876892089844, weights = tensor([6.2500, 0.4043], dtype=torch.float16)\n",
      "step №78: loss = 44.353981018066406, weights = tensor([6.2500, 0.4082], dtype=torch.float16)\n",
      "step №79: loss = 44.3292236328125, weights = tensor([6.2500, 0.4121], dtype=torch.float16)\n",
      "step №80: loss = 44.304500579833984, weights = tensor([6.2461, 0.4160], dtype=torch.float16)\n",
      "step №81: loss = 44.2786979675293, weights = tensor([6.2461, 0.4199], dtype=torch.float16)\n",
      "step №82: loss = 44.25389862060547, weights = tensor([6.2461, 0.4238], dtype=torch.float16)\n",
      "step №83: loss = 44.229129791259766, weights = tensor([6.2461, 0.4277], dtype=torch.float16)\n",
      "step №84: loss = 44.204383850097656, weights = tensor([6.2461, 0.4316], dtype=torch.float16)\n",
      "step №85: loss = 44.1796760559082, weights = tensor([6.2461, 0.4355], dtype=torch.float16)\n",
      "step №86: loss = 44.154998779296875, weights = tensor([6.2422, 0.4395], dtype=torch.float16)\n",
      "step №87: loss = 44.12928771972656, weights = tensor([6.2422, 0.4434], dtype=torch.float16)\n",
      "step №88: loss = 44.10453414916992, weights = tensor([6.2422, 0.4473], dtype=torch.float16)\n",
      "step №89: loss = 44.079811096191406, weights = tensor([6.2422, 0.4512], dtype=torch.float16)\n",
      "step №90: loss = 44.055110931396484, weights = tensor([6.2422, 0.4551], dtype=torch.float16)\n",
      "step №91: loss = 44.03044891357422, weights = tensor([6.2422, 0.4590], dtype=torch.float16)\n",
      "step №92: loss = 44.00581741333008, weights = tensor([6.2383, 0.4626], dtype=torch.float16)\n",
      "step №93: loss = 43.981746673583984, weights = tensor([6.2383, 0.4666], dtype=torch.float16)\n",
      "step №94: loss = 43.95703887939453, weights = tensor([6.2383, 0.4705], dtype=torch.float16)\n",
      "step №95: loss = 43.93235397338867, weights = tensor([6.2383, 0.4744], dtype=torch.float16)\n",
      "step №96: loss = 43.90769958496094, weights = tensor([6.2383, 0.4780], dtype=torch.float16)\n",
      "step №97: loss = 43.884620666503906, weights = tensor([6.2383, 0.4817], dtype=torch.float16)\n",
      "step №98: loss = 43.86156463623047, weights = tensor([6.2383, 0.4854], dtype=torch.float16)\n",
      "step №99: loss = 43.83853530883789, weights = tensor([6.2344, 0.4890], dtype=torch.float16)\n",
      "step №100: loss = 43.81447219848633, weights = tensor([6.2344, 0.4929], dtype=torch.float16)\n",
      "step №101: loss = 43.789825439453125, weights = tensor([6.2344, 0.4966], dtype=torch.float16)\n",
      "step №102: loss = 43.766754150390625, weights = tensor([6.2344, 0.5005], dtype=torch.float16)\n",
      "step №103: loss = 43.74216842651367, weights = tensor([6.2344, 0.5044], dtype=torch.float16)\n",
      "step №104: loss = 43.71761703491211, weights = tensor([6.2344, 0.5083], dtype=torch.float16)\n",
      "step №105: loss = 43.693092346191406, weights = tensor([6.2305, 0.5122], dtype=torch.float16)\n",
      "step №106: loss = 43.6675910949707, weights = tensor([6.2305, 0.5161], dtype=torch.float16)\n",
      "step №107: loss = 43.64299392700195, weights = tensor([6.2305, 0.5200], dtype=torch.float16)\n",
      "step №108: loss = 43.61842346191406, weights = tensor([6.2305, 0.5239], dtype=torch.float16)\n",
      "step №109: loss = 43.59387969970703, weights = tensor([6.2305, 0.5278], dtype=torch.float16)\n",
      "step №110: loss = 43.56937789916992, weights = tensor([6.2305, 0.5317], dtype=torch.float16)\n",
      "step №111: loss = 43.544898986816406, weights = tensor([6.2305, 0.5356], dtype=torch.float16)\n",
      "step №112: loss = 43.520450592041016, weights = tensor([6.2266, 0.5396], dtype=torch.float16)\n",
      "step №113: loss = 43.494937896728516, weights = tensor([6.2266, 0.5435], dtype=torch.float16)\n",
      "step №114: loss = 43.47041320800781, weights = tensor([6.2266, 0.5474], dtype=torch.float16)\n",
      "step №115: loss = 43.445919036865234, weights = tensor([6.2266, 0.5513], dtype=torch.float16)\n",
      "step №116: loss = 43.42145919799805, weights = tensor([6.2266, 0.5552], dtype=torch.float16)\n",
      "step №117: loss = 43.397029876708984, weights = tensor([6.2266, 0.5591], dtype=torch.float16)\n",
      "step №118: loss = 43.37262725830078, weights = tensor([6.2227, 0.5630], dtype=torch.float16)\n",
      "step №119: loss = 43.34720230102539, weights = tensor([6.2227, 0.5669], dtype=torch.float16)\n",
      "step №120: loss = 43.322723388671875, weights = tensor([6.2227, 0.5708], dtype=torch.float16)\n",
      "step №121: loss = 43.29827880859375, weights = tensor([6.2227, 0.5747], dtype=torch.float16)\n",
      "step №122: loss = 43.27386474609375, weights = tensor([6.2227, 0.5786], dtype=torch.float16)\n",
      "step №123: loss = 43.249473571777344, weights = tensor([6.2227, 0.5825], dtype=torch.float16)\n",
      "step №124: loss = 43.22512435913086, weights = tensor([6.2188, 0.5864], dtype=torch.float16)\n",
      "step №125: loss = 43.199790954589844, weights = tensor([6.2188, 0.5903], dtype=torch.float16)\n",
      "step №126: loss = 43.175350189208984, weights = tensor([6.2188, 0.5942], dtype=torch.float16)\n",
      "step №127: loss = 43.15095520019531, weights = tensor([6.2188, 0.5981], dtype=torch.float16)\n",
      "step №128: loss = 43.12659454345703, weights = tensor([6.2188, 0.6021], dtype=torch.float16)\n",
      "step №129: loss = 43.10224151611328, weights = tensor([6.2188, 0.6060], dtype=torch.float16)\n",
      "step №130: loss = 43.077938079833984, weights = tensor([6.2188, 0.6099], dtype=torch.float16)\n",
      "step №131: loss = 43.05365753173828, weights = tensor([6.2148, 0.6138], dtype=torch.float16)\n",
      "step №132: loss = 43.0283088684082, weights = tensor([6.2148, 0.6177], dtype=torch.float16)\n",
      "step №133: loss = 43.00395584106445, weights = tensor([6.2148, 0.6216], dtype=torch.float16)\n",
      "step №134: loss = 42.9796257019043, weights = tensor([6.2148, 0.6255], dtype=torch.float16)\n",
      "step №135: loss = 42.95533752441406, weights = tensor([6.2148, 0.6294], dtype=torch.float16)\n",
      "step №136: loss = 42.93107223510742, weights = tensor([6.2148, 0.6333], dtype=torch.float16)\n",
      "step №137: loss = 42.906837463378906, weights = tensor([6.2109, 0.6372], dtype=torch.float16)\n",
      "step №138: loss = 42.88158416748047, weights = tensor([6.2109, 0.6411], dtype=torch.float16)\n",
      "step №139: loss = 42.85727310180664, weights = tensor([6.2109, 0.6450], dtype=torch.float16)\n",
      "step №140: loss = 42.83299255371094, weights = tensor([6.2109, 0.6489], dtype=torch.float16)\n",
      "step №141: loss = 42.80874252319336, weights = tensor([6.2109, 0.6528], dtype=torch.float16)\n",
      "step №142: loss = 42.78452682495117, weights = tensor([6.2109, 0.6567], dtype=torch.float16)\n",
      "step №143: loss = 42.760337829589844, weights = tensor([6.2070, 0.6606], dtype=torch.float16)\n",
      "step №144: loss = 42.73517608642578, weights = tensor([6.2070, 0.6646], dtype=torch.float16)\n",
      "step №145: loss = 42.710914611816406, weights = tensor([6.2070, 0.6685], dtype=torch.float16)\n",
      "step №146: loss = 42.686676025390625, weights = tensor([6.2070, 0.6724], dtype=torch.float16)\n",
      "step №147: loss = 42.662471771240234, weights = tensor([6.2070, 0.6763], dtype=torch.float16)\n",
      "step №148: loss = 42.6383056640625, weights = tensor([6.2070, 0.6802], dtype=torch.float16)\n",
      "step №149: loss = 42.614158630371094, weights = tensor([6.2070, 0.6841], dtype=torch.float16)\n",
      "step №150: loss = 42.590049743652344, weights = tensor([6.2031, 0.6880], dtype=torch.float16)\n",
      "step №151: loss = 42.56487274169922, weights = tensor([6.2031, 0.6919], dtype=torch.float16)\n",
      "step №152: loss = 42.540687561035156, weights = tensor([6.2031, 0.6958], dtype=torch.float16)\n",
      "step №153: loss = 42.51652526855469, weights = tensor([6.2031, 0.6997], dtype=torch.float16)\n",
      "step №154: loss = 42.492393493652344, weights = tensor([6.2031, 0.7036], dtype=torch.float16)\n",
      "step №155: loss = 42.468299865722656, weights = tensor([6.2031, 0.7075], dtype=torch.float16)\n",
      "step №156: loss = 42.44424057006836, weights = tensor([6.1992, 0.7114], dtype=torch.float16)\n",
      "step №157: loss = 42.41914749145508, weights = tensor([6.1992, 0.7153], dtype=torch.float16)\n",
      "step №158: loss = 42.39501190185547, weights = tensor([6.1992, 0.7192], dtype=torch.float16)\n",
      "step №159: loss = 42.37089920043945, weights = tensor([6.1992, 0.7231], dtype=torch.float16)\n",
      "step №160: loss = 42.3468132019043, weights = tensor([6.1992, 0.7271], dtype=torch.float16)\n",
      "step №161: loss = 42.3227653503418, weights = tensor([6.1992, 0.7310], dtype=torch.float16)\n",
      "step №162: loss = 42.298744201660156, weights = tensor([6.1953, 0.7349], dtype=torch.float16)\n",
      "step №163: loss = 42.273746490478516, weights = tensor([6.1953, 0.7388], dtype=torch.float16)\n",
      "step №164: loss = 42.249656677246094, weights = tensor([6.1953, 0.7427], dtype=torch.float16)\n",
      "step №165: loss = 42.225589752197266, weights = tensor([6.1953, 0.7466], dtype=torch.float16)\n",
      "step №166: loss = 42.20155715942383, weights = tensor([6.1953, 0.7505], dtype=torch.float16)\n",
      "step №167: loss = 42.17755126953125, weights = tensor([6.1953, 0.7544], dtype=torch.float16)\n",
      "step №168: loss = 42.15357208251953, weights = tensor([6.1953, 0.7583], dtype=torch.float16)\n",
      "step №169: loss = 42.12963104248047, weights = tensor([6.1914, 0.7622], dtype=torch.float16)\n",
      "step №170: loss = 42.10462188720703, weights = tensor([6.1914, 0.7661], dtype=torch.float16)\n",
      "step №171: loss = 42.08060073852539, weights = tensor([6.1914, 0.7700], dtype=torch.float16)\n",
      "step №172: loss = 42.056617736816406, weights = tensor([6.1914, 0.7739], dtype=torch.float16)\n",
      "step №173: loss = 42.03265380859375, weights = tensor([6.1914, 0.7778], dtype=torch.float16)\n",
      "step №174: loss = 42.00872802734375, weights = tensor([6.1914, 0.7817], dtype=torch.float16)\n",
      "step №175: loss = 41.984825134277344, weights = tensor([6.1875, 0.7856], dtype=torch.float16)\n",
      "step №176: loss = 41.95991516113281, weights = tensor([6.1875, 0.7896], dtype=torch.float16)\n",
      "step №177: loss = 41.93592834472656, weights = tensor([6.1875, 0.7935], dtype=torch.float16)\n",
      "step №178: loss = 41.91199493408203, weights = tensor([6.1875, 0.7974], dtype=torch.float16)\n",
      "step №179: loss = 41.888084411621094, weights = tensor([6.1875, 0.8013], dtype=torch.float16)\n",
      "step №180: loss = 41.864192962646484, weights = tensor([6.1875, 0.8052], dtype=torch.float16)\n",
      "step №181: loss = 41.84034729003906, weights = tensor([6.1836, 0.8086], dtype=torch.float16)\n",
      "step №182: loss = 41.818504333496094, weights = tensor([6.1836, 0.8125], dtype=torch.float16)\n",
      "step №183: loss = 41.79457473754883, weights = tensor([6.1836, 0.8164], dtype=torch.float16)\n",
      "step №184: loss = 41.77067184448242, weights = tensor([6.1836, 0.8203], dtype=torch.float16)\n",
      "step №185: loss = 41.746803283691406, weights = tensor([6.1836, 0.8242], dtype=torch.float16)\n",
      "step №186: loss = 41.72296142578125, weights = tensor([6.1836, 0.8276], dtype=torch.float16)\n",
      "step №187: loss = 41.702125549316406, weights = tensor([6.1836, 0.8311], dtype=torch.float16)\n",
      "step №188: loss = 41.68131637573242, weights = tensor([6.1797, 0.8345], dtype=torch.float16)\n",
      "step №189: loss = 41.65949630737305, weights = tensor([6.1797, 0.8384], dtype=torch.float16)\n",
      "step №190: loss = 41.63562774658203, weights = tensor([6.1797, 0.8423], dtype=torch.float16)\n",
      "step №191: loss = 41.611793518066406, weights = tensor([6.1797, 0.8457], dtype=torch.float16)\n",
      "step №192: loss = 41.59095764160156, weights = tensor([6.1797, 0.8491], dtype=torch.float16)\n",
      "step №193: loss = 41.570152282714844, weights = tensor([6.1797, 0.8525], dtype=torch.float16)\n",
      "step №194: loss = 41.54936981201172, weights = tensor([6.1797, 0.8560], dtype=torch.float16)\n",
      "step №195: loss = 41.528602600097656, weights = tensor([6.1758, 0.8594], dtype=torch.float16)\n",
      "step №196: loss = 41.50682830810547, weights = tensor([6.1758, 0.8628], dtype=torch.float16)\n",
      "step №197: loss = 41.485992431640625, weights = tensor([6.1758, 0.8662], dtype=torch.float16)\n",
      "step №198: loss = 41.465179443359375, weights = tensor([6.1758, 0.8696], dtype=torch.float16)\n",
      "step №199: loss = 41.44439697265625, weights = tensor([6.1758, 0.8730], dtype=torch.float16)\n",
      "step №200: loss = 41.42362976074219, weights = tensor([6.1758, 0.8765], dtype=torch.float16)\n",
      "step №201: loss = 41.402889251708984, weights = tensor([6.1758, 0.8799], dtype=torch.float16)\n",
      "step №202: loss = 41.382171630859375, weights = tensor([6.1719, 0.8833], dtype=torch.float16)\n",
      "step №203: loss = 41.36046600341797, weights = tensor([6.1719, 0.8867], dtype=torch.float16)\n",
      "step №204: loss = 41.33966827392578, weights = tensor([6.1719, 0.8901], dtype=torch.float16)\n",
      "step №205: loss = 41.318904876708984, weights = tensor([6.1719, 0.8936], dtype=torch.float16)\n",
      "step №206: loss = 41.29815673828125, weights = tensor([6.1719, 0.8970], dtype=torch.float16)\n",
      "step №207: loss = 41.27743911743164, weights = tensor([6.1719, 0.9004], dtype=torch.float16)\n",
      "step №208: loss = 41.25674057006836, weights = tensor([6.1719, 0.9038], dtype=torch.float16)\n",
      "step №209: loss = 41.23606872558594, weights = tensor([6.1680, 0.9072], dtype=torch.float16)\n",
      "step №210: loss = 41.214439392089844, weights = tensor([6.1680, 0.9106], dtype=torch.float16)\n",
      "step №211: loss = 41.193687438964844, weights = tensor([6.1680, 0.9141], dtype=torch.float16)\n",
      "step №212: loss = 41.1729621887207, weights = tensor([6.1680, 0.9175], dtype=torch.float16)\n",
      "step №213: loss = 41.15226364135742, weights = tensor([6.1680, 0.9209], dtype=torch.float16)\n",
      "step №214: loss = 41.13158416748047, weights = tensor([6.1680, 0.9243], dtype=torch.float16)\n",
      "step №215: loss = 41.11092758178711, weights = tensor([6.1680, 0.9277], dtype=torch.float16)\n",
      "step №216: loss = 41.090293884277344, weights = tensor([6.1680, 0.9312], dtype=torch.float16)\n",
      "step №217: loss = 41.06968688964844, weights = tensor([6.1641, 0.9346], dtype=torch.float16)\n",
      "step №218: loss = 41.048030853271484, weights = tensor([6.1641, 0.9380], dtype=torch.float16)\n",
      "step №219: loss = 41.02735137939453, weights = tensor([6.1641, 0.9414], dtype=torch.float16)\n",
      "step №220: loss = 41.006683349609375, weights = tensor([6.1641, 0.9448], dtype=torch.float16)\n",
      "step №221: loss = 40.98605728149414, weights = tensor([6.1641, 0.9482], dtype=torch.float16)\n",
      "step №222: loss = 40.96544647216797, weights = tensor([6.1641, 0.9517], dtype=torch.float16)\n",
      "step №223: loss = 40.944854736328125, weights = tensor([6.1641, 0.9551], dtype=torch.float16)\n",
      "step №224: loss = 40.92428970336914, weights = tensor([6.1602, 0.9585], dtype=torch.float16)\n",
      "step №225: loss = 40.902713775634766, weights = tensor([6.1602, 0.9619], dtype=torch.float16)\n",
      "step №226: loss = 40.88207244873047, weights = tensor([6.1602, 0.9653], dtype=torch.float16)\n",
      "step №227: loss = 40.86145782470703, weights = tensor([6.1602, 0.9688], dtype=torch.float16)\n",
      "step №228: loss = 40.84086227416992, weights = tensor([6.1602, 0.9722], dtype=torch.float16)\n",
      "step №229: loss = 40.82029342651367, weights = tensor([6.1602, 0.9756], dtype=torch.float16)\n",
      "step №230: loss = 40.79975128173828, weights = tensor([6.1602, 0.9790], dtype=torch.float16)\n",
      "step №231: loss = 40.77922821044922, weights = tensor([6.1562, 0.9824], dtype=torch.float16)\n",
      "step №232: loss = 40.757713317871094, weights = tensor([6.1562, 0.9858], dtype=torch.float16)\n",
      "step №233: loss = 40.737125396728516, weights = tensor([6.1562, 0.9893], dtype=torch.float16)\n",
      "step №234: loss = 40.716556549072266, weights = tensor([6.1562, 0.9927], dtype=torch.float16)\n",
      "step №235: loss = 40.696006774902344, weights = tensor([6.1562, 0.9961], dtype=torch.float16)\n",
      "step №236: loss = 40.67546844482422, weights = tensor([6.1562, 0.9995], dtype=torch.float16)\n",
      "step №237: loss = 40.65497589111328, weights = tensor([6.1562, 1.0029], dtype=torch.float16)\n",
      "step №238: loss = 40.63449478149414, weights = tensor([6.1523, 1.0068], dtype=torch.float16)\n",
      "step №239: loss = 40.61012649536133, weights = tensor([6.1523, 1.0107], dtype=torch.float16)\n",
      "step №240: loss = 40.58664321899414, weights = tensor([6.1523, 1.0146], dtype=torch.float16)\n",
      "step №241: loss = 40.56319046020508, weights = tensor([6.1523, 1.0186], dtype=torch.float16)\n",
      "step №242: loss = 40.539772033691406, weights = tensor([6.1523, 1.0225], dtype=torch.float16)\n",
      "step №243: loss = 40.516380310058594, weights = tensor([6.1523, 1.0264], dtype=torch.float16)\n",
      "step №244: loss = 40.493019104003906, weights = tensor([6.1523, 1.0303], dtype=torch.float16)\n",
      "step №245: loss = 40.469688415527344, weights = tensor([6.1484, 1.0342], dtype=torch.float16)\n",
      "step №246: loss = 40.44529724121094, weights = tensor([6.1484, 1.0381], dtype=torch.float16)\n",
      "step №247: loss = 40.42189407348633, weights = tensor([6.1484, 1.0420], dtype=torch.float16)\n",
      "step №248: loss = 40.39851760864258, weights = tensor([6.1484, 1.0459], dtype=torch.float16)\n",
      "step №249: loss = 40.37517547607422, weights = tensor([6.1484, 1.0498], dtype=torch.float16)\n",
      "step №250: loss = 40.35186004638672, weights = tensor([6.1484, 1.0537], dtype=torch.float16)\n",
      "step №251: loss = 40.328575134277344, weights = tensor([6.1445, 1.0576], dtype=torch.float16)\n",
      "step №252: loss = 40.30427551269531, weights = tensor([6.1445, 1.0615], dtype=torch.float16)\n",
      "step №253: loss = 40.28091812133789, weights = tensor([6.1445, 1.0654], dtype=torch.float16)\n",
      "step №254: loss = 40.25758743286133, weights = tensor([6.1445, 1.0693], dtype=torch.float16)\n",
      "step №255: loss = 40.23428726196289, weights = tensor([6.1445, 1.0732], dtype=torch.float16)\n",
      "step №256: loss = 40.211021423339844, weights = tensor([6.1445, 1.0771], dtype=torch.float16)\n",
      "step №257: loss = 40.187782287597656, weights = tensor([6.1406, 1.0811], dtype=torch.float16)\n",
      "step №258: loss = 40.16356658935547, weights = tensor([6.1406, 1.0850], dtype=torch.float16)\n",
      "step №259: loss = 40.140262603759766, weights = tensor([6.1406, 1.0889], dtype=torch.float16)\n",
      "step №260: loss = 40.116981506347656, weights = tensor([6.1406, 1.0928], dtype=torch.float16)\n",
      "step №261: loss = 40.09372329711914, weights = tensor([6.1406, 1.0967], dtype=torch.float16)\n",
      "step №262: loss = 40.07049560546875, weights = tensor([6.1406, 1.1006], dtype=torch.float16)\n",
      "step №263: loss = 40.04731369018555, weights = tensor([6.1406, 1.1045], dtype=torch.float16)\n",
      "step №264: loss = 40.02415084838867, weights = tensor([6.1367, 1.1084], dtype=torch.float16)\n",
      "step №265: loss = 39.99992370605469, weights = tensor([6.1367, 1.1123], dtype=torch.float16)\n",
      "step №266: loss = 39.9766845703125, weights = tensor([6.1367, 1.1162], dtype=torch.float16)\n",
      "step №267: loss = 39.95347595214844, weights = tensor([6.1367, 1.1201], dtype=torch.float16)\n",
      "step №268: loss = 39.93030548095703, weights = tensor([6.1367, 1.1240], dtype=torch.float16)\n",
      "step №269: loss = 39.90715789794922, weights = tensor([6.1367, 1.1279], dtype=torch.float16)\n",
      "step №270: loss = 39.88404083251953, weights = tensor([6.1328, 1.1318], dtype=torch.float16)\n",
      "step №271: loss = 39.85991287231445, weights = tensor([6.1328, 1.1357], dtype=torch.float16)\n",
      "step №272: loss = 39.83671569824219, weights = tensor([6.1328, 1.1396], dtype=torch.float16)\n",
      "step №273: loss = 39.81355667114258, weights = tensor([6.1328, 1.1436], dtype=torch.float16)\n",
      "step №274: loss = 39.79042434692383, weights = tensor([6.1328, 1.1475], dtype=torch.float16)\n",
      "step №275: loss = 39.76732635498047, weights = tensor([6.1328, 1.1514], dtype=torch.float16)\n",
      "step №276: loss = 39.74425506591797, weights = tensor([6.1289, 1.1553], dtype=torch.float16)\n",
      "step №277: loss = 39.72021484375, weights = tensor([6.1289, 1.1592], dtype=torch.float16)\n",
      "step №278: loss = 39.69706726074219, weights = tensor([6.1289, 1.1631], dtype=torch.float16)\n",
      "step №279: loss = 39.673954010009766, weights = tensor([6.1289, 1.1670], dtype=torch.float16)\n",
      "step №280: loss = 39.65087127685547, weights = tensor([6.1289, 1.1709], dtype=torch.float16)\n",
      "step №281: loss = 39.62781524658203, weights = tensor([6.1289, 1.1748], dtype=torch.float16)\n",
      "step №282: loss = 39.60478973388672, weights = tensor([6.1289, 1.1787], dtype=torch.float16)\n",
      "step №283: loss = 39.58179473876953, weights = tensor([6.1250, 1.1826], dtype=torch.float16)\n",
      "step №284: loss = 39.557743072509766, weights = tensor([6.1250, 1.1865], dtype=torch.float16)\n",
      "step №285: loss = 39.53466796875, weights = tensor([6.1250, 1.1904], dtype=torch.float16)\n",
      "step №286: loss = 39.51162338256836, weights = tensor([6.1250, 1.1943], dtype=torch.float16)\n",
      "step №287: loss = 39.488624572753906, weights = tensor([6.1250, 1.1982], dtype=torch.float16)\n",
      "step №288: loss = 39.46564483642578, weights = tensor([6.1250, 1.2021], dtype=torch.float16)\n",
      "step №289: loss = 39.44269561767578, weights = tensor([6.1211, 1.2061], dtype=torch.float16)\n",
      "step №290: loss = 39.418731689453125, weights = tensor([6.1211, 1.2100], dtype=torch.float16)\n",
      "step №291: loss = 39.39570617675781, weights = tensor([6.1211, 1.2139], dtype=torch.float16)\n",
      "step №292: loss = 39.37271499633789, weights = tensor([6.1211, 1.2178], dtype=torch.float16)\n",
      "step №293: loss = 39.34975051879883, weights = tensor([6.1211, 1.2217], dtype=torch.float16)\n",
      "step №294: loss = 39.326820373535156, weights = tensor([6.1211, 1.2256], dtype=torch.float16)\n",
      "step №295: loss = 39.30391311645508, weights = tensor([6.1172, 1.2295], dtype=torch.float16)\n",
      "step №296: loss = 39.2800407409668, weights = tensor([6.1172, 1.2334], dtype=torch.float16)\n",
      "step №297: loss = 39.2570686340332, weights = tensor([6.1172, 1.2373], dtype=torch.float16)\n",
      "step №298: loss = 39.23411560058594, weights = tensor([6.1172, 1.2412], dtype=torch.float16)\n",
      "step №299: loss = 39.21119689941406, weights = tensor([6.1172, 1.2451], dtype=torch.float16)\n",
      "step №300: loss = 39.18831253051758, weights = tensor([6.1172, 1.2490], dtype=torch.float16)\n",
      "step №301: loss = 39.16545867919922, weights = tensor([6.1172, 1.2529], dtype=torch.float16)\n",
      "step №302: loss = 39.14263153076172, weights = tensor([6.1133, 1.2568], dtype=torch.float16)\n",
      "step №303: loss = 39.11874008178711, weights = tensor([6.1133, 1.2607], dtype=torch.float16)\n",
      "step №304: loss = 39.09584045410156, weights = tensor([6.1133, 1.2646], dtype=torch.float16)\n",
      "step №305: loss = 39.07297134399414, weights = tensor([6.1133, 1.2686], dtype=torch.float16)\n",
      "step №306: loss = 39.05012893676758, weights = tensor([6.1133, 1.2725], dtype=torch.float16)\n",
      "step №307: loss = 39.027320861816406, weights = tensor([6.1133, 1.2764], dtype=torch.float16)\n",
      "step №308: loss = 39.004539489746094, weights = tensor([6.1094, 1.2803], dtype=torch.float16)\n",
      "step №309: loss = 38.98073959350586, weights = tensor([6.1094, 1.2842], dtype=torch.float16)\n",
      "step №310: loss = 38.95787811279297, weights = tensor([6.1094, 1.2881], dtype=torch.float16)\n",
      "step №311: loss = 38.93506622314453, weights = tensor([6.1094, 1.2920], dtype=torch.float16)\n",
      "step №312: loss = 38.912269592285156, weights = tensor([6.1094, 1.2959], dtype=torch.float16)\n",
      "step №313: loss = 38.88949966430664, weights = tensor([6.1094, 1.2998], dtype=torch.float16)\n",
      "step №314: loss = 38.86676025390625, weights = tensor([6.1055, 1.3037], dtype=torch.float16)\n",
      "step №315: loss = 38.843055725097656, weights = tensor([6.1055, 1.3076], dtype=torch.float16)\n",
      "step №316: loss = 38.82025146484375, weights = tensor([6.1055, 1.3115], dtype=torch.float16)\n",
      "step №317: loss = 38.79747009277344, weights = tensor([6.1055, 1.3154], dtype=torch.float16)\n",
      "step №318: loss = 38.77471923828125, weights = tensor([6.1055, 1.3193], dtype=torch.float16)\n",
      "step №319: loss = 38.75200271606445, weights = tensor([6.1055, 1.3232], dtype=torch.float16)\n",
      "step №320: loss = 38.72931671142578, weights = tensor([6.1055, 1.3271], dtype=torch.float16)\n",
      "step №321: loss = 38.70665740966797, weights = tensor([6.1016, 1.3311], dtype=torch.float16)\n",
      "step №322: loss = 38.68293380737305, weights = tensor([6.1016, 1.3350], dtype=torch.float16)\n",
      "step №323: loss = 38.66020202636719, weights = tensor([6.1016, 1.3389], dtype=torch.float16)\n",
      "step №324: loss = 38.63749694824219, weights = tensor([6.1016, 1.3428], dtype=torch.float16)\n",
      "step №325: loss = 38.61482620239258, weights = tensor([6.1016, 1.3467], dtype=torch.float16)\n",
      "step №326: loss = 38.59218215942383, weights = tensor([6.1016, 1.3506], dtype=torch.float16)\n",
      "step №327: loss = 38.56957244873047, weights = tensor([6.0977, 1.3545], dtype=torch.float16)\n",
      "step №328: loss = 38.54594421386719, weights = tensor([6.0977, 1.3584], dtype=torch.float16)\n",
      "step №329: loss = 38.52325439453125, weights = tensor([6.0977, 1.3623], dtype=torch.float16)\n",
      "step №330: loss = 38.50059509277344, weights = tensor([6.0977, 1.3662], dtype=torch.float16)\n",
      "step №331: loss = 38.47796630859375, weights = tensor([6.0977, 1.3701], dtype=torch.float16)\n",
      "step №332: loss = 38.45537567138672, weights = tensor([6.0977, 1.3740], dtype=torch.float16)\n",
      "step №333: loss = 38.43280792236328, weights = tensor([6.0938, 1.3779], dtype=torch.float16)\n",
      "step №334: loss = 38.409263610839844, weights = tensor([6.0938, 1.3818], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №335: loss = 38.386627197265625, weights = tensor([6.0938, 1.3857], dtype=torch.float16)\n",
      "step №336: loss = 38.364017486572266, weights = tensor([6.0938, 1.3896], dtype=torch.float16)\n",
      "step №337: loss = 38.3414306640625, weights = tensor([6.0938, 1.3936], dtype=torch.float16)\n",
      "step №338: loss = 38.31887435913086, weights = tensor([6.0938, 1.3975], dtype=torch.float16)\n",
      "step №339: loss = 38.296363830566406, weights = tensor([6.0938, 1.4014], dtype=torch.float16)\n",
      "step №340: loss = 38.27387237548828, weights = tensor([6.0898, 1.4053], dtype=torch.float16)\n",
      "step №341: loss = 38.25031661987305, weights = tensor([6.0898, 1.4092], dtype=torch.float16)\n",
      "step №342: loss = 38.227752685546875, weights = tensor([6.0898, 1.4131], dtype=torch.float16)\n",
      "step №343: loss = 38.2052116394043, weights = tensor([6.0898, 1.4170], dtype=torch.float16)\n",
      "step №344: loss = 38.18271255493164, weights = tensor([6.0898, 1.4209], dtype=torch.float16)\n",
      "step №345: loss = 38.16023635864258, weights = tensor([6.0898, 1.4248], dtype=torch.float16)\n",
      "step №346: loss = 38.137794494628906, weights = tensor([6.0859, 1.4287], dtype=torch.float16)\n",
      "step №347: loss = 38.11432647705078, weights = tensor([6.0859, 1.4326], dtype=torch.float16)\n",
      "step №348: loss = 38.0918083190918, weights = tensor([6.0859, 1.4365], dtype=torch.float16)\n",
      "step №349: loss = 38.0693244934082, weights = tensor([6.0859, 1.4404], dtype=torch.float16)\n",
      "step №350: loss = 38.04685974121094, weights = tensor([6.0859, 1.4443], dtype=torch.float16)\n",
      "step №351: loss = 38.02443313598633, weights = tensor([6.0859, 1.4482], dtype=torch.float16)\n",
      "step №352: loss = 38.00203323364258, weights = tensor([6.0820, 1.4521], dtype=torch.float16)\n",
      "step №353: loss = 37.97866439819336, weights = tensor([6.0820, 1.4561], dtype=torch.float16)\n",
      "step №354: loss = 37.9561882019043, weights = tensor([6.0820, 1.4600], dtype=torch.float16)\n",
      "step №355: loss = 37.933746337890625, weights = tensor([6.0820, 1.4639], dtype=torch.float16)\n",
      "step №356: loss = 37.91133117675781, weights = tensor([6.0820, 1.4678], dtype=torch.float16)\n",
      "step №357: loss = 37.88895034790039, weights = tensor([6.0820, 1.4717], dtype=torch.float16)\n",
      "step №358: loss = 37.86659622192383, weights = tensor([6.0820, 1.4756], dtype=torch.float16)\n",
      "step №359: loss = 37.84427261352539, weights = tensor([6.0781, 1.4795], dtype=torch.float16)\n",
      "step №360: loss = 37.820892333984375, weights = tensor([6.0781, 1.4834], dtype=torch.float16)\n",
      "step №361: loss = 37.79848861694336, weights = tensor([6.0781, 1.4873], dtype=torch.float16)\n",
      "step №362: loss = 37.77611541748047, weights = tensor([6.0781, 1.4912], dtype=torch.float16)\n",
      "step №363: loss = 37.753787994384766, weights = tensor([6.0781, 1.4951], dtype=torch.float16)\n",
      "step №364: loss = 37.731483459472656, weights = tensor([6.0781, 1.4990], dtype=torch.float16)\n",
      "step №365: loss = 37.70920181274414, weights = tensor([6.0742, 1.5029], dtype=torch.float16)\n",
      "step №366: loss = 37.68590545654297, weights = tensor([6.0742, 1.5068], dtype=torch.float16)\n",
      "step №367: loss = 37.66355514526367, weights = tensor([6.0742, 1.5107], dtype=torch.float16)\n",
      "step №368: loss = 37.6412353515625, weights = tensor([6.0742, 1.5146], dtype=torch.float16)\n",
      "step №369: loss = 37.61894226074219, weights = tensor([6.0742, 1.5186], dtype=torch.float16)\n",
      "step №370: loss = 37.5966796875, weights = tensor([6.0742, 1.5215], dtype=torch.float16)\n",
      "step №371: loss = 37.58000564575195, weights = tensor([6.0742, 1.5244], dtype=torch.float16)\n",
      "step №372: loss = 37.56334686279297, weights = tensor([6.0703, 1.5273], dtype=torch.float16)\n",
      "step №373: loss = 37.5456657409668, weights = tensor([6.0703, 1.5312], dtype=torch.float16)\n",
      "step №374: loss = 37.52336883544922, weights = tensor([6.0703, 1.5352], dtype=torch.float16)\n",
      "step №375: loss = 37.5010986328125, weights = tensor([6.0703, 1.5391], dtype=torch.float16)\n",
      "step №376: loss = 37.47886276245117, weights = tensor([6.0703, 1.5420], dtype=torch.float16)\n",
      "step №377: loss = 37.46220397949219, weights = tensor([6.0703, 1.5449], dtype=torch.float16)\n",
      "step №378: loss = 37.44556427001953, weights = tensor([6.0703, 1.5479], dtype=torch.float16)\n",
      "step №379: loss = 37.4289436340332, weights = tensor([6.0664, 1.5508], dtype=torch.float16)\n",
      "step №380: loss = 37.411338806152344, weights = tensor([6.0664, 1.5547], dtype=torch.float16)\n",
      "step №381: loss = 37.38908767700195, weights = tensor([6.0664, 1.5576], dtype=torch.float16)\n",
      "step №382: loss = 37.37242126464844, weights = tensor([6.0664, 1.5605], dtype=torch.float16)\n",
      "step №383: loss = 37.35576629638672, weights = tensor([6.0664, 1.5635], dtype=torch.float16)\n",
      "step №384: loss = 37.33913040161133, weights = tensor([6.0664, 1.5664], dtype=torch.float16)\n",
      "step №385: loss = 37.322509765625, weights = tensor([6.0664, 1.5693], dtype=torch.float16)\n",
      "step №386: loss = 37.305912017822266, weights = tensor([6.0664, 1.5723], dtype=torch.float16)\n",
      "step №387: loss = 37.28932571411133, weights = tensor([6.0625, 1.5752], dtype=torch.float16)\n",
      "step №388: loss = 37.27177429199219, weights = tensor([6.0625, 1.5781], dtype=torch.float16)\n",
      "step №389: loss = 37.255126953125, weights = tensor([6.0625, 1.5811], dtype=torch.float16)\n",
      "step №390: loss = 37.238487243652344, weights = tensor([6.0625, 1.5840], dtype=torch.float16)\n",
      "step №391: loss = 37.22187042236328, weights = tensor([6.0625, 1.5869], dtype=torch.float16)\n",
      "step №392: loss = 37.20527267456055, weights = tensor([6.0625, 1.5898], dtype=torch.float16)\n",
      "step №393: loss = 37.188682556152344, weights = tensor([6.0625, 1.5928], dtype=torch.float16)\n",
      "step №394: loss = 37.172119140625, weights = tensor([6.0625, 1.5957], dtype=torch.float16)\n",
      "step №395: loss = 37.15557098388672, weights = tensor([6.0625, 1.5986], dtype=torch.float16)\n",
      "step №396: loss = 37.139041900634766, weights = tensor([6.0586, 1.6016], dtype=torch.float16)\n",
      "step №397: loss = 37.121482849121094, weights = tensor([6.0586, 1.6045], dtype=torch.float16)\n",
      "step №398: loss = 37.10488510131836, weights = tensor([6.0586, 1.6074], dtype=torch.float16)\n",
      "step №399: loss = 37.08829879760742, weights = tensor([6.0586, 1.6104], dtype=torch.float16)\n",
      "step №400: loss = 37.07173538208008, weights = tensor([6.0586, 1.6133], dtype=torch.float16)\n",
      "step №401: loss = 37.05518341064453, weights = tensor([6.0586, 1.6162], dtype=torch.float16)\n",
      "step №402: loss = 37.03865051269531, weights = tensor([6.0586, 1.6191], dtype=torch.float16)\n",
      "step №403: loss = 37.022132873535156, weights = tensor([6.0586, 1.6221], dtype=torch.float16)\n",
      "step №404: loss = 37.005638122558594, weights = tensor([6.0547, 1.6250], dtype=torch.float16)\n",
      "step №405: loss = 36.98816680908203, weights = tensor([6.0547, 1.6279], dtype=torch.float16)\n",
      "step №406: loss = 36.971595764160156, weights = tensor([6.0547, 1.6309], dtype=torch.float16)\n",
      "step №407: loss = 36.955047607421875, weights = tensor([6.0547, 1.6338], dtype=torch.float16)\n",
      "step №408: loss = 36.938514709472656, weights = tensor([6.0547, 1.6367], dtype=torch.float16)\n",
      "step №409: loss = 36.9219970703125, weights = tensor([6.0547, 1.6396], dtype=torch.float16)\n",
      "step №410: loss = 36.9055061340332, weights = tensor([6.0547, 1.6426], dtype=torch.float16)\n",
      "step №411: loss = 36.88901901245117, weights = tensor([6.0547, 1.6455], dtype=torch.float16)\n",
      "step №412: loss = 36.872554779052734, weights = tensor([6.0508, 1.6484], dtype=torch.float16)\n",
      "step №413: loss = 36.85516357421875, weights = tensor([6.0508, 1.6514], dtype=torch.float16)\n",
      "step №414: loss = 36.83863067626953, weights = tensor([6.0508, 1.6543], dtype=torch.float16)\n",
      "step №415: loss = 36.82211685180664, weights = tensor([6.0508, 1.6572], dtype=torch.float16)\n",
      "step №416: loss = 36.80561828613281, weights = tensor([6.0508, 1.6602], dtype=torch.float16)\n",
      "step №417: loss = 36.78913497924805, weights = tensor([6.0508, 1.6631], dtype=torch.float16)\n",
      "step №418: loss = 36.77267074584961, weights = tensor([6.0508, 1.6660], dtype=torch.float16)\n",
      "step №419: loss = 36.7562255859375, weights = tensor([6.0508, 1.6689], dtype=torch.float16)\n",
      "step №420: loss = 36.73979568481445, weights = tensor([6.0508, 1.6719], dtype=torch.float16)\n",
      "step №421: loss = 36.72338104248047, weights = tensor([6.0469, 1.6748], dtype=torch.float16)\n",
      "step №422: loss = 36.70598220825195, weights = tensor([6.0469, 1.6777], dtype=torch.float16)\n",
      "step №423: loss = 36.68950653076172, weights = tensor([6.0469, 1.6807], dtype=torch.float16)\n",
      "step №424: loss = 36.67304611206055, weights = tensor([6.0469, 1.6836], dtype=torch.float16)\n",
      "step №425: loss = 36.656593322753906, weights = tensor([6.0469, 1.6865], dtype=torch.float16)\n",
      "step №426: loss = 36.64016342163086, weights = tensor([6.0469, 1.6895], dtype=torch.float16)\n",
      "step №427: loss = 36.62375259399414, weights = tensor([6.0469, 1.6924], dtype=torch.float16)\n",
      "step №428: loss = 36.60736083984375, weights = tensor([6.0469, 1.6953], dtype=torch.float16)\n",
      "step №429: loss = 36.590980529785156, weights = tensor([6.0430, 1.6982], dtype=torch.float16)\n",
      "step №430: loss = 36.57366180419922, weights = tensor([6.0430, 1.7012], dtype=torch.float16)\n",
      "step №431: loss = 36.557212829589844, weights = tensor([6.0430, 1.7041], dtype=torch.float16)\n",
      "step №432: loss = 36.54078674316406, weights = tensor([6.0430, 1.7070], dtype=torch.float16)\n",
      "step №433: loss = 36.52437210083008, weights = tensor([6.0430, 1.7100], dtype=torch.float16)\n",
      "step №434: loss = 36.507972717285156, weights = tensor([6.0430, 1.7129], dtype=torch.float16)\n",
      "step №435: loss = 36.491600036621094, weights = tensor([6.0430, 1.7158], dtype=torch.float16)\n",
      "step №436: loss = 36.47523880004883, weights = tensor([6.0430, 1.7188], dtype=torch.float16)\n",
      "step №437: loss = 36.458892822265625, weights = tensor([6.0430, 1.7217], dtype=torch.float16)\n",
      "step №438: loss = 36.44256591796875, weights = tensor([6.0391, 1.7246], dtype=torch.float16)\n",
      "step №439: loss = 36.42524337768555, weights = tensor([6.0391, 1.7275], dtype=torch.float16)\n",
      "step №440: loss = 36.40884780883789, weights = tensor([6.0391, 1.7305], dtype=torch.float16)\n",
      "step №441: loss = 36.3924674987793, weights = tensor([6.0391, 1.7334], dtype=torch.float16)\n",
      "step №442: loss = 36.3761100769043, weights = tensor([6.0391, 1.7363], dtype=torch.float16)\n",
      "step №443: loss = 36.359764099121094, weights = tensor([6.0391, 1.7393], dtype=torch.float16)\n",
      "step №444: loss = 36.34343719482422, weights = tensor([6.0391, 1.7422], dtype=torch.float16)\n",
      "step №445: loss = 36.32712936401367, weights = tensor([6.0391, 1.7451], dtype=torch.float16)\n",
      "step №446: loss = 36.31083679199219, weights = tensor([6.0352, 1.7480], dtype=torch.float16)\n",
      "step №447: loss = 36.29359436035156, weights = tensor([6.0352, 1.7510], dtype=torch.float16)\n",
      "step №448: loss = 36.2772331237793, weights = tensor([6.0352, 1.7539], dtype=torch.float16)\n",
      "step №449: loss = 36.260887145996094, weights = tensor([6.0352, 1.7568], dtype=torch.float16)\n",
      "step №450: loss = 36.244564056396484, weights = tensor([6.0352, 1.7598], dtype=torch.float16)\n",
      "step №451: loss = 36.22825241088867, weights = tensor([6.0352, 1.7627], dtype=torch.float16)\n",
      "step №452: loss = 36.21196365356445, weights = tensor([6.0352, 1.7656], dtype=torch.float16)\n",
      "step №453: loss = 36.19568634033203, weights = tensor([6.0352, 1.7686], dtype=torch.float16)\n",
      "step №454: loss = 36.17942810058594, weights = tensor([6.0352, 1.7715], dtype=torch.float16)\n",
      "step №455: loss = 36.163185119628906, weights = tensor([6.0312, 1.7744], dtype=torch.float16)\n",
      "step №456: loss = 36.14594268798828, weights = tensor([6.0312, 1.7773], dtype=torch.float16)\n",
      "step №457: loss = 36.1296272277832, weights = tensor([6.0312, 1.7803], dtype=torch.float16)\n",
      "step №458: loss = 36.11333465576172, weights = tensor([6.0312, 1.7832], dtype=torch.float16)\n",
      "step №459: loss = 36.09706115722656, weights = tensor([6.0312, 1.7861], dtype=torch.float16)\n",
      "step №460: loss = 36.080806732177734, weights = tensor([6.0312, 1.7891], dtype=torch.float16)\n",
      "step №461: loss = 36.0645637512207, weights = tensor([6.0312, 1.7920], dtype=torch.float16)\n",
      "step №462: loss = 36.048343658447266, weights = tensor([6.0312, 1.7949], dtype=torch.float16)\n",
      "step №463: loss = 36.03213119506836, weights = tensor([6.0273, 1.7979], dtype=torch.float16)\n",
      "step №464: loss = 36.01496505737305, weights = tensor([6.0273, 1.8008], dtype=torch.float16)\n",
      "step №465: loss = 35.998687744140625, weights = tensor([6.0273, 1.8037], dtype=torch.float16)\n",
      "step №466: loss = 35.98242950439453, weights = tensor([6.0273, 1.8066], dtype=torch.float16)\n",
      "step №467: loss = 35.966190338134766, weights = tensor([6.0273, 1.8096], dtype=torch.float16)\n",
      "step №468: loss = 35.94996643066406, weights = tensor([6.0273, 1.8125], dtype=torch.float16)\n",
      "step №469: loss = 35.93375778198242, weights = tensor([6.0273, 1.8154], dtype=torch.float16)\n",
      "step №470: loss = 35.917572021484375, weights = tensor([6.0273, 1.8184], dtype=torch.float16)\n",
      "step №471: loss = 35.901397705078125, weights = tensor([6.0273, 1.8213], dtype=torch.float16)\n",
      "step №472: loss = 35.8852424621582, weights = tensor([6.0234, 1.8242], dtype=torch.float16)\n",
      "step №473: loss = 35.868064880371094, weights = tensor([6.0234, 1.8271], dtype=torch.float16)\n",
      "step №474: loss = 35.85184860229492, weights = tensor([6.0234, 1.8301], dtype=torch.float16)\n",
      "step №475: loss = 35.83563995361328, weights = tensor([6.0234, 1.8330], dtype=torch.float16)\n",
      "step №476: loss = 35.81945037841797, weights = tensor([6.0234, 1.8359], dtype=torch.float16)\n",
      "step №477: loss = 35.80327606201172, weights = tensor([6.0234, 1.8389], dtype=torch.float16)\n",
      "step №478: loss = 35.7871208190918, weights = tensor([6.0234, 1.8418], dtype=torch.float16)\n",
      "step №479: loss = 35.7709846496582, weights = tensor([6.0234, 1.8447], dtype=torch.float16)\n",
      "step №480: loss = 35.75486373901367, weights = tensor([6.0195, 1.8477], dtype=torch.float16)\n",
      "step №481: loss = 35.737770080566406, weights = tensor([6.0195, 1.8506], dtype=torch.float16)\n",
      "step №482: loss = 35.721580505371094, weights = tensor([6.0195, 1.8535], dtype=torch.float16)\n",
      "step №483: loss = 35.705406188964844, weights = tensor([6.0195, 1.8564], dtype=torch.float16)\n",
      "step №484: loss = 35.68925094604492, weights = tensor([6.0195, 1.8594], dtype=torch.float16)\n",
      "step №485: loss = 35.67311477661133, weights = tensor([6.0195, 1.8623], dtype=torch.float16)\n",
      "step №486: loss = 35.6569938659668, weights = tensor([6.0195, 1.8652], dtype=torch.float16)\n",
      "step №487: loss = 35.640892028808594, weights = tensor([6.0195, 1.8682], dtype=torch.float16)\n",
      "step №488: loss = 35.62480545043945, weights = tensor([6.0156, 1.8711], dtype=torch.float16)\n",
      "step №489: loss = 35.6077880859375, weights = tensor([6.0156, 1.8740], dtype=torch.float16)\n",
      "step №490: loss = 35.591636657714844, weights = tensor([6.0156, 1.8770], dtype=torch.float16)\n",
      "step №491: loss = 35.575496673583984, weights = tensor([6.0156, 1.8799], dtype=torch.float16)\n",
      "step №492: loss = 35.55937957763672, weights = tensor([6.0156, 1.8828], dtype=torch.float16)\n",
      "step №493: loss = 35.54327392578125, weights = tensor([6.0156, 1.8857], dtype=torch.float16)\n",
      "step №494: loss = 35.527191162109375, weights = tensor([6.0156, 1.8887], dtype=torch.float16)\n",
      "step №495: loss = 35.51111602783203, weights = tensor([6.0156, 1.8916], dtype=torch.float16)\n",
      "step №496: loss = 35.49506378173828, weights = tensor([6.0156, 1.8945], dtype=torch.float16)\n",
      "step №497: loss = 35.479026794433594, weights = tensor([6.0117, 1.8975], dtype=torch.float16)\n",
      "step №498: loss = 35.46200942993164, weights = tensor([6.0117, 1.9004], dtype=torch.float16)\n",
      "step №499: loss = 35.44590759277344, weights = tensor([6.0117, 1.9033], dtype=torch.float16)\n",
      "step №500: loss = 35.4298210144043, weights = tensor([6.0117, 1.9062], dtype=torch.float16)\n",
      "step №501: loss = 35.41374969482422, weights = tensor([6.0117, 1.9092], dtype=torch.float16)\n",
      "step №502: loss = 35.39769744873047, weights = tensor([6.0117, 1.9121], dtype=torch.float16)\n",
      "step №503: loss = 35.38166427612305, weights = tensor([6.0117, 1.9150], dtype=torch.float16)\n",
      "step №504: loss = 35.36565017700195, weights = tensor([6.0117, 1.9180], dtype=torch.float16)\n",
      "step №505: loss = 35.349647521972656, weights = tensor([6.0078, 1.9209], dtype=torch.float16)\n",
      "step №506: loss = 35.33271026611328, weights = tensor([6.0078, 1.9238], dtype=torch.float16)\n",
      "step №507: loss = 35.3166389465332, weights = tensor([6.0078, 1.9268], dtype=torch.float16)\n",
      "step №508: loss = 35.30058670043945, weights = tensor([6.0078, 1.9297], dtype=torch.float16)\n",
      "step №509: loss = 35.28455352783203, weights = tensor([6.0078, 1.9326], dtype=torch.float16)\n",
      "step №510: loss = 35.268531799316406, weights = tensor([6.0078, 1.9355], dtype=torch.float16)\n",
      "step №511: loss = 35.252532958984375, weights = tensor([6.0078, 1.9385], dtype=torch.float16)\n",
      "step №512: loss = 35.23655319213867, weights = tensor([6.0078, 1.9414], dtype=torch.float16)\n",
      "step №513: loss = 35.2205810546875, weights = tensor([6.0078, 1.9443], dtype=torch.float16)\n",
      "step №514: loss = 35.20463943481445, weights = tensor([6.0039, 1.9473], dtype=torch.float16)\n",
      "step №515: loss = 35.18769073486328, weights = tensor([6.0039, 1.9502], dtype=torch.float16)\n",
      "step №516: loss = 35.17167282104492, weights = tensor([6.0039, 1.9531], dtype=torch.float16)\n",
      "step №517: loss = 35.155670166015625, weights = tensor([6.0039, 1.9561], dtype=torch.float16)\n",
      "step №518: loss = 35.139686584472656, weights = tensor([6.0039, 1.9590], dtype=torch.float16)\n",
      "step №519: loss = 35.123722076416016, weights = tensor([6.0039, 1.9619], dtype=torch.float16)\n",
      "step №520: loss = 35.10776901245117, weights = tensor([6.0039, 1.9648], dtype=torch.float16)\n",
      "step №521: loss = 35.09183883666992, weights = tensor([6.0039, 1.9678], dtype=torch.float16)\n",
      "step №522: loss = 35.075927734375, weights = tensor([6.0000, 1.9707], dtype=torch.float16)\n",
      "step №523: loss = 35.059059143066406, weights = tensor([6.0000, 1.9736], dtype=torch.float16)\n",
      "step №524: loss = 35.04308319091797, weights = tensor([6.0000, 1.9766], dtype=torch.float16)\n",
      "step №525: loss = 35.0271110534668, weights = tensor([6.0000, 1.9795], dtype=torch.float16)\n",
      "step №526: loss = 35.011165618896484, weights = tensor([6.0000, 1.9824], dtype=torch.float16)\n",
      "step №527: loss = 34.99523162841797, weights = tensor([6.0000, 1.9854], dtype=torch.float16)\n",
      "step №528: loss = 34.97931671142578, weights = tensor([6.0000, 1.9883], dtype=torch.float16)\n",
      "step №529: loss = 34.963417053222656, weights = tensor([6.0000, 1.9912], dtype=torch.float16)\n",
      "step №530: loss = 34.94754409790039, weights = tensor([6.0000, 1.9941], dtype=torch.float16)\n",
      "step №531: loss = 34.93167495727539, weights = tensor([5.9961, 1.9971], dtype=torch.float16)\n",
      "step №532: loss = 34.91480255126953, weights = tensor([5.9961, 2.0000], dtype=torch.float16)\n",
      "step №533: loss = 34.89887237548828, weights = tensor([5.9961, 2.0039], dtype=torch.float16)\n",
      "step №534: loss = 34.877655029296875, weights = tensor([5.9961, 2.0078], dtype=torch.float16)\n",
      "step №535: loss = 34.856468200683594, weights = tensor([5.9961, 2.0117], dtype=torch.float16)\n",
      "step №536: loss = 34.8353157043457, weights = tensor([5.9961, 2.0156], dtype=torch.float16)\n",
      "step №537: loss = 34.81418991088867, weights = tensor([5.9961, 2.0195], dtype=torch.float16)\n",
      "step №538: loss = 34.79309844970703, weights = tensor([5.9922, 2.0234], dtype=torch.float16)\n",
      "step №539: loss = 34.77095413208008, weights = tensor([5.9922, 2.0273], dtype=torch.float16)\n",
      "step №540: loss = 34.749778747558594, weights = tensor([5.9922, 2.0312], dtype=torch.float16)\n",
      "step №541: loss = 34.72864532470703, weights = tensor([5.9922, 2.0352], dtype=torch.float16)\n",
      "step №542: loss = 34.7075309753418, weights = tensor([5.9922, 2.0391], dtype=torch.float16)\n",
      "step №543: loss = 34.68645477294922, weights = tensor([5.9922, 2.0430], dtype=torch.float16)\n",
      "step №544: loss = 34.6654052734375, weights = tensor([5.9883, 2.0469], dtype=torch.float16)\n",
      "step №545: loss = 34.64335250854492, weights = tensor([5.9883, 2.0508], dtype=torch.float16)\n",
      "step №546: loss = 34.622230529785156, weights = tensor([5.9883, 2.0547], dtype=torch.float16)\n",
      "step №547: loss = 34.60113525390625, weights = tensor([5.9883, 2.0586], dtype=torch.float16)\n",
      "step №548: loss = 34.58007049560547, weights = tensor([5.9883, 2.0625], dtype=torch.float16)\n",
      "step №549: loss = 34.55904006958008, weights = tensor([5.9883, 2.0664], dtype=torch.float16)\n",
      "step №550: loss = 34.53803634643555, weights = tensor([5.9844, 2.0703], dtype=torch.float16)\n",
      "step №551: loss = 34.516075134277344, weights = tensor([5.9844, 2.0742], dtype=torch.float16)\n",
      "step №552: loss = 34.4949951171875, weights = tensor([5.9844, 2.0781], dtype=torch.float16)\n",
      "step №553: loss = 34.47394943237305, weights = tensor([5.9844, 2.0820], dtype=torch.float16)\n",
      "step №554: loss = 34.45293045043945, weights = tensor([5.9844, 2.0859], dtype=torch.float16)\n",
      "step №555: loss = 34.43194580078125, weights = tensor([5.9844, 2.0898], dtype=torch.float16)\n",
      "step №556: loss = 34.410987854003906, weights = tensor([5.9844, 2.0938], dtype=torch.float16)\n",
      "step №557: loss = 34.39006423950195, weights = tensor([5.9805, 2.0977], dtype=torch.float16)\n",
      "step №558: loss = 34.36808395385742, weights = tensor([5.9805, 2.1016], dtype=torch.float16)\n",
      "step №559: loss = 34.347084045410156, weights = tensor([5.9805, 2.1055], dtype=torch.float16)\n",
      "step №560: loss = 34.32611083984375, weights = tensor([5.9805, 2.1094], dtype=torch.float16)\n",
      "step №561: loss = 34.30516815185547, weights = tensor([5.9805, 2.1133], dtype=torch.float16)\n",
      "step №562: loss = 34.28425979614258, weights = tensor([5.9805, 2.1172], dtype=torch.float16)\n",
      "step №563: loss = 34.26337814331055, weights = tensor([5.9766, 2.1211], dtype=torch.float16)\n",
      "step №564: loss = 34.241493225097656, weights = tensor([5.9766, 2.1250], dtype=torch.float16)\n",
      "step №565: loss = 34.22053909301758, weights = tensor([5.9766, 2.1289], dtype=torch.float16)\n",
      "step №566: loss = 34.199607849121094, weights = tensor([5.9766, 2.1328], dtype=torch.float16)\n",
      "step №567: loss = 34.17871856689453, weights = tensor([5.9766, 2.1367], dtype=torch.float16)\n",
      "step №568: loss = 34.1578483581543, weights = tensor([5.9766, 2.1406], dtype=torch.float16)\n",
      "step №569: loss = 34.13701629638672, weights = tensor([5.9727, 2.1445], dtype=torch.float16)\n",
      "step №570: loss = 34.1152229309082, weights = tensor([5.9727, 2.1484], dtype=torch.float16)\n",
      "step №571: loss = 34.09431076049805, weights = tensor([5.9727, 2.1523], dtype=torch.float16)\n",
      "step №572: loss = 34.07343292236328, weights = tensor([5.9727, 2.1562], dtype=torch.float16)\n",
      "step №573: loss = 34.052581787109375, weights = tensor([5.9727, 2.1602], dtype=torch.float16)\n",
      "step №574: loss = 34.031761169433594, weights = tensor([5.9727, 2.1641], dtype=torch.float16)\n",
      "step №575: loss = 34.0109748840332, weights = tensor([5.9727, 2.1680], dtype=torch.float16)\n",
      "step №576: loss = 33.99021530151367, weights = tensor([5.9688, 2.1719], dtype=torch.float16)\n",
      "step №577: loss = 33.968406677246094, weights = tensor([5.9688, 2.1758], dtype=torch.float16)\n",
      "step №578: loss = 33.94757080078125, weights = tensor([5.9688, 2.1797], dtype=torch.float16)\n",
      "step №579: loss = 33.9267692565918, weights = tensor([5.9688, 2.1836], dtype=torch.float16)\n",
      "step №580: loss = 33.9059944152832, weights = tensor([5.9688, 2.1875], dtype=torch.float16)\n",
      "step №581: loss = 33.88525390625, weights = tensor([5.9688, 2.1914], dtype=torch.float16)\n",
      "step №582: loss = 33.864540100097656, weights = tensor([5.9648, 2.1953], dtype=torch.float16)\n",
      "step №583: loss = 33.84282302856445, weights = tensor([5.9648, 2.1992], dtype=torch.float16)\n",
      "step №584: loss = 33.8220329284668, weights = tensor([5.9648, 2.2031], dtype=torch.float16)\n",
      "step №585: loss = 33.80127716064453, weights = tensor([5.9648, 2.2070], dtype=torch.float16)\n",
      "step №586: loss = 33.780548095703125, weights = tensor([5.9648, 2.2109], dtype=torch.float16)\n",
      "step №587: loss = 33.759849548339844, weights = tensor([5.9648, 2.2148], dtype=torch.float16)\n",
      "step №588: loss = 33.73918533325195, weights = tensor([5.9609, 2.2188], dtype=torch.float16)\n",
      "step №589: loss = 33.717559814453125, weights = tensor([5.9609, 2.2227], dtype=torch.float16)\n",
      "step №590: loss = 33.696815490722656, weights = tensor([5.9609, 2.2266], dtype=torch.float16)\n",
      "step №591: loss = 33.67610549926758, weights = tensor([5.9609, 2.2305], dtype=torch.float16)\n",
      "step №592: loss = 33.655418395996094, weights = tensor([5.9609, 2.2344], dtype=torch.float16)\n",
      "step №593: loss = 33.63477325439453, weights = tensor([5.9609, 2.2383], dtype=torch.float16)\n",
      "step №594: loss = 33.6141471862793, weights = tensor([5.9609, 2.2422], dtype=torch.float16)\n",
      "step №595: loss = 33.59355926513672, weights = tensor([5.9570, 2.2461], dtype=torch.float16)\n",
      "step №596: loss = 33.57191848754883, weights = tensor([5.9570, 2.2500], dtype=torch.float16)\n",
      "step №597: loss = 33.55125045776367, weights = tensor([5.9570, 2.2539], dtype=torch.float16)\n",
      "step №598: loss = 33.530616760253906, weights = tensor([5.9570, 2.2578], dtype=torch.float16)\n",
      "step №599: loss = 33.510009765625, weights = tensor([5.9570, 2.2617], dtype=torch.float16)\n",
      "step №600: loss = 33.48943328857422, weights = tensor([5.9570, 2.2656], dtype=torch.float16)\n",
      "step №601: loss = 33.46889114379883, weights = tensor([5.9531, 2.2695], dtype=torch.float16)\n",
      "step №602: loss = 33.44733810424805, weights = tensor([5.9531, 2.2734], dtype=torch.float16)\n",
      "step №603: loss = 33.426719665527344, weights = tensor([5.9531, 2.2773], dtype=torch.float16)\n",
      "step №604: loss = 33.4061279296875, weights = tensor([5.9531, 2.2812], dtype=torch.float16)\n",
      "step №605: loss = 33.38557052612305, weights = tensor([5.9531, 2.2852], dtype=torch.float16)\n",
      "step №606: loss = 33.36503982543945, weights = tensor([5.9531, 2.2891], dtype=torch.float16)\n",
      "step №607: loss = 33.34454345703125, weights = tensor([5.9492, 2.2930], dtype=torch.float16)\n",
      "step №608: loss = 33.323081970214844, weights = tensor([5.9492, 2.2969], dtype=torch.float16)\n",
      "step №609: loss = 33.30250930786133, weights = tensor([5.9492, 2.3008], dtype=torch.float16)\n",
      "step №610: loss = 33.28196334838867, weights = tensor([5.9492, 2.3047], dtype=torch.float16)\n",
      "step №611: loss = 33.261451721191406, weights = tensor([5.9492, 2.3086], dtype=torch.float16)\n",
      "step №612: loss = 33.240966796875, weights = tensor([5.9492, 2.3125], dtype=torch.float16)\n",
      "step №613: loss = 33.22051239013672, weights = tensor([5.9492, 2.3164], dtype=torch.float16)\n",
      "step №614: loss = 33.20009231567383, weights = tensor([5.9453, 2.3203], dtype=torch.float16)\n",
      "step №615: loss = 33.178619384765625, weights = tensor([5.9453, 2.3242], dtype=torch.float16)\n",
      "step №616: loss = 33.158119201660156, weights = tensor([5.9453, 2.3281], dtype=torch.float16)\n",
      "step №617: loss = 33.13765335083008, weights = tensor([5.9453, 2.3320], dtype=torch.float16)\n",
      "step №618: loss = 33.117210388183594, weights = tensor([5.9453, 2.3359], dtype=torch.float16)\n",
      "step №619: loss = 33.09680938720703, weights = tensor([5.9453, 2.3398], dtype=torch.float16)\n",
      "step №620: loss = 33.0764274597168, weights = tensor([5.9414, 2.3438], dtype=torch.float16)\n",
      "step №621: loss = 33.05504608154297, weights = tensor([5.9414, 2.3477], dtype=torch.float16)\n",
      "step №622: loss = 33.03459548950195, weights = tensor([5.9414, 2.3516], dtype=torch.float16)\n",
      "step №623: loss = 33.0141716003418, weights = tensor([5.9414, 2.3555], dtype=torch.float16)\n",
      "step №624: loss = 32.99378204345703, weights = tensor([5.9414, 2.3594], dtype=torch.float16)\n",
      "step №625: loss = 32.973419189453125, weights = tensor([5.9414, 2.3633], dtype=torch.float16)\n",
      "step №626: loss = 32.953086853027344, weights = tensor([5.9375, 2.3672], dtype=torch.float16)\n",
      "step №627: loss = 32.931800842285156, weights = tensor([5.9375, 2.3711], dtype=torch.float16)\n",
      "step №628: loss = 32.9113883972168, weights = tensor([5.9375, 2.3750], dtype=torch.float16)\n",
      "step №629: loss = 32.891014099121094, weights = tensor([5.9375, 2.3789], dtype=torch.float16)\n",
      "step №630: loss = 32.87066650390625, weights = tensor([5.9375, 2.3828], dtype=torch.float16)\n",
      "step №631: loss = 32.8503532409668, weights = tensor([5.9375, 2.3867], dtype=torch.float16)\n",
      "step №632: loss = 32.8300666809082, weights = tensor([5.9375, 2.3906], dtype=torch.float16)\n",
      "step №633: loss = 32.809814453125, weights = tensor([5.9336, 2.3945], dtype=torch.float16)\n",
      "step №634: loss = 32.78850555419922, weights = tensor([5.9336, 2.3984], dtype=torch.float16)\n",
      "step №635: loss = 32.7681770324707, weights = tensor([5.9336, 2.4023], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №636: loss = 32.74787521362305, weights = tensor([5.9336, 2.4062], dtype=torch.float16)\n",
      "step №637: loss = 32.72760772705078, weights = tensor([5.9336, 2.4102], dtype=torch.float16)\n",
      "step №638: loss = 32.707366943359375, weights = tensor([5.9336, 2.4141], dtype=torch.float16)\n",
      "step №639: loss = 32.687156677246094, weights = tensor([5.9297, 2.4180], dtype=torch.float16)\n",
      "step №640: loss = 32.66594314575195, weights = tensor([5.9297, 2.4219], dtype=torch.float16)\n",
      "step №641: loss = 32.645660400390625, weights = tensor([5.9297, 2.4258], dtype=torch.float16)\n",
      "step №642: loss = 32.625404357910156, weights = tensor([5.9297, 2.4297], dtype=torch.float16)\n",
      "step №643: loss = 32.60518264770508, weights = tensor([5.9297, 2.4336], dtype=torch.float16)\n",
      "step №644: loss = 32.584983825683594, weights = tensor([5.9297, 2.4375], dtype=torch.float16)\n",
      "step №645: loss = 32.56482696533203, weights = tensor([5.9258, 2.4414], dtype=torch.float16)\n",
      "step №646: loss = 32.543701171875, weights = tensor([5.9258, 2.4453], dtype=torch.float16)\n",
      "step №647: loss = 32.523460388183594, weights = tensor([5.9258, 2.4492], dtype=torch.float16)\n",
      "step №648: loss = 32.50325393676758, weights = tensor([5.9258, 2.4531], dtype=torch.float16)\n",
      "step №649: loss = 32.48307418823242, weights = tensor([5.9258, 2.4570], dtype=torch.float16)\n",
      "step №650: loss = 32.462928771972656, weights = tensor([5.9258, 2.4609], dtype=torch.float16)\n",
      "step №651: loss = 32.44281005859375, weights = tensor([5.9258, 2.4648], dtype=torch.float16)\n",
      "step №652: loss = 32.42272186279297, weights = tensor([5.9219, 2.4688], dtype=torch.float16)\n",
      "step №653: loss = 32.401588439941406, weights = tensor([5.9219, 2.4727], dtype=torch.float16)\n",
      "step №654: loss = 32.38142013549805, weights = tensor([5.9219, 2.4766], dtype=torch.float16)\n",
      "step №655: loss = 32.361289978027344, weights = tensor([5.9219, 2.4805], dtype=torch.float16)\n",
      "step №656: loss = 32.3411865234375, weights = tensor([5.9219, 2.4844], dtype=torch.float16)\n",
      "step №657: loss = 32.32111740112305, weights = tensor([5.9219, 2.4883], dtype=torch.float16)\n",
      "step №658: loss = 32.30107498168945, weights = tensor([5.9180, 2.4922], dtype=torch.float16)\n",
      "step №659: loss = 32.280029296875, weights = tensor([5.9180, 2.4961], dtype=torch.float16)\n",
      "step №660: loss = 32.259910583496094, weights = tensor([5.9180, 2.5000], dtype=torch.float16)\n",
      "step №661: loss = 32.23982620239258, weights = tensor([5.9180, 2.5039], dtype=torch.float16)\n",
      "step №662: loss = 32.21976852416992, weights = tensor([5.9180, 2.5078], dtype=torch.float16)\n",
      "step №663: loss = 32.199745178222656, weights = tensor([5.9180, 2.5117], dtype=torch.float16)\n",
      "step №664: loss = 32.17974853515625, weights = tensor([5.9141, 2.5156], dtype=torch.float16)\n",
      "step №665: loss = 32.15879440307617, weights = tensor([5.9141, 2.5195], dtype=torch.float16)\n",
      "step №666: loss = 32.13872146606445, weights = tensor([5.9141, 2.5234], dtype=torch.float16)\n",
      "step №667: loss = 32.118682861328125, weights = tensor([5.9141, 2.5273], dtype=torch.float16)\n",
      "step №668: loss = 32.098670959472656, weights = tensor([5.9141, 2.5312], dtype=torch.float16)\n",
      "step №669: loss = 32.07869338989258, weights = tensor([5.9141, 2.5352], dtype=torch.float16)\n",
      "step №670: loss = 32.058738708496094, weights = tensor([5.9141, 2.5391], dtype=torch.float16)\n",
      "step №671: loss = 32.03882598876953, weights = tensor([5.9102, 2.5430], dtype=torch.float16)\n",
      "step №672: loss = 32.017852783203125, weights = tensor([5.9102, 2.5469], dtype=torch.float16)\n",
      "step №673: loss = 31.99785804748535, weights = tensor([5.9102, 2.5508], dtype=torch.float16)\n",
      "step №674: loss = 31.977893829345703, weights = tensor([5.9102, 2.5547], dtype=torch.float16)\n",
      "step №675: loss = 31.957958221435547, weights = tensor([5.9102, 2.5586], dtype=torch.float16)\n",
      "step №676: loss = 31.93805503845215, weights = tensor([5.9102, 2.5625], dtype=torch.float16)\n",
      "step №677: loss = 31.918182373046875, weights = tensor([5.9062, 2.5664], dtype=torch.float16)\n",
      "step №678: loss = 31.897302627563477, weights = tensor([5.9062, 2.5703], dtype=torch.float16)\n",
      "step №679: loss = 31.877355575561523, weights = tensor([5.9062, 2.5742], dtype=torch.float16)\n",
      "step №680: loss = 31.857433319091797, weights = tensor([5.9062, 2.5781], dtype=torch.float16)\n",
      "step №681: loss = 31.837549209594727, weights = tensor([5.9062, 2.5820], dtype=torch.float16)\n",
      "step №682: loss = 31.81768798828125, weights = tensor([5.9062, 2.5859], dtype=torch.float16)\n",
      "step №683: loss = 31.797863006591797, weights = tensor([5.9023, 2.5898], dtype=torch.float16)\n",
      "step №684: loss = 31.777074813842773, weights = tensor([5.9023, 2.5938], dtype=torch.float16)\n",
      "step №685: loss = 31.757171630859375, weights = tensor([5.9023, 2.5977], dtype=torch.float16)\n",
      "step №686: loss = 31.7372989654541, weights = tensor([5.9023, 2.6016], dtype=torch.float16)\n",
      "step №687: loss = 31.717456817626953, weights = tensor([5.9023, 2.6055], dtype=torch.float16)\n",
      "step №688: loss = 31.697643280029297, weights = tensor([5.9023, 2.6094], dtype=torch.float16)\n",
      "step №689: loss = 31.6778621673584, weights = tensor([5.9023, 2.6133], dtype=torch.float16)\n",
      "step №690: loss = 31.658111572265625, weights = tensor([5.8984, 2.6172], dtype=torch.float16)\n",
      "step №691: loss = 31.637310028076172, weights = tensor([5.8984, 2.6211], dtype=torch.float16)\n",
      "step №692: loss = 31.617481231689453, weights = tensor([5.8984, 2.6250], dtype=torch.float16)\n",
      "step №693: loss = 31.597686767578125, weights = tensor([5.8984, 2.6289], dtype=torch.float16)\n",
      "step №694: loss = 31.577917098999023, weights = tensor([5.8984, 2.6328], dtype=torch.float16)\n",
      "step №695: loss = 31.558185577392578, weights = tensor([5.8984, 2.6367], dtype=torch.float16)\n",
      "step №696: loss = 31.538476943969727, weights = tensor([5.8945, 2.6406], dtype=torch.float16)\n",
      "step №697: loss = 31.51776695251465, weights = tensor([5.8945, 2.6445], dtype=torch.float16)\n",
      "step №698: loss = 31.49798583984375, weights = tensor([5.8945, 2.6484], dtype=torch.float16)\n",
      "step №699: loss = 31.478235244750977, weights = tensor([5.8945, 2.6523], dtype=torch.float16)\n",
      "step №700: loss = 31.458515167236328, weights = tensor([5.8945, 2.6562], dtype=torch.float16)\n",
      "step №701: loss = 31.438823699951172, weights = tensor([5.8945, 2.6602], dtype=torch.float16)\n",
      "step №702: loss = 31.419164657592773, weights = tensor([5.8906, 2.6641], dtype=torch.float16)\n",
      "step №703: loss = 31.398548126220703, weights = tensor([5.8906, 2.6680], dtype=torch.float16)\n",
      "step №704: loss = 31.378808975219727, weights = tensor([5.8906, 2.6719], dtype=torch.float16)\n",
      "step №705: loss = 31.359106063842773, weights = tensor([5.8906, 2.6758], dtype=torch.float16)\n",
      "step №706: loss = 31.339427947998047, weights = tensor([5.8906, 2.6797], dtype=torch.float16)\n",
      "step №707: loss = 31.319787979125977, weights = tensor([5.8906, 2.6836], dtype=torch.float16)\n",
      "step №708: loss = 31.3001708984375, weights = tensor([5.8906, 2.6875], dtype=torch.float16)\n",
      "step №709: loss = 31.280590057373047, weights = tensor([5.8867, 2.6914], dtype=torch.float16)\n",
      "step №710: loss = 31.25995445251465, weights = tensor([5.8867, 2.6953], dtype=torch.float16)\n",
      "step №711: loss = 31.24029541015625, weights = tensor([5.8867, 2.6992], dtype=torch.float16)\n",
      "step №712: loss = 31.220666885375977, weights = tensor([5.8867, 2.7031], dtype=torch.float16)\n",
      "step №713: loss = 31.201068878173828, weights = tensor([5.8867, 2.7070], dtype=torch.float16)\n",
      "step №714: loss = 31.181499481201172, weights = tensor([5.8867, 2.7109], dtype=torch.float16)\n",
      "step №715: loss = 31.161962509155273, weights = tensor([5.8828, 2.7148], dtype=torch.float16)\n",
      "step №716: loss = 31.14141845703125, weights = tensor([5.8828, 2.7188], dtype=torch.float16)\n",
      "step №717: loss = 31.121807098388672, weights = tensor([5.8828, 2.7227], dtype=torch.float16)\n",
      "step №718: loss = 31.102222442626953, weights = tensor([5.8828, 2.7266], dtype=torch.float16)\n",
      "step №719: loss = 31.082672119140625, weights = tensor([5.8828, 2.7305], dtype=torch.float16)\n",
      "step №720: loss = 31.063146591186523, weights = tensor([5.8828, 2.7344], dtype=torch.float16)\n",
      "step №721: loss = 31.043659210205078, weights = tensor([5.8789, 2.7383], dtype=torch.float16)\n",
      "step №722: loss = 31.023204803466797, weights = tensor([5.8789, 2.7422], dtype=torch.float16)\n",
      "step №723: loss = 31.003637313842773, weights = tensor([5.8789, 2.7461], dtype=torch.float16)\n",
      "step №724: loss = 30.984100341796875, weights = tensor([5.8789, 2.7500], dtype=torch.float16)\n",
      "step №725: loss = 30.9645938873291, weights = tensor([5.8789, 2.7539], dtype=torch.float16)\n",
      "step №726: loss = 30.945117950439453, weights = tensor([5.8789, 2.7578], dtype=torch.float16)\n",
      "step №727: loss = 30.925670623779297, weights = tensor([5.8789, 2.7617], dtype=torch.float16)\n",
      "step №728: loss = 30.9062557220459, weights = tensor([5.8750, 2.7656], dtype=torch.float16)\n",
      "step №729: loss = 30.885791778564453, weights = tensor([5.8750, 2.7695], dtype=torch.float16)\n",
      "step №730: loss = 30.866296768188477, weights = tensor([5.8750, 2.7734], dtype=torch.float16)\n",
      "step №731: loss = 30.846837997436523, weights = tensor([5.8750, 2.7773], dtype=torch.float16)\n",
      "step №732: loss = 30.827404022216797, weights = tensor([5.8750, 2.7812], dtype=torch.float16)\n",
      "step №733: loss = 30.808008193969727, weights = tensor([5.8750, 2.7852], dtype=torch.float16)\n",
      "step №734: loss = 30.78863525390625, weights = tensor([5.8711, 2.7891], dtype=torch.float16)\n",
      "step №735: loss = 30.768260955810547, weights = tensor([5.8711, 2.7930], dtype=torch.float16)\n",
      "step №736: loss = 30.748815536499023, weights = tensor([5.8711, 2.7969], dtype=torch.float16)\n",
      "step №737: loss = 30.729400634765625, weights = tensor([5.8711, 2.8008], dtype=torch.float16)\n",
      "step №738: loss = 30.71001625061035, weights = tensor([5.8711, 2.8047], dtype=torch.float16)\n",
      "step №739: loss = 30.690662384033203, weights = tensor([5.8711, 2.8086], dtype=torch.float16)\n",
      "step №740: loss = 30.671337127685547, weights = tensor([5.8672, 2.8125], dtype=torch.float16)\n",
      "step №741: loss = 30.65105628967285, weights = tensor([5.8672, 2.8164], dtype=torch.float16)\n",
      "step №742: loss = 30.63165283203125, weights = tensor([5.8672, 2.8203], dtype=torch.float16)\n",
      "step №743: loss = 30.612285614013672, weights = tensor([5.8672, 2.8242], dtype=torch.float16)\n",
      "step №744: loss = 30.592945098876953, weights = tensor([5.8672, 2.8281], dtype=torch.float16)\n",
      "step №745: loss = 30.573638916015625, weights = tensor([5.8672, 2.8320], dtype=torch.float16)\n",
      "step №746: loss = 30.554357528686523, weights = tensor([5.8672, 2.8359], dtype=torch.float16)\n",
      "step №747: loss = 30.535114288330078, weights = tensor([5.8633, 2.8398], dtype=torch.float16)\n",
      "step №748: loss = 30.514812469482422, weights = tensor([5.8633, 2.8438], dtype=torch.float16)\n",
      "step №749: loss = 30.4954891204834, weights = tensor([5.8633, 2.8477], dtype=torch.float16)\n",
      "step №750: loss = 30.4761962890625, weights = tensor([5.8633, 2.8516], dtype=torch.float16)\n",
      "step №751: loss = 30.456933975219727, weights = tensor([5.8633, 2.8555], dtype=torch.float16)\n",
      "step №752: loss = 30.437702178955078, weights = tensor([5.8633, 2.8594], dtype=torch.float16)\n",
      "step №753: loss = 30.418498992919922, weights = tensor([5.8594, 2.8633], dtype=torch.float16)\n",
      "step №754: loss = 30.398290634155273, weights = tensor([5.8594, 2.8672], dtype=torch.float16)\n",
      "step №755: loss = 30.379016876220703, weights = tensor([5.8594, 2.8711], dtype=torch.float16)\n",
      "step №756: loss = 30.359766006469727, weights = tensor([5.8594, 2.8750], dtype=torch.float16)\n",
      "step №757: loss = 30.340551376342773, weights = tensor([5.8594, 2.8789], dtype=torch.float16)\n",
      "step №758: loss = 30.321361541748047, weights = tensor([5.8594, 2.8828], dtype=torch.float16)\n",
      "step №759: loss = 30.302209854125977, weights = tensor([5.8555, 2.8867], dtype=torch.float16)\n",
      "step №760: loss = 30.282093048095703, weights = tensor([5.8555, 2.8906], dtype=torch.float16)\n",
      "step №761: loss = 30.262859344482422, weights = tensor([5.8555, 2.8945], dtype=torch.float16)\n",
      "step №762: loss = 30.2436580657959, weights = tensor([5.8555, 2.8984], dtype=torch.float16)\n",
      "step №763: loss = 30.2244873046875, weights = tensor([5.8555, 2.9023], dtype=torch.float16)\n",
      "step №764: loss = 30.205347061157227, weights = tensor([5.8555, 2.9062], dtype=torch.float16)\n",
      "step №765: loss = 30.186237335205078, weights = tensor([5.8555, 2.9102], dtype=torch.float16)\n",
      "step №766: loss = 30.167156219482422, weights = tensor([5.8516, 2.9121], dtype=torch.float16)\n",
      "step №767: loss = 30.1566162109375, weights = tensor([5.8516, 2.9160], dtype=torch.float16)\n",
      "step №768: loss = 30.1374454498291, weights = tensor([5.8516, 2.9199], dtype=torch.float16)\n",
      "step №769: loss = 30.118305206298828, weights = tensor([5.8516, 2.9238], dtype=torch.float16)\n",
      "step №770: loss = 30.099193572998047, weights = tensor([5.8516, 2.9277], dtype=torch.float16)\n",
      "step №771: loss = 30.080114364624023, weights = tensor([5.8516, 2.9297], dtype=torch.float16)\n",
      "step №772: loss = 30.070587158203125, weights = tensor([5.8516, 2.9316], dtype=torch.float16)\n",
      "step №773: loss = 30.061065673828125, weights = tensor([5.8516, 2.9336], dtype=torch.float16)\n",
      "step №774: loss = 30.051549911499023, weights = tensor([5.8477, 2.9355], dtype=torch.float16)\n",
      "step №775: loss = 30.041080474853516, weights = tensor([5.8477, 2.9395], dtype=torch.float16)\n",
      "step №776: loss = 30.021953582763672, weights = tensor([5.8477, 2.9434], dtype=torch.float16)\n",
      "step №777: loss = 30.002859115600586, weights = tensor([5.8477, 2.9473], dtype=torch.float16)\n",
      "step №778: loss = 29.983795166015625, weights = tensor([5.8477, 2.9492], dtype=torch.float16)\n",
      "step №779: loss = 29.974273681640625, weights = tensor([5.8477, 2.9512], dtype=torch.float16)\n",
      "step №780: loss = 29.96476173400879, weights = tensor([5.8477, 2.9531], dtype=torch.float16)\n",
      "step №781: loss = 29.95525550842285, weights = tensor([5.8477, 2.9551], dtype=torch.float16)\n",
      "step №782: loss = 29.945758819580078, weights = tensor([5.8477, 2.9570], dtype=torch.float16)\n",
      "step №783: loss = 29.936267852783203, weights = tensor([5.8438, 2.9590], dtype=torch.float16)\n",
      "step №784: loss = 29.92586326599121, weights = tensor([5.8438, 2.9629], dtype=torch.float16)\n",
      "step №785: loss = 29.906784057617188, weights = tensor([5.8438, 2.9648], dtype=torch.float16)\n",
      "step №786: loss = 29.897253036499023, weights = tensor([5.8438, 2.9668], dtype=torch.float16)\n",
      "step №787: loss = 29.88773536682129, weights = tensor([5.8438, 2.9688], dtype=torch.float16)\n",
      "step №788: loss = 29.878223419189453, weights = tensor([5.8438, 2.9707], dtype=torch.float16)\n",
      "step №789: loss = 29.868717193603516, weights = tensor([5.8438, 2.9727], dtype=torch.float16)\n",
      "step №790: loss = 29.859216690063477, weights = tensor([5.8438, 2.9746], dtype=torch.float16)\n",
      "step №791: loss = 29.849727630615234, weights = tensor([5.8438, 2.9766], dtype=torch.float16)\n",
      "step №792: loss = 29.840246200561523, weights = tensor([5.8438, 2.9785], dtype=torch.float16)\n",
      "step №793: loss = 29.83077049255371, weights = tensor([5.8438, 2.9805], dtype=torch.float16)\n",
      "step №794: loss = 29.821300506591797, weights = tensor([5.8438, 2.9824], dtype=torch.float16)\n",
      "step №795: loss = 29.811843872070312, weights = tensor([5.8398, 2.9844], dtype=torch.float16)\n",
      "step №796: loss = 29.80144691467285, weights = tensor([5.8398, 2.9863], dtype=torch.float16)\n",
      "step №797: loss = 29.791934967041016, weights = tensor([5.8398, 2.9883], dtype=torch.float16)\n",
      "step №798: loss = 29.782428741455078, weights = tensor([5.8398, 2.9902], dtype=torch.float16)\n",
      "step №799: loss = 29.772930145263672, weights = tensor([5.8398, 2.9922], dtype=torch.float16)\n",
      "step №800: loss = 29.763439178466797, weights = tensor([5.8398, 2.9941], dtype=torch.float16)\n",
      "step №801: loss = 29.753957748413086, weights = tensor([5.8398, 2.9961], dtype=torch.float16)\n",
      "step №802: loss = 29.744482040405273, weights = tensor([5.8398, 2.9980], dtype=torch.float16)\n",
      "step №803: loss = 29.735015869140625, weights = tensor([5.8398, 3.0000], dtype=torch.float16)\n",
      "step №804: loss = 29.725555419921875, weights = tensor([5.8398, 3.0020], dtype=torch.float16)\n",
      "step №805: loss = 29.71610450744629, weights = tensor([5.8398, 3.0039], dtype=torch.float16)\n",
      "step №806: loss = 29.7066593170166, weights = tensor([5.8398, 3.0059], dtype=torch.float16)\n",
      "step №807: loss = 29.697223663330078, weights = tensor([5.8398, 3.0078], dtype=torch.float16)\n",
      "step №808: loss = 29.687793731689453, weights = tensor([5.8359, 3.0098], dtype=torch.float16)\n",
      "step №809: loss = 29.677404403686523, weights = tensor([5.8359, 3.0117], dtype=torch.float16)\n",
      "step №810: loss = 29.667919158935547, weights = tensor([5.8359, 3.0137], dtype=torch.float16)\n",
      "step №811: loss = 29.658447265625, weights = tensor([5.8359, 3.0156], dtype=torch.float16)\n",
      "step №812: loss = 29.64898109436035, weights = tensor([5.8359, 3.0176], dtype=torch.float16)\n",
      "step №813: loss = 29.6395206451416, weights = tensor([5.8359, 3.0195], dtype=torch.float16)\n",
      "step №814: loss = 29.63006591796875, weights = tensor([5.8359, 3.0215], dtype=torch.float16)\n",
      "step №815: loss = 29.620624542236328, weights = tensor([5.8359, 3.0234], dtype=torch.float16)\n",
      "step №816: loss = 29.611186981201172, weights = tensor([5.8359, 3.0254], dtype=torch.float16)\n",
      "step №817: loss = 29.601757049560547, weights = tensor([5.8359, 3.0273], dtype=torch.float16)\n",
      "step №818: loss = 29.592334747314453, weights = tensor([5.8359, 3.0293], dtype=torch.float16)\n",
      "step №819: loss = 29.582921981811523, weights = tensor([5.8359, 3.0312], dtype=torch.float16)\n",
      "step №820: loss = 29.573516845703125, weights = tensor([5.8320, 3.0332], dtype=torch.float16)\n",
      "step №821: loss = 29.563196182250977, weights = tensor([5.8320, 3.0352], dtype=torch.float16)\n",
      "step №822: loss = 29.553735733032227, weights = tensor([5.8320, 3.0371], dtype=torch.float16)\n",
      "step №823: loss = 29.54428482055664, weights = tensor([5.8320, 3.0391], dtype=torch.float16)\n",
      "step №824: loss = 29.534839630126953, weights = tensor([5.8320, 3.0410], dtype=torch.float16)\n",
      "step №825: loss = 29.525402069091797, weights = tensor([5.8320, 3.0430], dtype=torch.float16)\n",
      "step №826: loss = 29.515972137451172, weights = tensor([5.8320, 3.0449], dtype=torch.float16)\n",
      "step №827: loss = 29.50655174255371, weights = tensor([5.8320, 3.0469], dtype=torch.float16)\n",
      "step №828: loss = 29.49713706970215, weights = tensor([5.8320, 3.0488], dtype=torch.float16)\n",
      "step №829: loss = 29.48773193359375, weights = tensor([5.8320, 3.0508], dtype=torch.float16)\n",
      "step №830: loss = 29.47833251953125, weights = tensor([5.8320, 3.0527], dtype=torch.float16)\n",
      "step №831: loss = 29.468942642211914, weights = tensor([5.8320, 3.0547], dtype=torch.float16)\n",
      "step №832: loss = 29.459558486938477, weights = tensor([5.8320, 3.0566], dtype=torch.float16)\n",
      "step №833: loss = 29.450183868408203, weights = tensor([5.8281, 3.0586], dtype=torch.float16)\n",
      "step №834: loss = 29.439868927001953, weights = tensor([5.8281, 3.0605], dtype=torch.float16)\n",
      "step №835: loss = 29.43044090270996, weights = tensor([5.8281, 3.0625], dtype=torch.float16)\n",
      "step №836: loss = 29.4210205078125, weights = tensor([5.8281, 3.0645], dtype=torch.float16)\n",
      "step №837: loss = 29.411605834960938, weights = tensor([5.8281, 3.0664], dtype=torch.float16)\n",
      "step №838: loss = 29.402196884155273, weights = tensor([5.8281, 3.0684], dtype=torch.float16)\n",
      "step №839: loss = 29.39280128479004, weights = tensor([5.8281, 3.0703], dtype=torch.float16)\n",
      "step №840: loss = 29.383411407470703, weights = tensor([5.8281, 3.0723], dtype=torch.float16)\n",
      "step №841: loss = 29.374027252197266, weights = tensor([5.8281, 3.0742], dtype=torch.float16)\n",
      "step №842: loss = 29.364648818969727, weights = tensor([5.8281, 3.0762], dtype=torch.float16)\n",
      "step №843: loss = 29.355281829833984, weights = tensor([5.8281, 3.0781], dtype=torch.float16)\n",
      "step №844: loss = 29.345922470092773, weights = tensor([5.8281, 3.0801], dtype=torch.float16)\n",
      "step №845: loss = 29.33656883239746, weights = tensor([5.8281, 3.0820], dtype=torch.float16)\n",
      "step №846: loss = 29.327220916748047, weights = tensor([5.8242, 3.0840], dtype=torch.float16)\n",
      "step №847: loss = 29.316919326782227, weights = tensor([5.8242, 3.0859], dtype=torch.float16)\n",
      "step №848: loss = 29.307519912719727, weights = tensor([5.8242, 3.0879], dtype=torch.float16)\n",
      "step №849: loss = 29.29813003540039, weights = tensor([5.8242, 3.0898], dtype=torch.float16)\n",
      "step №850: loss = 29.288745880126953, weights = tensor([5.8242, 3.0918], dtype=torch.float16)\n",
      "step №851: loss = 29.279369354248047, weights = tensor([5.8242, 3.0938], dtype=torch.float16)\n",
      "step №852: loss = 29.270000457763672, weights = tensor([5.8242, 3.0957], dtype=torch.float16)\n",
      "step №853: loss = 29.26064109802246, weights = tensor([5.8242, 3.0977], dtype=torch.float16)\n",
      "step №854: loss = 29.25128746032715, weights = tensor([5.8242, 3.0996], dtype=torch.float16)\n",
      "step №855: loss = 29.241943359375, weights = tensor([5.8242, 3.1016], dtype=torch.float16)\n",
      "step №856: loss = 29.23260498046875, weights = tensor([5.8242, 3.1035], dtype=torch.float16)\n",
      "step №857: loss = 29.223276138305664, weights = tensor([5.8242, 3.1055], dtype=torch.float16)\n",
      "step №858: loss = 29.213953018188477, weights = tensor([5.8203, 3.1074], dtype=torch.float16)\n",
      "step №859: loss = 29.203716278076172, weights = tensor([5.8203, 3.1094], dtype=torch.float16)\n",
      "step №860: loss = 29.1943416595459, weights = tensor([5.8203, 3.1113], dtype=torch.float16)\n",
      "step №861: loss = 29.184972763061523, weights = tensor([5.8203, 3.1133], dtype=torch.float16)\n",
      "step №862: loss = 29.175609588623047, weights = tensor([5.8203, 3.1152], dtype=torch.float16)\n",
      "step №863: loss = 29.166259765625, weights = tensor([5.8203, 3.1172], dtype=torch.float16)\n",
      "step №864: loss = 29.15691566467285, weights = tensor([5.8203, 3.1191], dtype=torch.float16)\n",
      "step №865: loss = 29.1475772857666, weights = tensor([5.8203, 3.1211], dtype=torch.float16)\n",
      "step №866: loss = 29.13824462890625, weights = tensor([5.8203, 3.1230], dtype=torch.float16)\n",
      "step №867: loss = 29.128925323486328, weights = tensor([5.8203, 3.1250], dtype=torch.float16)\n",
      "step №868: loss = 29.119609832763672, weights = tensor([5.8203, 3.1270], dtype=torch.float16)\n",
      "step №869: loss = 29.110301971435547, weights = tensor([5.8203, 3.1289], dtype=torch.float16)\n",
      "step №870: loss = 29.101001739501953, weights = tensor([5.8203, 3.1309], dtype=torch.float16)\n",
      "step №871: loss = 29.091711044311523, weights = tensor([5.8164, 3.1328], dtype=torch.float16)\n",
      "step №872: loss = 29.08148193359375, weights = tensor([5.8164, 3.1348], dtype=torch.float16)\n",
      "step №873: loss = 29.0721378326416, weights = tensor([5.8164, 3.1367], dtype=torch.float16)\n",
      "step №874: loss = 29.06279945373535, weights = tensor([5.8164, 3.1387], dtype=torch.float16)\n",
      "step №875: loss = 29.053470611572266, weights = tensor([5.8164, 3.1406], dtype=torch.float16)\n",
      "step №876: loss = 29.044147491455078, weights = tensor([5.8164, 3.1426], dtype=torch.float16)\n",
      "step №877: loss = 29.034832000732422, weights = tensor([5.8164, 3.1445], dtype=torch.float16)\n",
      "step №878: loss = 29.025524139404297, weights = tensor([5.8164, 3.1465], dtype=torch.float16)\n",
      "step №879: loss = 29.016225814819336, weights = tensor([5.8164, 3.1484], dtype=torch.float16)\n",
      "step №880: loss = 29.006933212280273, weights = tensor([5.8164, 3.1504], dtype=torch.float16)\n",
      "step №881: loss = 28.997650146484375, weights = tensor([5.8164, 3.1523], dtype=torch.float16)\n",
      "step №882: loss = 28.988372802734375, weights = tensor([5.8164, 3.1543], dtype=torch.float16)\n",
      "step №883: loss = 28.97910499572754, weights = tensor([5.8164, 3.1562], dtype=torch.float16)\n",
      "step №884: loss = 28.9698429107666, weights = tensor([5.8125, 3.1582], dtype=torch.float16)\n",
      "step №885: loss = 28.95962142944336, weights = tensor([5.8125, 3.1602], dtype=torch.float16)\n",
      "step №886: loss = 28.950305938720703, weights = tensor([5.8125, 3.1621], dtype=torch.float16)\n",
      "step №887: loss = 28.94099998474121, weights = tensor([5.8125, 3.1641], dtype=torch.float16)\n",
      "step №888: loss = 28.93170166015625, weights = tensor([5.8125, 3.1660], dtype=torch.float16)\n",
      "step №889: loss = 28.922409057617188, weights = tensor([5.8125, 3.1680], dtype=torch.float16)\n",
      "step №890: loss = 28.913122177124023, weights = tensor([5.8125, 3.1699], dtype=torch.float16)\n",
      "step №891: loss = 28.90384864807129, weights = tensor([5.8125, 3.1719], dtype=torch.float16)\n",
      "step №892: loss = 28.894580841064453, weights = tensor([5.8125, 3.1738], dtype=torch.float16)\n",
      "step №893: loss = 28.885318756103516, weights = tensor([5.8125, 3.1758], dtype=torch.float16)\n",
      "step №894: loss = 28.876062393188477, weights = tensor([5.8125, 3.1777], dtype=torch.float16)\n",
      "step №895: loss = 28.866817474365234, weights = tensor([5.8125, 3.1797], dtype=torch.float16)\n",
      "step №896: loss = 28.857580184936523, weights = tensor([5.8086, 3.1816], dtype=torch.float16)\n",
      "step №897: loss = 28.847427368164062, weights = tensor([5.8086, 3.1836], dtype=torch.float16)\n",
      "step №898: loss = 28.838134765625, weights = tensor([5.8086, 3.1855], dtype=torch.float16)\n",
      "step №899: loss = 28.8288516998291, weights = tensor([5.8086, 3.1875], dtype=torch.float16)\n",
      "step №900: loss = 28.8195743560791, weights = tensor([5.8086, 3.1895], dtype=torch.float16)\n",
      "step №901: loss = 28.810306549072266, weights = tensor([5.8086, 3.1914], dtype=torch.float16)\n",
      "step №902: loss = 28.801044464111328, weights = tensor([5.8086, 3.1934], dtype=torch.float16)\n",
      "step №903: loss = 28.791790008544922, weights = tensor([5.8086, 3.1953], dtype=torch.float16)\n",
      "step №904: loss = 28.782543182373047, weights = tensor([5.8086, 3.1973], dtype=torch.float16)\n",
      "step №905: loss = 28.773305892944336, weights = tensor([5.8086, 3.1992], dtype=torch.float16)\n",
      "step №906: loss = 28.764074325561523, weights = tensor([5.8086, 3.2012], dtype=torch.float16)\n",
      "step №907: loss = 28.754852294921875, weights = tensor([5.8086, 3.2031], dtype=torch.float16)\n",
      "step №908: loss = 28.745635986328125, weights = tensor([5.8086, 3.2051], dtype=torch.float16)\n",
      "step №909: loss = 28.73642921447754, weights = tensor([5.8047, 3.2070], dtype=torch.float16)\n",
      "step №910: loss = 28.726282119750977, weights = tensor([5.8047, 3.2090], dtype=torch.float16)\n",
      "step №911: loss = 28.717021942138672, weights = tensor([5.8047, 3.2109], dtype=torch.float16)\n",
      "step №912: loss = 28.7077693939209, weights = tensor([5.8047, 3.2129], dtype=torch.float16)\n",
      "step №913: loss = 28.698522567749023, weights = tensor([5.8047, 3.2148], dtype=torch.float16)\n",
      "step №914: loss = 28.689281463623047, weights = tensor([5.8047, 3.2168], dtype=torch.float16)\n",
      "step №915: loss = 28.6800537109375, weights = tensor([5.8047, 3.2188], dtype=torch.float16)\n",
      "step №916: loss = 28.67083168029785, weights = tensor([5.8047, 3.2207], dtype=torch.float16)\n",
      "step №917: loss = 28.6616153717041, weights = tensor([5.8047, 3.2227], dtype=torch.float16)\n",
      "step №918: loss = 28.65240478515625, weights = tensor([5.8047, 3.2246], dtype=torch.float16)\n",
      "step №919: loss = 28.643207550048828, weights = tensor([5.8047, 3.2266], dtype=torch.float16)\n",
      "step №920: loss = 28.634014129638672, weights = tensor([5.8047, 3.2285], dtype=torch.float16)\n",
      "step №921: loss = 28.624828338623047, weights = tensor([5.8047, 3.2305], dtype=torch.float16)\n",
      "step №922: loss = 28.615650177001953, weights = tensor([5.8008, 3.2324], dtype=torch.float16)\n",
      "step №923: loss = 28.605514526367188, weights = tensor([5.8008, 3.2344], dtype=torch.float16)\n",
      "step №924: loss = 28.596282958984375, weights = tensor([5.8008, 3.2363], dtype=torch.float16)\n",
      "step №925: loss = 28.587060928344727, weights = tensor([5.8008, 3.2383], dtype=torch.float16)\n",
      "step №926: loss = 28.577844619750977, weights = tensor([5.8008, 3.2402], dtype=torch.float16)\n",
      "step №927: loss = 28.56863784790039, weights = tensor([5.8008, 3.2422], dtype=torch.float16)\n",
      "step №928: loss = 28.559436798095703, weights = tensor([5.8008, 3.2441], dtype=torch.float16)\n",
      "step №929: loss = 28.550243377685547, weights = tensor([5.8008, 3.2461], dtype=torch.float16)\n",
      "step №930: loss = 28.541057586669922, weights = tensor([5.8008, 3.2480], dtype=torch.float16)\n",
      "step №931: loss = 28.53188133239746, weights = tensor([5.8008, 3.2500], dtype=torch.float16)\n",
      "step №932: loss = 28.5227108001709, weights = tensor([5.8008, 3.2520], dtype=torch.float16)\n",
      "step №933: loss = 28.5135498046875, weights = tensor([5.8008, 3.2539], dtype=torch.float16)\n",
      "step №934: loss = 28.50439453125, weights = tensor([5.7969, 3.2559], dtype=torch.float16)\n",
      "step №935: loss = 28.494327545166016, weights = tensor([5.7969, 3.2578], dtype=torch.float16)\n",
      "step №936: loss = 28.485118865966797, weights = tensor([5.7969, 3.2598], dtype=torch.float16)\n",
      "step №937: loss = 28.47591781616211, weights = tensor([5.7969, 3.2617], dtype=torch.float16)\n",
      "step №938: loss = 28.466724395751953, weights = tensor([5.7969, 3.2637], dtype=torch.float16)\n",
      "step №939: loss = 28.45754051208496, weights = tensor([5.7969, 3.2656], dtype=torch.float16)\n",
      "step №940: loss = 28.4483642578125, weights = tensor([5.7969, 3.2676], dtype=torch.float16)\n",
      "step №941: loss = 28.439193725585938, weights = tensor([5.7969, 3.2695], dtype=torch.float16)\n",
      "step №942: loss = 28.430028915405273, weights = tensor([5.7969, 3.2715], dtype=torch.float16)\n",
      "step №943: loss = 28.42087745666504, weights = tensor([5.7969, 3.2734], dtype=torch.float16)\n",
      "step №944: loss = 28.411731719970703, weights = tensor([5.7969, 3.2754], dtype=torch.float16)\n",
      "step №945: loss = 28.402591705322266, weights = tensor([5.7969, 3.2773], dtype=torch.float16)\n",
      "step №946: loss = 28.393457412719727, weights = tensor([5.7969, 3.2793], dtype=torch.float16)\n",
      "step №947: loss = 28.384334564208984, weights = tensor([5.7930, 3.2812], dtype=torch.float16)\n",
      "step №948: loss = 28.3742733001709, weights = tensor([5.7930, 3.2832], dtype=torch.float16)\n",
      "step №949: loss = 28.365097045898438, weights = tensor([5.7930, 3.2852], dtype=torch.float16)\n",
      "step №950: loss = 28.355926513671875, weights = tensor([5.7930, 3.2871], dtype=torch.float16)\n",
      "step №951: loss = 28.346765518188477, weights = tensor([5.7930, 3.2891], dtype=torch.float16)\n",
      "step №952: loss = 28.337610244750977, weights = tensor([5.7930, 3.2910], dtype=torch.float16)\n",
      "step №953: loss = 28.32846450805664, weights = tensor([5.7930, 3.2930], dtype=torch.float16)\n",
      "step №954: loss = 28.319324493408203, weights = tensor([5.7930, 3.2949], dtype=torch.float16)\n",
      "step №955: loss = 28.310192108154297, weights = tensor([5.7930, 3.2969], dtype=torch.float16)\n",
      "step №956: loss = 28.301067352294922, weights = tensor([5.7930, 3.2988], dtype=torch.float16)\n",
      "step №957: loss = 28.29195213317871, weights = tensor([5.7930, 3.3008], dtype=torch.float16)\n",
      "step №958: loss = 28.2828426361084, weights = tensor([5.7930, 3.3027], dtype=torch.float16)\n",
      "step №959: loss = 28.27374267578125, weights = tensor([5.7930, 3.3047], dtype=torch.float16)\n",
      "step №960: loss = 28.2646484375, weights = tensor([5.7891, 3.3066], dtype=torch.float16)\n",
      "step №961: loss = 28.254596710205078, weights = tensor([5.7891, 3.3086], dtype=torch.float16)\n",
      "step №962: loss = 28.245447158813477, weights = tensor([5.7891, 3.3105], dtype=torch.float16)\n",
      "step №963: loss = 28.236309051513672, weights = tensor([5.7891, 3.3125], dtype=torch.float16)\n",
      "step №964: loss = 28.2271785736084, weights = tensor([5.7891, 3.3145], dtype=torch.float16)\n",
      "step №965: loss = 28.218053817749023, weights = tensor([5.7891, 3.3164], dtype=torch.float16)\n",
      "step №966: loss = 28.208934783935547, weights = tensor([5.7891, 3.3184], dtype=torch.float16)\n",
      "step №967: loss = 28.1998291015625, weights = tensor([5.7891, 3.3203], dtype=torch.float16)\n",
      "step №968: loss = 28.19072914123535, weights = tensor([5.7891, 3.3223], dtype=torch.float16)\n",
      "step №969: loss = 28.1816349029541, weights = tensor([5.7891, 3.3242], dtype=torch.float16)\n",
      "step №970: loss = 28.17254638671875, weights = tensor([5.7891, 3.3262], dtype=torch.float16)\n",
      "step №971: loss = 28.163471221923828, weights = tensor([5.7891, 3.3281], dtype=torch.float16)\n",
      "step №972: loss = 28.154399871826172, weights = tensor([5.7852, 3.3301], dtype=torch.float16)\n",
      "step №973: loss = 28.1444149017334, weights = tensor([5.7852, 3.3320], dtype=torch.float16)\n",
      "step №974: loss = 28.135290145874023, weights = tensor([5.7852, 3.3340], dtype=torch.float16)\n",
      "step №975: loss = 28.126174926757812, weights = tensor([5.7852, 3.3359], dtype=torch.float16)\n",
      "step №976: loss = 28.1170654296875, weights = tensor([5.7852, 3.3379], dtype=torch.float16)\n",
      "step №977: loss = 28.10796546936035, weights = tensor([5.7852, 3.3398], dtype=torch.float16)\n",
      "step №978: loss = 28.0988712310791, weights = tensor([5.7852, 3.3418], dtype=torch.float16)\n",
      "step №979: loss = 28.089786529541016, weights = tensor([5.7852, 3.3438], dtype=torch.float16)\n",
      "step №980: loss = 28.080707550048828, weights = tensor([5.7852, 3.3457], dtype=torch.float16)\n",
      "step №981: loss = 28.071636199951172, weights = tensor([5.7852, 3.3477], dtype=torch.float16)\n",
      "step №982: loss = 28.062572479248047, weights = tensor([5.7852, 3.3496], dtype=torch.float16)\n",
      "step №983: loss = 28.053518295288086, weights = tensor([5.7852, 3.3516], dtype=torch.float16)\n",
      "step №984: loss = 28.044469833374023, weights = tensor([5.7852, 3.3535], dtype=torch.float16)\n",
      "step №985: loss = 28.035430908203125, weights = tensor([5.7812, 3.3555], dtype=torch.float16)\n",
      "step №986: loss = 28.02545166015625, weights = tensor([5.7812, 3.3574], dtype=torch.float16)\n",
      "step №987: loss = 28.016361236572266, weights = tensor([5.7812, 3.3594], dtype=torch.float16)\n",
      "step №988: loss = 28.007274627685547, weights = tensor([5.7812, 3.3613], dtype=torch.float16)\n",
      "step №989: loss = 27.99819564819336, weights = tensor([5.7812, 3.3633], dtype=torch.float16)\n",
      "step №990: loss = 27.989124298095703, weights = tensor([5.7812, 3.3652], dtype=torch.float16)\n",
      "step №991: loss = 27.98006248474121, weights = tensor([5.7812, 3.3672], dtype=torch.float16)\n",
      "step №992: loss = 27.97100830078125, weights = tensor([5.7812, 3.3691], dtype=torch.float16)\n",
      "step №993: loss = 27.961959838867188, weights = tensor([5.7812, 3.3711], dtype=torch.float16)\n",
      "step №994: loss = 27.952917098999023, weights = tensor([5.7812, 3.3730], dtype=torch.float16)\n",
      "step №995: loss = 27.94388771057129, weights = tensor([5.7812, 3.3750], dtype=torch.float16)\n",
      "step №996: loss = 27.934864044189453, weights = tensor([5.7812, 3.3770], dtype=torch.float16)\n",
      "step №997: loss = 27.925846099853516, weights = tensor([5.7812, 3.3789], dtype=torch.float16)\n",
      "step №998: loss = 27.916833877563477, weights = tensor([5.7773, 3.3809], dtype=torch.float16)\n",
      "step №999: loss = 27.9068660736084, weights = tensor([5.7773, 3.3828], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#first let's train our model with a stopping criteria as numbers of steps \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "793f260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative. Slope = 5.77734375, intercept = 3.3828125, loss = 27.9068660736084')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1693c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 278.4984436035156\n",
      "did one more step, loss reduced by 120.25773620605469\n",
      "did one more step, loss reduced by 52.01158905029297\n",
      "did one more step, loss reduced by 22.412334442138672\n",
      "did one more step, loss reduced by 9.601085662841797\n",
      "did one more step, loss reduced by 4.173526763916016\n",
      "did one more step, loss reduced by 1.8298072814941406\n",
      "did one more step, loss reduced by 0.8069534301757812\n",
      "did one more step, loss reduced by 0.37647247314453125\n",
      "did one more step, loss reduced by 0.16679763793945312\n",
      "did one more step, loss reduced by 0.085235595703125\n",
      "did one more step, loss reduced by 0.05072021484375\n",
      "did one more step, loss reduced by 0.037418365478515625\n",
      "did one more step, loss reduced by 0.033336639404296875\n",
      "did one more step, loss reduced by 0.028400421142578125\n",
      "did one more step, loss reduced by 0.026409149169921875\n",
      "did one more step, loss reduced by 0.025524139404296875\n",
      "did one more step, loss reduced by 0.025485992431640625\n",
      "did one more step, loss reduced by 0.025455474853515625\n",
      "did one more step, loss reduced by 0.02542877197265625\n",
      "did one more step, loss reduced by 0.02539825439453125\n",
      "did one more step, loss reduced by 0.02536773681640625\n",
      "did one more step, loss reduced by 0.02532958984375\n",
      "did one more step, loss reduced by 0.02530670166015625\n",
      "did one more step, loss reduced by 0.02527618408203125\n",
      "did one more step, loss reduced by 0.02524566650390625\n",
      "did one more step, loss reduced by 0.025211334228515625\n",
      "did one more step, loss reduced by 0.025180816650390625\n",
      "did one more step, loss reduced by 0.026214599609375\n",
      "did one more step, loss reduced by 0.025257110595703125\n",
      "did one more step, loss reduced by 0.025234222412109375\n",
      "did one more step, loss reduced by 0.025196075439453125\n",
      "did one more step, loss reduced by 0.025165557861328125\n",
      "did one more step, loss reduced by 0.02513885498046875\n",
      "did one more step, loss reduced by 0.026119232177734375\n",
      "did one more step, loss reduced by 0.025211334228515625\n",
      "did one more step, loss reduced by 0.025188446044921875\n",
      "did one more step, loss reduced by 0.025150299072265625\n",
      "did one more step, loss reduced by 0.025119781494140625\n",
      "did one more step, loss reduced by 0.02509307861328125\n",
      "did one more step, loss reduced by 0.02506256103515625\n",
      "did one more step, loss reduced by 0.026134490966796875\n",
      "did one more step, loss reduced by 0.02513885498046875\n",
      "did one more step, loss reduced by 0.02510833740234375\n",
      "did one more step, loss reduced by 0.0250701904296875\n",
      "did one more step, loss reduced by 0.02504730224609375\n",
      "did one more step, loss reduced by 0.02501678466796875\n",
      "did one more step, loss reduced by 0.026042938232421875\n",
      "did one more step, loss reduced by 0.02509307861328125\n",
      "did one more step, loss reduced by 0.02506256103515625\n",
      "did one more step, loss reduced by 0.025028228759765625\n",
      "did one more step, loss reduced by 0.024997711181640625\n",
      "did one more step, loss reduced by 0.024974822998046875\n",
      "did one more step, loss reduced by 0.02594757080078125\n",
      "did one more step, loss reduced by 0.02504730224609375\n",
      "did one more step, loss reduced by 0.02501678466796875\n",
      "did one more step, loss reduced by 0.024982452392578125\n",
      "did one more step, loss reduced by 0.024951934814453125\n",
      "did one more step, loss reduced by 0.024929046630859375\n",
      "did one more step, loss reduced by 0.024890899658203125\n",
      "did one more step, loss reduced by 0.025970458984375\n",
      "did one more step, loss reduced by 0.024967193603515625\n",
      "did one more step, loss reduced by 0.024936676025390625\n",
      "did one more step, loss reduced by 0.02490997314453125\n",
      "did one more step, loss reduced by 0.02487945556640625\n",
      "did one more step, loss reduced by 0.02484893798828125\n",
      "did one more step, loss reduced by 0.025875091552734375\n",
      "did one more step, loss reduced by 0.024921417236328125\n",
      "did one more step, loss reduced by 0.024890899658203125\n",
      "did one more step, loss reduced by 0.02486419677734375\n",
      "did one more step, loss reduced by 0.02483367919921875\n",
      "did one more step, loss reduced by 0.02480316162109375\n",
      "did one more step, loss reduced by 0.025783538818359375\n",
      "did one more step, loss reduced by 0.02487945556640625\n",
      "did one more step, loss reduced by 0.02484130859375\n",
      "did one more step, loss reduced by 0.02481842041015625\n",
      "did one more step, loss reduced by 0.02478790283203125\n",
      "did one more step, loss reduced by 0.02475738525390625\n",
      "did one more step, loss reduced by 0.024723052978515625\n",
      "did one more step, loss reduced by 0.0258026123046875\n",
      "did one more step, loss reduced by 0.024799346923828125\n",
      "did one more step, loss reduced by 0.024768829345703125\n",
      "did one more step, loss reduced by 0.024745941162109375\n",
      "did one more step, loss reduced by 0.024707794189453125\n",
      "did one more step, loss reduced by 0.024677276611328125\n",
      "did one more step, loss reduced by 0.0257110595703125\n",
      "did one more step, loss reduced by 0.024753570556640625\n",
      "did one more step, loss reduced by 0.024723052978515625\n",
      "did one more step, loss reduced by 0.024700164794921875\n",
      "did one more step, loss reduced by 0.024662017822265625\n",
      "did one more step, loss reduced by 0.024631500244140625\n",
      "did one more step, loss reduced by 0.02407073974609375\n",
      "did one more step, loss reduced by 0.024707794189453125\n",
      "did one more step, loss reduced by 0.024684906005859375\n",
      "did one more step, loss reduced by 0.024654388427734375\n",
      "did one more step, loss reduced by 0.02307891845703125\n",
      "did one more step, loss reduced by 0.0230560302734375\n",
      "did one more step, loss reduced by 0.023029327392578125\n",
      "did one more step, loss reduced by 0.0240631103515625\n",
      "did one more step, loss reduced by 0.024646759033203125\n",
      "did one more step, loss reduced by 0.0230712890625\n",
      "did one more step, loss reduced by 0.024585723876953125\n",
      "did one more step, loss reduced by 0.0245513916015625\n",
      "did one more step, loss reduced by 0.024524688720703125\n",
      "did one more step, loss reduced by 0.025501251220703125\n",
      "did one more step, loss reduced by 0.02459716796875\n",
      "did one more step, loss reduced by 0.024570465087890625\n",
      "did one more step, loss reduced by 0.02454376220703125\n",
      "did one more step, loss reduced by 0.024501800537109375\n",
      "did one more step, loss reduced by 0.024478912353515625\n",
      "did one more step, loss reduced by 0.024448394775390625\n",
      "did one more step, loss reduced by 0.0255126953125\n",
      "did one more step, loss reduced by 0.024524688720703125\n",
      "did one more step, loss reduced by 0.024494171142578125\n",
      "did one more step, loss reduced by 0.0244598388671875\n",
      "did one more step, loss reduced by 0.0244293212890625\n",
      "did one more step, loss reduced by 0.024402618408203125\n",
      "did one more step, loss reduced by 0.025424957275390625\n",
      "did one more step, loss reduced by 0.024478912353515625\n",
      "did one more step, loss reduced by 0.024444580078125\n",
      "did one more step, loss reduced by 0.0244140625\n",
      "did one more step, loss reduced by 0.02439117431640625\n",
      "did one more step, loss reduced by 0.024349212646484375\n",
      "did one more step, loss reduced by 0.025333404541015625\n",
      "did one more step, loss reduced by 0.024440765380859375\n",
      "did one more step, loss reduced by 0.024394989013671875\n",
      "did one more step, loss reduced by 0.02436065673828125\n",
      "did one more step, loss reduced by 0.02435302734375\n",
      "did one more step, loss reduced by 0.024303436279296875\n",
      "did one more step, loss reduced by 0.024280548095703125\n",
      "did one more step, loss reduced by 0.025348663330078125\n",
      "did one more step, loss reduced by 0.02435302734375\n",
      "did one more step, loss reduced by 0.02433013916015625\n",
      "did one more step, loss reduced by 0.024288177490234375\n",
      "did one more step, loss reduced by 0.024265289306640625\n",
      "did one more step, loss reduced by 0.024234771728515625\n",
      "did one more step, loss reduced by 0.0252532958984375\n",
      "did one more step, loss reduced by 0.024311065673828125\n",
      "did one more step, loss reduced by 0.024280548095703125\n",
      "did one more step, loss reduced by 0.024250030517578125\n",
      "did one more step, loss reduced by 0.0242156982421875\n",
      "did one more step, loss reduced by 0.024188995361328125\n",
      "did one more step, loss reduced by 0.0251617431640625\n",
      "did one more step, loss reduced by 0.024261474609375\n",
      "did one more step, loss reduced by 0.02423858642578125\n",
      "did one more step, loss reduced by 0.024204254150390625\n",
      "did one more step, loss reduced by 0.024166107177734375\n",
      "did one more step, loss reduced by 0.02414703369140625\n",
      "did one more step, loss reduced by 0.02410888671875\n",
      "did one more step, loss reduced by 0.025177001953125\n",
      "did one more step, loss reduced by 0.0241851806640625\n",
      "did one more step, loss reduced by 0.02416229248046875\n",
      "did one more step, loss reduced by 0.02413177490234375\n",
      "did one more step, loss reduced by 0.0240936279296875\n",
      "did one more step, loss reduced by 0.024059295654296875\n",
      "did one more step, loss reduced by 0.02509307861328125\n",
      "did one more step, loss reduced by 0.024135589599609375\n",
      "did one more step, loss reduced by 0.024112701416015625\n",
      "did one more step, loss reduced by 0.02408599853515625\n",
      "did one more step, loss reduced by 0.0240478515625\n",
      "did one more step, loss reduced by 0.024021148681640625\n",
      "did one more step, loss reduced by 0.024997711181640625\n",
      "did one more step, loss reduced by 0.024089813232421875\n",
      "did one more step, loss reduced by 0.024066925048828125\n",
      "did one more step, loss reduced by 0.0240325927734375\n",
      "did one more step, loss reduced by 0.024005889892578125\n",
      "did one more step, loss reduced by 0.02397918701171875\n",
      "did one more step, loss reduced by 0.0239410400390625\n",
      "did one more step, loss reduced by 0.0250091552734375\n",
      "did one more step, loss reduced by 0.024021148681640625\n",
      "did one more step, loss reduced by 0.023983001708984375\n",
      "did one more step, loss reduced by 0.02396392822265625\n",
      "did one more step, loss reduced by 0.02392578125\n",
      "did one more step, loss reduced by 0.02390289306640625\n",
      "did one more step, loss reduced by 0.02490997314453125\n",
      "did one more step, loss reduced by 0.02398681640625\n",
      "did one more step, loss reduced by 0.02393341064453125\n",
      "did one more step, loss reduced by 0.0239105224609375\n",
      "did one more step, loss reduced by 0.023891448974609375\n",
      "did one more step, loss reduced by 0.023845672607421875\n",
      "did one more step, loss reduced by 0.02184295654296875\n",
      "did one more step, loss reduced by 0.023929595947265625\n",
      "did one more step, loss reduced by 0.02390289306640625\n",
      "did one more step, loss reduced by 0.023868560791015625\n",
      "did one more step, loss reduced by 0.02384185791015625\n",
      "did one more step, loss reduced by 0.02083587646484375\n",
      "did one more step, loss reduced by 0.020809173583984375\n",
      "did one more step, loss reduced by 0.021820068359375\n",
      "did one more step, loss reduced by 0.023868560791015625\n",
      "did one more step, loss reduced by 0.023834228515625\n",
      "did one more step, loss reduced by 0.02083587646484375\n",
      "did one more step, loss reduced by 0.02080535888671875\n",
      "did one more step, loss reduced by 0.020782470703125\n",
      "did one more step, loss reduced by 0.0207672119140625\n",
      "did one more step, loss reduced by 0.0217742919921875\n",
      "did one more step, loss reduced by 0.02083587646484375\n",
      "did one more step, loss reduced by 0.02081298828125\n",
      "did one more step, loss reduced by 0.020782470703125\n",
      "did one more step, loss reduced by 0.0207672119140625\n",
      "did one more step, loss reduced by 0.020740509033203125\n",
      "did one more step, loss reduced by 0.020717620849609375\n",
      "did one more step, loss reduced by 0.02170562744140625\n",
      "did one more step, loss reduced by 0.0207977294921875\n",
      "did one more step, loss reduced by 0.020763397216796875\n",
      "did one more step, loss reduced by 0.020748138427734375\n",
      "did one more step, loss reduced by 0.020717620849609375\n",
      "did one more step, loss reduced by 0.02069854736328125\n",
      "did one more step, loss reduced by 0.020671844482421875\n",
      "did one more step, loss reduced by 0.02162933349609375\n",
      "did one more step, loss reduced by 0.020751953125\n",
      "did one more step, loss reduced by 0.020725250244140625\n",
      "did one more step, loss reduced by 0.02069854736328125\n",
      "did one more step, loss reduced by 0.020679473876953125\n",
      "did one more step, loss reduced by 0.020656585693359375\n",
      "did one more step, loss reduced by 0.020633697509765625\n",
      "did one more step, loss reduced by 0.02060699462890625\n",
      "did one more step, loss reduced by 0.021656036376953125\n",
      "did one more step, loss reduced by 0.020679473876953125\n",
      "did one more step, loss reduced by 0.02066802978515625\n",
      "did one more step, loss reduced by 0.020626068115234375\n",
      "did one more step, loss reduced by 0.020610809326171875\n",
      "did one more step, loss reduced by 0.02059173583984375\n",
      "did one more step, loss reduced by 0.020565032958984375\n",
      "did one more step, loss reduced by 0.021575927734375\n",
      "did one more step, loss reduced by 0.020641326904296875\n",
      "did one more step, loss reduced by 0.0206146240234375\n",
      "did one more step, loss reduced by 0.020595550537109375\n",
      "did one more step, loss reduced by 0.02056884765625\n",
      "did one more step, loss reduced by 0.020542144775390625\n",
      "did one more step, loss reduced by 0.0205230712890625\n",
      "did one more step, loss reduced by 0.021514892578125\n",
      "did one more step, loss reduced by 0.020587921142578125\n",
      "did one more step, loss reduced by 0.02056884765625\n",
      "did one more step, loss reduced by 0.020549774169921875\n",
      "did one more step, loss reduced by 0.020538330078125\n",
      "did one more step, loss reduced by 0.0204925537109375\n",
      "did one more step, loss reduced by 0.020481109619140625\n",
      "did one more step, loss reduced by 0.0243682861328125\n",
      "did one more step, loss reduced by 0.0234832763671875\n",
      "did one more step, loss reduced by 0.0234527587890625\n",
      "did one more step, loss reduced by 0.023418426513671875\n",
      "did one more step, loss reduced by 0.0233917236328125\n",
      "did one more step, loss reduced by 0.0233612060546875\n",
      "did one more step, loss reduced by 0.0233306884765625\n",
      "did one more step, loss reduced by 0.02439117431640625\n",
      "did one more step, loss reduced by 0.023403167724609375\n",
      "did one more step, loss reduced by 0.02337646484375\n",
      "did one more step, loss reduced by 0.023342132568359375\n",
      "did one more step, loss reduced by 0.0233154296875\n",
      "did one more step, loss reduced by 0.023284912109375\n",
      "did one more step, loss reduced by 0.02429962158203125\n",
      "did one more step, loss reduced by 0.023357391357421875\n",
      "did one more step, loss reduced by 0.0233306884765625\n",
      "did one more step, loss reduced by 0.0233001708984375\n",
      "did one more step, loss reduced by 0.023265838623046875\n",
      "did one more step, loss reduced by 0.0232391357421875\n",
      "did one more step, loss reduced by 0.0242156982421875\n",
      "did one more step, loss reduced by 0.023303985595703125\n",
      "did one more step, loss reduced by 0.023281097412109375\n",
      "did one more step, loss reduced by 0.023258209228515625\n",
      "did one more step, loss reduced by 0.023227691650390625\n",
      "did one more step, loss reduced by 0.023181915283203125\n",
      "did one more step, loss reduced by 0.023162841796875\n",
      "did one more step, loss reduced by 0.024227142333984375\n",
      "did one more step, loss reduced by 0.0232391357421875\n",
      "did one more step, loss reduced by 0.0232086181640625\n",
      "did one more step, loss reduced by 0.02317047119140625\n",
      "did one more step, loss reduced by 0.0231475830078125\n",
      "did one more step, loss reduced by 0.0231170654296875\n",
      "did one more step, loss reduced by 0.024127960205078125\n",
      "did one more step, loss reduced by 0.023197174072265625\n",
      "did one more step, loss reduced by 0.023159027099609375\n",
      "did one more step, loss reduced by 0.02313232421875\n",
      "did one more step, loss reduced by 0.023097991943359375\n",
      "did one more step, loss reduced by 0.0230712890625\n",
      "did one more step, loss reduced by 0.02404022216796875\n",
      "did one more step, loss reduced by 0.0231475830078125\n",
      "did one more step, loss reduced by 0.023113250732421875\n",
      "did one more step, loss reduced by 0.023082733154296875\n",
      "did one more step, loss reduced by 0.0230560302734375\n",
      "did one more step, loss reduced by 0.0230255126953125\n",
      "did one more step, loss reduced by 0.0229949951171875\n",
      "did one more step, loss reduced by 0.024051666259765625\n",
      "did one more step, loss reduced by 0.023075103759765625\n",
      "did one more step, loss reduced by 0.023044586181640625\n",
      "did one more step, loss reduced by 0.022998809814453125\n",
      "did one more step, loss reduced by 0.022979736328125\n",
      "did one more step, loss reduced by 0.02294921875\n",
      "did one more step, loss reduced by 0.02396392822265625\n",
      "did one more step, loss reduced by 0.0230255126953125\n",
      "did one more step, loss reduced by 0.022991180419921875\n",
      "did one more step, loss reduced by 0.0229644775390625\n",
      "did one more step, loss reduced by 0.022930145263671875\n",
      "did one more step, loss reduced by 0.022907257080078125\n",
      "did one more step, loss reduced by 0.02387237548828125\n",
      "did one more step, loss reduced by 0.02297210693359375\n",
      "did one more step, loss reduced by 0.022953033447265625\n",
      "did one more step, loss reduced by 0.022918701171875\n",
      "did one more step, loss reduced by 0.022884368896484375\n",
      "did one more step, loss reduced by 0.022853851318359375\n",
      "did one more step, loss reduced by 0.0228271484375\n",
      "did one more step, loss reduced by 0.023891448974609375\n",
      "did one more step, loss reduced by 0.022899627685546875\n",
      "did one more step, loss reduced by 0.022869110107421875\n",
      "did one more step, loss reduced by 0.0228424072265625\n",
      "did one more step, loss reduced by 0.022808074951171875\n",
      "did one more step, loss reduced by 0.0227813720703125\n",
      "did one more step, loss reduced by 0.023799896240234375\n",
      "did one more step, loss reduced by 0.022861480712890625\n",
      "did one more step, loss reduced by 0.0228118896484375\n",
      "did one more step, loss reduced by 0.022796630859375\n",
      "did one more step, loss reduced by 0.022769927978515625\n",
      "did one more step, loss reduced by 0.022739410400390625\n",
      "did one more step, loss reduced by 0.02370452880859375\n",
      "did one more step, loss reduced by 0.02280426025390625\n",
      "did one more step, loss reduced by 0.0227813720703125\n",
      "did one more step, loss reduced by 0.0227508544921875\n",
      "did one more step, loss reduced by 0.022716522216796875\n",
      "did one more step, loss reduced by 0.022686004638671875\n",
      "did one more step, loss reduced by 0.0226593017578125\n",
      "did one more step, loss reduced by 0.023723602294921875\n",
      "did one more step, loss reduced by 0.022731781005859375\n",
      "did one more step, loss reduced by 0.022705078125\n",
      "did one more step, loss reduced by 0.022670745849609375\n",
      "did one more step, loss reduced by 0.02264404296875\n",
      "did one more step, loss reduced by 0.022609710693359375\n",
      "did one more step, loss reduced by 0.02362823486328125\n",
      "did one more step, loss reduced by 0.0226898193359375\n",
      "did one more step, loss reduced by 0.0226593017578125\n",
      "did one more step, loss reduced by 0.0226287841796875\n",
      "did one more step, loss reduced by 0.02259063720703125\n",
      "did one more step, loss reduced by 0.0225677490234375\n",
      "did one more step, loss reduced by 0.0235443115234375\n",
      "did one more step, loss reduced by 0.02263641357421875\n",
      "did one more step, loss reduced by 0.022609710693359375\n",
      "did one more step, loss reduced by 0.022586822509765625\n",
      "did one more step, loss reduced by 0.022556304931640625\n",
      "did one more step, loss reduced by 0.022510528564453125\n",
      "did one more step, loss reduced by 0.022491455078125\n",
      "did one more step, loss reduced by 0.023555755615234375\n",
      "did one more step, loss reduced by 0.022563934326171875\n",
      "did one more step, loss reduced by 0.022541046142578125\n",
      "did one more step, loss reduced by 0.02249908447265625\n",
      "did one more step, loss reduced by 0.0224761962890625\n",
      "did one more step, loss reduced by 0.022441864013671875\n",
      "did one more step, loss reduced by 0.023468017578125\n",
      "did one more step, loss reduced by 0.022518157958984375\n",
      "did one more step, loss reduced by 0.02248382568359375\n",
      "did one more step, loss reduced by 0.022464752197265625\n",
      "did one more step, loss reduced by 0.022426605224609375\n",
      "did one more step, loss reduced by 0.02239990234375\n",
      "did one more step, loss reduced by 0.02336883544921875\n",
      "did one more step, loss reduced by 0.0224761962890625\n",
      "did one more step, loss reduced by 0.022441864013671875\n",
      "did one more step, loss reduced by 0.0224151611328125\n",
      "did one more step, loss reduced by 0.022380828857421875\n",
      "did one more step, loss reduced by 0.0223541259765625\n",
      "did one more step, loss reduced by 0.0223236083984375\n",
      "did one more step, loss reduced by 0.023380279541015625\n",
      "did one more step, loss reduced by 0.022403717041015625\n",
      "did one more step, loss reduced by 0.022373199462890625\n",
      "did one more step, loss reduced by 0.022327423095703125\n",
      "did one more step, loss reduced by 0.022304534912109375\n",
      "did one more step, loss reduced by 0.022281646728515625\n",
      "did one more step, loss reduced by 0.023296356201171875\n",
      "did one more step, loss reduced by 0.022350311279296875\n",
      "did one more step, loss reduced by 0.022319793701171875\n",
      "did one more step, loss reduced by 0.0222930908203125\n",
      "did one more step, loss reduced by 0.0222625732421875\n",
      "did one more step, loss reduced by 0.016674041748046875\n",
      "did one more step, loss reduced by 0.016658782958984375\n",
      "did one more step, loss reduced by 0.017681121826171875\n",
      "did one more step, loss reduced by 0.022296905517578125\n",
      "did one more step, loss reduced by 0.02227020263671875\n",
      "did one more step, loss reduced by 0.022235870361328125\n",
      "did one more step, loss reduced by 0.016658782958984375\n",
      "did one more step, loss reduced by 0.01663970947265625\n",
      "did one more step, loss reduced by 0.016620635986328125\n",
      "did one more step, loss reduced by 0.017604827880859375\n",
      "did one more step, loss reduced by 0.022251129150390625\n",
      "did one more step, loss reduced by 0.016666412353515625\n",
      "did one more step, loss reduced by 0.01665496826171875\n",
      "did one more step, loss reduced by 0.016635894775390625\n",
      "did one more step, loss reduced by 0.016620635986328125\n",
      "did one more step, loss reduced by 0.016597747802734375\n",
      "did one more step, loss reduced by 0.0165863037109375\n",
      "did one more step, loss reduced by 0.017551422119140625\n",
      "did one more step, loss reduced by 0.0166473388671875\n",
      "did one more step, loss reduced by 0.01663970947265625\n",
      "did one more step, loss reduced by 0.0166168212890625\n",
      "did one more step, loss reduced by 0.016597747802734375\n",
      "did one more step, loss reduced by 0.016590118408203125\n",
      "did one more step, loss reduced by 0.01656341552734375\n",
      "did one more step, loss reduced by 0.01654815673828125\n",
      "did one more step, loss reduced by 0.016529083251953125\n",
      "did one more step, loss reduced by 0.017559051513671875\n",
      "did one more step, loss reduced by 0.016597747802734375\n",
      "did one more step, loss reduced by 0.0165863037109375\n",
      "did one more step, loss reduced by 0.01656341552734375\n",
      "did one more step, loss reduced by 0.016551971435546875\n",
      "did one more step, loss reduced by 0.01653289794921875\n",
      "did one more step, loss reduced by 0.01651763916015625\n",
      "did one more step, loss reduced by 0.0164947509765625\n",
      "did one more step, loss reduced by 0.0174713134765625\n",
      "did one more step, loss reduced by 0.016571044921875\n",
      "did one more step, loss reduced by 0.01654815673828125\n",
      "did one more step, loss reduced by 0.01653289794921875\n",
      "did one more step, loss reduced by 0.01651763916015625\n",
      "did one more step, loss reduced by 0.016490936279296875\n",
      "did one more step, loss reduced by 0.01648712158203125\n",
      "did one more step, loss reduced by 0.0164642333984375\n",
      "did one more step, loss reduced by 0.017391204833984375\n",
      "did one more step, loss reduced by 0.01653289794921875\n",
      "did one more step, loss reduced by 0.016513824462890625\n",
      "did one more step, loss reduced by 0.016498565673828125\n",
      "did one more step, loss reduced by 0.016483306884765625\n",
      "did one more step, loss reduced by 0.0164642333984375\n",
      "did one more step, loss reduced by 0.016445159912109375\n",
      "did one more step, loss reduced by 0.016429901123046875\n",
      "did one more step, loss reduced by 0.016414642333984375\n",
      "did one more step, loss reduced by 0.017398834228515625\n",
      "did one more step, loss reduced by 0.016475677490234375\n",
      "did one more step, loss reduced by 0.016460418701171875\n",
      "did one more step, loss reduced by 0.016452789306640625\n",
      "did one more step, loss reduced by 0.016429901123046875\n",
      "did one more step, loss reduced by 0.01641082763671875\n",
      "did one more step, loss reduced by 0.016391754150390625\n",
      "did one more step, loss reduced by 0.01638031005859375\n",
      "did one more step, loss reduced by 0.0173187255859375\n",
      "did one more step, loss reduced by 0.016448974609375\n",
      "did one more step, loss reduced by 0.01642608642578125\n",
      "did one more step, loss reduced by 0.016414642333984375\n",
      "did one more step, loss reduced by 0.016399383544921875\n",
      "did one more step, loss reduced by 0.0163726806640625\n",
      "did one more step, loss reduced by 0.016361236572265625\n",
      "did one more step, loss reduced by 0.016345977783203125\n",
      "did one more step, loss reduced by 0.016326904296875\n",
      "did one more step, loss reduced by 0.017322540283203125\n",
      "did one more step, loss reduced by 0.01639556884765625\n",
      "did one more step, loss reduced by 0.01638031005859375\n",
      "did one more step, loss reduced by 0.016357421875\n",
      "did one more step, loss reduced by 0.016345977783203125\n",
      "did one more step, loss reduced by 0.016326904296875\n",
      "did one more step, loss reduced by 0.016307830810546875\n",
      "did one more step, loss reduced by 0.016292572021484375\n",
      "did one more step, loss reduced by 0.017242431640625\n",
      "did one more step, loss reduced by 0.016361236572265625\n",
      "did one more step, loss reduced by 0.016345977783203125\n",
      "did one more step, loss reduced by 0.016323089599609375\n",
      "did one more step, loss reduced by 0.0163116455078125\n",
      "did one more step, loss reduced by 0.01628875732421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.016277313232421875\n",
      "did one more step, loss reduced by 0.01625823974609375\n",
      "did one more step, loss reduced by 0.01624298095703125\n",
      "did one more step, loss reduced by 0.017242431640625\n",
      "did one more step, loss reduced by 0.016315460205078125\n",
      "did one more step, loss reduced by 0.016292572021484375\n",
      "did one more step, loss reduced by 0.01627349853515625\n",
      "did one more step, loss reduced by 0.016254425048828125\n",
      "did one more step, loss reduced by 0.01624298095703125\n",
      "did one more step, loss reduced by 0.0162200927734375\n",
      "did one more step, loss reduced by 0.01621246337890625\n",
      "did one more step, loss reduced by 0.0171661376953125\n",
      "did one more step, loss reduced by 0.016277313232421875\n",
      "did one more step, loss reduced by 0.01625823974609375\n",
      "did one more step, loss reduced by 0.016239166259765625\n",
      "did one more step, loss reduced by 0.016223907470703125\n",
      "did one more step, loss reduced by 0.016208648681640625\n",
      "did one more step, loss reduced by 0.016185760498046875\n",
      "did one more step, loss reduced by 0.01617431640625\n",
      "did one more step, loss reduced by 0.016155242919921875\n",
      "did one more step, loss reduced by 0.017177581787109375\n",
      "did one more step, loss reduced by 0.016216278076171875\n",
      "did one more step, loss reduced by 0.016208648681640625\n",
      "did one more step, loss reduced by 0.0161895751953125\n",
      "did one more step, loss reduced by 0.01617431640625\n",
      "did one more step, loss reduced by 0.016155242919921875\n",
      "did one more step, loss reduced by 0.01613616943359375\n",
      "did one more step, loss reduced by 0.01612091064453125\n",
      "did one more step, loss reduced by 0.017093658447265625\n",
      "did one more step, loss reduced by 0.0161895751953125\n",
      "did one more step, loss reduced by 0.01617431640625\n",
      "did one more step, loss reduced by 0.016155242919921875\n",
      "did one more step, loss reduced by 0.01613616943359375\n",
      "did one more step, loss reduced by 0.01612091064453125\n",
      "did one more step, loss reduced by 0.016101837158203125\n",
      "did one more step, loss reduced by 0.016086578369140625\n",
      "did one more step, loss reduced by 0.017017364501953125\n",
      "did one more step, loss reduced by 0.01615142822265625\n",
      "did one more step, loss reduced by 0.016139984130859375\n",
      "did one more step, loss reduced by 0.016117095947265625\n",
      "did one more step, loss reduced by 0.01610565185546875\n",
      "did one more step, loss reduced by 0.016082763671875\n",
      "did one more step, loss reduced by 0.01607513427734375\n",
      "did one more step, loss reduced by 0.01605224609375\n",
      "did one more step, loss reduced by 0.0160369873046875\n",
      "did one more step, loss reduced by 0.017017364501953125\n",
      "did one more step, loss reduced by 0.016101837158203125\n",
      "did one more step, loss reduced by 0.016086578369140625\n",
      "did one more step, loss reduced by 0.016071319580078125\n",
      "did one more step, loss reduced by 0.01605224609375\n",
      "did one more step, loss reduced by 0.016033172607421875\n",
      "did one more step, loss reduced by 0.01601409912109375\n",
      "did one more step, loss reduced by 0.016002655029296875\n",
      "did one more step, loss reduced by 0.016937255859375\n",
      "did one more step, loss reduced by 0.016071319580078125\n",
      "did one more step, loss reduced by 0.01605224609375\n",
      "did one more step, loss reduced by 0.016033172607421875\n",
      "did one more step, loss reduced by 0.016021728515625\n",
      "did one more step, loss reduced by 0.01599884033203125\n",
      "did one more step, loss reduced by 0.015979766845703125\n",
      "did one more step, loss reduced by 0.015972137451171875\n",
      "did one more step, loss reduced by 0.015941619873046875\n",
      "did one more step, loss reduced by 0.016948699951171875\n",
      "did one more step, loss reduced by 0.016017913818359375\n",
      "did one more step, loss reduced by 0.016002655029296875\n",
      "did one more step, loss reduced by 0.01598358154296875\n",
      "did one more step, loss reduced by 0.015964508056640625\n",
      "did one more step, loss reduced by 0.01595306396484375\n",
      "did one more step, loss reduced by 0.01593017578125\n",
      "did one more step, loss reduced by 0.015911102294921875\n",
      "did one more step, loss reduced by 0.01686859130859375\n",
      "did one more step, loss reduced by 0.0159759521484375\n",
      "did one more step, loss reduced by 0.015972137451171875\n",
      "did one more step, loss reduced by 0.0159454345703125\n",
      "did one more step, loss reduced by 0.015933990478515625\n",
      "did one more step, loss reduced by 0.0159149169921875\n",
      "did one more step, loss reduced by 0.015899658203125\n",
      "did one more step, loss reduced by 0.015872955322265625\n",
      "did one more step, loss reduced by 0.015869140625\n",
      "did one more step, loss reduced by 0.016872406005859375\n",
      "did one more step, loss reduced by 0.01593017578125\n",
      "did one more step, loss reduced by 0.02121734619140625\n",
      "did one more step, loss reduced by 0.02118682861328125\n",
      "did one more step, loss reduced by 0.021152496337890625\n",
      "did one more step, loss reduced by 0.02112579345703125\n",
      "did one more step, loss reduced by 0.021091461181640625\n",
      "did one more step, loss reduced by 0.022144317626953125\n",
      "did one more step, loss reduced by 0.021175384521484375\n",
      "did one more step, loss reduced by 0.0211334228515625\n",
      "did one more step, loss reduced by 0.021114349365234375\n",
      "did one more step, loss reduced by 0.021076202392578125\n",
      "did one more step, loss reduced by 0.02104949951171875\n",
      "did one more step, loss reduced by 0.022052764892578125\n",
      "did one more step, loss reduced by 0.021121978759765625\n",
      "did one more step, loss reduced by 0.02109527587890625\n",
      "did one more step, loss reduced by 0.02106475830078125\n",
      "did one more step, loss reduced by 0.021030426025390625\n",
      "did one more step, loss reduced by 0.02100372314453125\n",
      "did one more step, loss reduced by 0.021961212158203125\n",
      "did one more step, loss reduced by 0.02108001708984375\n",
      "did one more step, loss reduced by 0.021045684814453125\n",
      "did one more step, loss reduced by 0.02101898193359375\n",
      "did one more step, loss reduced by 0.020984649658203125\n",
      "did one more step, loss reduced by 0.02095794677734375\n",
      "did one more step, loss reduced by 0.020923614501953125\n",
      "did one more step, loss reduced by 0.02198028564453125\n",
      "did one more step, loss reduced by 0.020999908447265625\n",
      "did one more step, loss reduced by 0.02097320556640625\n",
      "did one more step, loss reduced by 0.02094268798828125\n",
      "did one more step, loss reduced by 0.020908355712890625\n",
      "did one more step, loss reduced by 0.02088165283203125\n",
      "did one more step, loss reduced by 0.021884918212890625\n",
      "did one more step, loss reduced by 0.020954132080078125\n",
      "did one more step, loss reduced by 0.020931243896484375\n",
      "did one more step, loss reduced by 0.0208892822265625\n",
      "did one more step, loss reduced by 0.020870208740234375\n",
      "did one more step, loss reduced by 0.020832061767578125\n",
      "did one more step, loss reduced by 0.021793365478515625\n",
      "did one more step, loss reduced by 0.02091217041015625\n",
      "did one more step, loss reduced by 0.020877838134765625\n",
      "did one more step, loss reduced by 0.02085113525390625\n",
      "did one more step, loss reduced by 0.02082061767578125\n",
      "did one more step, loss reduced by 0.020786285400390625\n",
      "did one more step, loss reduced by 0.02075958251953125\n",
      "did one more step, loss reduced by 0.021808624267578125\n",
      "did one more step, loss reduced by 0.02083587646484375\n",
      "did one more step, loss reduced by 0.020801544189453125\n",
      "did one more step, loss reduced by 0.02077484130859375\n",
      "did one more step, loss reduced by 0.020740509033203125\n",
      "did one more step, loss reduced by 0.02071380615234375\n",
      "did one more step, loss reduced by 0.021717071533203125\n",
      "did one more step, loss reduced by 0.02079010009765625\n",
      "did one more step, loss reduced by 0.020755767822265625\n",
      "did one more step, loss reduced by 0.02072906494140625\n",
      "did one more step, loss reduced by 0.02069854736328125\n",
      "did one more step, loss reduced by 0.020664215087890625\n",
      "did one more step, loss reduced by 0.021625518798828125\n",
      "did one more step, loss reduced by 0.02074432373046875\n",
      "did one more step, loss reduced by 0.020709991455078125\n",
      "did one more step, loss reduced by 0.020687103271484375\n",
      "did one more step, loss reduced by 0.0206451416015625\n",
      "did one more step, loss reduced by 0.020626068115234375\n",
      "did one more step, loss reduced by 0.020587921142578125\n",
      "did one more step, loss reduced by 0.021640777587890625\n",
      "did one more step, loss reduced by 0.02066802978515625\n",
      "did one more step, loss reduced by 0.020633697509765625\n",
      "did one more step, loss reduced by 0.02060699462890625\n",
      "did one more step, loss reduced by 0.02057647705078125\n",
      "did one more step, loss reduced by 0.020542144775390625\n",
      "did one more step, loss reduced by 0.02155303955078125\n",
      "did one more step, loss reduced by 0.020618438720703125\n",
      "did one more step, loss reduced by 0.02059173583984375\n",
      "did one more step, loss reduced by 0.020557403564453125\n",
      "did one more step, loss reduced by 0.02053070068359375\n",
      "did one more step, loss reduced by 0.020496368408203125\n",
      "did one more step, loss reduced by 0.02146148681640625\n",
      "did one more step, loss reduced by 0.020572662353515625\n",
      "did one more step, loss reduced by 0.02054595947265625\n",
      "did one more step, loss reduced by 0.020511627197265625\n",
      "did one more step, loss reduced by 0.02048492431640625\n",
      "did one more step, loss reduced by 0.02045440673828125\n",
      "did one more step, loss reduced by 0.020420074462890625\n",
      "did one more step, loss reduced by 0.021472930908203125\n",
      "did one more step, loss reduced by 0.02050018310546875\n",
      "did one more step, loss reduced by 0.020465850830078125\n",
      "did one more step, loss reduced by 0.020442962646484375\n",
      "did one more step, loss reduced by 0.0204010009765625\n",
      "did one more step, loss reduced by 0.020381927490234375\n",
      "did one more step, loss reduced by 0.021381378173828125\n",
      "did one more step, loss reduced by 0.020450592041015625\n",
      "did one more step, loss reduced by 0.02042388916015625\n",
      "did one more step, loss reduced by 0.020389556884765625\n",
      "did one more step, loss reduced by 0.02036285400390625\n",
      "did one more step, loss reduced by 0.02033233642578125\n",
      "did one more step, loss reduced by 0.0212860107421875\n",
      "did one more step, loss reduced by 0.020412445068359375\n",
      "did one more step, loss reduced by 0.020374298095703125\n",
      "did one more step, loss reduced by 0.02034759521484375\n",
      "did one more step, loss reduced by 0.020313262939453125\n",
      "did one more step, loss reduced by 0.02028656005859375\n",
      "did one more step, loss reduced by 0.020252227783203125\n",
      "did one more step, loss reduced by 0.02130889892578125\n",
      "did one more step, loss reduced by 0.020328521728515625\n",
      "did one more step, loss reduced by 0.02030181884765625\n",
      "did one more step, loss reduced by 0.020267486572265625\n",
      "did one more step, loss reduced by 0.02024078369140625\n",
      "did one more step, loss reduced by 0.02021026611328125\n",
      "did one more step, loss reduced by 0.021213531494140625\n",
      "did one more step, loss reduced by 0.020282745361328125\n",
      "did one more step, loss reduced by 0.02025604248046875\n",
      "did one more step, loss reduced by 0.020221710205078125\n",
      "did one more step, loss reduced by 0.020198822021484375\n",
      "did one more step, loss reduced by 0.0201568603515625\n",
      "did one more step, loss reduced by 0.02112579345703125\n",
      "did one more step, loss reduced by 0.02024078369140625\n",
      "did one more step, loss reduced by 0.020206451416015625\n",
      "did one more step, loss reduced by 0.02017974853515625\n",
      "did one more step, loss reduced by 0.020145416259765625\n",
      "did one more step, loss reduced by 0.02011871337890625\n",
      "did one more step, loss reduced by 0.02008819580078125\n",
      "did one more step, loss reduced by 0.0211334228515625\n",
      "did one more step, loss reduced by 0.020168304443359375\n",
      "did one more step, loss reduced by 0.020130157470703125\n",
      "did one more step, loss reduced by 0.02010345458984375\n",
      "did one more step, loss reduced by 0.020069122314453125\n",
      "did one more step, loss reduced by 0.02004241943359375\n",
      "did one more step, loss reduced by 0.021045684814453125\n",
      "did one more step, loss reduced by 0.02011871337890625\n",
      "did one more step, loss reduced by 0.020084381103515625\n",
      "did one more step, loss reduced by 0.02005767822265625\n",
      "did one more step, loss reduced by 0.020023345947265625\n",
      "did one more step, loss reduced by 0.01999664306640625\n",
      "did one more step, loss reduced by 0.020954132080078125\n",
      "did one more step, loss reduced by 0.02007293701171875\n",
      "did one more step, loss reduced by 0.020038604736328125\n",
      "did one more step, loss reduced by 0.02001190185546875\n",
      "did one more step, loss reduced by 0.019977569580078125\n",
      "did one more step, loss reduced by 0.019954681396484375\n",
      "did one more step, loss reduced by 0.0199127197265625\n",
      "did one more step, loss reduced by 0.02097320556640625\n",
      "did one more step, loss reduced by 0.019994735717773438\n",
      "did one more step, loss reduced by 0.019964218139648438\n",
      "did one more step, loss reduced by 0.01993560791015625\n",
      "did one more step, loss reduced by 0.019903182983398438\n",
      "did one more step, loss reduced by 0.019872665405273438\n",
      "did one more step, loss reduced by 0.020879745483398438\n",
      "did one more step, loss reduced by 0.019947052001953125\n",
      "did one more step, loss reduced by 0.019922256469726562\n",
      "did one more step, loss reduced by 0.019884109497070312\n",
      "did one more step, loss reduced by 0.019861221313476562\n",
      "did one more step, loss reduced by 0.019824981689453125\n",
      "did one more step, loss reduced by 0.020788192749023438\n",
      "did one more step, loss reduced by 0.019903182983398438\n",
      "did one more step, loss reduced by 0.019872665405273438\n",
      "did one more step, loss reduced by 0.019842147827148438\n",
      "did one more step, loss reduced by 0.01981353759765625\n",
      "did one more step, loss reduced by 0.019781112670898438\n",
      "did one more step, loss reduced by 0.019750595092773438\n",
      "did one more step, loss reduced by 0.020801544189453125\n",
      "did one more step, loss reduced by 0.01982879638671875\n",
      "did one more step, loss reduced by 0.019794464111328125\n",
      "did one more step, loss reduced by 0.019769668579101562\n",
      "did one more step, loss reduced by 0.019731521606445312\n",
      "did one more step, loss reduced by 0.019708633422851562\n",
      "did one more step, loss reduced by 0.020709991455078125\n",
      "did one more step, loss reduced by 0.019781112670898438\n",
      "did one more step, loss reduced by 0.019750595092773438\n",
      "did one more step, loss reduced by 0.019720077514648438\n",
      "did one more step, loss reduced by 0.01969146728515625\n",
      "did one more step, loss reduced by 0.019659042358398438\n",
      "did one more step, loss reduced by 0.020616531372070312\n",
      "did one more step, loss reduced by 0.019739151000976562\n",
      "did one more step, loss reduced by 0.019702911376953125\n",
      "did one more step, loss reduced by 0.019678115844726562\n",
      "did one more step, loss reduced by 0.019639968872070312\n",
      "did one more step, loss reduced by 0.019617080688476562\n",
      "did one more step, loss reduced by 0.019580841064453125\n",
      "did one more step, loss reduced by 0.020635604858398438\n",
      "did one more step, loss reduced by 0.019659042358398438\n",
      "did one more step, loss reduced by 0.019628524780273438\n",
      "did one more step, loss reduced by 0.019598007202148438\n",
      "did one more step, loss reduced by 0.01956939697265625\n",
      "did one more step, loss reduced by 0.019536972045898438\n",
      "did one more step, loss reduced by 0.020544052124023438\n",
      "did one more step, loss reduced by 0.019611358642578125\n",
      "did one more step, loss reduced by 0.01958465576171875\n",
      "did one more step, loss reduced by 0.019550323486328125\n",
      "did one more step, loss reduced by 0.019525527954101562\n",
      "did one more step, loss reduced by 0.019487380981445312\n",
      "did one more step, loss reduced by 0.02045440673828125\n",
      "did one more step, loss reduced by 0.019567489624023438\n",
      "did one more step, loss reduced by 0.019536972045898438\n",
      "did one more step, loss reduced by 0.019506454467773438\n",
      "did one more step, loss reduced by 0.019475936889648438\n",
      "did one more step, loss reduced by 0.01944732666015625\n",
      "did one more step, loss reduced by 0.019414901733398438\n",
      "did one more step, loss reduced by 0.020463943481445312\n",
      "did one more step, loss reduced by 0.019495010375976562\n",
      "did one more step, loss reduced by 0.019458770751953125\n",
      "did one more step, loss reduced by 0.019433975219726562\n",
      "did one more step, loss reduced by 0.019395828247070312\n",
      "did one more step, loss reduced by 0.019372940063476562\n",
      "did one more step, loss reduced by 0.020374298095703125\n",
      "did one more step, loss reduced by 0.019445419311523438\n",
      "did one more step, loss reduced by 0.019414901733398438\n",
      "did one more step, loss reduced by 0.019384384155273438\n",
      "did one more step, loss reduced by 0.019353866577148438\n",
      "did one more step, loss reduced by 0.01932525634765625\n",
      "did one more step, loss reduced by 0.020280838012695312\n",
      "did one more step, loss reduced by 0.019403457641601562\n",
      "did one more step, loss reduced by 0.019367218017578125\n",
      "did one more step, loss reduced by 0.01934051513671875\n",
      "did one more step, loss reduced by 0.019306182861328125\n",
      "did one more step, loss reduced by 0.019281387329101562\n",
      "did one more step, loss reduced by 0.019243240356445312\n",
      "did one more step, loss reduced by 0.02030181884765625\n",
      "did one more step, loss reduced by 0.019323348999023438\n",
      "did one more step, loss reduced by 0.019292831420898438\n",
      "did one more step, loss reduced by 0.019262313842773438\n",
      "did one more step, loss reduced by 0.019231796264648438\n",
      "did one more step, loss reduced by 0.01920318603515625\n",
      "did one more step, loss reduced by 0.020208358764648438\n",
      "did one more step, loss reduced by 0.019273757934570312\n",
      "did one more step, loss reduced by 0.019250869750976562\n",
      "did one more step, loss reduced by 0.019214630126953125\n",
      "did one more step, loss reduced by 0.019189834594726562\n",
      "did one more step, loss reduced by 0.019151687622070312\n",
      "did one more step, loss reduced by 0.020116806030273438\n",
      "did one more step, loss reduced by 0.01923370361328125\n",
      "did one more step, loss reduced by 0.019201278686523438\n",
      "did one more step, loss reduced by 0.019170761108398438\n",
      "did one more step, loss reduced by 0.019140243530273438\n",
      "did one more step, loss reduced by 0.019109725952148438\n",
      "did one more step, loss reduced by 0.01908111572265625\n",
      "did one more step, loss reduced by 0.010540008544921875\n",
      "did one more step, loss reduced by 0.019170761108398438\n",
      "did one more step, loss reduced by 0.019140243530273438\n",
      "did one more step, loss reduced by 0.01911163330078125\n",
      "did one more step, loss reduced by 0.019079208374023438\n",
      "did one more step, loss reduced by 0.009527206420898438\n",
      "did one more step, loss reduced by 0.009521484375\n",
      "did one more step, loss reduced by 0.009515762329101562\n",
      "did one more step, loss reduced by 0.010469436645507812\n",
      "did one more step, loss reduced by 0.01912689208984375\n",
      "did one more step, loss reduced by 0.019094467163085938\n",
      "did one more step, loss reduced by 0.019063949584960938\n",
      "did one more step, loss reduced by 0.009521484375\n",
      "did one more step, loss reduced by 0.009511947631835938\n",
      "did one more step, loss reduced by 0.0095062255859375\n",
      "did one more step, loss reduced by 0.009496688842773438\n",
      "did one more step, loss reduced by 0.009490966796875\n",
      "did one more step, loss reduced by 0.010404586791992188\n",
      "did one more step, loss reduced by 0.019079208374023438\n",
      "did one more step, loss reduced by 0.009531021118164062\n",
      "did one more step, loss reduced by 0.009517669677734375\n",
      "did one more step, loss reduced by 0.009511947631835938\n",
      "did one more step, loss reduced by 0.0095062255859375\n",
      "did one more step, loss reduced by 0.009500503540039062\n",
      "did one more step, loss reduced by 0.009489059448242188\n",
      "did one more step, loss reduced by 0.009481430053710938\n",
      "did one more step, loss reduced by 0.0094757080078125\n",
      "did one more step, loss reduced by 0.009469985961914062\n",
      "did one more step, loss reduced by 0.009456634521484375\n",
      "did one more step, loss reduced by 0.010396957397460938\n",
      "did one more step, loss reduced by 0.009511947631835938\n",
      "did one more step, loss reduced by 0.0095062255859375\n",
      "did one more step, loss reduced by 0.00949859619140625\n",
      "did one more step, loss reduced by 0.009490966796875\n",
      "did one more step, loss reduced by 0.009481430053710938\n",
      "did one more step, loss reduced by 0.0094757080078125\n",
      "did one more step, loss reduced by 0.009466171264648438\n",
      "did one more step, loss reduced by 0.00946044921875\n",
      "did one more step, loss reduced by 0.009450912475585938\n",
      "did one more step, loss reduced by 0.0094451904296875\n",
      "did one more step, loss reduced by 0.009435653686523438\n",
      "did one more step, loss reduced by 0.009429931640625\n",
      "did one more step, loss reduced by 0.010389328002929688\n",
      "did one more step, loss reduced by 0.009485244750976562\n",
      "did one more step, loss reduced by 0.009471893310546875\n",
      "did one more step, loss reduced by 0.009466171264648438\n",
      "did one more step, loss reduced by 0.00946044921875\n",
      "did one more step, loss reduced by 0.009454727172851562\n",
      "did one more step, loss reduced by 0.009441375732421875\n",
      "did one more step, loss reduced by 0.00943756103515625\n",
      "did one more step, loss reduced by 0.009429931640625\n",
      "did one more step, loss reduced by 0.00942230224609375\n",
      "did one more step, loss reduced by 0.009412765502929688\n",
      "did one more step, loss reduced by 0.009405136108398438\n",
      "did one more step, loss reduced by 0.010320663452148438\n",
      "did one more step, loss reduced by 0.00946044921875\n",
      "did one more step, loss reduced by 0.009450912475585938\n",
      "did one more step, loss reduced by 0.0094451904296875\n",
      "did one more step, loss reduced by 0.00943756103515625\n",
      "did one more step, loss reduced by 0.009429931640625\n",
      "did one more step, loss reduced by 0.009420394897460938\n",
      "did one more step, loss reduced by 0.0094146728515625\n",
      "did one more step, loss reduced by 0.009405136108398438\n",
      "did one more step, loss reduced by 0.0093994140625\n",
      "did one more step, loss reduced by 0.009389877319335938\n",
      "did one more step, loss reduced by 0.0093841552734375\n",
      "did one more step, loss reduced by 0.009374618530273438\n",
      "did one more step, loss reduced by 0.01031494140625\n",
      "did one more step, loss reduced by 0.009428024291992188\n",
      "did one more step, loss reduced by 0.009420394897460938\n",
      "did one more step, loss reduced by 0.0094146728515625\n",
      "did one more step, loss reduced by 0.009408950805664062\n",
      "did one more step, loss reduced by 0.009395599365234375\n",
      "did one more step, loss reduced by 0.009389877319335938\n",
      "did one more step, loss reduced by 0.0093841552734375\n",
      "did one more step, loss reduced by 0.009378433227539062\n",
      "did one more step, loss reduced by 0.009366989135742188\n",
      "did one more step, loss reduced by 0.009359359741210938\n",
      "did one more step, loss reduced by 0.0093536376953125\n",
      "did one more step, loss reduced by 0.009347915649414062\n",
      "did one more step, loss reduced by 0.010301589965820312\n",
      "did one more step, loss reduced by 0.0093994140625\n",
      "did one more step, loss reduced by 0.009389877319335938\n",
      "did one more step, loss reduced by 0.0093841552734375\n",
      "did one more step, loss reduced by 0.00937652587890625\n",
      "did one more step, loss reduced by 0.009368896484375\n",
      "did one more step, loss reduced by 0.009359359741210938\n",
      "did one more step, loss reduced by 0.0093536376953125\n",
      "did one more step, loss reduced by 0.009344100952148438\n",
      "did one more step, loss reduced by 0.00933837890625\n",
      "did one more step, loss reduced by 0.009328842163085938\n",
      "did one more step, loss reduced by 0.0093231201171875\n",
      "did one more step, loss reduced by 0.010236740112304688\n",
      "did one more step, loss reduced by 0.009374618530273438\n",
      "did one more step, loss reduced by 0.009368896484375\n",
      "did one more step, loss reduced by 0.009363174438476562\n",
      "did one more step, loss reduced by 0.009349822998046875\n",
      "did one more step, loss reduced by 0.009344100952148438\n",
      "did one more step, loss reduced by 0.00933837890625\n",
      "did one more step, loss reduced by 0.009332656860351562\n",
      "did one more step, loss reduced by 0.009319305419921875\n",
      "did one more step, loss reduced by 0.00931549072265625\n",
      "did one more step, loss reduced by 0.009307861328125\n",
      "did one more step, loss reduced by 0.00930023193359375\n",
      "did one more step, loss reduced by 0.009290695190429688\n",
      "did one more step, loss reduced by 0.010229110717773438\n",
      "did one more step, loss reduced by 0.009344100952148438\n",
      "did one more step, loss reduced by 0.00933837890625\n",
      "did one more step, loss reduced by 0.009328842163085938\n",
      "did one more step, loss reduced by 0.0093231201171875\n",
      "did one more step, loss reduced by 0.00931549072265625\n",
      "did one more step, loss reduced by 0.009307861328125\n",
      "did one more step, loss reduced by 0.009298324584960938\n",
      "did one more step, loss reduced by 0.0092926025390625\n",
      "did one more step, loss reduced by 0.009283065795898438\n",
      "did one more step, loss reduced by 0.00927734375\n",
      "did one more step, loss reduced by 0.009267807006835938\n",
      "did one more step, loss reduced by 0.0092620849609375\n",
      "did one more step, loss reduced by 0.010221481323242188\n",
      "did one more step, loss reduced by 0.00931549072265625\n",
      "did one more step, loss reduced by 0.009305953979492188\n",
      "did one more step, loss reduced by 0.009298324584960938\n",
      "did one more step, loss reduced by 0.0092926025390625\n",
      "did one more step, loss reduced by 0.009286880493164062\n",
      "did one more step, loss reduced by 0.009273529052734375\n",
      "did one more step, loss reduced by 0.009267807006835938\n",
      "did one more step, loss reduced by 0.0092620849609375\n",
      "did one more step, loss reduced by 0.009256362915039062\n",
      "did one more step, loss reduced by 0.009244918823242188\n",
      "did one more step, loss reduced by 0.009237289428710938\n",
      "did one more step, loss reduced by 0.010152816772460938\n",
      "did one more step, loss reduced by 0.0092926025390625\n",
      "did one more step, loss reduced by 0.009283065795898438\n",
      "did one more step, loss reduced by 0.00927734375\n",
      "did one more step, loss reduced by 0.009267807006835938\n",
      "did one more step, loss reduced by 0.0092620849609375\n",
      "did one more step, loss reduced by 0.00925445556640625\n",
      "did one more step, loss reduced by 0.009246826171875\n",
      "did one more step, loss reduced by 0.009237289428710938\n",
      "did one more step, loss reduced by 0.0092315673828125\n",
      "did one more step, loss reduced by 0.009222030639648438\n",
      "did one more step, loss reduced by 0.00921630859375\n",
      "did one more step, loss reduced by 0.009206771850585938\n",
      "did one more step, loss reduced by 0.0101470947265625\n",
      "did one more step, loss reduced by 0.009260177612304688\n",
      "did one more step, loss reduced by 0.009252548217773438\n",
      "did one more step, loss reduced by 0.009246826171875\n",
      "did one more step, loss reduced by 0.009241104125976562\n",
      "did one more step, loss reduced by 0.009227752685546875\n",
      "did one more step, loss reduced by 0.009222030639648438\n",
      "did one more step, loss reduced by 0.00921630859375\n",
      "did one more step, loss reduced by 0.009210586547851562\n",
      "did one more step, loss reduced by 0.009197235107421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.00919342041015625\n",
      "did one more step, loss reduced by 0.009185791015625\n",
      "did one more step, loss reduced by 0.00917816162109375\n",
      "did one more step, loss reduced by 0.010135650634765625\n",
      "did one more step, loss reduced by 0.0092315673828125\n",
      "did one more step, loss reduced by 0.009222030639648438\n",
      "did one more step, loss reduced by 0.00921630859375\n",
      "did one more step, loss reduced by 0.009206771850585938\n",
      "did one more step, loss reduced by 0.0092010498046875\n",
      "did one more step, loss reduced by 0.00919342041015625\n",
      "did one more step, loss reduced by 0.009185791015625\n",
      "did one more step, loss reduced by 0.009176254272460938\n",
      "did one more step, loss reduced by 0.0091705322265625\n",
      "did one more step, loss reduced by 0.009160995483398438\n",
      "did one more step, loss reduced by 0.0091552734375\n",
      "did one more step, loss reduced by 0.010066986083984375\n",
      "did one more step, loss reduced by 0.00920867919921875\n",
      "did one more step, loss reduced by 0.0092010498046875\n",
      "did one more step, loss reduced by 0.00919342041015625\n",
      "did one more step, loss reduced by 0.009183883666992188\n",
      "did one more step, loss reduced by 0.009176254272460938\n",
      "did one more step, loss reduced by 0.0091705322265625\n",
      "did one more step, loss reduced by 0.009164810180664062\n",
      "did one more step, loss reduced by 0.009151458740234375\n",
      "did one more step, loss reduced by 0.009145736694335938\n",
      "did one more step, loss reduced by 0.0091400146484375\n",
      "did one more step, loss reduced by 0.009134292602539062\n",
      "did one more step, loss reduced by 0.009122848510742188\n",
      "did one more step, loss reduced by 0.010061264038085938\n",
      "did one more step, loss reduced by 0.009176254272460938\n",
      "did one more step, loss reduced by 0.0091705322265625\n",
      "did one more step, loss reduced by 0.009160995483398438\n",
      "did one more step, loss reduced by 0.0091552734375\n",
      "did one more step, loss reduced by 0.009145736694335938\n",
      "did one more step, loss reduced by 0.0091400146484375\n",
      "did one more step, loss reduced by 0.00913238525390625\n",
      "did one more step, loss reduced by 0.009124755859375\n",
      "did one more step, loss reduced by 0.009115219116210938\n",
      "did one more step, loss reduced by 0.0091094970703125\n",
      "did one more step, loss reduced by 0.009099960327148438\n",
      "did one more step, loss reduced by 0.00909423828125\n",
      "did one more step, loss reduced by 0.010051727294921875\n",
      "did one more step, loss reduced by 0.009149551391601562\n",
      "did one more step, loss reduced by 0.009138107299804688\n",
      "did one more step, loss reduced by 0.009130477905273438\n",
      "did one more step, loss reduced by 0.009124755859375\n",
      "did one more step, loss reduced by 0.009119033813476562\n",
      "did one more step, loss reduced by 0.009105682373046875\n",
      "did one more step, loss reduced by 0.009099960327148438\n",
      "did one more step, loss reduced by 0.00909423828125\n",
      "did one more step, loss reduced by 0.009088516235351562\n",
      "did one more step, loss reduced by 0.009075164794921875\n",
      "did one more step, loss reduced by 0.00907135009765625\n",
      "did one more step, loss reduced by 0.009984970092773438\n",
      "did one more step, loss reduced by 0.009124755859375\n",
      "did one more step, loss reduced by 0.009115219116210938\n",
      "did one more step, loss reduced by 0.0091094970703125\n",
      "did one more step, loss reduced by 0.009099960327148438\n",
      "did one more step, loss reduced by 0.00909423828125\n",
      "did one more step, loss reduced by 0.009084701538085938\n",
      "did one more step, loss reduced by 0.0090789794921875\n",
      "did one more step, loss reduced by 0.00907135009765625\n",
      "did one more step, loss reduced by 0.009063720703125\n",
      "did one more step, loss reduced by 0.009054183959960938\n",
      "did one more step, loss reduced by 0.0090484619140625\n",
      "did one more step, loss reduced by 0.009038925170898438\n",
      "did one more step, loss reduced by 0.009979248046875\n",
      "did one more step, loss reduced by 0.009090423583984375\n",
      "did one more step, loss reduced by 0.00908660888671875\n",
      "did one more step, loss reduced by 0.0090789794921875\n",
      "did one more step, loss reduced by 0.00907135009765625\n",
      "did one more step, loss reduced by 0.009061813354492188\n",
      "did one more step, loss reduced by 0.009054183959960938\n",
      "did one more step, loss reduced by 0.0090484619140625\n",
      "did one more step, loss reduced by 0.009042739868164062\n",
      "did one more step, loss reduced by 0.009029388427734375\n",
      "did one more step, loss reduced by 0.009023666381835938\n",
      "did one more step, loss reduced by 0.0090179443359375\n",
      "did one more step, loss reduced by 0.009012222290039062\n",
      "did one more step, loss reduced by 0.009967803955078125\n",
      "did one more step, loss reduced by 0.009063720703125\n",
      "did one more step, loss reduced by 0.009054183959960938\n",
      "did one more step, loss reduced by 0.0090484619140625\n",
      "did one more step, loss reduced by 0.009038925170898438\n",
      "did one more step, loss reduced by 0.009033203125\n",
      "did one more step, loss reduced by 0.009023666381835938\n",
      "did one more step, loss reduced by 0.0090179443359375\n",
      "did one more step, loss reduced by 0.00901031494140625\n",
      "did one more step, loss reduced by 0.009002685546875\n",
      "did one more step, loss reduced by 0.008993148803710938\n",
      "did one more step, loss reduced by 0.0089874267578125\n",
      "did one more step, loss reduced by 0.009899139404296875\n",
      "did one more step, loss reduced by 0.009038925170898438\n",
      "did one more step, loss reduced by 0.009033203125\n",
      "did one more step, loss reduced by 0.009027481079101562\n",
      "did one more step, loss reduced by 0.009016036987304688\n",
      "did one more step, loss reduced by 0.009008407592773438\n",
      "did one more step, loss reduced by 0.009002685546875\n",
      "did one more step, loss reduced by 0.008996963500976562\n",
      "did one more step, loss reduced by 0.008983612060546875\n",
      "did one more step, loss reduced by 0.008977890014648438\n",
      "did one more step, loss reduced by 0.00897216796875\n",
      "did one more step, loss reduced by 0.008966445922851562\n",
      "did one more step, loss reduced by 0.008953094482421875\n",
      "did one more step, loss reduced by 0.00989532470703125\n",
      "did one more step, loss reduced by 0.009008407592773438\n",
      "did one more step, loss reduced by 0.009002685546875\n",
      "did one more step, loss reduced by 0.008993148803710938\n",
      "did one more step, loss reduced by 0.0089874267578125\n",
      "did one more step, loss reduced by 0.008977890014648438\n",
      "did one more step, loss reduced by 0.00897216796875\n",
      "did one more step, loss reduced by 0.008962631225585938\n",
      "did one more step, loss reduced by 0.0089569091796875\n",
      "did one more step, loss reduced by 0.00894927978515625\n",
      "did one more step, loss reduced by 0.008941650390625\n",
      "did one more step, loss reduced by 0.008932113647460938\n",
      "did one more step, loss reduced by 0.0089263916015625\n",
      "did one more step, loss reduced by 0.009883880615234375\n",
      "did one more step, loss reduced by 0.008981704711914062\n",
      "did one more step, loss reduced by 0.008968353271484375\n",
      "did one more step, loss reduced by 0.00896453857421875\n",
      "did one more step, loss reduced by 0.0089569091796875\n",
      "did one more step, loss reduced by 0.00894927978515625\n",
      "did one more step, loss reduced by 0.008939743041992188\n",
      "did one more step, loss reduced by 0.008932113647460938\n",
      "did one more step, loss reduced by 0.0089263916015625\n",
      "did one more step, loss reduced by 0.008920669555664062\n",
      "did one more step, loss reduced by 0.008907318115234375\n",
      "did one more step, loss reduced by 0.008901596069335938\n",
      "did one more step, loss reduced by 0.00981903076171875\n",
      "did one more step, loss reduced by 0.0089569091796875\n",
      "did one more step, loss reduced by 0.008947372436523438\n",
      "did one more step, loss reduced by 0.008941650390625\n",
      "did one more step, loss reduced by 0.008932113647460938\n",
      "did one more step, loss reduced by 0.0089263916015625\n",
      "did one more step, loss reduced by 0.008916854858398438\n",
      "did one more step, loss reduced by 0.0089111328125\n",
      "did one more step, loss reduced by 0.008901596069335938\n",
      "did one more step, loss reduced by 0.0088958740234375\n",
      "did one more step, loss reduced by 0.00888824462890625\n",
      "did one more step, loss reduced by 0.008880615234375\n",
      "did one more step, loss reduced by 0.008871078491210938\n",
      "did one more step, loss reduced by 0.0098114013671875\n",
      "did one more step, loss reduced by 0.008922576904296875\n",
      "did one more step, loss reduced by 0.008916854858398438\n",
      "did one more step, loss reduced by 0.0089111328125\n",
      "did one more step, loss reduced by 0.008905410766601562\n",
      "did one more step, loss reduced by 0.008893966674804688\n",
      "did one more step, loss reduced by 0.008886337280273438\n",
      "did one more step, loss reduced by 0.008880615234375\n",
      "did one more step, loss reduced by 0.008874893188476562\n",
      "did one more step, loss reduced by 0.008861541748046875\n",
      "did one more step, loss reduced by 0.008855819702148438\n",
      "did one more step, loss reduced by 0.00885009765625\n",
      "did one more step, loss reduced by 0.008844375610351562\n",
      "did one more step, loss reduced by 0.009799957275390625\n",
      "did one more step, loss reduced by 0.0088958740234375\n",
      "did one more step, loss reduced by 0.008886337280273438\n",
      "did one more step, loss reduced by 0.008880615234375\n",
      "did one more step, loss reduced by 0.008871078491210938\n",
      "did one more step, loss reduced by 0.0088653564453125\n",
      "did one more step, loss reduced by 0.008855819702148438\n",
      "did one more step, loss reduced by 0.00885009765625\n",
      "did one more step, loss reduced by 0.008840560913085938\n",
      "did one more step, loss reduced by 0.0088348388671875\n",
      "did one more step, loss reduced by 0.00882720947265625\n",
      "did one more step, loss reduced by 0.008819580078125\n",
      "did one more step, loss reduced by 0.009731292724609375\n",
      "did one more step, loss reduced by 0.008871078491210938\n",
      "did one more step, loss reduced by 0.0088653564453125\n",
      "did one more step, loss reduced by 0.008859634399414062\n",
      "did one more step, loss reduced by 0.008846282958984375\n",
      "did one more step, loss reduced by 0.00884246826171875\n",
      "did one more step, loss reduced by 0.0088348388671875\n",
      "did one more step, loss reduced by 0.00882720947265625\n",
      "did one more step, loss reduced by 0.008817672729492188\n",
      "did one more step, loss reduced by 0.008810043334960938\n",
      "did one more step, loss reduced by 0.0088043212890625\n",
      "did one more step, loss reduced by 0.008798599243164062\n",
      "did one more step, loss reduced by 0.008785247802734375\n",
      "did one more step, loss reduced by 0.009725570678710938\n",
      "did one more step, loss reduced by 0.00884246826171875\n",
      "did one more step, loss reduced by 0.0088348388671875\n",
      "did one more step, loss reduced by 0.008825302124023438\n",
      "did one more step, loss reduced by 0.008819580078125\n",
      "did one more step, loss reduced by 0.008810043334960938\n",
      "did one more step, loss reduced by 0.0088043212890625\n",
      "did one more step, loss reduced by 0.008794784545898438\n",
      "did one more step, loss reduced by 0.0087890625\n",
      "did one more step, loss reduced by 0.008779525756835938\n",
      "did one more step, loss reduced by 0.0087738037109375\n",
      "did one more step, loss reduced by 0.00876617431640625\n",
      "did one more step, loss reduced by 0.008758544921875\n",
      "did one more step, loss reduced by 0.009716033935546875\n",
      "did one more step, loss reduced by 0.008813858032226562\n",
      "did one more step, loss reduced by 0.008800506591796875\n",
      "did one more step, loss reduced by 0.008794784545898438\n",
      "did one more step, loss reduced by 0.0087890625\n",
      "did one more step, loss reduced by 0.008783340454101562\n",
      "did one more step, loss reduced by 0.008771896362304688\n",
      "did one more step, loss reduced by 0.008764266967773438\n",
      "did one more step, loss reduced by 0.008758544921875\n",
      "did one more step, loss reduced by 0.008752822875976562\n",
      "did one more step, loss reduced by 0.008739471435546875\n",
      "did one more step, loss reduced by 0.008733749389648438\n",
      "did one more step, loss reduced by 0.009649276733398438\n",
      "did one more step, loss reduced by 0.0087890625\n",
      "did one more step, loss reduced by 0.00878143310546875\n",
      "did one more step, loss reduced by 0.0087738037109375\n",
      "did one more step, loss reduced by 0.008764266967773438\n",
      "did one more step, loss reduced by 0.008758544921875\n",
      "did one more step, loss reduced by 0.008749008178710938\n",
      "did one more step, loss reduced by 0.0087432861328125\n",
      "did one more step, loss reduced by 0.008733749389648438\n",
      "did one more step, loss reduced by 0.00872802734375\n",
      "did one more step, loss reduced by 0.008718490600585938\n",
      "did one more step, loss reduced by 0.0087127685546875\n",
      "did one more step, loss reduced by 0.00870513916015625\n",
      "did one more step, loss reduced by 0.0096435546875\n",
      "did one more step, loss reduced by 0.008754730224609375\n",
      "did one more step, loss reduced by 0.008749008178710938\n",
      "did one more step, loss reduced by 0.0087432861328125\n",
      "did one more step, loss reduced by 0.008737564086914062\n",
      "did one more step, loss reduced by 0.008724212646484375\n",
      "did one more step, loss reduced by 0.00872039794921875\n",
      "did one more step, loss reduced by 0.0087127685546875\n",
      "did one more step, loss reduced by 0.00870513916015625\n",
      "did one more step, loss reduced by 0.008695602416992188\n",
      "did one more step, loss reduced by 0.008687973022460938\n",
      "did one more step, loss reduced by 0.0086822509765625\n",
      "did one more step, loss reduced by 0.008676528930664062\n",
      "did one more step, loss reduced by 0.009630203247070312\n",
      "did one more step, loss reduced by 0.00872802734375\n",
      "did one more step, loss reduced by 0.00872039794921875\n",
      "did one more step, loss reduced by 0.0087127685546875\n",
      "did one more step, loss reduced by 0.008703231811523438\n",
      "did one more step, loss reduced by 0.008697509765625\n",
      "did one more step, loss reduced by 0.008687973022460938\n",
      "did one more step, loss reduced by 0.0086822509765625\n",
      "did one more step, loss reduced by 0.008672714233398438\n",
      "did one more step, loss reduced by 0.0086669921875\n",
      "did one more step, loss reduced by 0.008657455444335938\n",
      "did one more step, loss reduced by 0.0086517333984375\n",
      "did one more step, loss reduced by 0.009565353393554688\n",
      "did one more step, loss reduced by 0.008703231811523438\n",
      "did one more step, loss reduced by 0.008697509765625\n",
      "did one more step, loss reduced by 0.008691787719726562\n",
      "did one more step, loss reduced by 0.008678436279296875\n",
      "did one more step, loss reduced by 0.008672714233398438\n",
      "did one more step, loss reduced by 0.0086669921875\n",
      "did one more step, loss reduced by 0.008661270141601562\n",
      "did one more step, loss reduced by 0.008649826049804688\n",
      "did one more step, loss reduced by 0.008642196655273438\n",
      "did one more step, loss reduced by 0.008636474609375\n",
      "did one more step, loss reduced by 0.008630752563476562\n",
      "did one more step, loss reduced by 0.008617401123046875\n",
      "did one more step, loss reduced by 0.009557723999023438\n",
      "did one more step, loss reduced by 0.008672714233398438\n",
      "did one more step, loss reduced by 0.0086669921875\n",
      "did one more step, loss reduced by 0.00865936279296875\n",
      "did one more step, loss reduced by 0.0086517333984375\n",
      "did one more step, loss reduced by 0.008642196655273438\n",
      "did one more step, loss reduced by 0.008636474609375\n",
      "did one more step, loss reduced by 0.008626937866210938\n",
      "did one more step, loss reduced by 0.0086212158203125\n",
      "did one more step, loss reduced by 0.008611679077148438\n",
      "did one more step, loss reduced by 0.00860595703125\n",
      "did one more step, loss reduced by 0.008596420288085938\n",
      "did one more step, loss reduced by 0.0085906982421875\n",
      "did one more step, loss reduced by 0.009550094604492188\n",
      "did one more step, loss reduced by 0.008646011352539062\n",
      "did one more step, loss reduced by 0.008632659912109375\n",
      "did one more step, loss reduced by 0.008626937866210938\n",
      "did one more step, loss reduced by 0.0086212158203125\n",
      "did one more step, loss reduced by 0.008615493774414062\n",
      "did one more step, loss reduced by 0.008602142333984375\n",
      "did one more step, loss reduced by 0.00859832763671875\n",
      "did one more step, loss reduced by 0.0085906982421875\n",
      "did one more step, loss reduced by 0.00858306884765625\n",
      "did one more step, loss reduced by 0.008573532104492188\n",
      "did one more step, loss reduced by 0.008565902709960938\n",
      "did one more step, loss reduced by 0.009481430053710938\n",
      "did one more step, loss reduced by 0.0086212158203125\n",
      "did one more step, loss reduced by 0.008611679077148438\n",
      "did one more step, loss reduced by 0.00860595703125\n",
      "did one more step, loss reduced by 0.00859832763671875\n",
      "did one more step, loss reduced by 0.0085906982421875\n",
      "did one more step, loss reduced by 0.008581161499023438\n",
      "did one more step, loss reduced by 0.008575439453125\n",
      "did one more step, loss reduced by 0.008565902709960938\n",
      "did one more step, loss reduced by 0.0085601806640625\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.008544921875\n",
      "did one more step, loss reduced by 0.008535385131835938\n",
      "did one more step, loss reduced by 0.0094757080078125\n",
      "did one more step, loss reduced by 0.008588790893554688\n",
      "did one more step, loss reduced by 0.008581161499023438\n",
      "did one more step, loss reduced by 0.008575439453125\n",
      "did one more step, loss reduced by 0.008569717407226562\n",
      "did one more step, loss reduced by 0.008556365966796875\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.008544921875\n",
      "did one more step, loss reduced by 0.008539199829101562\n",
      "did one more step, loss reduced by 0.008527755737304688\n",
      "did one more step, loss reduced by 0.008520126342773438\n",
      "did one more step, loss reduced by 0.008514404296875\n",
      "did one more step, loss reduced by 0.008508682250976562\n",
      "did one more step, loss reduced by 0.009462356567382812\n",
      "did one more step, loss reduced by 0.0085601806640625\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.008544921875\n",
      "did one more step, loss reduced by 0.00853729248046875\n",
      "did one more step, loss reduced by 0.0085296630859375\n",
      "did one more step, loss reduced by 0.008520126342773438\n",
      "did one more step, loss reduced by 0.008514404296875\n",
      "did one more step, loss reduced by 0.008504867553710938\n",
      "did one more step, loss reduced by 0.0084991455078125\n",
      "did one more step, loss reduced by 0.008489608764648438\n",
      "did one more step, loss reduced by 0.00848388671875\n",
      "did one more step, loss reduced by 0.009397506713867188\n",
      "did one more step, loss reduced by 0.008535385131835938\n",
      "did one more step, loss reduced by 0.0085296630859375\n",
      "did one more step, loss reduced by 0.008523941040039062\n",
      "did one more step, loss reduced by 0.008510589599609375\n",
      "did one more step, loss reduced by 0.008504867553710938\n",
      "did one more step, loss reduced by 0.0084991455078125\n",
      "did one more step, loss reduced by 0.008493423461914062\n",
      "did one more step, loss reduced by 0.008480072021484375\n",
      "did one more step, loss reduced by 0.00847625732421875\n",
      "did one more step, loss reduced by 0.0084686279296875\n",
      "did one more step, loss reduced by 0.00846099853515625\n",
      "did one more step, loss reduced by 0.008451461791992188\n",
      "did one more step, loss reduced by 0.009389877319335938\n",
      "did one more step, loss reduced by 0.008504867553710938\n",
      "did one more step, loss reduced by 0.0084991455078125\n",
      "did one more step, loss reduced by 0.008489608764648438\n",
      "did one more step, loss reduced by 0.00848388671875\n",
      "did one more step, loss reduced by 0.00847625732421875\n",
      "did one more step, loss reduced by 0.0084686279296875\n",
      "did one more step, loss reduced by 0.008459091186523438\n",
      "did one more step, loss reduced by 0.008453369140625\n",
      "did one more step, loss reduced by 0.00844573974609375\n",
      "did one more step, loss reduced by 0.008434295654296875\n",
      "did one more step, loss reduced by 0.008432388305664062\n",
      "did one more step, loss reduced by 0.008419036865234375\n",
      "did one more step, loss reduced by 0.0093841552734375\n",
      "did one more step, loss reduced by 0.008474349975585938\n",
      "did one more step, loss reduced by 0.008470535278320312\n",
      "did one more step, loss reduced by 0.008457183837890625\n",
      "did one more step, loss reduced by 0.008455276489257812\n",
      "did one more step, loss reduced by 0.008441925048828125\n",
      "did one more step, loss reduced by 0.008440017700195312\n",
      "did one more step, loss reduced by 0.008426666259765625\n",
      "did one more step, loss reduced by 0.008424758911132812\n",
      "did one more step, loss reduced by 0.008413314819335938\n",
      "did one more step, loss reduced by 0.0084075927734375\n",
      "did one more step, loss reduced by 0.008398056030273438\n",
      "did one more step, loss reduced by 0.00931549072265625\n",
      "did one more step, loss reduced by 0.008449554443359375\n",
      "did one more step, loss reduced by 0.008447647094726562\n",
      "did one more step, loss reduced by 0.008434295654296875\n",
      "did one more step, loss reduced by 0.008432388305664062\n",
      "did one more step, loss reduced by 0.008420944213867188\n",
      "did one more step, loss reduced by 0.00841522216796875\n",
      "did one more step, loss reduced by 0.008405685424804688\n",
      "did one more step, loss reduced by 0.008401870727539062\n",
      "did one more step, loss reduced by 0.008388519287109375\n",
      "did one more step, loss reduced by 0.008386611938476562\n",
      "did one more step, loss reduced by 0.008373260498046875\n",
      "did one more step, loss reduced by 0.008371353149414062\n",
      "did one more step, loss reduced by 0.009304046630859375\n",
      "did one more step, loss reduced by 0.0084228515625\n",
      "did one more step, loss reduced by 0.008413314819335938\n",
      "did one more step, loss reduced by 0.008409500122070312\n",
      "did one more step, loss reduced by 0.008396148681640625\n",
      "did one more step, loss reduced by 0.008394241333007812\n",
      "did one more step, loss reduced by 0.008380889892578125\n",
      "did one more step, loss reduced by 0.008378982543945312\n",
      "did one more step, loss reduced by 0.008365631103515625\n",
      "did one more step, loss reduced by 0.008363723754882812\n",
      "did one more step, loss reduced by 0.008352279663085938\n",
      "did one more step, loss reduced by 0.0083465576171875\n",
      "did one more step, loss reduced by 0.008337020874023438\n",
      "did one more step, loss reduced by 0.00930023193359375\n",
      "did one more step, loss reduced by 0.008388519287109375\n",
      "did one more step, loss reduced by 0.008386611938476562\n",
      "did one more step, loss reduced by 0.008373260498046875\n",
      "did one more step, loss reduced by 0.008371353149414062\n",
      "did one more step, loss reduced by 0.008359909057617188\n",
      "did one more step, loss reduced by 0.00835418701171875\n",
      "did one more step, loss reduced by 0.008344650268554688\n",
      "did one more step, loss reduced by 0.008340835571289062\n",
      "did one more step, loss reduced by 0.008327484130859375\n",
      "did one more step, loss reduced by 0.008325576782226562\n",
      "did one more step, loss reduced by 0.008312225341796875\n",
      "did one more step, loss reduced by 0.0092315673828125\n",
      "did one more step, loss reduced by 0.008367538452148438\n",
      "did one more step, loss reduced by 0.016714096069335938\n",
      "did one more step, loss reduced by 0.016683578491210938\n",
      "did one more step, loss reduced by 0.016653060913085938\n",
      "did one more step, loss reduced by 0.016622543334960938\n",
      "did one more step, loss reduced by 0.01659393310546875\n",
      "did one more step, loss reduced by 0.016561508178710938\n",
      "did one more step, loss reduced by 0.017612457275390625\n",
      "did one more step, loss reduced by 0.016637802124023438\n",
      "did one more step, loss reduced by 0.01660919189453125\n",
      "did one more step, loss reduced by 0.016576766967773438\n",
      "did one more step, loss reduced by 0.016546249389648438\n",
      "did one more step, loss reduced by 0.016515731811523438\n",
      "did one more step, loss reduced by 0.017522811889648438\n",
      "did one more step, loss reduced by 0.016592025756835938\n",
      "did one more step, loss reduced by 0.016561508178710938\n",
      "did one more step, loss reduced by 0.016530990600585938\n",
      "did one more step, loss reduced by 0.016500473022460938\n",
      "did one more step, loss reduced by 0.01647186279296875\n",
      "did one more step, loss reduced by 0.017429351806640625\n",
      "did one more step, loss reduced by 0.016546249389648438\n",
      "did one more step, loss reduced by 0.016515731811523438\n",
      "did one more step, loss reduced by 0.01648712158203125\n",
      "did one more step, loss reduced by 0.016454696655273438\n",
      "did one more step, loss reduced by 0.016424179077148438\n",
      "did one more step, loss reduced by 0.016393661499023438\n",
      "did one more step, loss reduced by 0.017446517944335938\n",
      "did one more step, loss reduced by 0.016469955444335938\n",
      "did one more step, loss reduced by 0.016439437866210938\n",
      "did one more step, loss reduced by 0.016408920288085938\n",
      "did one more step, loss reduced by 0.016378402709960938\n",
      "did one more step, loss reduced by 0.01634979248046875\n",
      "did one more step, loss reduced by 0.017353057861328125\n",
      "did one more step, loss reduced by 0.016424179077148438\n",
      "did one more step, loss reduced by 0.016393661499023438\n",
      "did one more step, loss reduced by 0.01636505126953125\n",
      "did one more step, loss reduced by 0.016332626342773438\n",
      "did one more step, loss reduced by 0.016302108764648438\n",
      "did one more step, loss reduced by 0.017261505126953125\n",
      "did one more step, loss reduced by 0.01638031005859375\n",
      "did one more step, loss reduced by 0.016347885131835938\n",
      "did one more step, loss reduced by 0.016317367553710938\n",
      "did one more step, loss reduced by 0.016286849975585938\n",
      "did one more step, loss reduced by 0.016256332397460938\n",
      "did one more step, loss reduced by 0.01622772216796875\n",
      "did one more step, loss reduced by 0.017276763916015625\n",
      "did one more step, loss reduced by 0.016302108764648438\n",
      "did one more step, loss reduced by 0.016271591186523438\n",
      "did one more step, loss reduced by 0.01624298095703125\n",
      "did one more step, loss reduced by 0.016210556030273438\n",
      "did one more step, loss reduced by 0.016180038452148438\n",
      "did one more step, loss reduced by 0.017185211181640625\n",
      "did one more step, loss reduced by 0.01625823974609375\n",
      "did one more step, loss reduced by 0.016225814819335938\n",
      "did one more step, loss reduced by 0.016195297241210938\n",
      "did one more step, loss reduced by 0.016164779663085938\n",
      "did one more step, loss reduced by 0.016134262084960938\n",
      "did one more step, loss reduced by 0.017095565795898438\n",
      "did one more step, loss reduced by 0.016210556030273438\n",
      "did one more step, loss reduced by 0.016180038452148438\n",
      "did one more step, loss reduced by 0.016149520874023438\n",
      "did one more step, loss reduced by 0.01612091064453125\n",
      "did one more step, loss reduced by 0.016088485717773438\n",
      "did one more step, loss reduced by 0.016057968139648438\n",
      "did one more step, loss reduced by 0.017108917236328125\n",
      "did one more step, loss reduced by 0.01613616943359375\n",
      "did one more step, loss reduced by 0.016103744506835938\n",
      "did one more step, loss reduced by 0.016073226928710938\n",
      "did one more step, loss reduced by 0.016042709350585938\n",
      "did one more step, loss reduced by 0.016012191772460938\n",
      "did one more step, loss reduced by 0.017019271850585938\n",
      "did one more step, loss reduced by 0.016088485717773438\n",
      "did one more step, loss reduced by 0.016057968139648438\n",
      "did one more step, loss reduced by 0.016027450561523438\n",
      "did one more step, loss reduced by 0.01599884033203125\n",
      "did one more step, loss reduced by 0.015966415405273438\n",
      "did one more step, loss reduced by 0.016925811767578125\n",
      "did one more step, loss reduced by 0.016042709350585938\n",
      "did one more step, loss reduced by 0.01601409912109375\n",
      "did one more step, loss reduced by 0.015981674194335938\n",
      "did one more step, loss reduced by 0.015951156616210938\n",
      "did one more step, loss reduced by 0.015920639038085938\n",
      "did one more step, loss reduced by 0.015890121459960938\n",
      "did one more step, loss reduced by 0.016942977905273438\n",
      "did one more step, loss reduced by 0.015966415405273438\n",
      "did one more step, loss reduced by 0.015935897827148438\n",
      "did one more step, loss reduced by 0.015905380249023438\n",
      "did one more step, loss reduced by 0.01587677001953125\n",
      "did one more step, loss reduced by 0.015844345092773438\n",
      "did one more step, loss reduced by 0.016849517822265625\n",
      "did one more step, loss reduced by 0.015920639038085938\n",
      "did one more step, loss reduced by 0.01589202880859375\n",
      "did one more step, loss reduced by 0.015859603881835938\n",
      "did one more step, loss reduced by 0.015829086303710938\n",
      "did one more step, loss reduced by 0.015798568725585938\n",
      "did one more step, loss reduced by 0.016759872436523438\n",
      "did one more step, loss reduced by 0.015874862670898438\n",
      "did one more step, loss reduced by 0.015844345092773438\n",
      "did one more step, loss reduced by 0.015813827514648438\n",
      "did one more step, loss reduced by 0.015783309936523438\n",
      "did one more step, loss reduced by 0.01575469970703125\n",
      "did one more step, loss reduced by 0.015722274780273438\n",
      "did one more step, loss reduced by 0.016773223876953125\n",
      "did one more step, loss reduced by 0.015798568725585938\n",
      "did one more step, loss reduced by 0.01576995849609375\n",
      "did one more step, loss reduced by 0.015737533569335938\n",
      "did one more step, loss reduced by 0.015707015991210938\n",
      "did one more step, loss reduced by 0.015676498413085938\n",
      "did one more step, loss reduced by 0.016683578491210938\n",
      "did one more step, loss reduced by 0.015752792358398438\n",
      "did one more step, loss reduced by 0.015722274780273438\n",
      "did one more step, loss reduced by 0.015691757202148438\n",
      "did one more step, loss reduced by 0.015661239624023438\n",
      "did one more step, loss reduced by 0.01563262939453125\n",
      "did one more step, loss reduced by 0.016590118408203125\n",
      "did one more step, loss reduced by 0.015707015991210938\n",
      "did one more step, loss reduced by 0.015676498413085938\n",
      "did one more step, loss reduced by 0.01564788818359375\n",
      "did one more step, loss reduced by 0.015615463256835938\n",
      "did one more step, loss reduced by 0.015584945678710938\n",
      "did one more step, loss reduced by 0.015554428100585938\n",
      "did one more step, loss reduced by 0.016607284545898438\n",
      "did one more step, loss reduced by 0.015630722045898438\n",
      "did one more step, loss reduced by 0.015600204467773438\n",
      "did one more step, loss reduced by 0.015569686889648438\n",
      "did one more step, loss reduced by 0.015539169311523438\n",
      "did one more step, loss reduced by 0.01551055908203125\n",
      "did one more step, loss reduced by 0.016513824462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.015584945678710938\n",
      "did one more step, loss reduced by 0.015554428100585938\n",
      "did one more step, loss reduced by 0.01552581787109375\n",
      "did one more step, loss reduced by 0.015493392944335938\n",
      "did one more step, loss reduced by 0.015462875366210938\n",
      "did one more step, loss reduced by 0.016422271728515625\n",
      "did one more step, loss reduced by 0.01554107666015625\n",
      "did one more step, loss reduced by 0.015508651733398438\n",
      "did one more step, loss reduced by 0.015478134155273438\n",
      "did one more step, loss reduced by 0.015447616577148438\n",
      "did one more step, loss reduced by 0.015417098999023438\n",
      "did one more step, loss reduced by 0.01538848876953125\n",
      "did one more step, loss reduced by 0.016437530517578125\n",
      "did one more step, loss reduced by 0.015462875366210938\n",
      "did one more step, loss reduced by 0.015432357788085938\n",
      "did one more step, loss reduced by 0.01540374755859375\n",
      "did one more step, loss reduced by 0.015371322631835938\n",
      "did one more step, loss reduced by 0.015340805053710938\n",
      "did one more step, loss reduced by 0.016345977783203125\n",
      "did one more step, loss reduced by 0.01541900634765625\n",
      "did one more step, loss reduced by 0.015386581420898438\n",
      "did one more step, loss reduced by 0.015356063842773438\n",
      "did one more step, loss reduced by 0.015325546264648438\n",
      "did one more step, loss reduced by 0.015295028686523438\n",
      "did one more step, loss reduced by 0.016256332397460938\n",
      "did one more step, loss reduced by 0.015371322631835938\n",
      "did one more step, loss reduced by 0.015340805053710938\n",
      "did one more step, loss reduced by 0.015310287475585938\n",
      "did one more step, loss reduced by 0.01528167724609375\n",
      "did one more step, loss reduced by 0.015249252319335938\n",
      "did one more step, loss reduced by 0.015218734741210938\n",
      "did one more step, loss reduced by 0.016269683837890625\n",
      "did one more step, loss reduced by 0.01529693603515625\n",
      "did one more step, loss reduced by 0.015264511108398438\n",
      "did one more step, loss reduced by 0.015233993530273438\n",
      "did one more step, loss reduced by 0.015203475952148438\n",
      "did one more step, loss reduced by 0.015172958374023438\n",
      "did one more step, loss reduced by 0.016180038452148438\n",
      "did one more step, loss reduced by 0.015249252319335938\n",
      "did one more step, loss reduced by 0.015218734741210938\n",
      "did one more step, loss reduced by 0.015188217163085938\n",
      "did one more step, loss reduced by 0.01515960693359375\n",
      "did one more step, loss reduced by 0.015127182006835938\n",
      "did one more step, loss reduced by 0.016086578369140625\n",
      "did one more step, loss reduced by 0.015203475952148438\n",
      "did one more step, loss reduced by 0.01517486572265625\n",
      "did one more step, loss reduced by 0.015142440795898438\n",
      "did one more step, loss reduced by 0.015111923217773438\n",
      "did one more step, loss reduced by 0.015081405639648438\n",
      "did one more step, loss reduced by 0.015050888061523438\n",
      "did one more step, loss reduced by 0.016103744506835938\n",
      "did one more step, loss reduced by 0.015127182006835938\n",
      "did one more step, loss reduced by 0.015096664428710938\n",
      "did one more step, loss reduced by 0.015066146850585938\n",
      "did one more step, loss reduced by 0.01503753662109375\n",
      "did one more step, loss reduced by 0.015005111694335938\n",
      "did one more step, loss reduced by 0.016010284423828125\n",
      "did one more step, loss reduced by 0.015081405639648438\n",
      "did one more step, loss reduced by 0.01505279541015625\n",
      "did one more step, loss reduced by 0.015020370483398438\n",
      "did one more step, loss reduced by 0.014989852905273438\n",
      "did one more step, loss reduced by 0.014959335327148438\n",
      "did one more step, loss reduced by 0.015920639038085938\n",
      "did one more step, loss reduced by 0.015035629272460938\n",
      "did one more step, loss reduced by 0.015005111694335938\n",
      "did one more step, loss reduced by 0.014974594116210938\n",
      "did one more step, loss reduced by 0.014944076538085938\n",
      "did one more step, loss reduced by 0.01491546630859375\n",
      "did one more step, loss reduced by 0.014883041381835938\n",
      "did one more step, loss reduced by 0.015933990478515625\n",
      "did one more step, loss reduced by 0.014959335327148438\n",
      "did one more step, loss reduced by 0.01493072509765625\n",
      "did one more step, loss reduced by 0.014898300170898438\n",
      "did one more step, loss reduced by 0.014867782592773438\n",
      "did one more step, loss reduced by 0.014837265014648438\n",
      "did one more step, loss reduced by 0.015844345092773438\n",
      "did one more step, loss reduced by 0.014913558959960938\n",
      "did one more step, loss reduced by 0.014883041381835938\n",
      "did one more step, loss reduced by 0.014852523803710938\n",
      "did one more step, loss reduced by 0.014822006225585938\n",
      "did one more step, loss reduced by 0.01479339599609375\n",
      "did one more step, loss reduced by 0.015750885009765625\n",
      "did one more step, loss reduced by 0.014867782592773438\n",
      "did one more step, loss reduced by 0.014837265014648438\n",
      "did one more step, loss reduced by 0.01480865478515625\n",
      "did one more step, loss reduced by 0.014776229858398438\n",
      "did one more step, loss reduced by 0.014745712280273438\n",
      "did one more step, loss reduced by 0.014715194702148438\n",
      "did one more step, loss reduced by 0.015768051147460938\n",
      "did one more step, loss reduced by 0.014791488647460938\n",
      "did one more step, loss reduced by 0.014760971069335938\n",
      "did one more step, loss reduced by 0.014730453491210938\n",
      "did one more step, loss reduced by 0.014699935913085938\n",
      "did one more step, loss reduced by 0.01467132568359375\n",
      "did one more step, loss reduced by 0.015674591064453125\n",
      "did one more step, loss reduced by 0.014745712280273438\n",
      "did one more step, loss reduced by 0.014715194702148438\n",
      "did one more step, loss reduced by 0.01468658447265625\n",
      "did one more step, loss reduced by 0.014654159545898438\n",
      "did one more step, loss reduced by 0.014623641967773438\n",
      "did one more step, loss reduced by 0.015583038330078125\n",
      "did one more step, loss reduced by 0.01470184326171875\n",
      "did one more step, loss reduced by 0.014669418334960938\n",
      "did one more step, loss reduced by 0.014638900756835938\n",
      "did one more step, loss reduced by 0.014608383178710938\n",
      "did one more step, loss reduced by 0.014577865600585938\n",
      "did one more step, loss reduced by 0.01454925537109375\n",
      "did one more step, loss reduced by 0.015598297119140625\n",
      "did one more step, loss reduced by 0.014623641967773438\n",
      "did one more step, loss reduced by 0.014593124389648438\n",
      "did one more step, loss reduced by 0.01456451416015625\n",
      "did one more step, loss reduced by 0.014532089233398438\n",
      "did one more step, loss reduced by 0.014501571655273438\n",
      "did one more step, loss reduced by 0.015506744384765625\n",
      "did one more step, loss reduced by 0.01457977294921875\n",
      "did one more step, loss reduced by 0.014547348022460938\n",
      "did one more step, loss reduced by 0.014516830444335938\n",
      "did one more step, loss reduced by 0.014486312866210938\n",
      "did one more step, loss reduced by 0.014455795288085938\n",
      "did one more step, loss reduced by 0.015417098999023438\n",
      "did one more step, loss reduced by 0.014532089233398438\n",
      "did one more step, loss reduced by 0.014501571655273438\n",
      "did one more step, loss reduced by 0.014471054077148438\n",
      "did one more step, loss reduced by 0.01444244384765625\n",
      "did one more step, loss reduced by 0.014410018920898438\n",
      "did one more step, loss reduced by 0.014379501342773438\n",
      "did one more step, loss reduced by 0.015430450439453125\n",
      "did one more step, loss reduced by 0.01445770263671875\n",
      "did one more step, loss reduced by 0.014425277709960938\n",
      "did one more step, loss reduced by 0.014394760131835938\n",
      "did one more step, loss reduced by 0.014364242553710938\n",
      "did one more step, loss reduced by 0.014333724975585938\n",
      "did one more step, loss reduced by 0.015340805053710938\n",
      "did one more step, loss reduced by 0.014410018920898438\n",
      "did one more step, loss reduced by 0.014379501342773438\n",
      "did one more step, loss reduced by 0.014348983764648438\n",
      "did one more step, loss reduced by 0.01432037353515625\n",
      "did one more step, loss reduced by 0.014287948608398438\n",
      "did one more step, loss reduced by 0.015247344970703125\n",
      "did one more step, loss reduced by 0.014364242553710938\n",
      "did one more step, loss reduced by 0.01433563232421875\n",
      "did one more step, loss reduced by 0.014303207397460938\n",
      "did one more step, loss reduced by 0.014272689819335938\n",
      "did one more step, loss reduced by 0.014242172241210938\n",
      "did one more step, loss reduced by 0.014211654663085938\n",
      "did one more step, loss reduced by 0.015264511108398438\n",
      "did one more step, loss reduced by 0.014287948608398438\n",
      "did one more step, loss reduced by 0.014257431030273438\n",
      "did one more step, loss reduced by 0.014226913452148438\n",
      "did one more step, loss reduced by 0.01419830322265625\n",
      "did one more step, loss reduced by 0.014165878295898438\n",
      "did one more step, loss reduced by 0.015171051025390625\n",
      "did one more step, loss reduced by 0.014242172241210938\n",
      "did one more step, loss reduced by 0.01421356201171875\n",
      "did one more step, loss reduced by 0.014181137084960938\n",
      "did one more step, loss reduced by 0.014150619506835938\n",
      "did one more step, loss reduced by 0.014120101928710938\n",
      "did one more step, loss reduced by 0.015081405639648438\n",
      "did one more step, loss reduced by 0.014196395874023438\n",
      "did one more step, loss reduced by 0.014165878295898438\n",
      "did one more step, loss reduced by 0.014135360717773438\n",
      "did one more step, loss reduced by 0.014104843139648438\n",
      "did one more step, loss reduced by 0.01407623291015625\n",
      "did one more step, loss reduced by 0.014043807983398438\n",
      "did one more step, loss reduced by 0.015094757080078125\n",
      "did one more step, loss reduced by 0.014120101928710938\n",
      "did one more step, loss reduced by 0.01409149169921875\n",
      "did one more step, loss reduced by 0.014059066772460938\n",
      "did one more step, loss reduced by 0.014028549194335938\n",
      "did one more step, loss reduced by 0.013998031616210938\n",
      "did one more step, loss reduced by 0.015005111694335938\n",
      "did one more step, loss reduced by 0.014074325561523438\n",
      "did one more step, loss reduced by 0.014043807983398438\n",
      "did one more step, loss reduced by 0.014013290405273438\n",
      "did one more step, loss reduced by 0.013982772827148438\n",
      "did one more step, loss reduced by 0.01395416259765625\n",
      "did one more step, loss reduced by 0.014911651611328125\n",
      "did one more step, loss reduced by 0.014028549194335938\n",
      "did one more step, loss reduced by 0.013998031616210938\n",
      "did one more step, loss reduced by 0.01396942138671875\n",
      "did one more step, loss reduced by 0.013936996459960938\n",
      "did one more step, loss reduced by 0.013906478881835938\n",
      "did one more step, loss reduced by 0.013875961303710938\n",
      "did one more step, loss reduced by 0.014928817749023438\n",
      "did one more step, loss reduced by 0.013952255249023438\n",
      "did one more step, loss reduced by 0.013921737670898438\n",
      "did one more step, loss reduced by 0.013891220092773438\n",
      "did one more step, loss reduced by 0.013860702514648438\n",
      "did one more step, loss reduced by 0.01383209228515625\n",
      "did one more step, loss reduced by 0.014835357666015625\n",
      "did one more step, loss reduced by 0.013906478881835938\n",
      "did one more step, loss reduced by 0.013875961303710938\n",
      "did one more step, loss reduced by 0.01384735107421875\n",
      "did one more step, loss reduced by 0.013814926147460938\n",
      "did one more step, loss reduced by 0.013784408569335938\n",
      "did one more step, loss reduced by 0.014743804931640625\n",
      "did one more step, loss reduced by 0.01386260986328125\n",
      "did one more step, loss reduced by 0.013830184936523438\n",
      "did one more step, loss reduced by 0.013799667358398438\n",
      "did one more step, loss reduced by 0.013769149780273438\n",
      "did one more step, loss reduced by 0.013738632202148438\n",
      "did one more step, loss reduced by 0.01371002197265625\n",
      "did one more step, loss reduced by 0.014759063720703125\n",
      "did one more step, loss reduced by 0.013784408569335938\n",
      "did one more step, loss reduced by 0.013753890991210938\n",
      "did one more step, loss reduced by 0.01372528076171875\n",
      "did one more step, loss reduced by 0.013692855834960938\n",
      "did one more step, loss reduced by 0.013662338256835938\n",
      "did one more step, loss reduced by 0.014667510986328125\n",
      "did one more step, loss reduced by 0.01374053955078125\n",
      "did one more step, loss reduced by 0.013708114624023438\n",
      "did one more step, loss reduced by 0.013677597045898438\n",
      "did one more step, loss reduced by 0.013647079467773438\n",
      "did one more step, loss reduced by 0.013616561889648438\n",
      "did one more step, loss reduced by 0.014577865600585938\n",
      "did one more step, loss reduced by 0.013692855834960938\n",
      "did one more step, loss reduced by 0.013662338256835938\n",
      "did one more step, loss reduced by 0.013631820678710938\n",
      "did one more step, loss reduced by 0.01360321044921875\n",
      "did one more step, loss reduced by 0.013570785522460938\n",
      "did one more step, loss reduced by 0.013540267944335938\n",
      "did one more step, loss reduced by 0.014591217041015625\n",
      "did one more step, loss reduced by 0.01361846923828125\n",
      "did one more step, loss reduced by 0.013586044311523438\n",
      "did one more step, loss reduced by 0.013555526733398438\n",
      "did one more step, loss reduced by 0.013525009155273438\n",
      "did one more step, loss reduced by 0.013494491577148438\n",
      "did one more step, loss reduced by 0.014501571655273438\n",
      "did one more step, loss reduced by 0.013570785522460938\n",
      "did one more step, loss reduced by 0.013540267944335938\n",
      "did one more step, loss reduced by 0.013509750366210938\n",
      "did one more step, loss reduced by 0.01348114013671875\n",
      "did one more step, loss reduced by 0.013448715209960938\n",
      "did one more step, loss reduced by 0.014408111572265625\n",
      "did one more step, loss reduced by 0.013525009155273438\n",
      "did one more step, loss reduced by 0.01349639892578125\n",
      "did one more step, loss reduced by 0.013463973999023438\n",
      "did one more step, loss reduced by 0.013433456420898438\n",
      "did one more step, loss reduced by 0.013402938842773438\n",
      "did one more step, loss reduced by 0.013372421264648438\n",
      "did one more step, loss reduced by 0.014425277709960938\n",
      "did one more step, loss reduced by 0.013448715209960938\n",
      "did one more step, loss reduced by 0.013418197631835938\n",
      "did one more step, loss reduced by 0.013387680053710938\n",
      "did one more step, loss reduced by 0.01335906982421875\n",
      "did one more step, loss reduced by 0.013326644897460938\n",
      "did one more step, loss reduced by 0.014331817626953125\n",
      "did one more step, loss reduced by 0.013402938842773438\n",
      "did one more step, loss reduced by 0.01337432861328125\n",
      "did one more step, loss reduced by 0.013341903686523438\n",
      "did one more step, loss reduced by 0.013311386108398438\n",
      "did one more step, loss reduced by 0.013280868530273438\n",
      "did one more step, loss reduced by 0.014242172241210938\n",
      "did one more step, loss reduced by 0.013357162475585938\n",
      "did one more step, loss reduced by 0.013326644897460938\n",
      "did one more step, loss reduced by 0.013296127319335938\n",
      "did one more step, loss reduced by 0.013265609741210938\n",
      "did one more step, loss reduced by 0.01323699951171875\n",
      "did one more step, loss reduced by 0.013204574584960938\n",
      "did one more step, loss reduced by 0.014255523681640625\n",
      "did one more step, loss reduced by 0.013280868530273438\n",
      "did one more step, loss reduced by 0.01325225830078125\n",
      "did one more step, loss reduced by 0.013219833374023438\n",
      "did one more step, loss reduced by 0.013189315795898438\n",
      "did one more step, loss reduced by 0.013158798217773438\n",
      "did one more step, loss reduced by 0.014165878295898438\n",
      "did one more step, loss reduced by 0.013235092163085938\n",
      "did one more step, loss reduced by 0.013204574584960938\n",
      "did one more step, loss reduced by 0.013174057006835938\n",
      "did one more step, loss reduced by 0.013143539428710938\n",
      "did one more step, loss reduced by 0.01311492919921875\n",
      "did one more step, loss reduced by 0.014072418212890625\n",
      "did one more step, loss reduced by 0.013189315795898438\n",
      "did one more step, loss reduced by 0.013158798217773438\n",
      "did one more step, loss reduced by 0.01313018798828125\n",
      "did one more step, loss reduced by 0.013097763061523438\n",
      "did one more step, loss reduced by 0.013067245483398438\n",
      "did one more step, loss reduced by 0.013036727905273438\n",
      "did one more step, loss reduced by 0.014089584350585938\n",
      "did one more step, loss reduced by 0.013113021850585938\n",
      "did one more step, loss reduced by 0.013082504272460938\n",
      "did one more step, loss reduced by 0.013051986694335938\n",
      "did one more step, loss reduced by 0.013021469116210938\n",
      "did one more step, loss reduced by 0.01299285888671875\n",
      "did one more step, loss reduced by 0.013996124267578125\n",
      "did one more step, loss reduced by 0.013067245483398438\n",
      "did one more step, loss reduced by 0.013036727905273438\n",
      "did one more step, loss reduced by 0.01300811767578125\n",
      "did one more step, loss reduced by 0.012975692749023438\n",
      "did one more step, loss reduced by 0.012945175170898438\n",
      "did one more step, loss reduced by 0.013904571533203125\n",
      "did one more step, loss reduced by 0.01302337646484375\n",
      "did one more step, loss reduced by 0.012990951538085938\n",
      "did one more step, loss reduced by 0.012960433959960938\n",
      "did one more step, loss reduced by 0.012929916381835938\n",
      "did one more step, loss reduced by 0.012899398803710938\n",
      "did one more step, loss reduced by 0.01287078857421875\n",
      "did one more step, loss reduced by 0.013919830322265625\n",
      "did one more step, loss reduced by 0.012945175170898438\n",
      "did one more step, loss reduced by 0.012914657592773438\n",
      "did one more step, loss reduced by 0.01288604736328125\n",
      "did one more step, loss reduced by 0.012853622436523438\n",
      "did one more step, loss reduced by 0.012823104858398438\n",
      "did one more step, loss reduced by 0.013828277587890625\n",
      "did one more step, loss reduced by 0.01290130615234375\n",
      "did one more step, loss reduced by 0.012868881225585938\n",
      "did one more step, loss reduced by 0.012838363647460938\n",
      "did one more step, loss reduced by 0.012807846069335938\n",
      "did one more step, loss reduced by 0.012777328491210938\n",
      "did one more step, loss reduced by 0.013738632202148438\n",
      "did one more step, loss reduced by 0.012853622436523438\n",
      "did one more step, loss reduced by 0.012823104858398438\n",
      "did one more step, loss reduced by 0.012792587280273438\n",
      "did one more step, loss reduced by 0.01276397705078125\n",
      "did one more step, loss reduced by 0.012731552124023438\n",
      "did one more step, loss reduced by 0.0\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#now stopping criteria is difference between errors  \n",
    "learning_rait = 0.0006 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "y_pred = predict(w, x)\n",
    "previous_error = mseerror(y, y_pred)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "error = mseerror(y, y_pred)\n",
    "y_pred = predict(w, x)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "previous_error = error\n",
    "error = mseerror(y, y_pred)\n",
    "while previous_error - error > 0.0000000005:\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    previous_error = error \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'did one more step, loss reduced by {previous_error - error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad64464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative. Slope = 5.41015625, intercept = 5.7265625, loss = 18.56068229675293')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "688ca691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1181.300048828125, weights = tensor([0.2157, 0.0380], dtype=torch.float16, requires_grad=True)\n",
      "step №1: loss = 1102.725341796875, weights = tensor([0.4238, 0.0748], dtype=torch.float16, requires_grad=True)\n",
      "step №2: loss = 1029.557861328125, weights = tensor([0.6245, 0.1105], dtype=torch.float16, requires_grad=True)\n",
      "step №3: loss = 961.4700927734375, weights = tensor([0.8184, 0.1450], dtype=torch.float16, requires_grad=True)\n",
      "step №4: loss = 897.9996948242188, weights = tensor([1.0059, 0.1785], dtype=torch.float16, requires_grad=True)\n",
      "step №5: loss = 838.7605590820312, weights = tensor([1.1865, 0.2108], dtype=torch.float16, requires_grad=True)\n",
      "step №6: loss = 783.6795654296875, weights = tensor([1.3604, 0.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №7: loss = 732.5275268554688, weights = tensor([1.5283, 0.2727], dtype=torch.float16, requires_grad=True)\n",
      "step №8: loss = 684.8211669921875, weights = tensor([1.6904, 0.3022], dtype=torch.float16, requires_grad=True)\n",
      "step №9: loss = 640.3889770507812, weights = tensor([1.8467, 0.3308], dtype=torch.float16, requires_grad=True)\n",
      "step №10: loss = 599.0595703125, weights = tensor([1.9971, 0.3584], dtype=torch.float16, requires_grad=True)\n",
      "step №11: loss = 560.66748046875, weights = tensor([2.1426, 0.3853], dtype=torch.float16, requires_grad=True)\n",
      "step №12: loss = 524.814453125, weights = tensor([2.2832, 0.4111], dtype=torch.float16, requires_grad=True)\n",
      "step №13: loss = 491.38262939453125, weights = tensor([2.4180, 0.4363], dtype=torch.float16, requires_grad=True)\n",
      "step №14: loss = 460.4501953125, weights = tensor([2.5488, 0.4607], dtype=torch.float16, requires_grad=True)\n",
      "step №15: loss = 431.4646911621094, weights = tensor([2.6758, 0.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №16: loss = 404.3334655761719, weights = tensor([2.7969, 0.5073], dtype=torch.float16, requires_grad=True)\n",
      "step №17: loss = 379.34857177734375, weights = tensor([2.9141, 0.5298], dtype=torch.float16, requires_grad=True)\n",
      "step №18: loss = 356.0050354003906, weights = tensor([3.0273, 0.5513], dtype=torch.float16, requires_grad=True)\n",
      "step №19: loss = 334.2376403808594, weights = tensor([3.1367, 0.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №20: loss = 313.94873046875, weights = tensor([3.2422, 0.5928], dtype=torch.float16, requires_grad=True)\n",
      "step №21: loss = 295.06158447265625, weights = tensor([3.3438, 0.6128], dtype=torch.float16, requires_grad=True)\n",
      "step №22: loss = 277.5021057128906, weights = tensor([3.4414, 0.6318], dtype=torch.float16, requires_grad=True)\n",
      "step №23: loss = 261.21417236328125, weights = tensor([3.5352, 0.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №24: loss = 246.11306762695312, weights = tensor([3.6270, 0.6685], dtype=torch.float16, requires_grad=True)\n",
      "step №25: loss = 231.84597778320312, weights = tensor([3.7148, 0.6860], dtype=torch.float16, requires_grad=True)\n",
      "step №26: loss = 218.65664672851562, weights = tensor([3.8008, 0.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №27: loss = 206.2163848876953, weights = tensor([3.8828, 0.7197], dtype=torch.float16, requires_grad=True)\n",
      "step №28: loss = 194.75167846679688, weights = tensor([3.9609, 0.7358], dtype=torch.float16, requires_grad=True)\n",
      "step №29: loss = 184.2048797607422, weights = tensor([4.0391, 0.7515], dtype=torch.float16, requires_grad=True)\n",
      "step №30: loss = 174.04161071777344, weights = tensor([4.1133, 0.7666], dtype=torch.float16, requires_grad=True)\n",
      "step №31: loss = 164.72230529785156, weights = tensor([4.1836, 0.7817], dtype=torch.float16, requires_grad=True)\n",
      "step №32: loss = 156.18289184570312, weights = tensor([4.2539, 0.7964], dtype=torch.float16, requires_grad=True)\n",
      "step №33: loss = 147.95639038085938, weights = tensor([4.3203, 0.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №34: loss = 140.45562744140625, weights = tensor([4.3828, 0.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №35: loss = 133.63385009765625, weights = tensor([4.4453, 0.8374], dtype=torch.float16, requires_grad=True)\n",
      "step №36: loss = 127.06108093261719, weights = tensor([4.5039, 0.8506], dtype=torch.float16, requires_grad=True)\n",
      "step №37: loss = 121.0976791381836, weights = tensor([4.5625, 0.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №38: loss = 115.35428619384766, weights = tensor([4.6172, 0.8755], dtype=torch.float16, requires_grad=True)\n",
      "step №39: loss = 110.17552185058594, weights = tensor([4.6719, 0.8877], dtype=torch.float16, requires_grad=True)\n",
      "step №40: loss = 105.17955017089844, weights = tensor([4.7227, 0.8994], dtype=torch.float16, requires_grad=True)\n",
      "step №41: loss = 100.69676208496094, weights = tensor([4.7734, 0.9106], dtype=torch.float16, requires_grad=True)\n",
      "step №42: loss = 96.38104248046875, weights = tensor([4.8203, 0.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №43: loss = 92.52139282226562, weights = tensor([4.8672, 0.9326], dtype=torch.float16, requires_grad=True)\n",
      "step №44: loss = 88.80538177490234, weights = tensor([4.9102, 0.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №45: loss = 85.501708984375, weights = tensor([4.9531, 0.9536], dtype=torch.float16, requires_grad=True)\n",
      "step №46: loss = 82.32008361816406, weights = tensor([4.9961, 0.9639], dtype=torch.float16, requires_grad=True)\n",
      "step №47: loss = 79.25183868408203, weights = tensor([5.0352, 0.9736], dtype=torch.float16, requires_grad=True)\n",
      "step №48: loss = 76.55375671386719, weights = tensor([5.0742, 0.9834], dtype=torch.float16, requires_grad=True)\n",
      "step №49: loss = 73.94969177246094, weights = tensor([5.1094, 0.9927], dtype=torch.float16, requires_grad=True)\n",
      "step №50: loss = 71.67890930175781, weights = tensor([5.1445, 1.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №51: loss = 69.48460388183594, weights = tensor([5.1797, 1.0107], dtype=torch.float16, requires_grad=True)\n",
      "step №52: loss = 67.37400817871094, weights = tensor([5.2109, 1.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №53: loss = 65.54773712158203, weights = tensor([5.2422, 1.0283], dtype=torch.float16, requires_grad=True)\n",
      "step №54: loss = 63.7822380065918, weights = tensor([5.2734, 1.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №55: loss = 62.07749557495117, weights = tensor([5.3047, 1.0459], dtype=torch.float16, requires_grad=True)\n",
      "step №56: loss = 60.43351364135742, weights = tensor([5.3320, 1.0537], dtype=torch.float16, requires_grad=True)\n",
      "step №57: loss = 59.04325485229492, weights = tensor([5.3594, 1.0615], dtype=torch.float16, requires_grad=True)\n",
      "step №58: loss = 57.6995735168457, weights = tensor([5.3867, 1.0693], dtype=torch.float16, requires_grad=True)\n",
      "step №59: loss = 56.40248489379883, weights = tensor([5.4141, 1.0771], dtype=torch.float16, requires_grad=True)\n",
      "step №60: loss = 55.1519775390625, weights = tensor([5.4375, 1.0850], dtype=torch.float16, requires_grad=True)\n",
      "step №61: loss = 54.10346603393555, weights = tensor([5.4609, 1.0928], dtype=torch.float16, requires_grad=True)\n",
      "step №62: loss = 53.089691162109375, weights = tensor([5.4844, 1.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №63: loss = 52.1222038269043, weights = tensor([5.5078, 1.1064], dtype=torch.float16, requires_grad=True)\n",
      "step №64: loss = 51.18901443481445, weights = tensor([5.5273, 1.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №65: loss = 50.42451858520508, weights = tensor([5.5469, 1.1201], dtype=torch.float16, requires_grad=True)\n",
      "step №66: loss = 49.68427276611328, weights = tensor([5.5664, 1.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №67: loss = 48.96825408935547, weights = tensor([5.5859, 1.1338], dtype=torch.float16, requires_grad=True)\n",
      "step №68: loss = 48.276485443115234, weights = tensor([5.6055, 1.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №69: loss = 47.60895538330078, weights = tensor([5.6250, 1.1475], dtype=torch.float16, requires_grad=True)\n",
      "step №70: loss = 46.965667724609375, weights = tensor([5.6406, 1.1533], dtype=torch.float16, requires_grad=True)\n",
      "step №71: loss = 46.464447021484375, weights = tensor([5.6562, 1.1592], dtype=torch.float16, requires_grad=True)\n",
      "step №72: loss = 45.97886657714844, weights = tensor([5.6719, 1.1650], dtype=torch.float16, requires_grad=True)\n",
      "step №73: loss = 45.50892639160156, weights = tensor([5.6875, 1.1709], dtype=torch.float16, requires_grad=True)\n",
      "step №74: loss = 45.05460739135742, weights = tensor([5.7031, 1.1768], dtype=torch.float16, requires_grad=True)\n",
      "step №75: loss = 44.615928649902344, weights = tensor([5.7188, 1.1826], dtype=torch.float16, requires_grad=True)\n",
      "step №76: loss = 44.19287872314453, weights = tensor([5.7344, 1.1885], dtype=torch.float16, requires_grad=True)\n",
      "step №77: loss = 43.785457611083984, weights = tensor([5.7461, 1.1943], dtype=torch.float16, requires_grad=True)\n",
      "step №78: loss = 43.476531982421875, weights = tensor([5.7578, 1.2002], dtype=torch.float16, requires_grad=True)\n",
      "step №79: loss = 43.17674255371094, weights = tensor([5.7695, 1.2061], dtype=torch.float16, requires_grad=True)\n",
      "step №80: loss = 42.886085510253906, weights = tensor([5.7812, 1.2119], dtype=torch.float16, requires_grad=True)\n",
      "step №81: loss = 42.60456085205078, weights = tensor([5.7930, 1.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №82: loss = 42.340782165527344, weights = tensor([5.8047, 1.2217], dtype=torch.float16, requires_grad=True)\n",
      "step №83: loss = 42.085914611816406, weights = tensor([5.8164, 1.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №84: loss = 41.8399543762207, weights = tensor([5.8281, 1.2314], dtype=torch.float16, requires_grad=True)\n",
      "step №85: loss = 41.6028938293457, weights = tensor([5.8398, 1.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №86: loss = 41.3747444152832, weights = tensor([5.8477, 1.2412], dtype=torch.float16, requires_grad=True)\n",
      "step №87: loss = 41.2140998840332, weights = tensor([5.8555, 1.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №88: loss = 41.05766677856445, weights = tensor([5.8633, 1.2510], dtype=torch.float16, requires_grad=True)\n",
      "step №89: loss = 40.90544891357422, weights = tensor([5.8711, 1.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №90: loss = 40.75743865966797, weights = tensor([5.8789, 1.2607], dtype=torch.float16, requires_grad=True)\n",
      "step №91: loss = 40.613651275634766, weights = tensor([5.8867, 1.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №92: loss = 40.47406768798828, weights = tensor([5.8945, 1.2705], dtype=torch.float16, requires_grad=True)\n",
      "step №93: loss = 40.33869934082031, weights = tensor([5.9023, 1.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №94: loss = 40.207542419433594, weights = tensor([5.9102, 1.2803], dtype=torch.float16, requires_grad=True)\n",
      "step №95: loss = 40.08060836791992, weights = tensor([5.9180, 1.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №96: loss = 39.95787811279297, weights = tensor([5.9258, 1.2900], dtype=torch.float16, requires_grad=True)\n",
      "step №97: loss = 39.83936309814453, weights = tensor([5.9336, 1.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №98: loss = 39.725067138671875, weights = tensor([5.9375, 1.2998], dtype=torch.float16, requires_grad=True)\n",
      "step №99: loss = 39.651512145996094, weights = tensor([5.9414, 1.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №100: loss = 39.5792236328125, weights = tensor([5.9453, 1.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №101: loss = 39.515296936035156, weights = tensor([5.9492, 1.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №102: loss = 39.452545166015625, weights = tensor([5.9531, 1.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №103: loss = 39.390968322753906, weights = tensor([5.9570, 1.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №104: loss = 39.33056640625, weights = tensor([5.9609, 1.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №105: loss = 39.271339416503906, weights = tensor([5.9648, 1.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №106: loss = 39.213287353515625, weights = tensor([5.9688, 1.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №107: loss = 39.156410217285156, weights = tensor([5.9727, 1.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №108: loss = 39.1007080078125, weights = tensor([5.9766, 1.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №109: loss = 39.046180725097656, weights = tensor([5.9805, 1.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №110: loss = 38.992828369140625, weights = tensor([5.9844, 1.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №111: loss = 38.940650939941406, weights = tensor([5.9883, 1.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №112: loss = 38.8896484375, weights = tensor([5.9922, 1.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №113: loss = 38.839820861816406, weights = tensor([5.9961, 1.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №114: loss = 38.791168212890625, weights = tensor([6.0000, 1.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №115: loss = 38.743690490722656, weights = tensor([6.0039, 1.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №116: loss = 38.6973876953125, weights = tensor([6.0078, 1.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №117: loss = 38.652259826660156, weights = tensor([6.0117, 1.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №118: loss = 38.608306884765625, weights = tensor([6.0156, 1.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №119: loss = 38.565528869628906, weights = tensor([6.0195, 1.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №120: loss = 38.52392578125, weights = tensor([6.0234, 1.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №121: loss = 38.483497619628906, weights = tensor([6.0273, 1.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №122: loss = 38.444244384765625, weights = tensor([6.0312, 1.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №123: loss = 38.406166076660156, weights = tensor([6.0352, 1.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №124: loss = 38.3692626953125, weights = tensor([6.0352, 1.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №125: loss = 38.34471893310547, weights = tensor([6.0352, 1.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №126: loss = 38.32020950317383, weights = tensor([6.0352, 1.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №127: loss = 38.29572677612305, weights = tensor([6.0352, 1.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №128: loss = 38.271278381347656, weights = tensor([6.0352, 1.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №129: loss = 38.246856689453125, weights = tensor([6.0352, 1.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №130: loss = 38.22246551513672, weights = tensor([6.0352, 1.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №131: loss = 38.1981086730957, weights = tensor([6.0352, 1.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №132: loss = 38.17377853393555, weights = tensor([6.0352, 1.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №133: loss = 38.14948272705078, weights = tensor([6.0352, 1.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №134: loss = 38.125213623046875, weights = tensor([6.0352, 1.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №135: loss = 38.100975036621094, weights = tensor([6.0352, 1.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №136: loss = 38.0767707824707, weights = tensor([6.0352, 1.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №137: loss = 38.05259323120117, weights = tensor([6.0352, 1.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №138: loss = 38.02845001220703, weights = tensor([6.0352, 1.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №139: loss = 38.00433349609375, weights = tensor([6.0352, 1.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №140: loss = 37.980247497558594, weights = tensor([6.0352, 1.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №141: loss = 37.95619583129883, weights = tensor([6.0352, 1.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №142: loss = 37.93217086791992, weights = tensor([6.0352, 1.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №143: loss = 37.908180236816406, weights = tensor([6.0352, 1.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №144: loss = 37.88421630859375, weights = tensor([6.0352, 1.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №145: loss = 37.86028289794922, weights = tensor([6.0352, 1.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №146: loss = 37.83638381958008, weights = tensor([6.0352, 1.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №147: loss = 37.8125114440918, weights = tensor([6.0352, 1.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №148: loss = 37.788673400878906, weights = tensor([6.0352, 1.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №149: loss = 37.764862060546875, weights = tensor([6.0352, 1.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №150: loss = 37.74108123779297, weights = tensor([6.0352, 1.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №151: loss = 37.71733474731445, weights = tensor([6.0352, 1.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №152: loss = 37.6936149597168, weights = tensor([6.0352, 1.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №153: loss = 37.66992950439453, weights = tensor([6.0352, 1.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №154: loss = 37.646270751953125, weights = tensor([6.0352, 1.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №155: loss = 37.622642517089844, weights = tensor([6.0352, 1.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №156: loss = 37.59904861450195, weights = tensor([6.0352, 1.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №157: loss = 37.57548141479492, weights = tensor([6.0352, 1.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №158: loss = 37.55194854736328, weights = tensor([6.0352, 1.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №159: loss = 37.5284423828125, weights = tensor([6.0352, 1.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №160: loss = 37.504966735839844, weights = tensor([6.0352, 1.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №161: loss = 37.48152542114258, weights = tensor([6.0352, 1.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №162: loss = 37.45811080932617, weights = tensor([6.0352, 1.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №163: loss = 37.434730529785156, weights = tensor([6.0352, 1.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №164: loss = 37.411376953125, weights = tensor([6.0352, 1.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №165: loss = 37.38805389404297, weights = tensor([6.0352, 1.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №166: loss = 37.36476516723633, weights = tensor([6.0352, 1.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №167: loss = 37.34150314331055, weights = tensor([6.0352, 1.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №168: loss = 37.318275451660156, weights = tensor([6.0352, 1.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №169: loss = 37.295074462890625, weights = tensor([6.0352, 1.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №170: loss = 37.27190399169922, weights = tensor([6.0352, 1.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №171: loss = 37.2487678527832, weights = tensor([6.0352, 1.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №172: loss = 37.22565841674805, weights = tensor([6.0352, 1.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №173: loss = 37.20258331298828, weights = tensor([6.0352, 1.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №174: loss = 37.179534912109375, weights = tensor([6.0352, 1.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №175: loss = 37.156517028808594, weights = tensor([6.0352, 1.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №176: loss = 37.1335334777832, weights = tensor([6.0352, 1.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №177: loss = 37.11057662963867, weights = tensor([6.0352, 1.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №178: loss = 37.08765411376953, weights = tensor([6.0352, 1.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №179: loss = 37.06475830078125, weights = tensor([6.0352, 1.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №180: loss = 37.041893005371094, weights = tensor([6.0352, 1.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №181: loss = 37.01906204223633, weights = tensor([6.0352, 1.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №182: loss = 36.99625778198242, weights = tensor([6.0352, 1.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №183: loss = 36.973487854003906, weights = tensor([6.0352, 1.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №184: loss = 36.95074462890625, weights = tensor([6.0352, 1.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №185: loss = 36.92803192138672, weights = tensor([6.0352, 1.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №186: loss = 36.90535354614258, weights = tensor([6.0352, 1.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №187: loss = 36.8827018737793, weights = tensor([6.0352, 1.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №188: loss = 36.860084533691406, weights = tensor([6.0352, 1.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №189: loss = 36.837493896484375, weights = tensor([6.0352, 1.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №190: loss = 36.81493377685547, weights = tensor([6.0352, 1.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №191: loss = 36.79240798950195, weights = tensor([6.0352, 1.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №192: loss = 36.7699089050293, weights = tensor([6.0352, 1.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №193: loss = 36.74744415283203, weights = tensor([6.0352, 1.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №194: loss = 36.725006103515625, weights = tensor([6.0352, 1.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №195: loss = 36.702598571777344, weights = tensor([6.0352, 1.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №196: loss = 36.68022537231445, weights = tensor([6.0352, 1.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №197: loss = 36.65787887573242, weights = tensor([6.0352, 1.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №198: loss = 36.63556671142578, weights = tensor([6.0352, 1.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №199: loss = 36.61328125, weights = tensor([6.0352, 1.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №200: loss = 36.591026306152344, weights = tensor([6.0352, 1.6982], dtype=torch.float16, requires_grad=True)\n",
      "step №201: loss = 36.574363708496094, weights = tensor([6.0352, 1.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №202: loss = 36.55770492553711, weights = tensor([6.0352, 1.7041], dtype=torch.float16, requires_grad=True)\n",
      "step №203: loss = 36.54106903076172, weights = tensor([6.0352, 1.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №204: loss = 36.524452209472656, weights = tensor([6.0352, 1.7100], dtype=torch.float16, requires_grad=True)\n",
      "step №205: loss = 36.507850646972656, weights = tensor([6.0352, 1.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №206: loss = 36.49126434326172, weights = tensor([6.0352, 1.7158], dtype=torch.float16, requires_grad=True)\n",
      "step №207: loss = 36.474700927734375, weights = tensor([6.0352, 1.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №208: loss = 36.45814895629883, weights = tensor([6.0352, 1.7217], dtype=torch.float16, requires_grad=True)\n",
      "step №209: loss = 36.44161605834961, weights = tensor([6.0352, 1.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №210: loss = 36.42510223388672, weights = tensor([6.0352, 1.7275], dtype=torch.float16, requires_grad=True)\n",
      "step №211: loss = 36.40860366821289, weights = tensor([6.0352, 1.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №212: loss = 36.392120361328125, weights = tensor([6.0352, 1.7334], dtype=torch.float16, requires_grad=True)\n",
      "step №213: loss = 36.37565994262695, weights = tensor([6.0352, 1.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №214: loss = 36.35921096801758, weights = tensor([6.0352, 1.7393], dtype=torch.float16, requires_grad=True)\n",
      "step №215: loss = 36.34278106689453, weights = tensor([6.0352, 1.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №216: loss = 36.32636642456055, weights = tensor([6.0352, 1.7451], dtype=torch.float16, requires_grad=True)\n",
      "step №217: loss = 36.309974670410156, weights = tensor([6.0352, 1.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №218: loss = 36.29359436035156, weights = tensor([6.0352, 1.7510], dtype=torch.float16, requires_grad=True)\n",
      "step №219: loss = 36.2772331237793, weights = tensor([6.0352, 1.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №220: loss = 36.260887145996094, weights = tensor([6.0352, 1.7568], dtype=torch.float16, requires_grad=True)\n",
      "step №221: loss = 36.244564056396484, weights = tensor([6.0352, 1.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №222: loss = 36.22825241088867, weights = tensor([6.0352, 1.7627], dtype=torch.float16, requires_grad=True)\n",
      "step №223: loss = 36.21196365356445, weights = tensor([6.0352, 1.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №224: loss = 36.19568634033203, weights = tensor([6.0352, 1.7686], dtype=torch.float16, requires_grad=True)\n",
      "step №225: loss = 36.17942810058594, weights = tensor([6.0352, 1.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №226: loss = 36.163185119628906, weights = tensor([6.0352, 1.7744], dtype=torch.float16, requires_grad=True)\n",
      "step №227: loss = 36.1469612121582, weights = tensor([6.0352, 1.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №228: loss = 36.13075637817383, weights = tensor([6.0352, 1.7803], dtype=torch.float16, requires_grad=True)\n",
      "step №229: loss = 36.11457061767578, weights = tensor([6.0352, 1.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №230: loss = 36.09839630126953, weights = tensor([6.0352, 1.7861], dtype=torch.float16, requires_grad=True)\n",
      "step №231: loss = 36.08224105834961, weights = tensor([6.0352, 1.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №232: loss = 36.06610107421875, weights = tensor([6.0352, 1.7920], dtype=torch.float16, requires_grad=True)\n",
      "step №233: loss = 36.049983978271484, weights = tensor([6.0352, 1.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №234: loss = 36.033878326416016, weights = tensor([6.0352, 1.7979], dtype=torch.float16, requires_grad=True)\n",
      "step №235: loss = 36.017791748046875, weights = tensor([6.0352, 1.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №236: loss = 36.0017204284668, weights = tensor([6.0352, 1.8037], dtype=torch.float16, requires_grad=True)\n",
      "step №237: loss = 35.98566818237305, weights = tensor([6.0352, 1.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №238: loss = 35.969635009765625, weights = tensor([6.0352, 1.8096], dtype=torch.float16, requires_grad=True)\n",
      "step №239: loss = 35.95362091064453, weights = tensor([6.0352, 1.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №240: loss = 35.93761444091797, weights = tensor([6.0352, 1.8154], dtype=torch.float16, requires_grad=True)\n",
      "step №241: loss = 35.921634674072266, weights = tensor([6.0352, 1.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №242: loss = 35.90566635131836, weights = tensor([6.0352, 1.8213], dtype=torch.float16, requires_grad=True)\n",
      "step №243: loss = 35.88971710205078, weights = tensor([6.0352, 1.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №244: loss = 35.87378692626953, weights = tensor([6.0352, 1.8271], dtype=torch.float16, requires_grad=True)\n",
      "step №245: loss = 35.857872009277344, weights = tensor([6.0352, 1.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №246: loss = 35.84197235107422, weights = tensor([6.0352, 1.8330], dtype=torch.float16, requires_grad=True)\n",
      "step №247: loss = 35.82609558105469, weights = tensor([6.0352, 1.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №248: loss = 35.81023025512695, weights = tensor([6.0352, 1.8389], dtype=torch.float16, requires_grad=True)\n",
      "step №249: loss = 35.79438781738281, weights = tensor([6.0352, 1.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №250: loss = 35.77855682373047, weights = tensor([6.0352, 1.8447], dtype=torch.float16, requires_grad=True)\n",
      "step №251: loss = 35.76274490356445, weights = tensor([6.0352, 1.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №252: loss = 35.7469482421875, weights = tensor([6.0352, 1.8506], dtype=torch.float16, requires_grad=True)\n",
      "step №253: loss = 35.73117446899414, weights = tensor([6.0352, 1.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №254: loss = 35.71541213989258, weights = tensor([6.0352, 1.8564], dtype=torch.float16, requires_grad=True)\n",
      "step №255: loss = 35.699668884277344, weights = tensor([6.0352, 1.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №256: loss = 35.68394088745117, weights = tensor([6.0352, 1.8623], dtype=torch.float16, requires_grad=True)\n",
      "step №257: loss = 35.668235778808594, weights = tensor([6.0352, 1.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №258: loss = 35.65254211425781, weights = tensor([6.0352, 1.8682], dtype=torch.float16, requires_grad=True)\n",
      "step №259: loss = 35.636863708496094, weights = tensor([6.0352, 1.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №260: loss = 35.62120819091797, weights = tensor([6.0352, 1.8740], dtype=torch.float16, requires_grad=True)\n",
      "step №261: loss = 35.60557174682617, weights = tensor([6.0352, 1.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №262: loss = 35.58994674682617, weights = tensor([6.0352, 1.8799], dtype=torch.float16, requires_grad=True)\n",
      "step №263: loss = 35.574344635009766, weights = tensor([6.0352, 1.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №264: loss = 35.558753967285156, weights = tensor([6.0352, 1.8857], dtype=torch.float16, requires_grad=True)\n",
      "step №265: loss = 35.54318618774414, weights = tensor([6.0352, 1.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №266: loss = 35.527626037597656, weights = tensor([6.0352, 1.8916], dtype=torch.float16, requires_grad=True)\n",
      "step №267: loss = 35.51209259033203, weights = tensor([6.0352, 1.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №268: loss = 35.4965705871582, weights = tensor([6.0352, 1.8975], dtype=torch.float16, requires_grad=True)\n",
      "step №269: loss = 35.4810676574707, weights = tensor([6.0352, 1.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №270: loss = 35.46558380126953, weights = tensor([6.0352, 1.9033], dtype=torch.float16, requires_grad=True)\n",
      "step №271: loss = 35.45011520385742, weights = tensor([6.0352, 1.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №272: loss = 35.434661865234375, weights = tensor([6.0352, 1.9092], dtype=torch.float16, requires_grad=True)\n",
      "step №273: loss = 35.419227600097656, weights = tensor([6.0352, 1.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №274: loss = 35.403812408447266, weights = tensor([6.0352, 1.9150], dtype=torch.float16, requires_grad=True)\n",
      "step №275: loss = 35.38841247558594, weights = tensor([6.0352, 1.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №276: loss = 35.37302780151367, weights = tensor([6.0352, 1.9209], dtype=torch.float16, requires_grad=True)\n",
      "step №277: loss = 35.357666015625, weights = tensor([6.0352, 1.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №278: loss = 35.342315673828125, weights = tensor([6.0352, 1.9268], dtype=torch.float16, requires_grad=True)\n",
      "step №279: loss = 35.326988220214844, weights = tensor([6.0352, 1.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №280: loss = 35.311668395996094, weights = tensor([6.0352, 1.9326], dtype=torch.float16, requires_grad=True)\n",
      "step №281: loss = 35.29637908935547, weights = tensor([6.0352, 1.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №282: loss = 35.28109359741211, weights = tensor([6.0352, 1.9385], dtype=torch.float16, requires_grad=True)\n",
      "step №283: loss = 35.265830993652344, weights = tensor([6.0352, 1.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №284: loss = 35.250587463378906, weights = tensor([6.0352, 1.9443], dtype=torch.float16, requires_grad=True)\n",
      "step №285: loss = 35.23535919189453, weights = tensor([6.0352, 1.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №286: loss = 35.22014617919922, weights = tensor([6.0352, 1.9502], dtype=torch.float16, requires_grad=True)\n",
      "step №287: loss = 35.2049560546875, weights = tensor([6.0352, 1.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №288: loss = 35.18977737426758, weights = tensor([6.0352, 1.9561], dtype=torch.float16, requires_grad=True)\n",
      "step №289: loss = 35.174617767333984, weights = tensor([6.0352, 1.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №290: loss = 35.15947723388672, weights = tensor([6.0352, 1.9619], dtype=torch.float16, requires_grad=True)\n",
      "step №291: loss = 35.14434814453125, weights = tensor([6.0352, 1.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №292: loss = 35.129241943359375, weights = tensor([6.0352, 1.9678], dtype=torch.float16, requires_grad=True)\n",
      "step №293: loss = 35.11415481567383, weights = tensor([6.0352, 1.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №294: loss = 35.09907913208008, weights = tensor([6.0352, 1.9736], dtype=torch.float16, requires_grad=True)\n",
      "step №295: loss = 35.084022521972656, weights = tensor([6.0352, 1.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №296: loss = 35.0689811706543, weights = tensor([6.0352, 1.9795], dtype=torch.float16, requires_grad=True)\n",
      "step №297: loss = 35.05396270751953, weights = tensor([6.0352, 1.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №298: loss = 35.03895568847656, weights = tensor([6.0352, 1.9854], dtype=torch.float16, requires_grad=True)\n",
      "step №299: loss = 35.02396774291992, weights = tensor([6.0352, 1.9883], dtype=torch.float16, requires_grad=True)\n",
      "step №300: loss = 35.008995056152344, weights = tensor([6.0352, 1.9912], dtype=torch.float16, requires_grad=True)\n",
      "step №301: loss = 34.994041442871094, weights = tensor([6.0352, 1.9941], dtype=torch.float16, requires_grad=True)\n",
      "step №302: loss = 34.97910690307617, weights = tensor([6.0352, 1.9971], dtype=torch.float16, requires_grad=True)\n",
      "step №303: loss = 34.96419143676758, weights = tensor([6.0352, 2.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №304: loss = 34.94928741455078, weights = tensor([6.0352, 2.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №305: loss = 34.929443359375, weights = tensor([6.0352, 2.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №306: loss = 34.909629821777344, weights = tensor([6.0352, 2.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №307: loss = 34.88985061645508, weights = tensor([6.0352, 2.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №308: loss = 34.87009811401367, weights = tensor([6.0352, 2.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №309: loss = 34.850379943847656, weights = tensor([6.0352, 2.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №310: loss = 34.8306884765625, weights = tensor([6.0352, 2.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №311: loss = 34.81102752685547, weights = tensor([6.0352, 2.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №312: loss = 34.79140090942383, weights = tensor([6.0352, 2.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №313: loss = 34.77180099487305, weights = tensor([6.0352, 2.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №314: loss = 34.752235412597656, weights = tensor([6.0352, 2.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №315: loss = 34.732696533203125, weights = tensor([6.0352, 2.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №316: loss = 34.71318817138672, weights = tensor([6.0352, 2.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №317: loss = 34.6937141418457, weights = tensor([6.0352, 2.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №318: loss = 34.67426681518555, weights = tensor([6.0352, 2.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №319: loss = 34.65485382080078, weights = tensor([6.0352, 2.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №320: loss = 34.635467529296875, weights = tensor([6.0352, 2.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №321: loss = 34.616111755371094, weights = tensor([6.0352, 2.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №322: loss = 34.5967903137207, weights = tensor([6.0352, 2.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №323: loss = 34.57749557495117, weights = tensor([6.0352, 2.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №324: loss = 34.55823516845703, weights = tensor([6.0352, 2.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №325: loss = 34.53900146484375, weights = tensor([6.0352, 2.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №326: loss = 34.519798278808594, weights = tensor([6.0352, 2.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №327: loss = 34.50062942504883, weights = tensor([6.0352, 2.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №328: loss = 34.48148727416992, weights = tensor([6.0352, 2.0977], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №329: loss = 34.462379455566406, weights = tensor([6.0312, 2.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №330: loss = 34.4307746887207, weights = tensor([6.0312, 2.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №331: loss = 34.411582946777344, weights = tensor([6.0312, 2.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №332: loss = 34.392433166503906, weights = tensor([6.0312, 2.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №333: loss = 34.3733024597168, weights = tensor([6.0312, 2.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №334: loss = 34.354209899902344, weights = tensor([6.0312, 2.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №335: loss = 34.33514404296875, weights = tensor([6.0273, 2.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №336: loss = 34.31321334838867, weights = tensor([6.0273, 2.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №337: loss = 34.29405975341797, weights = tensor([6.0273, 2.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №338: loss = 34.274932861328125, weights = tensor([6.0273, 2.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №339: loss = 34.255836486816406, weights = tensor([6.0273, 2.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №340: loss = 34.246299743652344, weights = tensor([6.0273, 2.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №341: loss = 34.23677444458008, weights = tensor([6.0273, 2.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №342: loss = 34.22725296020508, weights = tensor([6.0273, 2.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №343: loss = 34.21773910522461, weights = tensor([6.0273, 2.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №344: loss = 34.20823287963867, weights = tensor([6.0234, 2.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №345: loss = 34.186370849609375, weights = tensor([6.0234, 2.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №346: loss = 34.167259216308594, weights = tensor([6.0234, 2.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №347: loss = 34.1481819152832, weights = tensor([6.0234, 2.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №348: loss = 34.13865280151367, weights = tensor([6.0234, 2.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №349: loss = 34.12913131713867, weights = tensor([6.0234, 2.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №350: loss = 34.1196174621582, weights = tensor([6.0234, 2.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №351: loss = 34.11011505126953, weights = tensor([6.0234, 2.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №352: loss = 34.100616455078125, weights = tensor([6.0234, 2.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №353: loss = 34.09112548828125, weights = tensor([6.0234, 2.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №354: loss = 34.081642150878906, weights = tensor([6.0234, 2.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №355: loss = 34.072166442871094, weights = tensor([6.0195, 2.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №356: loss = 34.05031204223633, weights = tensor([6.0195, 2.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №357: loss = 34.0407829284668, weights = tensor([6.0195, 2.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №358: loss = 34.0312614440918, weights = tensor([6.0195, 2.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №359: loss = 34.021751403808594, weights = tensor([6.0195, 2.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №360: loss = 34.012245178222656, weights = tensor([6.0195, 2.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №361: loss = 34.00274658203125, weights = tensor([6.0195, 2.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №362: loss = 33.993255615234375, weights = tensor([6.0195, 2.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №363: loss = 33.98377227783203, weights = tensor([6.0195, 2.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №364: loss = 33.97429656982422, weights = tensor([6.0195, 2.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №365: loss = 33.9648323059082, weights = tensor([6.0195, 2.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №366: loss = 33.95537185668945, weights = tensor([6.0195, 2.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №367: loss = 33.945919036865234, weights = tensor([6.0195, 2.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №368: loss = 33.93647384643555, weights = tensor([6.0156, 2.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №369: loss = 33.91462707519531, weights = tensor([6.0156, 2.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №370: loss = 33.905128479003906, weights = tensor([6.0156, 2.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №371: loss = 33.89563751220703, weights = tensor([6.0156, 2.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №372: loss = 33.88615798950195, weights = tensor([6.0156, 2.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №373: loss = 33.87668228149414, weights = tensor([6.0156, 2.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №374: loss = 33.867210388183594, weights = tensor([6.0156, 2.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №375: loss = 33.85775375366211, weights = tensor([6.0156, 2.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №376: loss = 33.848304748535156, weights = tensor([6.0156, 2.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №377: loss = 33.83885955810547, weights = tensor([6.0156, 2.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №378: loss = 33.82941818237305, weights = tensor([6.0156, 2.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №379: loss = 33.81999206542969, weights = tensor([6.0156, 2.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №380: loss = 33.810569763183594, weights = tensor([6.0117, 2.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №381: loss = 33.78879165649414, weights = tensor([6.0117, 2.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №382: loss = 33.77931594848633, weights = tensor([6.0117, 2.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №383: loss = 33.76984786987305, weights = tensor([6.0117, 2.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №384: loss = 33.7603874206543, weights = tensor([6.0117, 2.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №385: loss = 33.750938415527344, weights = tensor([6.0117, 2.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №386: loss = 33.741493225097656, weights = tensor([6.0117, 2.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №387: loss = 33.7320556640625, weights = tensor([6.0117, 2.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №388: loss = 33.722625732421875, weights = tensor([6.0117, 2.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №389: loss = 33.71320343017578, weights = tensor([6.0117, 2.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №390: loss = 33.70378875732422, weights = tensor([6.0117, 2.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №391: loss = 33.69438552856445, weights = tensor([6.0117, 2.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №392: loss = 33.68498611450195, weights = tensor([6.0117, 2.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №393: loss = 33.675594329833984, weights = tensor([6.0078, 2.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №394: loss = 33.6538200378418, weights = tensor([6.0078, 2.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №395: loss = 33.644378662109375, weights = tensor([6.0078, 2.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №396: loss = 33.63494110107422, weights = tensor([6.0078, 2.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №397: loss = 33.625511169433594, weights = tensor([6.0078, 2.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №398: loss = 33.6160888671875, weights = tensor([6.0078, 2.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №399: loss = 33.6066780090332, weights = tensor([6.0078, 2.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №400: loss = 33.59727096557617, weights = tensor([6.0078, 2.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №401: loss = 33.58787155151367, weights = tensor([6.0078, 2.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №402: loss = 33.5784797668457, weights = tensor([6.0078, 2.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №403: loss = 33.56909942626953, weights = tensor([6.0078, 2.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №404: loss = 33.559722900390625, weights = tensor([6.0078, 2.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №405: loss = 33.55035400390625, weights = tensor([6.0078, 2.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №406: loss = 33.540992736816406, weights = tensor([6.0039, 2.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №407: loss = 33.519229888916016, weights = tensor([6.0039, 2.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №408: loss = 33.50981521606445, weights = tensor([6.0039, 2.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №409: loss = 33.50040817260742, weights = tensor([6.0039, 2.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №410: loss = 33.49100875854492, weights = tensor([6.0039, 2.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №411: loss = 33.48162078857422, weights = tensor([6.0039, 2.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №412: loss = 33.47223663330078, weights = tensor([6.0039, 2.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №413: loss = 33.462860107421875, weights = tensor([6.0039, 2.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №414: loss = 33.4534912109375, weights = tensor([6.0039, 2.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №415: loss = 33.444129943847656, weights = tensor([6.0039, 2.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №416: loss = 33.434776306152344, weights = tensor([6.0039, 2.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №417: loss = 33.42543411254883, weights = tensor([6.0039, 2.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №418: loss = 33.41609573364258, weights = tensor([6.0000, 2.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №419: loss = 33.39440155029297, weights = tensor([6.0000, 2.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №420: loss = 33.385009765625, weights = tensor([6.0000, 2.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №421: loss = 33.37562561035156, weights = tensor([6.0000, 2.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №422: loss = 33.366249084472656, weights = tensor([6.0000, 2.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №423: loss = 33.35688018798828, weights = tensor([6.0000, 2.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №424: loss = 33.3475227355957, weights = tensor([6.0000, 2.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №425: loss = 33.33816909790039, weights = tensor([6.0000, 2.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №426: loss = 33.328819274902344, weights = tensor([6.0000, 2.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №427: loss = 33.31948471069336, weights = tensor([6.0000, 2.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №428: loss = 33.310157775878906, weights = tensor([6.0000, 2.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №429: loss = 33.30083465576172, weights = tensor([6.0000, 2.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №430: loss = 33.2915153503418, weights = tensor([6.0000, 2.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №431: loss = 33.28221130371094, weights = tensor([5.9961, 2.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №432: loss = 33.260520935058594, weights = tensor([5.9961, 2.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №433: loss = 33.251163482666016, weights = tensor([5.9961, 2.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №434: loss = 33.2418098449707, weights = tensor([5.9961, 2.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №435: loss = 33.23246383666992, weights = tensor([5.9961, 2.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №436: loss = 33.22312545776367, weights = tensor([5.9961, 2.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №437: loss = 33.21379852294922, weights = tensor([5.9961, 2.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №438: loss = 33.20447540283203, weights = tensor([5.9961, 2.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №439: loss = 33.195159912109375, weights = tensor([5.9961, 2.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №440: loss = 33.18585205078125, weights = tensor([5.9961, 2.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №441: loss = 33.176551818847656, weights = tensor([5.9961, 2.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №442: loss = 33.167259216308594, weights = tensor([5.9961, 2.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №443: loss = 33.15797805786133, weights = tensor([5.9961, 2.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №444: loss = 33.14870071411133, weights = tensor([5.9922, 2.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №445: loss = 33.12702178955078, weights = tensor([5.9922, 2.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №446: loss = 33.1176872253418, weights = tensor([5.9922, 2.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №447: loss = 33.108367919921875, weights = tensor([5.9922, 2.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №448: loss = 33.09905242919922, weights = tensor([5.9922, 2.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №449: loss = 33.089744567871094, weights = tensor([5.9922, 2.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №450: loss = 33.0804443359375, weights = tensor([5.9922, 2.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №451: loss = 33.0711555480957, weights = tensor([5.9922, 2.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №452: loss = 33.06187057495117, weights = tensor([5.9922, 2.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №453: loss = 33.05259323120117, weights = tensor([5.9922, 2.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №454: loss = 33.0433235168457, weights = tensor([5.9922, 2.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №455: loss = 33.03406524658203, weights = tensor([5.9922, 2.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №456: loss = 33.024810791015625, weights = tensor([5.9883, 2.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №457: loss = 33.003196716308594, weights = tensor([5.9883, 2.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №458: loss = 32.99388885498047, weights = tensor([5.9883, 2.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №459: loss = 32.98459243774414, weights = tensor([5.9883, 2.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №460: loss = 32.97529983520508, weights = tensor([5.9883, 2.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №461: loss = 32.96601486206055, weights = tensor([5.9883, 2.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №462: loss = 32.95673751831055, weights = tensor([5.9883, 2.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №463: loss = 32.947471618652344, weights = tensor([5.9883, 2.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №464: loss = 32.938209533691406, weights = tensor([5.9883, 2.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №465: loss = 32.928955078125, weights = tensor([5.9883, 2.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №466: loss = 32.919708251953125, weights = tensor([5.9883, 2.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №467: loss = 32.91046905517578, weights = tensor([5.9883, 2.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №468: loss = 32.90123748779297, weights = tensor([5.9883, 2.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №469: loss = 32.89201736450195, weights = tensor([5.9844, 2.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №470: loss = 32.87041091918945, weights = tensor([5.9844, 2.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №471: loss = 32.86113739013672, weights = tensor([5.9844, 2.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №472: loss = 32.85186767578125, weights = tensor([5.9844, 2.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №473: loss = 32.84260559082031, weights = tensor([5.9844, 2.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №474: loss = 32.833351135253906, weights = tensor([5.9844, 2.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №475: loss = 32.82410430908203, weights = tensor([5.9844, 2.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №476: loss = 32.81486892700195, weights = tensor([5.9844, 2.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №477: loss = 32.80563735961914, weights = tensor([5.9844, 2.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №478: loss = 32.796409606933594, weights = tensor([5.9844, 2.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №479: loss = 32.78719711303711, weights = tensor([5.9844, 2.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №480: loss = 32.777992248535156, weights = tensor([5.9844, 2.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №481: loss = 32.76879119873047, weights = tensor([5.9844, 2.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №482: loss = 32.75959396362305, weights = tensor([5.9805, 2.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №483: loss = 32.737998962402344, weights = tensor([5.9805, 2.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №484: loss = 32.72875213623047, weights = tensor([5.9805, 2.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №485: loss = 32.71951675415039, weights = tensor([5.9805, 2.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №486: loss = 32.71028518676758, weights = tensor([5.9805, 2.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №487: loss = 32.7010612487793, weights = tensor([5.9805, 2.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №488: loss = 32.69184494018555, weights = tensor([5.9805, 2.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №489: loss = 32.682640075683594, weights = tensor([5.9805, 2.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №490: loss = 32.673439025878906, weights = tensor([5.9805, 2.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №491: loss = 32.66424560546875, weights = tensor([5.9805, 2.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №492: loss = 32.655059814453125, weights = tensor([5.9805, 2.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №493: loss = 32.64588165283203, weights = tensor([5.9805, 2.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №494: loss = 32.63671112060547, weights = tensor([5.9766, 2.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №495: loss = 32.61518478393555, weights = tensor([5.9766, 2.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №496: loss = 32.60596466064453, weights = tensor([5.9766, 2.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №497: loss = 32.59674835205078, weights = tensor([5.9766, 2.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №498: loss = 32.5875358581543, weights = tensor([5.9766, 2.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №499: loss = 32.578338623046875, weights = tensor([5.9766, 2.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №500: loss = 32.56914520263672, weights = tensor([5.9766, 2.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №501: loss = 32.559959411621094, weights = tensor([5.9766, 2.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №502: loss = 32.55078125, weights = tensor([5.9766, 2.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №503: loss = 32.5416145324707, weights = tensor([5.9766, 2.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №504: loss = 32.53245162963867, weights = tensor([5.9766, 2.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №505: loss = 32.52329635620117, weights = tensor([5.9766, 2.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №506: loss = 32.5141487121582, weights = tensor([5.9766, 2.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №507: loss = 32.50501251220703, weights = tensor([5.9727, 2.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №508: loss = 32.483489990234375, weights = tensor([5.9727, 2.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №509: loss = 32.47429656982422, weights = tensor([5.9727, 2.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №510: loss = 32.465110778808594, weights = tensor([5.9727, 2.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №511: loss = 32.455936431884766, weights = tensor([5.9727, 2.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №512: loss = 32.4467658996582, weights = tensor([5.9727, 2.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №513: loss = 32.43760299682617, weights = tensor([5.9727, 2.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №514: loss = 32.42844772338867, weights = tensor([5.9727, 2.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №515: loss = 32.41930389404297, weights = tensor([5.9727, 2.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №516: loss = 32.41016387939453, weights = tensor([5.9727, 2.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №517: loss = 32.401031494140625, weights = tensor([5.9727, 2.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №518: loss = 32.39190673828125, weights = tensor([5.9727, 2.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №519: loss = 32.382789611816406, weights = tensor([5.9727, 2.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №520: loss = 32.373680114746094, weights = tensor([5.9688, 2.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №521: loss = 32.352169036865234, weights = tensor([5.9688, 2.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №522: loss = 32.3430061340332, weights = tensor([5.9688, 2.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №523: loss = 32.33385467529297, weights = tensor([5.9688, 2.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №524: loss = 32.32470703125, weights = tensor([5.9688, 2.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №525: loss = 32.31556701660156, weights = tensor([5.9688, 2.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №526: loss = 32.306434631347656, weights = tensor([5.9688, 2.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №527: loss = 32.29730987548828, weights = tensor([5.9688, 2.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №528: loss = 32.2881965637207, weights = tensor([5.9688, 2.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №529: loss = 32.27908706665039, weights = tensor([5.9688, 2.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №530: loss = 32.269981384277344, weights = tensor([5.9688, 2.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №531: loss = 32.26089096069336, weights = tensor([5.9688, 2.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №532: loss = 32.251808166503906, weights = tensor([5.9648, 2.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №533: loss = 32.23036193847656, weights = tensor([5.9648, 2.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №534: loss = 32.221221923828125, weights = tensor([5.9648, 2.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №535: loss = 32.21208953857422, weights = tensor([5.9648, 2.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №536: loss = 32.202964782714844, weights = tensor([5.9648, 2.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №537: loss = 32.193851470947266, weights = tensor([5.9648, 2.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №538: loss = 32.18474197387695, weights = tensor([5.9648, 2.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №539: loss = 32.17564010620117, weights = tensor([5.9648, 2.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №540: loss = 32.16654586791992, weights = tensor([5.9648, 2.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №541: loss = 32.15746307373047, weights = tensor([5.9648, 2.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №542: loss = 32.14838409423828, weights = tensor([5.9648, 2.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №543: loss = 32.139312744140625, weights = tensor([5.9648, 2.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №544: loss = 32.1302490234375, weights = tensor([5.9648, 2.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №545: loss = 32.121192932128906, weights = tensor([5.9609, 2.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №546: loss = 32.099754333496094, weights = tensor([5.9609, 2.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №547: loss = 32.09064865112305, weights = tensor([5.9609, 2.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №548: loss = 32.08155059814453, weights = tensor([5.9609, 2.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №549: loss = 32.07245635986328, weights = tensor([5.9609, 2.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №550: loss = 32.0633659362793, weights = tensor([5.9609, 2.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №551: loss = 32.054290771484375, weights = tensor([5.9609, 2.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №552: loss = 32.04521942138672, weights = tensor([5.9609, 2.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №553: loss = 32.036155700683594, weights = tensor([5.9609, 2.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №554: loss = 32.027099609375, weights = tensor([5.9609, 2.5605], dtype=torch.float16, requires_grad=True)\n",
      "step №555: loss = 32.0180549621582, weights = tensor([5.9609, 2.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №556: loss = 32.00901412963867, weights = tensor([5.9609, 2.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №557: loss = 31.999980926513672, weights = tensor([5.9609, 2.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №558: loss = 31.990955352783203, weights = tensor([5.9570, 2.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №559: loss = 31.969528198242188, weights = tensor([5.9570, 2.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №560: loss = 31.96044921875, weights = tensor([5.9570, 2.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №561: loss = 31.951379776000977, weights = tensor([5.9570, 2.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №562: loss = 31.94231605529785, weights = tensor([5.9570, 2.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №563: loss = 31.93326187133789, weights = tensor([5.9570, 2.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №564: loss = 31.924213409423828, weights = tensor([5.9570, 2.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №565: loss = 31.915172576904297, weights = tensor([5.9570, 2.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №566: loss = 31.906139373779297, weights = tensor([5.9570, 2.5840], dtype=torch.float16, requires_grad=True)\n",
      "step №567: loss = 31.89711570739746, weights = tensor([5.9570, 2.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №568: loss = 31.888097763061523, weights = tensor([5.9570, 2.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №569: loss = 31.87908935546875, weights = tensor([5.9570, 2.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №570: loss = 31.870086669921875, weights = tensor([5.9531, 2.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №571: loss = 31.84872817993164, weights = tensor([5.9531, 2.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №572: loss = 31.839672088623047, weights = tensor([5.9531, 2.5957], dtype=torch.float16, requires_grad=True)\n",
      "step №573: loss = 31.830623626708984, weights = tensor([5.9531, 2.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №574: loss = 31.821582794189453, weights = tensor([5.9531, 2.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №575: loss = 31.812551498413086, weights = tensor([5.9531, 2.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №576: loss = 31.80352783203125, weights = tensor([5.9531, 2.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №577: loss = 31.794509887695312, weights = tensor([5.9531, 2.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №578: loss = 31.785497665405273, weights = tensor([5.9531, 2.6074], dtype=torch.float16, requires_grad=True)\n",
      "step №579: loss = 31.776498794555664, weights = tensor([5.9531, 2.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №580: loss = 31.767505645751953, weights = tensor([5.9531, 2.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №581: loss = 31.75851821899414, weights = tensor([5.9531, 2.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №582: loss = 31.749536514282227, weights = tensor([5.9531, 2.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №583: loss = 31.74056625366211, weights = tensor([5.9492, 2.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №584: loss = 31.719213485717773, weights = tensor([5.9492, 2.6191], dtype=torch.float16, requires_grad=True)\n",
      "step №585: loss = 31.710189819335938, weights = tensor([5.9492, 2.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №586: loss = 31.701171875, weights = tensor([5.9492, 2.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №587: loss = 31.692163467407227, weights = tensor([5.9492, 2.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №588: loss = 31.68316078186035, weights = tensor([5.9492, 2.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №589: loss = 31.67416763305664, weights = tensor([5.9492, 2.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №590: loss = 31.665180206298828, weights = tensor([5.9492, 2.6309], dtype=torch.float16, requires_grad=True)\n",
      "step №591: loss = 31.656200408935547, weights = tensor([5.9492, 2.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №592: loss = 31.647228240966797, weights = tensor([5.9492, 2.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №593: loss = 31.63826560974121, weights = tensor([5.9492, 2.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №594: loss = 31.629308700561523, weights = tensor([5.9492, 2.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №595: loss = 31.620361328125, weights = tensor([5.9492, 2.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №596: loss = 31.611419677734375, weights = tensor([5.9453, 2.6426], dtype=torch.float16, requires_grad=True)\n",
      "step №597: loss = 31.590076446533203, weights = tensor([5.9453, 2.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №598: loss = 31.581079483032227, weights = tensor([5.9453, 2.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №599: loss = 31.572093963623047, weights = tensor([5.9453, 2.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №600: loss = 31.5631160736084, weights = tensor([5.9453, 2.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №601: loss = 31.55414390563965, weights = tensor([5.9453, 2.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №602: loss = 31.545177459716797, weights = tensor([5.9453, 2.6543], dtype=torch.float16, requires_grad=True)\n",
      "step №603: loss = 31.536224365234375, weights = tensor([5.9453, 2.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №604: loss = 31.52727699279785, weights = tensor([5.9453, 2.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №605: loss = 31.518335342407227, weights = tensor([5.9453, 2.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №606: loss = 31.5093994140625, weights = tensor([5.9453, 2.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №607: loss = 31.500476837158203, weights = tensor([5.9453, 2.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №608: loss = 31.491558074951172, weights = tensor([5.9414, 2.6660], dtype=torch.float16, requires_grad=True)\n",
      "step №609: loss = 31.47028160095215, weights = tensor([5.9414, 2.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №610: loss = 31.4613094329834, weights = tensor([5.9414, 2.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №611: loss = 31.452346801757812, weights = tensor([5.9414, 2.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №612: loss = 31.443389892578125, weights = tensor([5.9414, 2.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №613: loss = 31.4344425201416, weights = tensor([5.9414, 2.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №614: loss = 31.425500869750977, weights = tensor([5.9414, 2.6777], dtype=torch.float16, requires_grad=True)\n",
      "step №615: loss = 31.416568756103516, weights = tensor([5.9414, 2.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №616: loss = 31.407642364501953, weights = tensor([5.9414, 2.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №617: loss = 31.398723602294922, weights = tensor([5.9414, 2.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №618: loss = 31.389812469482422, weights = tensor([5.9414, 2.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №619: loss = 31.380910873413086, weights = tensor([5.9414, 2.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №620: loss = 31.37201499938965, weights = tensor([5.9414, 2.6895], dtype=torch.float16, requires_grad=True)\n",
      "step №621: loss = 31.363128662109375, weights = tensor([5.9375, 2.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №622: loss = 31.34185791015625, weights = tensor([5.9375, 2.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №623: loss = 31.33292007446289, weights = tensor([5.9375, 2.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №624: loss = 31.323986053466797, weights = tensor([5.9375, 2.6973], dtype=torch.float16, requires_grad=True)\n",
      "step №625: loss = 31.315059661865234, weights = tensor([5.9375, 2.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №626: loss = 31.306140899658203, weights = tensor([5.9375, 2.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №627: loss = 31.297231674194336, weights = tensor([5.9375, 2.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №628: loss = 31.288330078125, weights = tensor([5.9375, 2.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №629: loss = 31.279434204101562, weights = tensor([5.9375, 2.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №630: loss = 31.270544052124023, weights = tensor([5.9375, 2.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №631: loss = 31.261667251586914, weights = tensor([5.9375, 2.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №632: loss = 31.252796173095703, weights = tensor([5.9375, 2.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №633: loss = 31.24393081665039, weights = tensor([5.9375, 2.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №634: loss = 31.235071182250977, weights = tensor([5.9336, 2.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №635: loss = 31.21381187438965, weights = tensor([5.9336, 2.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №636: loss = 31.20490074157715, weights = tensor([5.9336, 2.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №637: loss = 31.195999145507812, weights = tensor([5.9336, 2.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №638: loss = 31.187103271484375, weights = tensor([5.9336, 2.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №639: loss = 31.1782169342041, weights = tensor([5.9336, 2.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №640: loss = 31.169336318969727, weights = tensor([5.9336, 2.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №641: loss = 31.160465240478516, weights = tensor([5.9336, 2.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №642: loss = 31.151599884033203, weights = tensor([5.9336, 2.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №643: loss = 31.142742156982422, weights = tensor([5.9336, 2.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №644: loss = 31.133892059326172, weights = tensor([5.9336, 2.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №645: loss = 31.125051498413086, weights = tensor([5.9336, 2.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №646: loss = 31.1162166595459, weights = tensor([5.9297, 2.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №647: loss = 31.09502601623535, weights = tensor([5.9297, 2.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №648: loss = 31.086139678955078, weights = tensor([5.9297, 2.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №649: loss = 31.077259063720703, weights = tensor([5.9297, 2.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №650: loss = 31.068384170532227, weights = tensor([5.9297, 2.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №651: loss = 31.059520721435547, weights = tensor([5.9297, 2.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №652: loss = 31.0506649017334, weights = tensor([5.9297, 2.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №653: loss = 31.04181480407715, weights = tensor([5.9297, 2.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №654: loss = 31.032970428466797, weights = tensor([5.9297, 2.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №655: loss = 31.024139404296875, weights = tensor([5.9297, 2.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №656: loss = 31.01531410217285, weights = tensor([5.9297, 2.7598], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №657: loss = 31.006494522094727, weights = tensor([5.9297, 2.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №658: loss = 30.9976806640625, weights = tensor([5.9297, 2.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №659: loss = 30.988880157470703, weights = tensor([5.9258, 2.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №660: loss = 30.967693328857422, weights = tensor([5.9258, 2.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №661: loss = 30.958837509155273, weights = tensor([5.9258, 2.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №662: loss = 30.949987411499023, weights = tensor([5.9258, 2.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №663: loss = 30.941146850585938, weights = tensor([5.9258, 2.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №664: loss = 30.93231201171875, weights = tensor([5.9258, 2.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №665: loss = 30.923486709594727, weights = tensor([5.9258, 2.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №666: loss = 30.9146671295166, weights = tensor([5.9258, 2.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №667: loss = 30.90585708618164, weights = tensor([5.9258, 2.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №668: loss = 30.897052764892578, weights = tensor([5.9258, 2.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №669: loss = 30.888256072998047, weights = tensor([5.9258, 2.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №670: loss = 30.879467010498047, weights = tensor([5.9258, 2.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №671: loss = 30.87068748474121, weights = tensor([5.9258, 2.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №672: loss = 30.861913681030273, weights = tensor([5.9219, 2.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №673: loss = 30.84073829650879, weights = tensor([5.9219, 2.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №674: loss = 30.8319091796875, weights = tensor([5.9219, 2.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №675: loss = 30.82309341430664, weights = tensor([5.9219, 2.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №676: loss = 30.814281463623047, weights = tensor([5.9219, 2.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №677: loss = 30.805477142333984, weights = tensor([5.9219, 2.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №678: loss = 30.796680450439453, weights = tensor([5.9219, 2.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №679: loss = 30.787893295288086, weights = tensor([5.9219, 2.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №680: loss = 30.77911376953125, weights = tensor([5.9219, 2.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №681: loss = 30.770339965820312, weights = tensor([5.9219, 2.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №682: loss = 30.761571884155273, weights = tensor([5.9219, 2.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №683: loss = 30.752817153930664, weights = tensor([5.9219, 2.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №684: loss = 30.744068145751953, weights = tensor([5.9180, 2.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №685: loss = 30.722957611083984, weights = tensor([5.9180, 2.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №686: loss = 30.714153289794922, weights = tensor([5.9180, 2.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №687: loss = 30.705358505249023, weights = tensor([5.9180, 2.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №688: loss = 30.696569442749023, weights = tensor([5.9180, 2.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №689: loss = 30.687789916992188, weights = tensor([5.9180, 2.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №690: loss = 30.67901611328125, weights = tensor([5.9180, 2.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №691: loss = 30.670251846313477, weights = tensor([5.9180, 2.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №692: loss = 30.6614933013916, weights = tensor([5.9180, 2.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №693: loss = 30.65274429321289, weights = tensor([5.9180, 2.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №694: loss = 30.644001007080078, weights = tensor([5.9180, 2.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №695: loss = 30.635265350341797, weights = tensor([5.9180, 2.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №696: loss = 30.626537322998047, weights = tensor([5.9180, 2.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №697: loss = 30.61781883239746, weights = tensor([5.9141, 2.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №698: loss = 30.596715927124023, weights = tensor([5.9141, 2.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №699: loss = 30.58794593811035, weights = tensor([5.9141, 2.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №700: loss = 30.579181671142578, weights = tensor([5.9141, 2.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №701: loss = 30.570423126220703, weights = tensor([5.9141, 2.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №702: loss = 30.561670303344727, weights = tensor([5.9141, 2.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №703: loss = 30.552928924560547, weights = tensor([5.9141, 2.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №704: loss = 30.5441951751709, weights = tensor([5.9141, 2.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №705: loss = 30.53546714782715, weights = tensor([5.9141, 2.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №706: loss = 30.526744842529297, weights = tensor([5.9141, 2.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №707: loss = 30.518035888671875, weights = tensor([5.9141, 2.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №708: loss = 30.50933265686035, weights = tensor([5.9141, 2.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №709: loss = 30.500635147094727, weights = tensor([5.9141, 2.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №710: loss = 30.491943359375, weights = tensor([5.9102, 2.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №711: loss = 30.47085189819336, weights = tensor([5.9102, 2.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №712: loss = 30.462108612060547, weights = tensor([5.9102, 2.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №713: loss = 30.4533748626709, weights = tensor([5.9102, 2.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №714: loss = 30.44464683532715, weights = tensor([5.9102, 2.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №715: loss = 30.435928344726562, weights = tensor([5.9102, 2.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №716: loss = 30.427215576171875, weights = tensor([5.9102, 2.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №717: loss = 30.41851234436035, weights = tensor([5.9102, 2.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №718: loss = 30.409814834594727, weights = tensor([5.9102, 2.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №719: loss = 30.401126861572266, weights = tensor([5.9102, 2.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №720: loss = 30.392444610595703, weights = tensor([5.9102, 2.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №721: loss = 30.383769989013672, weights = tensor([5.9102, 2.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №722: loss = 30.375102996826172, weights = tensor([5.9062, 2.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №723: loss = 30.354080200195312, weights = tensor([5.9062, 2.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №724: loss = 30.345361709594727, weights = tensor([5.9062, 2.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №725: loss = 30.33664894104004, weights = tensor([5.9062, 2.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №726: loss = 30.32794189453125, weights = tensor([5.9062, 2.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №727: loss = 30.31924819946289, weights = tensor([5.9062, 2.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №728: loss = 30.310558319091797, weights = tensor([5.9062, 2.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №729: loss = 30.301876068115234, weights = tensor([5.9062, 2.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №730: loss = 30.293201446533203, weights = tensor([5.9062, 2.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №731: loss = 30.284536361694336, weights = tensor([5.9062, 2.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №732: loss = 30.27587890625, weights = tensor([5.9062, 2.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №733: loss = 30.267227172851562, weights = tensor([5.9062, 2.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №734: loss = 30.258581161499023, weights = tensor([5.9062, 2.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №735: loss = 30.249948501586914, weights = tensor([5.9023, 2.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №736: loss = 30.228931427001953, weights = tensor([5.9023, 2.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №737: loss = 30.22024154663086, weights = tensor([5.9023, 2.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №738: loss = 30.211559295654297, weights = tensor([5.9023, 2.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №739: loss = 30.2028865814209, weights = tensor([5.9023, 2.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №740: loss = 30.1942195892334, weights = tensor([5.9023, 2.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №741: loss = 30.185562133789062, weights = tensor([5.9023, 2.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №742: loss = 30.176910400390625, weights = tensor([5.9023, 2.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №743: loss = 30.16826820373535, weights = tensor([5.9023, 2.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №744: loss = 30.159631729125977, weights = tensor([5.9023, 2.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №745: loss = 30.151004791259766, weights = tensor([5.9023, 2.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №746: loss = 30.142383575439453, weights = tensor([5.9023, 2.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №747: loss = 30.133769989013672, weights = tensor([5.9023, 2.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №748: loss = 30.125164031982422, weights = tensor([5.8984, 2.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №749: loss = 30.104156494140625, weights = tensor([5.8984, 2.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №750: loss = 30.095495223999023, weights = tensor([5.8984, 2.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №751: loss = 30.08684730529785, weights = tensor([5.8984, 2.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №752: loss = 30.078205108642578, weights = tensor([5.8984, 2.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №753: loss = 30.069568634033203, weights = tensor([5.8984, 2.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №754: loss = 30.060937881469727, weights = tensor([5.8984, 2.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №755: loss = 30.052318572998047, weights = tensor([5.8984, 2.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №756: loss = 30.0437068939209, weights = tensor([5.8984, 2.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №757: loss = 30.03510093688965, weights = tensor([5.8984, 2.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №758: loss = 30.026500701904297, weights = tensor([5.8984, 2.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №759: loss = 30.017913818359375, weights = tensor([5.8984, 2.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №760: loss = 30.00933265686035, weights = tensor([5.8945, 2.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №761: loss = 29.988391876220703, weights = tensor([5.8945, 2.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №762: loss = 29.979755401611328, weights = tensor([5.8945, 2.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №763: loss = 29.971126556396484, weights = tensor([5.8945, 2.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №764: loss = 29.962505340576172, weights = tensor([5.8945, 2.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №765: loss = 29.953893661499023, weights = tensor([5.8945, 2.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №766: loss = 29.945287704467773, weights = tensor([5.8945, 2.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №767: loss = 29.936691284179688, weights = tensor([5.8945, 2.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №768: loss = 29.9281005859375, weights = tensor([5.8945, 2.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №769: loss = 29.919519424438477, weights = tensor([5.8945, 2.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №770: loss = 29.91094398498535, weights = tensor([5.8945, 2.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №771: loss = 29.90237808227539, weights = tensor([5.8945, 2.9844], dtype=torch.float16, requires_grad=True)\n",
      "step №772: loss = 29.893817901611328, weights = tensor([5.8945, 2.9863], dtype=torch.float16, requires_grad=True)\n",
      "step №773: loss = 29.885265350341797, weights = tensor([5.8906, 2.9883], dtype=torch.float16, requires_grad=True)\n",
      "step №774: loss = 29.864330291748047, weights = tensor([5.8906, 2.9902], dtype=torch.float16, requires_grad=True)\n",
      "step №775: loss = 29.855728149414062, weights = tensor([5.8906, 2.9922], dtype=torch.float16, requires_grad=True)\n",
      "step №776: loss = 29.847131729125977, weights = tensor([5.8906, 2.9941], dtype=torch.float16, requires_grad=True)\n",
      "step №777: loss = 29.83854103088379, weights = tensor([5.8906, 2.9961], dtype=torch.float16, requires_grad=True)\n",
      "step №778: loss = 29.8299560546875, weights = tensor([5.8906, 2.9980], dtype=torch.float16, requires_grad=True)\n",
      "step №779: loss = 29.82138442993164, weights = tensor([5.8906, 3.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №780: loss = 29.812816619873047, weights = tensor([5.8906, 3.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №781: loss = 29.804256439208984, weights = tensor([5.8906, 3.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №782: loss = 29.795703887939453, weights = tensor([5.8906, 3.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №783: loss = 29.787160873413086, weights = tensor([5.8906, 3.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №784: loss = 29.77862548828125, weights = tensor([5.8906, 3.0098], dtype=torch.float16, requires_grad=True)\n",
      "step №785: loss = 29.770095825195312, weights = tensor([5.8906, 3.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №786: loss = 29.761571884155273, weights = tensor([5.8867, 3.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №787: loss = 29.740650177001953, weights = tensor([5.8867, 3.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №788: loss = 29.732074737548828, weights = tensor([5.8867, 3.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №789: loss = 29.723506927490234, weights = tensor([5.8867, 3.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №790: loss = 29.714946746826172, weights = tensor([5.8867, 3.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №791: loss = 29.706396102905273, weights = tensor([5.8867, 3.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №792: loss = 29.697851181030273, weights = tensor([5.8867, 3.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №793: loss = 29.689315795898438, weights = tensor([5.8867, 3.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №794: loss = 29.6807861328125, weights = tensor([5.8867, 3.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №795: loss = 29.672266006469727, weights = tensor([5.8867, 3.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №796: loss = 29.66375160217285, weights = tensor([5.8867, 3.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №797: loss = 29.65524673461914, weights = tensor([5.8867, 3.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №798: loss = 29.646747589111328, weights = tensor([5.8828, 3.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №799: loss = 29.625890731811523, weights = tensor([5.8828, 3.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №800: loss = 29.617340087890625, weights = tensor([5.8828, 3.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №801: loss = 29.608795166015625, weights = tensor([5.8828, 3.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №802: loss = 29.600255966186523, weights = tensor([5.8828, 3.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №803: loss = 29.59173011779785, weights = tensor([5.8828, 3.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №804: loss = 29.583209991455078, weights = tensor([5.8828, 3.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №805: loss = 29.574695587158203, weights = tensor([5.8828, 3.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №806: loss = 29.566186904907227, weights = tensor([5.8828, 3.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №807: loss = 29.557689666748047, weights = tensor([5.8828, 3.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №808: loss = 29.5492000579834, weights = tensor([5.8828, 3.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №809: loss = 29.54071617126465, weights = tensor([5.8828, 3.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №810: loss = 29.532238006591797, weights = tensor([5.8828, 3.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №811: loss = 29.523773193359375, weights = tensor([5.8789, 3.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №812: loss = 29.5029239654541, weights = tensor([5.8789, 3.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №813: loss = 29.494403839111328, weights = tensor([5.8789, 3.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №814: loss = 29.485889434814453, weights = tensor([5.8789, 3.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №815: loss = 29.47738265991211, weights = tensor([5.8789, 3.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №816: loss = 29.468883514404297, weights = tensor([5.8789, 3.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №817: loss = 29.46039390563965, weights = tensor([5.8789, 3.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №818: loss = 29.4519100189209, weights = tensor([5.8789, 3.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №819: loss = 29.443435668945312, weights = tensor([5.8789, 3.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №820: loss = 29.434967041015625, weights = tensor([5.8789, 3.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №821: loss = 29.4265079498291, weights = tensor([5.8789, 3.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №822: loss = 29.418054580688477, weights = tensor([5.8789, 3.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №823: loss = 29.409610748291016, weights = tensor([5.8789, 3.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №824: loss = 29.401172637939453, weights = tensor([5.8750, 3.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №825: loss = 29.38033103942871, weights = tensor([5.8750, 3.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №826: loss = 29.371837615966797, weights = tensor([5.8750, 3.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №827: loss = 29.363357543945312, weights = tensor([5.8750, 3.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №828: loss = 29.354883193969727, weights = tensor([5.8750, 3.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №829: loss = 29.34641456604004, weights = tensor([5.8750, 3.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №830: loss = 29.33795166015625, weights = tensor([5.8750, 3.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №831: loss = 29.32950210571289, weights = tensor([5.8750, 3.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №832: loss = 29.321056365966797, weights = tensor([5.8750, 3.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №833: loss = 29.312618255615234, weights = tensor([5.8750, 3.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №834: loss = 29.304187774658203, weights = tensor([5.8750, 3.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №835: loss = 29.295766830444336, weights = tensor([5.8750, 3.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №836: loss = 29.287353515625, weights = tensor([5.8711, 3.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №837: loss = 29.26658058166504, weights = tensor([5.8711, 3.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №838: loss = 29.25811195373535, weights = tensor([5.8711, 3.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №839: loss = 29.249652862548828, weights = tensor([5.8711, 3.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №840: loss = 29.241199493408203, weights = tensor([5.8711, 3.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №841: loss = 29.23275375366211, weights = tensor([5.8711, 3.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №842: loss = 29.224315643310547, weights = tensor([5.8711, 3.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №843: loss = 29.21588706970215, weights = tensor([5.8711, 3.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №844: loss = 29.20746421813965, weights = tensor([5.8711, 3.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №845: loss = 29.199050903320312, weights = tensor([5.8711, 3.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №846: loss = 29.190643310546875, weights = tensor([5.8711, 3.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №847: loss = 29.1822452545166, weights = tensor([5.8711, 3.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №848: loss = 29.173852920532227, weights = tensor([5.8711, 3.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №849: loss = 29.165470123291016, weights = tensor([5.8672, 3.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №850: loss = 29.144702911376953, weights = tensor([5.8672, 3.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №851: loss = 29.136266708374023, weights = tensor([5.8672, 3.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №852: loss = 29.127838134765625, weights = tensor([5.8672, 3.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №853: loss = 29.119415283203125, weights = tensor([5.8672, 3.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №854: loss = 29.110998153686523, weights = tensor([5.8672, 3.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №855: loss = 29.10259437561035, weights = tensor([5.8672, 3.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №856: loss = 29.094196319580078, weights = tensor([5.8672, 3.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №857: loss = 29.085803985595703, weights = tensor([5.8672, 3.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №858: loss = 29.077417373657227, weights = tensor([5.8672, 3.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №859: loss = 29.069042205810547, weights = tensor([5.8672, 3.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №860: loss = 29.0606746673584, weights = tensor([5.8672, 3.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №861: loss = 29.05231285095215, weights = tensor([5.8672, 3.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №862: loss = 29.043956756591797, weights = tensor([5.8633, 3.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №863: loss = 29.023202896118164, weights = tensor([5.8633, 3.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №864: loss = 29.014795303344727, weights = tensor([5.8633, 3.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №865: loss = 29.006397247314453, weights = tensor([5.8633, 3.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №866: loss = 28.998004913330078, weights = tensor([5.8633, 3.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №867: loss = 28.989620208740234, weights = tensor([5.8633, 3.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №868: loss = 28.981243133544922, weights = tensor([5.8633, 3.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №869: loss = 28.972875595092773, weights = tensor([5.8633, 3.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №870: loss = 28.964513778686523, weights = tensor([5.8633, 3.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №871: loss = 28.956161499023438, weights = tensor([5.8633, 3.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №872: loss = 28.94781494140625, weights = tensor([5.8633, 3.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №873: loss = 28.939477920532227, weights = tensor([5.8633, 3.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №874: loss = 28.9311466217041, weights = tensor([5.8594, 3.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №875: loss = 28.910457611083984, weights = tensor([5.8594, 3.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №876: loss = 28.902074813842773, weights = tensor([5.8594, 3.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №877: loss = 28.89369773864746, weights = tensor([5.8594, 3.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №878: loss = 28.885326385498047, weights = tensor([5.8594, 3.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №879: loss = 28.876968383789062, weights = tensor([5.8594, 3.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №880: loss = 28.868616104125977, weights = tensor([5.8594, 3.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №881: loss = 28.86026954650879, weights = tensor([5.8594, 3.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №882: loss = 28.8519287109375, weights = tensor([5.8594, 3.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №883: loss = 28.84360122680664, weights = tensor([5.8594, 3.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №884: loss = 28.835277557373047, weights = tensor([5.8594, 3.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №885: loss = 28.826961517333984, weights = tensor([5.8594, 3.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №886: loss = 28.818653106689453, weights = tensor([5.8594, 3.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №887: loss = 28.810354232788086, weights = tensor([5.8555, 3.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №888: loss = 28.7896728515625, weights = tensor([5.8555, 3.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №889: loss = 28.781320571899414, weights = tensor([5.8555, 3.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №890: loss = 28.772974014282227, weights = tensor([5.8555, 3.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №891: loss = 28.764636993408203, weights = tensor([5.8555, 3.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №892: loss = 28.756305694580078, weights = tensor([5.8555, 3.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №893: loss = 28.747982025146484, weights = tensor([5.8555, 3.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №894: loss = 28.739665985107422, weights = tensor([5.8555, 3.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №895: loss = 28.731359481811523, weights = tensor([5.8555, 3.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №896: loss = 28.723058700561523, weights = tensor([5.8555, 3.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №897: loss = 28.714767456054688, weights = tensor([5.8555, 3.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №898: loss = 28.70648193359375, weights = tensor([5.8555, 3.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №899: loss = 28.698205947875977, weights = tensor([5.8555, 3.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №900: loss = 28.6899356842041, weights = tensor([5.8516, 3.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №901: loss = 28.669261932373047, weights = tensor([5.8516, 3.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №902: loss = 28.660938262939453, weights = tensor([5.8516, 3.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №903: loss = 28.652624130249023, weights = tensor([5.8516, 3.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №904: loss = 28.644317626953125, weights = tensor([5.8516, 3.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №905: loss = 28.636016845703125, weights = tensor([5.8516, 3.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №906: loss = 28.627721786499023, weights = tensor([5.8516, 3.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №907: loss = 28.61944007873535, weights = tensor([5.8516, 3.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №908: loss = 28.611164093017578, weights = tensor([5.8516, 3.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №909: loss = 28.602893829345703, weights = tensor([5.8516, 3.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №910: loss = 28.594629287719727, weights = tensor([5.8516, 3.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №911: loss = 28.586376190185547, weights = tensor([5.8516, 3.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №912: loss = 28.5781307220459, weights = tensor([5.8477, 3.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №913: loss = 28.557525634765625, weights = tensor([5.8477, 3.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №914: loss = 28.549224853515625, weights = tensor([5.8477, 3.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №915: loss = 28.54093360900879, weights = tensor([5.8477, 3.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №916: loss = 28.53264808654785, weights = tensor([5.8477, 3.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №917: loss = 28.524372100830078, weights = tensor([5.8477, 3.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №918: loss = 28.516101837158203, weights = tensor([5.8477, 3.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №919: loss = 28.50783920288086, weights = tensor([5.8477, 3.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №920: loss = 28.499584197998047, weights = tensor([5.8477, 3.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №921: loss = 28.4913387298584, weights = tensor([5.8477, 3.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №922: loss = 28.48309898376465, weights = tensor([5.8477, 3.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №923: loss = 28.474868774414062, weights = tensor([5.8477, 3.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №924: loss = 28.466644287109375, weights = tensor([5.8477, 3.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №925: loss = 28.45842933654785, weights = tensor([5.8438, 3.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №926: loss = 28.437829971313477, weights = tensor([5.8438, 3.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №927: loss = 28.429561614990234, weights = tensor([5.8438, 3.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №928: loss = 28.421300888061523, weights = tensor([5.8438, 3.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №929: loss = 28.41304588317871, weights = tensor([5.8438, 3.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №930: loss = 28.404796600341797, weights = tensor([5.8438, 3.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №931: loss = 28.396560668945312, weights = tensor([5.8438, 3.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №932: loss = 28.388330459594727, weights = tensor([5.8438, 3.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №933: loss = 28.38010597229004, weights = tensor([5.8438, 3.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №934: loss = 28.37188720703125, weights = tensor([5.8438, 3.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №935: loss = 28.36368179321289, weights = tensor([5.8438, 3.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №936: loss = 28.355480194091797, weights = tensor([5.8438, 3.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №937: loss = 28.347286224365234, weights = tensor([5.8438, 3.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №938: loss = 28.339099884033203, weights = tensor([5.8398, 3.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №939: loss = 28.318511962890625, weights = tensor([5.8398, 3.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №940: loss = 28.310272216796875, weights = tensor([5.8398, 3.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №941: loss = 28.30204200744629, weights = tensor([5.8398, 3.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №942: loss = 28.2938175201416, weights = tensor([5.8398, 3.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №943: loss = 28.285602569580078, weights = tensor([5.8398, 3.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №944: loss = 28.277393341064453, weights = tensor([5.8398, 3.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №945: loss = 28.26919174194336, weights = tensor([5.8398, 3.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №946: loss = 28.260997772216797, weights = tensor([5.8398, 3.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №947: loss = 28.2528133392334, weights = tensor([5.8398, 3.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №948: loss = 28.2446346282959, weights = tensor([5.8398, 3.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №949: loss = 28.236465454101562, weights = tensor([5.8398, 3.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №950: loss = 28.228302001953125, weights = tensor([5.8359, 3.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №951: loss = 28.207782745361328, weights = tensor([5.8359, 3.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №952: loss = 28.199565887451172, weights = tensor([5.8359, 3.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №953: loss = 28.191356658935547, weights = tensor([5.8359, 3.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №954: loss = 28.183155059814453, weights = tensor([5.8359, 3.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №955: loss = 28.174962997436523, weights = tensor([5.8359, 3.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №956: loss = 28.166778564453125, weights = tensor([5.8359, 3.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №957: loss = 28.158599853515625, weights = tensor([5.8359, 3.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №958: loss = 28.150426864624023, weights = tensor([5.8359, 3.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №959: loss = 28.14226722717285, weights = tensor([5.8359, 3.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №960: loss = 28.134113311767578, weights = tensor([5.8359, 3.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №961: loss = 28.125965118408203, weights = tensor([5.8359, 3.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №962: loss = 28.117822647094727, weights = tensor([5.8359, 3.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №963: loss = 28.109691619873047, weights = tensor([5.8320, 3.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №964: loss = 28.08917808532715, weights = tensor([5.8320, 3.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №965: loss = 28.08099365234375, weights = tensor([5.8320, 3.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №966: loss = 28.07281494140625, weights = tensor([5.8320, 3.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №967: loss = 28.064645767211914, weights = tensor([5.8320, 3.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №968: loss = 28.056482315063477, weights = tensor([5.8320, 3.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №969: loss = 28.048328399658203, weights = tensor([5.8320, 3.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №970: loss = 28.040180206298828, weights = tensor([5.8320, 3.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №971: loss = 28.032039642333984, weights = tensor([5.8320, 3.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №972: loss = 28.023906707763672, weights = tensor([5.8320, 3.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №973: loss = 28.015783309936523, weights = tensor([5.8320, 3.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №974: loss = 28.007665634155273, weights = tensor([5.8320, 3.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №975: loss = 27.999557495117188, weights = tensor([5.8320, 3.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №976: loss = 27.991455078125, weights = tensor([5.8281, 3.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №977: loss = 27.970951080322266, weights = tensor([5.8281, 3.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №978: loss = 27.962793350219727, weights = tensor([5.8281, 3.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №979: loss = 27.954647064208984, weights = tensor([5.8281, 3.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №980: loss = 27.946508407592773, weights = tensor([5.8281, 3.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №981: loss = 27.93837547302246, weights = tensor([5.8281, 3.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №982: loss = 27.930248260498047, weights = tensor([5.8281, 3.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №983: loss = 27.922134399414062, weights = tensor([5.8281, 3.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №984: loss = 27.914026260375977, weights = tensor([5.8281, 3.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №985: loss = 27.90592384338379, weights = tensor([5.8281, 3.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №986: loss = 27.8978271484375, weights = tensor([5.8281, 3.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №987: loss = 27.88974380493164, weights = tensor([5.8281, 3.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №988: loss = 27.881664276123047, weights = tensor([5.8242, 3.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №989: loss = 27.86122703552246, weights = tensor([5.8242, 3.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №990: loss = 27.85309410095215, weights = tensor([5.8242, 3.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №991: loss = 27.844970703125, weights = tensor([5.8242, 3.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №992: loss = 27.83685302734375, weights = tensor([5.8242, 3.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №993: loss = 27.828744888305664, weights = tensor([5.8242, 3.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №994: loss = 27.820642471313477, weights = tensor([5.8242, 3.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №995: loss = 27.812549591064453, weights = tensor([5.8242, 3.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №996: loss = 27.804462432861328, weights = tensor([5.8242, 3.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №997: loss = 27.796382904052734, weights = tensor([5.8242, 3.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №998: loss = 27.788311004638672, weights = tensor([5.8242, 3.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №999: loss = 27.780248641967773, weights = tensor([5.8242, 3.4297], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#now let's use PyTorch gradient calculation. We don't need our gradient function. \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16, requires_grad = True) #it should be mentioned that in the future \n",
    "                                                            #you will need a derivative with respect to these values\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = mseerror(y, y_pred)\n",
    "    error.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rait*w.grad\n",
    "    w.grad.zero_()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "193b97bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Derivative is calculated by Pytorch. Slope = 5.82421875, intercept = 3.4296875, loss = 27.780248641967773')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Derivative is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7eecf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1181.300048828125, weights = tensor([0.2158, 0.0380], requires_grad=True)\n",
      "step №1: loss = 1102.7030029296875, weights = tensor([0.4239, 0.0749], requires_grad=True)\n",
      "step №2: loss = 1029.519287109375, weights = tensor([0.6248, 0.1105], requires_grad=True)\n",
      "step №3: loss = 961.3761596679688, weights = tensor([0.8186, 0.1451], requires_grad=True)\n",
      "step №4: loss = 897.9259033203125, weights = tensor([1.0056, 0.1785], requires_grad=True)\n",
      "step №5: loss = 838.8455810546875, weights = tensor([1.1860, 0.2109], requires_grad=True)\n",
      "step №6: loss = 783.8341064453125, weights = tensor([1.3600, 0.2423], requires_grad=True)\n",
      "step №7: loss = 732.6109619140625, weights = tensor([1.5280, 0.2727], requires_grad=True)\n",
      "step №8: loss = 684.9152221679688, weights = tensor([1.6900, 0.3022], requires_grad=True)\n",
      "step №9: loss = 640.504150390625, weights = tensor([1.8463, 0.3307], requires_grad=True)\n",
      "step №10: loss = 599.1510620117188, weights = tensor([1.9972, 0.3584], requires_grad=True)\n",
      "step №11: loss = 560.6455078125, weights = tensor([2.1427, 0.3852], requires_grad=True)\n",
      "step №12: loss = 524.7913208007812, weights = tensor([2.2831, 0.4112], requires_grad=True)\n",
      "step №13: loss = 491.40576171875, weights = tensor([2.4185, 0.4364], requires_grad=True)\n",
      "step №14: loss = 460.3187561035156, weights = tensor([2.5492, 0.4609], requires_grad=True)\n",
      "step №15: loss = 431.3719787597656, weights = tensor([2.6753, 0.4846], requires_grad=True)\n",
      "step №16: loss = 404.4179992675781, weights = tensor([2.7970, 0.5076], requires_grad=True)\n",
      "step №17: loss = 379.31951904296875, weights = tensor([2.9143, 0.5299], requires_grad=True)\n",
      "step №18: loss = 355.94873046875, weights = tensor([3.0276, 0.5516], requires_grad=True)\n",
      "step №19: loss = 334.1865234375, weights = tensor([3.1368, 0.5726], requires_grad=True)\n",
      "step №20: loss = 313.92218017578125, weights = tensor([3.2422, 0.5931], requires_grad=True)\n",
      "step №21: loss = 295.0523681640625, weights = tensor([3.3439, 0.6129], requires_grad=True)\n",
      "step №22: loss = 277.4812316894531, weights = tensor([3.4420, 0.6321], requires_grad=True)\n",
      "step №23: loss = 261.11907958984375, weights = tensor([3.5366, 0.6508], requires_grad=True)\n",
      "step №24: loss = 245.88284301757812, weights = tensor([3.6279, 0.6690], requires_grad=True)\n",
      "step №25: loss = 231.6947784423828, weights = tensor([3.7160, 0.6866], requires_grad=True)\n",
      "step №26: loss = 218.4827423095703, weights = tensor([3.8009, 0.7038], requires_grad=True)\n",
      "step №27: loss = 206.17947387695312, weights = tensor([3.8829, 0.7204], requires_grad=True)\n",
      "step №28: loss = 194.72238159179688, weights = tensor([3.9620, 0.7367], requires_grad=True)\n",
      "step №29: loss = 184.05320739746094, weights = tensor([4.0382, 0.7524], requires_grad=True)\n",
      "step №30: loss = 174.11758422851562, weights = tensor([4.1118, 0.7677], requires_grad=True)\n",
      "step №31: loss = 164.8650665283203, weights = tensor([4.1828, 0.7827], requires_grad=True)\n",
      "step №32: loss = 156.2484893798828, weights = tensor([4.2513, 0.7972], requires_grad=True)\n",
      "step №33: loss = 148.22415161132812, weights = tensor([4.3174, 0.8113], requires_grad=True)\n",
      "step №34: loss = 140.75119018554688, weights = tensor([4.3811, 0.8251], requires_grad=True)\n",
      "step №35: loss = 133.79164123535156, weights = tensor([4.4426, 0.8384], requires_grad=True)\n",
      "step №36: loss = 127.31010437011719, weights = tensor([4.5019, 0.8515], requires_grad=True)\n",
      "step №37: loss = 121.2737045288086, weights = tensor([4.5591, 0.8642], requires_grad=True)\n",
      "step №38: loss = 115.6517333984375, weights = tensor([4.6142, 0.8766], requires_grad=True)\n",
      "step №39: loss = 110.41563415527344, weights = tensor([4.6674, 0.8887], requires_grad=True)\n",
      "step №40: loss = 105.53886413574219, weights = tensor([4.7188, 0.9004], requires_grad=True)\n",
      "step №41: loss = 100.99662017822266, weights = tensor([4.7683, 0.9119], requires_grad=True)\n",
      "step №42: loss = 96.76591491699219, weights = tensor([4.8161, 0.9231], requires_grad=True)\n",
      "step №43: loss = 92.82520294189453, weights = tensor([4.8621, 0.9340], requires_grad=True)\n",
      "step №44: loss = 89.15455627441406, weights = tensor([4.9066, 0.9447], requires_grad=True)\n",
      "step №45: loss = 85.73541259765625, weights = tensor([4.9494, 0.9551], requires_grad=True)\n",
      "step №46: loss = 82.55039978027344, weights = tensor([4.9907, 0.9653], requires_grad=True)\n",
      "step №47: loss = 79.58336639404297, weights = tensor([5.0306, 0.9752], requires_grad=True)\n",
      "step №48: loss = 76.81935119628906, weights = tensor([5.0691, 0.9849], requires_grad=True)\n",
      "step №49: loss = 74.24434661865234, weights = tensor([5.1061, 0.9944], requires_grad=True)\n",
      "step №50: loss = 71.84530639648438, weights = tensor([5.1419, 1.0037], requires_grad=True)\n",
      "step №51: loss = 69.61014556884766, weights = tensor([5.1764, 1.0127], requires_grad=True)\n",
      "step №52: loss = 67.52757263183594, weights = tensor([5.2096, 1.0216], requires_grad=True)\n",
      "step №53: loss = 65.58702850341797, weights = tensor([5.2417, 1.0303], requires_grad=True)\n",
      "step №54: loss = 63.778770446777344, weights = tensor([5.2726, 1.0388], requires_grad=True)\n",
      "step №55: loss = 62.09366989135742, weights = tensor([5.3025, 1.0471], requires_grad=True)\n",
      "step №56: loss = 60.52326583862305, weights = tensor([5.3312, 1.0553], requires_grad=True)\n",
      "step №57: loss = 59.05965042114258, weights = tensor([5.3590, 1.0632], requires_grad=True)\n",
      "step №58: loss = 57.695457458496094, weights = tensor([5.3857, 1.0711], requires_grad=True)\n",
      "step №59: loss = 56.42383575439453, weights = tensor([5.4115, 1.0787], requires_grad=True)\n",
      "step №60: loss = 55.2384033203125, weights = tensor([5.4364, 1.0863], requires_grad=True)\n",
      "step №61: loss = 54.13324737548828, weights = tensor([5.4603, 1.0936], requires_grad=True)\n",
      "step №62: loss = 53.102813720703125, weights = tensor([5.4834, 1.1009], requires_grad=True)\n",
      "step №63: loss = 52.1419677734375, weights = tensor([5.5057, 1.1080], requires_grad=True)\n",
      "step №64: loss = 51.24592208862305, weights = tensor([5.5272, 1.1150], requires_grad=True)\n",
      "step №65: loss = 50.410194396972656, weights = tensor([5.5479, 1.1218], requires_grad=True)\n",
      "step №66: loss = 49.630653381347656, weights = tensor([5.5679, 1.1286], requires_grad=True)\n",
      "step №67: loss = 48.90341567993164, weights = tensor([5.5871, 1.1352], requires_grad=True)\n",
      "step №68: loss = 48.224876403808594, weights = tensor([5.6057, 1.1417], requires_grad=True)\n",
      "step №69: loss = 47.591697692871094, weights = tensor([5.6235, 1.1481], requires_grad=True)\n",
      "step №70: loss = 47.0007438659668, weights = tensor([5.6408, 1.1544], requires_grad=True)\n",
      "step №71: loss = 46.449119567871094, weights = tensor([5.6574, 1.1606], requires_grad=True)\n",
      "step №72: loss = 45.934085845947266, weights = tensor([5.6734, 1.1667], requires_grad=True)\n",
      "step №73: loss = 45.45314025878906, weights = tensor([5.6888, 1.1727], requires_grad=True)\n",
      "step №74: loss = 45.00394821166992, weights = tensor([5.7037, 1.1786], requires_grad=True)\n",
      "step №75: loss = 44.58430480957031, weights = tensor([5.7180, 1.1844], requires_grad=True)\n",
      "step №76: loss = 44.192176818847656, weights = tensor([5.7318, 1.1902], requires_grad=True)\n",
      "step №77: loss = 43.82567596435547, weights = tensor([5.7451, 1.1958], requires_grad=True)\n",
      "step №78: loss = 43.48303985595703, weights = tensor([5.7580, 1.2014], requires_grad=True)\n",
      "step №79: loss = 43.16260528564453, weights = tensor([5.7703, 1.2069], requires_grad=True)\n",
      "step №80: loss = 42.862876892089844, weights = tensor([5.7822, 1.2123], requires_grad=True)\n",
      "step №81: loss = 42.58240509033203, weights = tensor([5.7937, 1.2177], requires_grad=True)\n",
      "step №82: loss = 42.319862365722656, weights = tensor([5.8047, 1.2230], requires_grad=True)\n",
      "step №83: loss = 42.07404708862305, weights = tensor([5.8153, 1.2282], requires_grad=True)\n",
      "step №84: loss = 41.843772888183594, weights = tensor([5.8256, 1.2334], requires_grad=True)\n",
      "step №85: loss = 41.62797164916992, weights = tensor([5.8355, 1.2385], requires_grad=True)\n",
      "step №86: loss = 41.42566680908203, weights = tensor([5.8450, 1.2435], requires_grad=True)\n",
      "step №87: loss = 41.23590850830078, weights = tensor([5.8541, 1.2485], requires_grad=True)\n",
      "step №88: loss = 41.05785369873047, weights = tensor([5.8629, 1.2534], requires_grad=True)\n",
      "step №89: loss = 40.89067840576172, weights = tensor([5.8714, 1.2583], requires_grad=True)\n",
      "step №90: loss = 40.7336540222168, weights = tensor([5.8796, 1.2631], requires_grad=True)\n",
      "step №91: loss = 40.5860595703125, weights = tensor([5.8874, 1.2679], requires_grad=True)\n",
      "step №92: loss = 40.447261810302734, weights = tensor([5.8950, 1.2726], requires_grad=True)\n",
      "step №93: loss = 40.316650390625, weights = tensor([5.9023, 1.2773], requires_grad=True)\n",
      "step №94: loss = 40.19367218017578, weights = tensor([5.9093, 1.2820], requires_grad=True)\n",
      "step №95: loss = 40.077789306640625, weights = tensor([5.9160, 1.2865], requires_grad=True)\n",
      "step №96: loss = 39.9685173034668, weights = tensor([5.9225, 1.2911], requires_grad=True)\n",
      "step №97: loss = 39.865394592285156, weights = tensor([5.9287, 1.2956], requires_grad=True)\n",
      "step №98: loss = 39.76801300048828, weights = tensor([5.9347, 1.3001], requires_grad=True)\n",
      "step №99: loss = 39.67597961425781, weights = tensor([5.9405, 1.3045], requires_grad=True)\n",
      "step №100: loss = 39.588905334472656, weights = tensor([5.9460, 1.3089], requires_grad=True)\n",
      "step №101: loss = 39.506465911865234, weights = tensor([5.9514, 1.3133], requires_grad=True)\n",
      "step №102: loss = 39.428340911865234, weights = tensor([5.9565, 1.3176], requires_grad=True)\n",
      "step №103: loss = 39.35423278808594, weights = tensor([5.9614, 1.3219], requires_grad=True)\n",
      "step №104: loss = 39.28386306762695, weights = tensor([5.9662, 1.3261], requires_grad=True)\n",
      "step №105: loss = 39.21699142456055, weights = tensor([5.9707, 1.3304], requires_grad=True)\n",
      "step №106: loss = 39.153350830078125, weights = tensor([5.9751, 1.3346], requires_grad=True)\n",
      "step №107: loss = 39.09273910522461, weights = tensor([5.9793, 1.3388], requires_grad=True)\n",
      "step №108: loss = 39.034934997558594, weights = tensor([5.9834, 1.3429], requires_grad=True)\n",
      "step №109: loss = 38.97976303100586, weights = tensor([5.9872, 1.3470], requires_grad=True)\n",
      "step №110: loss = 38.92700958251953, weights = tensor([5.9910, 1.3511], requires_grad=True)\n",
      "step №111: loss = 38.876564025878906, weights = tensor([5.9945, 1.3552], requires_grad=True)\n",
      "step №112: loss = 38.828216552734375, weights = tensor([5.9980, 1.3592], requires_grad=True)\n",
      "step №113: loss = 38.7818489074707, weights = tensor([6.0013, 1.3632], requires_grad=True)\n",
      "step №114: loss = 38.737327575683594, weights = tensor([6.0044, 1.3672], requires_grad=True)\n",
      "step №115: loss = 38.69451141357422, weights = tensor([6.0074, 1.3712], requires_grad=True)\n",
      "step №116: loss = 38.65329360961914, weights = tensor([6.0103, 1.3752], requires_grad=True)\n",
      "step №117: loss = 38.61357116699219, weights = tensor([6.0131, 1.3791], requires_grad=True)\n",
      "step №118: loss = 38.575218200683594, weights = tensor([6.0158, 1.3830], requires_grad=True)\n",
      "step №119: loss = 38.53816604614258, weights = tensor([6.0183, 1.3869], requires_grad=True)\n",
      "step №120: loss = 38.50231170654297, weights = tensor([6.0208, 1.3908], requires_grad=True)\n",
      "step №121: loss = 38.46758270263672, weights = tensor([6.0231, 1.3946], requires_grad=True)\n",
      "step №122: loss = 38.43389892578125, weights = tensor([6.0254, 1.3985], requires_grad=True)\n",
      "step №123: loss = 38.401187896728516, weights = tensor([6.0275, 1.4023], requires_grad=True)\n",
      "step №124: loss = 38.36937713623047, weights = tensor([6.0295, 1.4061], requires_grad=True)\n",
      "step №125: loss = 38.33842468261719, weights = tensor([6.0315, 1.4099], requires_grad=True)\n",
      "step №126: loss = 38.30826187133789, weights = tensor([6.0334, 1.4137], requires_grad=True)\n",
      "step №127: loss = 38.2788200378418, weights = tensor([6.0352, 1.4175], requires_grad=True)\n",
      "step №128: loss = 38.25006866455078, weights = tensor([6.0369, 1.4212], requires_grad=True)\n",
      "step №129: loss = 38.221954345703125, weights = tensor([6.0385, 1.4249], requires_grad=True)\n",
      "step №130: loss = 38.1944580078125, weights = tensor([6.0400, 1.4287], requires_grad=True)\n",
      "step №131: loss = 38.167484283447266, weights = tensor([6.0415, 1.4324], requires_grad=True)\n",
      "step №132: loss = 38.141048431396484, weights = tensor([6.0429, 1.4361], requires_grad=True)\n",
      "step №133: loss = 38.115089416503906, weights = tensor([6.0443, 1.4398], requires_grad=True)\n",
      "step №134: loss = 38.0895881652832, weights = tensor([6.0455, 1.4434], requires_grad=True)\n",
      "step №135: loss = 38.06450653076172, weights = tensor([6.0467, 1.4471], requires_grad=True)\n",
      "step №136: loss = 38.03981399536133, weights = tensor([6.0479, 1.4507], requires_grad=True)\n",
      "step №137: loss = 38.01548767089844, weights = tensor([6.0490, 1.4544], requires_grad=True)\n",
      "step №138: loss = 37.99150085449219, weights = tensor([6.0500, 1.4580], requires_grad=True)\n",
      "step №139: loss = 37.96784210205078, weights = tensor([6.0510, 1.4616], requires_grad=True)\n",
      "step №140: loss = 37.94448471069336, weights = tensor([6.0519, 1.4652], requires_grad=True)\n",
      "step №141: loss = 37.9213981628418, weights = tensor([6.0528, 1.4688], requires_grad=True)\n",
      "step №142: loss = 37.898582458496094, weights = tensor([6.0536, 1.4724], requires_grad=True)\n",
      "step №143: loss = 37.8759880065918, weights = tensor([6.0544, 1.4760], requires_grad=True)\n",
      "step №144: loss = 37.853641510009766, weights = tensor([6.0551, 1.4796], requires_grad=True)\n",
      "step №145: loss = 37.83150100708008, weights = tensor([6.0558, 1.4832], requires_grad=True)\n",
      "step №146: loss = 37.80956268310547, weights = tensor([6.0564, 1.4867], requires_grad=True)\n",
      "step №147: loss = 37.787803649902344, weights = tensor([6.0570, 1.4903], requires_grad=True)\n",
      "step №148: loss = 37.76622772216797, weights = tensor([6.0576, 1.4938], requires_grad=True)\n",
      "step №149: loss = 37.744808197021484, weights = tensor([6.0581, 1.4974], requires_grad=True)\n",
      "step №150: loss = 37.723548889160156, weights = tensor([6.0586, 1.5009], requires_grad=True)\n",
      "step №151: loss = 37.70242691040039, weights = tensor([6.0591, 1.5044], requires_grad=True)\n",
      "step №152: loss = 37.68144226074219, weights = tensor([6.0595, 1.5079], requires_grad=True)\n",
      "step №153: loss = 37.66057586669922, weights = tensor([6.0599, 1.5114], requires_grad=True)\n",
      "step №154: loss = 37.63983917236328, weights = tensor([6.0602, 1.5149], requires_grad=True)\n",
      "step №155: loss = 37.61920928955078, weights = tensor([6.0605, 1.5184], requires_grad=True)\n",
      "step №156: loss = 37.59867477416992, weights = tensor([6.0608, 1.5219], requires_grad=True)\n",
      "step №157: loss = 37.5782470703125, weights = tensor([6.0611, 1.5254], requires_grad=True)\n",
      "step №158: loss = 37.55791473388672, weights = tensor([6.0613, 1.5289], requires_grad=True)\n",
      "step №159: loss = 37.53765869140625, weights = tensor([6.0615, 1.5324], requires_grad=True)\n",
      "step №160: loss = 37.517494201660156, weights = tensor([6.0617, 1.5358], requires_grad=True)\n",
      "step №161: loss = 37.49739456176758, weights = tensor([6.0619, 1.5393], requires_grad=True)\n",
      "step №162: loss = 37.47737503051758, weights = tensor([6.0620, 1.5428], requires_grad=True)\n",
      "step №163: loss = 37.457420349121094, weights = tensor([6.0621, 1.5462], requires_grad=True)\n",
      "step №164: loss = 37.43754577636719, weights = tensor([6.0622, 1.5497], requires_grad=True)\n",
      "step №165: loss = 37.41771697998047, weights = tensor([6.0623, 1.5531], requires_grad=True)\n",
      "step №166: loss = 37.39794158935547, weights = tensor([6.0623, 1.5565], requires_grad=True)\n",
      "step №167: loss = 37.37823486328125, weights = tensor([6.0623, 1.5600], requires_grad=True)\n",
      "step №168: loss = 37.35857009887695, weights = tensor([6.0623, 1.5634], requires_grad=True)\n",
      "step №169: loss = 37.338951110839844, weights = tensor([6.0623, 1.5668], requires_grad=True)\n",
      "step №170: loss = 37.31938934326172, weights = tensor([6.0623, 1.5703], requires_grad=True)\n",
      "step №171: loss = 37.299869537353516, weights = tensor([6.0622, 1.5737], requires_grad=True)\n",
      "step №172: loss = 37.28038787841797, weights = tensor([6.0622, 1.5771], requires_grad=True)\n",
      "step №173: loss = 37.26094055175781, weights = tensor([6.0621, 1.5805], requires_grad=True)\n",
      "step №174: loss = 37.24153518676758, weights = tensor([6.0620, 1.5839], requires_grad=True)\n",
      "step №175: loss = 37.22217559814453, weights = tensor([6.0619, 1.5873], requires_grad=True)\n",
      "step №176: loss = 37.202850341796875, weights = tensor([6.0617, 1.5907], requires_grad=True)\n",
      "step №177: loss = 37.18354034423828, weights = tensor([6.0616, 1.5941], requires_grad=True)\n",
      "step №178: loss = 37.16427230834961, weights = tensor([6.0614, 1.5975], requires_grad=True)\n",
      "step №179: loss = 37.145042419433594, weights = tensor([6.0613, 1.6009], requires_grad=True)\n",
      "step №180: loss = 37.125831604003906, weights = tensor([6.0611, 1.6043], requires_grad=True)\n",
      "step №181: loss = 37.106658935546875, weights = tensor([6.0609, 1.6077], requires_grad=True)\n",
      "step №182: loss = 37.087501525878906, weights = tensor([6.0607, 1.6111], requires_grad=True)\n",
      "step №183: loss = 37.06837844848633, weights = tensor([6.0605, 1.6144], requires_grad=True)\n",
      "step №184: loss = 37.04928207397461, weights = tensor([6.0603, 1.6178], requires_grad=True)\n",
      "step №185: loss = 37.03020477294922, weights = tensor([6.0600, 1.6212], requires_grad=True)\n",
      "step №186: loss = 37.01115036010742, weights = tensor([6.0598, 1.6246], requires_grad=True)\n",
      "step №187: loss = 36.99211883544922, weights = tensor([6.0595, 1.6279], requires_grad=True)\n",
      "step №188: loss = 36.973106384277344, weights = tensor([6.0593, 1.6313], requires_grad=True)\n",
      "step №189: loss = 36.954124450683594, weights = tensor([6.0590, 1.6347], requires_grad=True)\n",
      "step №190: loss = 36.93516159057617, weights = tensor([6.0587, 1.6380], requires_grad=True)\n",
      "step №191: loss = 36.916221618652344, weights = tensor([6.0584, 1.6414], requires_grad=True)\n",
      "step №192: loss = 36.89729690551758, weights = tensor([6.0581, 1.6447], requires_grad=True)\n",
      "step №193: loss = 36.878387451171875, weights = tensor([6.0578, 1.6481], requires_grad=True)\n",
      "step №194: loss = 36.859500885009766, weights = tensor([6.0575, 1.6514], requires_grad=True)\n",
      "step №195: loss = 36.84062576293945, weights = tensor([6.0572, 1.6548], requires_grad=True)\n",
      "step №196: loss = 36.82178497314453, weights = tensor([6.0568, 1.6581], requires_grad=True)\n",
      "step №197: loss = 36.802947998046875, weights = tensor([6.0565, 1.6615], requires_grad=True)\n",
      "step №198: loss = 36.78413009643555, weights = tensor([6.0561, 1.6648], requires_grad=True)\n",
      "step №199: loss = 36.76532745361328, weights = tensor([6.0558, 1.6681], requires_grad=True)\n",
      "step №200: loss = 36.746559143066406, weights = tensor([6.0554, 1.6715], requires_grad=True)\n",
      "step №201: loss = 36.727779388427734, weights = tensor([6.0551, 1.6748], requires_grad=True)\n",
      "step №202: loss = 36.709041595458984, weights = tensor([6.0547, 1.6781], requires_grad=True)\n",
      "step №203: loss = 36.6902961730957, weights = tensor([6.0543, 1.6815], requires_grad=True)\n",
      "step №204: loss = 36.67158126831055, weights = tensor([6.0540, 1.6848], requires_grad=True)\n",
      "step №205: loss = 36.65287780761719, weights = tensor([6.0536, 1.6881], requires_grad=True)\n",
      "step №206: loss = 36.634193420410156, weights = tensor([6.0532, 1.6915], requires_grad=True)\n",
      "step №207: loss = 36.615516662597656, weights = tensor([6.0528, 1.6948], requires_grad=True)\n",
      "step №208: loss = 36.596858978271484, weights = tensor([6.0524, 1.6981], requires_grad=True)\n",
      "step №209: loss = 36.578224182128906, weights = tensor([6.0520, 1.7014], requires_grad=True)\n",
      "step №210: loss = 36.55958938598633, weights = tensor([6.0516, 1.7047], requires_grad=True)\n",
      "step №211: loss = 36.54096603393555, weights = tensor([6.0512, 1.7081], requires_grad=True)\n",
      "step №212: loss = 36.52238082885742, weights = tensor([6.0508, 1.7114], requires_grad=True)\n",
      "step №213: loss = 36.5037841796875, weights = tensor([6.0503, 1.7147], requires_grad=True)\n",
      "step №214: loss = 36.48522186279297, weights = tensor([6.0499, 1.7180], requires_grad=True)\n",
      "step №215: loss = 36.4666633605957, weights = tensor([6.0495, 1.7213], requires_grad=True)\n",
      "step №216: loss = 36.44811248779297, weights = tensor([6.0491, 1.7246], requires_grad=True)\n",
      "step №217: loss = 36.42958450317383, weights = tensor([6.0486, 1.7279], requires_grad=True)\n",
      "step №218: loss = 36.41105651855469, weights = tensor([6.0482, 1.7312], requires_grad=True)\n",
      "step №219: loss = 36.39255905151367, weights = tensor([6.0478, 1.7345], requires_grad=True)\n",
      "step №220: loss = 36.37407684326172, weights = tensor([6.0473, 1.7378], requires_grad=True)\n",
      "step №221: loss = 36.35559844970703, weights = tensor([6.0469, 1.7411], requires_grad=True)\n",
      "step №222: loss = 36.33713150024414, weights = tensor([6.0464, 1.7444], requires_grad=True)\n",
      "step №223: loss = 36.31869125366211, weights = tensor([6.0460, 1.7477], requires_grad=True)\n",
      "step №224: loss = 36.300254821777344, weights = tensor([6.0455, 1.7510], requires_grad=True)\n",
      "step №225: loss = 36.281822204589844, weights = tensor([6.0451, 1.7543], requires_grad=True)\n",
      "step №226: loss = 36.263404846191406, weights = tensor([6.0446, 1.7576], requires_grad=True)\n",
      "step №227: loss = 36.245018005371094, weights = tensor([6.0442, 1.7609], requires_grad=True)\n",
      "step №228: loss = 36.22664260864258, weights = tensor([6.0437, 1.7642], requires_grad=True)\n",
      "step №229: loss = 36.20826721191406, weights = tensor([6.0433, 1.7675], requires_grad=True)\n",
      "step №230: loss = 36.189903259277344, weights = tensor([6.0428, 1.7707], requires_grad=True)\n",
      "step №231: loss = 36.17156219482422, weights = tensor([6.0423, 1.7740], requires_grad=True)\n",
      "step №232: loss = 36.153221130371094, weights = tensor([6.0419, 1.7773], requires_grad=True)\n",
      "step №233: loss = 36.13490676879883, weights = tensor([6.0414, 1.7806], requires_grad=True)\n",
      "step №234: loss = 36.116600036621094, weights = tensor([6.0409, 1.7839], requires_grad=True)\n",
      "step №235: loss = 36.09830093383789, weights = tensor([6.0404, 1.7871], requires_grad=True)\n",
      "step №236: loss = 36.08000946044922, weights = tensor([6.0400, 1.7904], requires_grad=True)\n",
      "step №237: loss = 36.06174850463867, weights = tensor([6.0395, 1.7937], requires_grad=True)\n",
      "step №238: loss = 36.043495178222656, weights = tensor([6.0390, 1.7970], requires_grad=True)\n",
      "step №239: loss = 36.025245666503906, weights = tensor([6.0385, 1.8002], requires_grad=True)\n",
      "step №240: loss = 36.00701141357422, weights = tensor([6.0381, 1.8035], requires_grad=True)\n",
      "step №241: loss = 35.98880386352539, weights = tensor([6.0376, 1.8068], requires_grad=True)\n",
      "step №242: loss = 35.9705924987793, weights = tensor([6.0371, 1.8101], requires_grad=True)\n",
      "step №243: loss = 35.952396392822266, weights = tensor([6.0366, 1.8133], requires_grad=True)\n",
      "step №244: loss = 35.934226989746094, weights = tensor([6.0361, 1.8166], requires_grad=True)\n",
      "step №245: loss = 35.91604995727539, weights = tensor([6.0356, 1.8199], requires_grad=True)\n",
      "step №246: loss = 35.89789581298828, weights = tensor([6.0352, 1.8231], requires_grad=True)\n",
      "step №247: loss = 35.87975311279297, weights = tensor([6.0347, 1.8264], requires_grad=True)\n",
      "step №248: loss = 35.86161804199219, weights = tensor([6.0342, 1.8296], requires_grad=True)\n",
      "step №249: loss = 35.843505859375, weights = tensor([6.0337, 1.8329], requires_grad=True)\n",
      "step №250: loss = 35.82539749145508, weights = tensor([6.0332, 1.8362], requires_grad=True)\n",
      "step №251: loss = 35.807289123535156, weights = tensor([6.0327, 1.8394], requires_grad=True)\n",
      "step №252: loss = 35.78921127319336, weights = tensor([6.0322, 1.8427], requires_grad=True)\n",
      "step №253: loss = 35.77115249633789, weights = tensor([6.0317, 1.8459], requires_grad=True)\n",
      "step №254: loss = 35.753089904785156, weights = tensor([6.0312, 1.8492], requires_grad=True)\n",
      "step №255: loss = 35.735042572021484, weights = tensor([6.0307, 1.8524], requires_grad=True)\n",
      "step №256: loss = 35.717018127441406, weights = tensor([6.0302, 1.8557], requires_grad=True)\n",
      "step №257: loss = 35.69899368286133, weights = tensor([6.0298, 1.8589], requires_grad=True)\n",
      "step №258: loss = 35.68098831176758, weights = tensor([6.0293, 1.8622], requires_grad=True)\n",
      "step №259: loss = 35.662986755371094, weights = tensor([6.0288, 1.8654], requires_grad=True)\n",
      "step №260: loss = 35.64500045776367, weights = tensor([6.0283, 1.8687], requires_grad=True)\n",
      "step №261: loss = 35.62704086303711, weights = tensor([6.0278, 1.8719], requires_grad=True)\n",
      "step №262: loss = 35.60907745361328, weights = tensor([6.0273, 1.8752], requires_grad=True)\n",
      "step №263: loss = 35.59113311767578, weights = tensor([6.0268, 1.8784], requires_grad=True)\n",
      "step №264: loss = 35.573204040527344, weights = tensor([6.0263, 1.8817], requires_grad=True)\n",
      "step №265: loss = 35.555267333984375, weights = tensor([6.0258, 1.8849], requires_grad=True)\n",
      "step №266: loss = 35.53736877441406, weights = tensor([6.0253, 1.8881], requires_grad=True)\n",
      "step №267: loss = 35.51947021484375, weights = tensor([6.0248, 1.8914], requires_grad=True)\n",
      "step №268: loss = 35.50157165527344, weights = tensor([6.0243, 1.8946], requires_grad=True)\n",
      "step №269: loss = 35.48370361328125, weights = tensor([6.0238, 1.8978], requires_grad=True)\n",
      "step №270: loss = 35.465843200683594, weights = tensor([6.0233, 1.9011], requires_grad=True)\n",
      "step №271: loss = 35.44799041748047, weights = tensor([6.0228, 1.9043], requires_grad=True)\n",
      "step №272: loss = 35.430152893066406, weights = tensor([6.0223, 1.9075], requires_grad=True)\n",
      "step №273: loss = 35.41233444213867, weights = tensor([6.0218, 1.9108], requires_grad=True)\n",
      "step №274: loss = 35.39452362060547, weights = tensor([6.0213, 1.9140], requires_grad=True)\n",
      "step №275: loss = 35.376712799072266, weights = tensor([6.0208, 1.9172], requires_grad=True)\n",
      "step №276: loss = 35.358924865722656, weights = tensor([6.0203, 1.9205], requires_grad=True)\n",
      "step №277: loss = 35.341148376464844, weights = tensor([6.0198, 1.9237], requires_grad=True)\n",
      "step №278: loss = 35.32338333129883, weights = tensor([6.0192, 1.9269], requires_grad=True)\n",
      "step №279: loss = 35.305641174316406, weights = tensor([6.0187, 1.9301], requires_grad=True)\n",
      "step №280: loss = 35.28789520263672, weights = tensor([6.0182, 1.9334], requires_grad=True)\n",
      "step №281: loss = 35.2701530456543, weights = tensor([6.0177, 1.9366], requires_grad=True)\n",
      "step №282: loss = 35.25244903564453, weights = tensor([6.0172, 1.9398], requires_grad=True)\n",
      "step №283: loss = 35.234745025634766, weights = tensor([6.0167, 1.9430], requires_grad=True)\n",
      "step №284: loss = 35.217044830322266, weights = tensor([6.0162, 1.9462], requires_grad=True)\n",
      "step №285: loss = 35.199371337890625, weights = tensor([6.0157, 1.9495], requires_grad=True)\n",
      "step №286: loss = 35.18170166015625, weights = tensor([6.0152, 1.9527], requires_grad=True)\n",
      "step №287: loss = 35.16404342651367, weights = tensor([6.0147, 1.9559], requires_grad=True)\n",
      "step №288: loss = 35.146400451660156, weights = tensor([6.0142, 1.9591], requires_grad=True)\n",
      "step №289: loss = 35.128761291503906, weights = tensor([6.0137, 1.9623], requires_grad=True)\n",
      "step №290: loss = 35.111141204833984, weights = tensor([6.0132, 1.9655], requires_grad=True)\n",
      "step №291: loss = 35.093528747558594, weights = tensor([6.0127, 1.9687], requires_grad=True)\n",
      "step №292: loss = 35.07593536376953, weights = tensor([6.0122, 1.9719], requires_grad=True)\n",
      "step №293: loss = 35.05835723876953, weights = tensor([6.0117, 1.9751], requires_grad=True)\n",
      "step №294: loss = 35.0407829284668, weights = tensor([6.0112, 1.9784], requires_grad=True)\n",
      "step №295: loss = 35.023216247558594, weights = tensor([6.0107, 1.9816], requires_grad=True)\n",
      "step №296: loss = 35.00566864013672, weights = tensor([6.0102, 1.9848], requires_grad=True)\n",
      "step №297: loss = 34.98813247680664, weights = tensor([6.0097, 1.9880], requires_grad=True)\n",
      "step №298: loss = 34.970603942871094, weights = tensor([6.0092, 1.9912], requires_grad=True)\n",
      "step №299: loss = 34.95309066772461, weights = tensor([6.0086, 1.9944], requires_grad=True)\n",
      "step №300: loss = 34.93559646606445, weights = tensor([6.0081, 1.9976], requires_grad=True)\n",
      "step №301: loss = 34.91809844970703, weights = tensor([6.0076, 2.0008], requires_grad=True)\n",
      "step №302: loss = 34.90061569213867, weights = tensor([6.0071, 2.0040], requires_grad=True)\n",
      "step №303: loss = 34.883155822753906, weights = tensor([6.0066, 2.0072], requires_grad=True)\n",
      "step №304: loss = 34.86570739746094, weights = tensor([6.0061, 2.0104], requires_grad=True)\n",
      "step №305: loss = 34.8482551574707, weights = tensor([6.0056, 2.0136], requires_grad=True)\n",
      "step №306: loss = 34.83082962036133, weights = tensor([6.0051, 2.0167], requires_grad=True)\n",
      "step №307: loss = 34.81340408325195, weights = tensor([6.0046, 2.0199], requires_grad=True)\n",
      "step №308: loss = 34.795997619628906, weights = tensor([6.0041, 2.0231], requires_grad=True)\n",
      "step №309: loss = 34.778602600097656, weights = tensor([6.0036, 2.0263], requires_grad=True)\n",
      "step №310: loss = 34.761226654052734, weights = tensor([6.0031, 2.0295], requires_grad=True)\n",
      "step №311: loss = 34.74385452270508, weights = tensor([6.0026, 2.0327], requires_grad=True)\n",
      "step №312: loss = 34.72649002075195, weights = tensor([6.0021, 2.0359], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №313: loss = 34.709144592285156, weights = tensor([6.0016, 2.0391], requires_grad=True)\n",
      "step №314: loss = 34.69180679321289, weights = tensor([6.0011, 2.0423], requires_grad=True)\n",
      "step №315: loss = 34.67448425292969, weights = tensor([6.0006, 2.0454], requires_grad=True)\n",
      "step №316: loss = 34.657161712646484, weights = tensor([6.0001, 2.0486], requires_grad=True)\n",
      "step №317: loss = 34.639862060546875, weights = tensor([5.9996, 2.0518], requires_grad=True)\n",
      "step №318: loss = 34.62257385253906, weights = tensor([5.9991, 2.0550], requires_grad=True)\n",
      "step №319: loss = 34.60530090332031, weights = tensor([5.9985, 2.0582], requires_grad=True)\n",
      "step №320: loss = 34.58802795410156, weights = tensor([5.9980, 2.0613], requires_grad=True)\n",
      "step №321: loss = 34.57078552246094, weights = tensor([5.9975, 2.0645], requires_grad=True)\n",
      "step №322: loss = 34.55352783203125, weights = tensor([5.9970, 2.0677], requires_grad=True)\n",
      "step №323: loss = 34.53630447387695, weights = tensor([5.9965, 2.0709], requires_grad=True)\n",
      "step №324: loss = 34.519081115722656, weights = tensor([5.9960, 2.0740], requires_grad=True)\n",
      "step №325: loss = 34.50187683105469, weights = tensor([5.9955, 2.0772], requires_grad=True)\n",
      "step №326: loss = 34.48468017578125, weights = tensor([5.9950, 2.0804], requires_grad=True)\n",
      "step №327: loss = 34.467498779296875, weights = tensor([5.9945, 2.0836], requires_grad=True)\n",
      "step №328: loss = 34.4503173828125, weights = tensor([5.9940, 2.0867], requires_grad=True)\n",
      "step №329: loss = 34.433162689208984, weights = tensor([5.9935, 2.0899], requires_grad=True)\n",
      "step №330: loss = 34.41602325439453, weights = tensor([5.9930, 2.0931], requires_grad=True)\n",
      "step №331: loss = 34.39888000488281, weights = tensor([5.9925, 2.0962], requires_grad=True)\n",
      "step №332: loss = 34.381752014160156, weights = tensor([5.9920, 2.0994], requires_grad=True)\n",
      "step №333: loss = 34.36464309692383, weights = tensor([5.9915, 2.1026], requires_grad=True)\n",
      "step №334: loss = 34.347530364990234, weights = tensor([5.9910, 2.1057], requires_grad=True)\n",
      "step №335: loss = 34.330440521240234, weights = tensor([5.9905, 2.1089], requires_grad=True)\n",
      "step №336: loss = 34.31336212158203, weights = tensor([5.9900, 2.1120], requires_grad=True)\n",
      "step №337: loss = 34.29629898071289, weights = tensor([5.9895, 2.1152], requires_grad=True)\n",
      "step №338: loss = 34.27923583984375, weights = tensor([5.9890, 2.1184], requires_grad=True)\n",
      "step №339: loss = 34.26219177246094, weights = tensor([5.9885, 2.1215], requires_grad=True)\n",
      "step №340: loss = 34.245155334472656, weights = tensor([5.9880, 2.1247], requires_grad=True)\n",
      "step №341: loss = 34.2281379699707, weights = tensor([5.9875, 2.1278], requires_grad=True)\n",
      "step №342: loss = 34.211124420166016, weights = tensor([5.9870, 2.1310], requires_grad=True)\n",
      "step №343: loss = 34.19412612915039, weights = tensor([5.9865, 2.1341], requires_grad=True)\n",
      "step №344: loss = 34.17713165283203, weights = tensor([5.9860, 2.1373], requires_grad=True)\n",
      "step №345: loss = 34.16016387939453, weights = tensor([5.9855, 2.1404], requires_grad=True)\n",
      "step №346: loss = 34.14319610595703, weights = tensor([5.9850, 2.1436], requires_grad=True)\n",
      "step №347: loss = 34.12624740600586, weights = tensor([5.9845, 2.1467], requires_grad=True)\n",
      "step №348: loss = 34.10930633544922, weights = tensor([5.9840, 2.1499], requires_grad=True)\n",
      "step №349: loss = 34.092369079589844, weights = tensor([5.9835, 2.1530], requires_grad=True)\n",
      "step №350: loss = 34.07545471191406, weights = tensor([5.9830, 2.1562], requires_grad=True)\n",
      "step №351: loss = 34.058555603027344, weights = tensor([5.9825, 2.1593], requires_grad=True)\n",
      "step №352: loss = 34.04165267944336, weights = tensor([5.9820, 2.1625], requires_grad=True)\n",
      "step №353: loss = 34.02476501464844, weights = tensor([5.9815, 2.1656], requires_grad=True)\n",
      "step №354: loss = 34.007896423339844, weights = tensor([5.9810, 2.1687], requires_grad=True)\n",
      "step №355: loss = 33.991031646728516, weights = tensor([5.9805, 2.1719], requires_grad=True)\n",
      "step №356: loss = 33.97418975830078, weights = tensor([5.9800, 2.1750], requires_grad=True)\n",
      "step №357: loss = 33.95734405517578, weights = tensor([5.9795, 2.1782], requires_grad=True)\n",
      "step №358: loss = 33.94052505493164, weights = tensor([5.9790, 2.1813], requires_grad=True)\n",
      "step №359: loss = 33.923702239990234, weights = tensor([5.9785, 2.1844], requires_grad=True)\n",
      "step №360: loss = 33.90690612792969, weights = tensor([5.9780, 2.1876], requires_grad=True)\n",
      "step №361: loss = 33.890098571777344, weights = tensor([5.9775, 2.1907], requires_grad=True)\n",
      "step №362: loss = 33.873329162597656, weights = tensor([5.9770, 2.1938], requires_grad=True)\n",
      "step №363: loss = 33.8565559387207, weights = tensor([5.9765, 2.1970], requires_grad=True)\n",
      "step №364: loss = 33.83979797363281, weights = tensor([5.9760, 2.2001], requires_grad=True)\n",
      "step №365: loss = 33.82305145263672, weights = tensor([5.9755, 2.2032], requires_grad=True)\n",
      "step №366: loss = 33.806312561035156, weights = tensor([5.9750, 2.2064], requires_grad=True)\n",
      "step №367: loss = 33.78958511352539, weights = tensor([5.9745, 2.2095], requires_grad=True)\n",
      "step №368: loss = 33.77287292480469, weights = tensor([5.9740, 2.2126], requires_grad=True)\n",
      "step №369: loss = 33.75617218017578, weights = tensor([5.9735, 2.2157], requires_grad=True)\n",
      "step №370: loss = 33.739479064941406, weights = tensor([5.9730, 2.2189], requires_grad=True)\n",
      "step №371: loss = 33.722801208496094, weights = tensor([5.9725, 2.2220], requires_grad=True)\n",
      "step №372: loss = 33.70613098144531, weights = tensor([5.9720, 2.2251], requires_grad=True)\n",
      "step №373: loss = 33.689476013183594, weights = tensor([5.9715, 2.2282], requires_grad=True)\n",
      "step №374: loss = 33.67283630371094, weights = tensor([5.9710, 2.2314], requires_grad=True)\n",
      "step №375: loss = 33.65619659423828, weights = tensor([5.9705, 2.2345], requires_grad=True)\n",
      "step №376: loss = 33.63957595825195, weights = tensor([5.9700, 2.2376], requires_grad=True)\n",
      "step №377: loss = 33.62297058105469, weights = tensor([5.9695, 2.2407], requires_grad=True)\n",
      "step №378: loss = 33.60636901855469, weights = tensor([5.9690, 2.2438], requires_grad=True)\n",
      "step №379: loss = 33.58976745605469, weights = tensor([5.9685, 2.2469], requires_grad=True)\n",
      "step №380: loss = 33.57319259643555, weights = tensor([5.9680, 2.2501], requires_grad=True)\n",
      "step №381: loss = 33.55662536621094, weights = tensor([5.9675, 2.2532], requires_grad=True)\n",
      "step №382: loss = 33.54007339477539, weights = tensor([5.9670, 2.2563], requires_grad=True)\n",
      "step №383: loss = 33.52352523803711, weights = tensor([5.9665, 2.2594], requires_grad=True)\n",
      "step №384: loss = 33.50699234008789, weights = tensor([5.9660, 2.2625], requires_grad=True)\n",
      "step №385: loss = 33.49047088623047, weights = tensor([5.9655, 2.2656], requires_grad=True)\n",
      "step №386: loss = 33.473960876464844, weights = tensor([5.9650, 2.2687], requires_grad=True)\n",
      "step №387: loss = 33.45746612548828, weights = tensor([5.9645, 2.2718], requires_grad=True)\n",
      "step №388: loss = 33.44097137451172, weights = tensor([5.9640, 2.2749], requires_grad=True)\n",
      "step №389: loss = 33.42449188232422, weights = tensor([5.9635, 2.2780], requires_grad=True)\n",
      "step №390: loss = 33.40802764892578, weights = tensor([5.9631, 2.2811], requires_grad=True)\n",
      "step №391: loss = 33.391578674316406, weights = tensor([5.9626, 2.2842], requires_grad=True)\n",
      "step №392: loss = 33.37513732910156, weights = tensor([5.9621, 2.2873], requires_grad=True)\n",
      "step №393: loss = 33.35871124267578, weights = tensor([5.9616, 2.2904], requires_grad=True)\n",
      "step №394: loss = 33.34227752685547, weights = tensor([5.9611, 2.2935], requires_grad=True)\n",
      "step №395: loss = 33.32587432861328, weights = tensor([5.9606, 2.2966], requires_grad=True)\n",
      "step №396: loss = 33.309471130371094, weights = tensor([5.9601, 2.2997], requires_grad=True)\n",
      "step №397: loss = 33.2930908203125, weights = tensor([5.9596, 2.3028], requires_grad=True)\n",
      "step №398: loss = 33.276702880859375, weights = tensor([5.9591, 2.3059], requires_grad=True)\n",
      "step №399: loss = 33.260337829589844, weights = tensor([5.9586, 2.3090], requires_grad=True)\n",
      "step №400: loss = 33.24398422241211, weights = tensor([5.9581, 2.3121], requires_grad=True)\n",
      "step №401: loss = 33.227638244628906, weights = tensor([5.9576, 2.3152], requires_grad=True)\n",
      "step №402: loss = 33.21131134033203, weights = tensor([5.9571, 2.3183], requires_grad=True)\n",
      "step №403: loss = 33.194984436035156, weights = tensor([5.9566, 2.3214], requires_grad=True)\n",
      "step №404: loss = 33.178672790527344, weights = tensor([5.9561, 2.3245], requires_grad=True)\n",
      "step №405: loss = 33.16237258911133, weights = tensor([5.9557, 2.3276], requires_grad=True)\n",
      "step №406: loss = 33.146087646484375, weights = tensor([5.9552, 2.3306], requires_grad=True)\n",
      "step №407: loss = 33.12981033325195, weights = tensor([5.9547, 2.3337], requires_grad=True)\n",
      "step №408: loss = 33.113548278808594, weights = tensor([5.9542, 2.3368], requires_grad=True)\n",
      "step №409: loss = 33.09728240966797, weights = tensor([5.9537, 2.3399], requires_grad=True)\n",
      "step №410: loss = 33.0810432434082, weights = tensor([5.9532, 2.3430], requires_grad=True)\n",
      "step №411: loss = 33.064815521240234, weights = tensor([5.9527, 2.3461], requires_grad=True)\n",
      "step №412: loss = 33.048583984375, weights = tensor([5.9522, 2.3491], requires_grad=True)\n",
      "step №413: loss = 33.032386779785156, weights = tensor([5.9517, 2.3522], requires_grad=True)\n",
      "step №414: loss = 33.016178131103516, weights = tensor([5.9512, 2.3553], requires_grad=True)\n",
      "step №415: loss = 32.9999885559082, weights = tensor([5.9507, 2.3584], requires_grad=True)\n",
      "step №416: loss = 32.98380661010742, weights = tensor([5.9502, 2.3614], requires_grad=True)\n",
      "step №417: loss = 32.96764373779297, weights = tensor([5.9498, 2.3645], requires_grad=True)\n",
      "step №418: loss = 32.95148849487305, weights = tensor([5.9493, 2.3676], requires_grad=True)\n",
      "step №419: loss = 32.935340881347656, weights = tensor([5.9488, 2.3707], requires_grad=True)\n",
      "step №420: loss = 32.9192008972168, weights = tensor([5.9483, 2.3737], requires_grad=True)\n",
      "step №421: loss = 32.903072357177734, weights = tensor([5.9478, 2.3768], requires_grad=True)\n",
      "step №422: loss = 32.886966705322266, weights = tensor([5.9473, 2.3799], requires_grad=True)\n",
      "step №423: loss = 32.8708610534668, weights = tensor([5.9468, 2.3829], requires_grad=True)\n",
      "step №424: loss = 32.85477066040039, weights = tensor([5.9463, 2.3860], requires_grad=True)\n",
      "step №425: loss = 32.83869552612305, weights = tensor([5.9458, 2.3891], requires_grad=True)\n",
      "step №426: loss = 32.82262420654297, weights = tensor([5.9454, 2.3921], requires_grad=True)\n",
      "step №427: loss = 32.80656814575195, weights = tensor([5.9449, 2.3952], requires_grad=True)\n",
      "step №428: loss = 32.7905158996582, weights = tensor([5.9444, 2.3983], requires_grad=True)\n",
      "step №429: loss = 32.77448654174805, weights = tensor([5.9439, 2.4013], requires_grad=True)\n",
      "step №430: loss = 32.758445739746094, weights = tensor([5.9434, 2.4044], requires_grad=True)\n",
      "step №431: loss = 32.74243927001953, weights = tensor([5.9429, 2.4075], requires_grad=True)\n",
      "step №432: loss = 32.726436614990234, weights = tensor([5.9424, 2.4105], requires_grad=True)\n",
      "step №433: loss = 32.71044921875, weights = tensor([5.9419, 2.4136], requires_grad=True)\n",
      "step №434: loss = 32.69446563720703, weights = tensor([5.9414, 2.4166], requires_grad=True)\n",
      "step №435: loss = 32.678489685058594, weights = tensor([5.9410, 2.4197], requires_grad=True)\n",
      "step №436: loss = 32.66252899169922, weights = tensor([5.9405, 2.4227], requires_grad=True)\n",
      "step №437: loss = 32.646575927734375, weights = tensor([5.9400, 2.4258], requires_grad=True)\n",
      "step №438: loss = 32.630638122558594, weights = tensor([5.9395, 2.4289], requires_grad=True)\n",
      "step №439: loss = 32.614707946777344, weights = tensor([5.9390, 2.4319], requires_grad=True)\n",
      "step №440: loss = 32.598793029785156, weights = tensor([5.9385, 2.4350], requires_grad=True)\n",
      "step №441: loss = 32.582889556884766, weights = tensor([5.9380, 2.4380], requires_grad=True)\n",
      "step №442: loss = 32.566993713378906, weights = tensor([5.9376, 2.4411], requires_grad=True)\n",
      "step №443: loss = 32.551109313964844, weights = tensor([5.9371, 2.4441], requires_grad=True)\n",
      "step №444: loss = 32.53523635864258, weights = tensor([5.9366, 2.4472], requires_grad=True)\n",
      "step №445: loss = 32.51937484741211, weights = tensor([5.9361, 2.4502], requires_grad=True)\n",
      "step №446: loss = 32.503517150878906, weights = tensor([5.9356, 2.4532], requires_grad=True)\n",
      "step №447: loss = 32.4876708984375, weights = tensor([5.9351, 2.4563], requires_grad=True)\n",
      "step №448: loss = 32.471839904785156, weights = tensor([5.9346, 2.4593], requires_grad=True)\n",
      "step №449: loss = 32.456024169921875, weights = tensor([5.9342, 2.4624], requires_grad=True)\n",
      "step №450: loss = 32.44021224975586, weights = tensor([5.9337, 2.4654], requires_grad=True)\n",
      "step №451: loss = 32.42441177368164, weights = tensor([5.9332, 2.4684], requires_grad=True)\n",
      "step №452: loss = 32.408626556396484, weights = tensor([5.9327, 2.4715], requires_grad=True)\n",
      "step №453: loss = 32.39284896850586, weights = tensor([5.9322, 2.4745], requires_grad=True)\n",
      "step №454: loss = 32.3770866394043, weights = tensor([5.9317, 2.4776], requires_grad=True)\n",
      "step №455: loss = 32.361331939697266, weights = tensor([5.9312, 2.4806], requires_grad=True)\n",
      "step №456: loss = 32.345577239990234, weights = tensor([5.9308, 2.4836], requires_grad=True)\n",
      "step №457: loss = 32.32984161376953, weights = tensor([5.9303, 2.4867], requires_grad=True)\n",
      "step №458: loss = 32.31412887573242, weights = tensor([5.9298, 2.4897], requires_grad=True)\n",
      "step №459: loss = 32.29840850830078, weights = tensor([5.9293, 2.4927], requires_grad=True)\n",
      "step №460: loss = 32.28270721435547, weights = tensor([5.9288, 2.4958], requires_grad=True)\n",
      "step №461: loss = 32.26701354980469, weights = tensor([5.9283, 2.4988], requires_grad=True)\n",
      "step №462: loss = 32.2513313293457, weights = tensor([5.9279, 2.5018], requires_grad=True)\n",
      "step №463: loss = 32.23566436767578, weights = tensor([5.9274, 2.5048], requires_grad=True)\n",
      "step №464: loss = 32.219993591308594, weights = tensor([5.9269, 2.5079], requires_grad=True)\n",
      "step №465: loss = 32.20435333251953, weights = tensor([5.9264, 2.5109], requires_grad=True)\n",
      "step №466: loss = 32.188716888427734, weights = tensor([5.9259, 2.5139], requires_grad=True)\n",
      "step №467: loss = 32.17307662963867, weights = tensor([5.9255, 2.5169], requires_grad=True)\n",
      "step №468: loss = 32.15746307373047, weights = tensor([5.9250, 2.5200], requires_grad=True)\n",
      "step №469: loss = 32.1418571472168, weights = tensor([5.9245, 2.5230], requires_grad=True)\n",
      "step №470: loss = 32.126258850097656, weights = tensor([5.9240, 2.5260], requires_grad=True)\n",
      "step №471: loss = 32.11066818237305, weights = tensor([5.9235, 2.5290], requires_grad=True)\n",
      "step №472: loss = 32.095088958740234, weights = tensor([5.9230, 2.5320], requires_grad=True)\n",
      "step №473: loss = 32.07953643798828, weights = tensor([5.9226, 2.5351], requires_grad=True)\n",
      "step №474: loss = 32.0639762878418, weights = tensor([5.9221, 2.5381], requires_grad=True)\n",
      "step №475: loss = 32.048431396484375, weights = tensor([5.9216, 2.5411], requires_grad=True)\n",
      "step №476: loss = 32.03289794921875, weights = tensor([5.9211, 2.5441], requires_grad=True)\n",
      "step №477: loss = 32.017372131347656, weights = tensor([5.9206, 2.5471], requires_grad=True)\n",
      "step №478: loss = 32.001861572265625, weights = tensor([5.9202, 2.5501], requires_grad=True)\n",
      "step №479: loss = 31.986358642578125, weights = tensor([5.9197, 2.5531], requires_grad=True)\n",
      "step №480: loss = 31.970867156982422, weights = tensor([5.9192, 2.5562], requires_grad=True)\n",
      "step №481: loss = 31.95538330078125, weights = tensor([5.9187, 2.5592], requires_grad=True)\n",
      "step №482: loss = 31.939916610717773, weights = tensor([5.9182, 2.5622], requires_grad=True)\n",
      "step №483: loss = 31.92445945739746, weights = tensor([5.9178, 2.5652], requires_grad=True)\n",
      "step №484: loss = 31.909011840820312, weights = tensor([5.9173, 2.5682], requires_grad=True)\n",
      "step №485: loss = 31.893566131591797, weights = tensor([5.9168, 2.5712], requires_grad=True)\n",
      "step №486: loss = 31.87813949584961, weights = tensor([5.9163, 2.5742], requires_grad=True)\n",
      "step №487: loss = 31.862720489501953, weights = tensor([5.9158, 2.5772], requires_grad=True)\n",
      "step №488: loss = 31.847309112548828, weights = tensor([5.9154, 2.5802], requires_grad=True)\n",
      "step №489: loss = 31.831918716430664, weights = tensor([5.9149, 2.5832], requires_grad=True)\n",
      "step №490: loss = 31.8165340423584, weights = tensor([5.9144, 2.5862], requires_grad=True)\n",
      "step №491: loss = 31.8011531829834, weights = tensor([5.9139, 2.5892], requires_grad=True)\n",
      "step №492: loss = 31.785785675048828, weights = tensor([5.9135, 2.5922], requires_grad=True)\n",
      "step №493: loss = 31.770427703857422, weights = tensor([5.9130, 2.5952], requires_grad=True)\n",
      "step №494: loss = 31.755090713500977, weights = tensor([5.9125, 2.5982], requires_grad=True)\n",
      "step №495: loss = 31.739749908447266, weights = tensor([5.9120, 2.6012], requires_grad=True)\n",
      "step №496: loss = 31.72442626953125, weights = tensor([5.9115, 2.6042], requires_grad=True)\n",
      "step №497: loss = 31.709117889404297, weights = tensor([5.9111, 2.6072], requires_grad=True)\n",
      "step №498: loss = 31.69380760192871, weights = tensor([5.9106, 2.6102], requires_grad=True)\n",
      "step №499: loss = 31.678512573242188, weights = tensor([5.9101, 2.6131], requires_grad=True)\n",
      "step №500: loss = 31.663219451904297, weights = tensor([5.9096, 2.6161], requires_grad=True)\n",
      "step №501: loss = 31.647960662841797, weights = tensor([5.9092, 2.6191], requires_grad=True)\n",
      "step №502: loss = 31.632699966430664, weights = tensor([5.9087, 2.6221], requires_grad=True)\n",
      "step №503: loss = 31.617443084716797, weights = tensor([5.9082, 2.6251], requires_grad=True)\n",
      "step №504: loss = 31.60220718383789, weights = tensor([5.9077, 2.6281], requires_grad=True)\n",
      "step №505: loss = 31.586963653564453, weights = tensor([5.9073, 2.6311], requires_grad=True)\n",
      "step №506: loss = 31.571752548217773, weights = tensor([5.9068, 2.6341], requires_grad=True)\n",
      "step №507: loss = 31.55653953552246, weights = tensor([5.9063, 2.6370], requires_grad=True)\n",
      "step №508: loss = 31.541339874267578, weights = tensor([5.9058, 2.6400], requires_grad=True)\n",
      "step №509: loss = 31.52614402770996, weights = tensor([5.9053, 2.6430], requires_grad=True)\n",
      "step №510: loss = 31.510976791381836, weights = tensor([5.9049, 2.6460], requires_grad=True)\n",
      "step №511: loss = 31.495803833007812, weights = tensor([5.9044, 2.6490], requires_grad=True)\n",
      "step №512: loss = 31.480640411376953, weights = tensor([5.9039, 2.6519], requires_grad=True)\n",
      "step №513: loss = 31.465490341186523, weights = tensor([5.9035, 2.6549], requires_grad=True)\n",
      "step №514: loss = 31.45034408569336, weights = tensor([5.9030, 2.6579], requires_grad=True)\n",
      "step №515: loss = 31.435222625732422, weights = tensor([5.9025, 2.6609], requires_grad=True)\n",
      "step №516: loss = 31.420101165771484, weights = tensor([5.9020, 2.6638], requires_grad=True)\n",
      "step №517: loss = 31.404998779296875, weights = tensor([5.9016, 2.6668], requires_grad=True)\n",
      "step №518: loss = 31.389907836914062, weights = tensor([5.9011, 2.6698], requires_grad=True)\n",
      "step №519: loss = 31.37481689453125, weights = tensor([5.9006, 2.6727], requires_grad=True)\n",
      "step №520: loss = 31.359725952148438, weights = tensor([5.9001, 2.6757], requires_grad=True)\n",
      "step №521: loss = 31.344669342041016, weights = tensor([5.8997, 2.6787], requires_grad=True)\n",
      "step №522: loss = 31.32961082458496, weights = tensor([5.8992, 2.6817], requires_grad=True)\n",
      "step №523: loss = 31.314565658569336, weights = tensor([5.8987, 2.6846], requires_grad=True)\n",
      "step №524: loss = 31.29952049255371, weights = tensor([5.8982, 2.6876], requires_grad=True)\n",
      "step №525: loss = 31.284500122070312, weights = tensor([5.8978, 2.6906], requires_grad=True)\n",
      "step №526: loss = 31.269489288330078, weights = tensor([5.8973, 2.6935], requires_grad=True)\n",
      "step №527: loss = 31.254480361938477, weights = tensor([5.8968, 2.6965], requires_grad=True)\n",
      "step №528: loss = 31.23948097229004, weights = tensor([5.8963, 2.6994], requires_grad=True)\n",
      "step №529: loss = 31.224506378173828, weights = tensor([5.8959, 2.7024], requires_grad=True)\n",
      "step №530: loss = 31.209529876708984, weights = tensor([5.8954, 2.7054], requires_grad=True)\n",
      "step №531: loss = 31.19455909729004, weights = tensor([5.8949, 2.7083], requires_grad=True)\n",
      "step №532: loss = 31.179601669311523, weights = tensor([5.8945, 2.7113], requires_grad=True)\n",
      "step №533: loss = 31.164663314819336, weights = tensor([5.8940, 2.7142], requires_grad=True)\n",
      "step №534: loss = 31.14971923828125, weights = tensor([5.8935, 2.7172], requires_grad=True)\n",
      "step №535: loss = 31.13479995727539, weights = tensor([5.8930, 2.7201], requires_grad=True)\n",
      "step №536: loss = 31.1198787689209, weights = tensor([5.8926, 2.7231], requires_grad=True)\n",
      "step №537: loss = 31.104984283447266, weights = tensor([5.8921, 2.7260], requires_grad=True)\n",
      "step №538: loss = 31.090087890625, weights = tensor([5.8916, 2.7290], requires_grad=True)\n",
      "step №539: loss = 31.0751953125, weights = tensor([5.8912, 2.7319], requires_grad=True)\n",
      "step №540: loss = 31.060327529907227, weights = tensor([5.8907, 2.7349], requires_grad=True)\n",
      "step №541: loss = 31.045461654663086, weights = tensor([5.8902, 2.7378], requires_grad=True)\n",
      "step №542: loss = 31.03061294555664, weights = tensor([5.8898, 2.7408], requires_grad=True)\n",
      "step №543: loss = 31.015758514404297, weights = tensor([5.8893, 2.7437], requires_grad=True)\n",
      "step №544: loss = 31.000930786132812, weights = tensor([5.8888, 2.7467], requires_grad=True)\n",
      "step №545: loss = 30.986103057861328, weights = tensor([5.8883, 2.7496], requires_grad=True)\n",
      "step №546: loss = 30.971288681030273, weights = tensor([5.8879, 2.7526], requires_grad=True)\n",
      "step №547: loss = 30.956485748291016, weights = tensor([5.8874, 2.7555], requires_grad=True)\n",
      "step №548: loss = 30.941699981689453, weights = tensor([5.8869, 2.7585], requires_grad=True)\n",
      "step №549: loss = 30.926916122436523, weights = tensor([5.8865, 2.7614], requires_grad=True)\n",
      "step №550: loss = 30.91214370727539, weights = tensor([5.8860, 2.7643], requires_grad=True)\n",
      "step №551: loss = 30.897369384765625, weights = tensor([5.8855, 2.7673], requires_grad=True)\n",
      "step №552: loss = 30.882614135742188, weights = tensor([5.8851, 2.7702], requires_grad=True)\n",
      "step №553: loss = 30.867870330810547, weights = tensor([5.8846, 2.7731], requires_grad=True)\n",
      "step №554: loss = 30.853139877319336, weights = tensor([5.8841, 2.7761], requires_grad=True)\n",
      "step №555: loss = 30.838424682617188, weights = tensor([5.8837, 2.7790], requires_grad=True)\n",
      "step №556: loss = 30.82370376586914, weights = tensor([5.8832, 2.7820], requires_grad=True)\n",
      "step №557: loss = 30.809001922607422, weights = tensor([5.8827, 2.7849], requires_grad=True)\n",
      "step №558: loss = 30.7943115234375, weights = tensor([5.8823, 2.7878], requires_grad=True)\n",
      "step №559: loss = 30.779621124267578, weights = tensor([5.8818, 2.7907], requires_grad=True)\n",
      "step №560: loss = 30.764949798583984, weights = tensor([5.8813, 2.7937], requires_grad=True)\n",
      "step №561: loss = 30.75028419494629, weights = tensor([5.8809, 2.7966], requires_grad=True)\n",
      "step №562: loss = 30.735631942749023, weights = tensor([5.8804, 2.7995], requires_grad=True)\n",
      "step №563: loss = 30.72098159790039, weights = tensor([5.8799, 2.8025], requires_grad=True)\n",
      "step №564: loss = 30.706344604492188, weights = tensor([5.8795, 2.8054], requires_grad=True)\n",
      "step №565: loss = 30.691722869873047, weights = tensor([5.8790, 2.8083], requires_grad=True)\n",
      "step №566: loss = 30.677114486694336, weights = tensor([5.8785, 2.8112], requires_grad=True)\n",
      "step №567: loss = 30.662506103515625, weights = tensor([5.8781, 2.8142], requires_grad=True)\n",
      "step №568: loss = 30.64790916442871, weights = tensor([5.8776, 2.8171], requires_grad=True)\n",
      "step №569: loss = 30.63332176208496, weights = tensor([5.8771, 2.8200], requires_grad=True)\n",
      "step №570: loss = 30.61875343322754, weights = tensor([5.8767, 2.8229], requires_grad=True)\n",
      "step №571: loss = 30.60418701171875, weights = tensor([5.8762, 2.8258], requires_grad=True)\n",
      "step №572: loss = 30.589630126953125, weights = tensor([5.8757, 2.8288], requires_grad=True)\n",
      "step №573: loss = 30.575088500976562, weights = tensor([5.8753, 2.8317], requires_grad=True)\n",
      "step №574: loss = 30.560558319091797, weights = tensor([5.8748, 2.8346], requires_grad=True)\n",
      "step №575: loss = 30.5460262298584, weights = tensor([5.8743, 2.8375], requires_grad=True)\n",
      "step №576: loss = 30.531509399414062, weights = tensor([5.8739, 2.8404], requires_grad=True)\n",
      "step №577: loss = 30.51700210571289, weights = tensor([5.8734, 2.8433], requires_grad=True)\n",
      "step №578: loss = 30.50250816345215, weights = tensor([5.8729, 2.8462], requires_grad=True)\n",
      "step №579: loss = 30.488027572631836, weights = tensor([5.8725, 2.8491], requires_grad=True)\n",
      "step №580: loss = 30.47355079650879, weights = tensor([5.8720, 2.8521], requires_grad=True)\n",
      "step №581: loss = 30.45907211303711, weights = tensor([5.8715, 2.8550], requires_grad=True)\n",
      "step №582: loss = 30.444622039794922, weights = tensor([5.8711, 2.8579], requires_grad=True)\n",
      "step №583: loss = 30.430179595947266, weights = tensor([5.8706, 2.8608], requires_grad=True)\n",
      "step №584: loss = 30.41573715209961, weights = tensor([5.8702, 2.8637], requires_grad=True)\n",
      "step №585: loss = 30.40131187438965, weights = tensor([5.8697, 2.8666], requires_grad=True)\n",
      "step №586: loss = 30.38689613342285, weights = tensor([5.8692, 2.8695], requires_grad=True)\n",
      "step №587: loss = 30.372478485107422, weights = tensor([5.8688, 2.8724], requires_grad=True)\n",
      "step №588: loss = 30.358081817626953, weights = tensor([5.8683, 2.8753], requires_grad=True)\n",
      "step №589: loss = 30.343704223632812, weights = tensor([5.8678, 2.8782], requires_grad=True)\n",
      "step №590: loss = 30.329320907592773, weights = tensor([5.8674, 2.8811], requires_grad=True)\n",
      "step №591: loss = 30.314956665039062, weights = tensor([5.8669, 2.8840], requires_grad=True)\n",
      "step №592: loss = 30.30059814453125, weights = tensor([5.8665, 2.8869], requires_grad=True)\n",
      "step №593: loss = 30.286239624023438, weights = tensor([5.8660, 2.8898], requires_grad=True)\n",
      "step №594: loss = 30.271902084350586, weights = tensor([5.8655, 2.8927], requires_grad=True)\n",
      "step №595: loss = 30.2575740814209, weights = tensor([5.8651, 2.8956], requires_grad=True)\n",
      "step №596: loss = 30.24325942993164, weights = tensor([5.8646, 2.8985], requires_grad=True)\n",
      "step №597: loss = 30.228946685791016, weights = tensor([5.8641, 2.9014], requires_grad=True)\n",
      "step №598: loss = 30.214645385742188, weights = tensor([5.8637, 2.9043], requires_grad=True)\n",
      "step №599: loss = 30.20035171508789, weights = tensor([5.8632, 2.9072], requires_grad=True)\n",
      "step №600: loss = 30.186077117919922, weights = tensor([5.8628, 2.9100], requires_grad=True)\n",
      "step №601: loss = 30.171794891357422, weights = tensor([5.8623, 2.9129], requires_grad=True)\n",
      "step №602: loss = 30.157541275024414, weights = tensor([5.8618, 2.9158], requires_grad=True)\n",
      "step №603: loss = 30.143285751342773, weights = tensor([5.8614, 2.9187], requires_grad=True)\n",
      "step №604: loss = 30.1290340423584, weights = tensor([5.8609, 2.9216], requires_grad=True)\n",
      "step №605: loss = 30.114803314208984, weights = tensor([5.8605, 2.9245], requires_grad=True)\n",
      "step №606: loss = 30.100576400756836, weights = tensor([5.8600, 2.9274], requires_grad=True)\n",
      "step №607: loss = 30.08636474609375, weights = tensor([5.8595, 2.9302], requires_grad=True)\n",
      "step №608: loss = 30.072162628173828, weights = tensor([5.8591, 2.9331], requires_grad=True)\n",
      "step №609: loss = 30.0579776763916, weights = tensor([5.8586, 2.9360], requires_grad=True)\n",
      "step №610: loss = 30.043777465820312, weights = tensor([5.8582, 2.9389], requires_grad=True)\n",
      "step №611: loss = 30.02960777282715, weights = tensor([5.8577, 2.9418], requires_grad=True)\n",
      "step №612: loss = 30.015445709228516, weights = tensor([5.8572, 2.9446], requires_grad=True)\n",
      "step №613: loss = 30.00128173828125, weights = tensor([5.8568, 2.9475], requires_grad=True)\n",
      "step №614: loss = 29.98714828491211, weights = tensor([5.8563, 2.9504], requires_grad=True)\n",
      "step №615: loss = 29.973007202148438, weights = tensor([5.8559, 2.9533], requires_grad=True)\n",
      "step №616: loss = 29.958866119384766, weights = tensor([5.8554, 2.9561], requires_grad=True)\n",
      "step №617: loss = 29.944759368896484, weights = tensor([5.8550, 2.9590], requires_grad=True)\n",
      "step №618: loss = 29.930652618408203, weights = tensor([5.8545, 2.9619], requires_grad=True)\n",
      "step №619: loss = 29.916553497314453, weights = tensor([5.8540, 2.9648], requires_grad=True)\n",
      "step №620: loss = 29.902462005615234, weights = tensor([5.8536, 2.9676], requires_grad=True)\n",
      "step №621: loss = 29.888378143310547, weights = tensor([5.8531, 2.9705], requires_grad=True)\n",
      "step №622: loss = 29.874313354492188, weights = tensor([5.8527, 2.9734], requires_grad=True)\n",
      "step №623: loss = 29.86025047302246, weights = tensor([5.8522, 2.9762], requires_grad=True)\n",
      "step №624: loss = 29.846195220947266, weights = tensor([5.8517, 2.9791], requires_grad=True)\n",
      "step №625: loss = 29.832157135009766, weights = tensor([5.8513, 2.9820], requires_grad=True)\n",
      "step №626: loss = 29.818126678466797, weights = tensor([5.8508, 2.9848], requires_grad=True)\n",
      "step №627: loss = 29.80410385131836, weights = tensor([5.8504, 2.9877], requires_grad=True)\n",
      "step №628: loss = 29.790096282958984, weights = tensor([5.8499, 2.9906], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №629: loss = 29.776092529296875, weights = tensor([5.8495, 2.9934], requires_grad=True)\n",
      "step №630: loss = 29.762094497680664, weights = tensor([5.8490, 2.9963], requires_grad=True)\n",
      "step №631: loss = 29.74810791015625, weights = tensor([5.8486, 2.9991], requires_grad=True)\n",
      "step №632: loss = 29.7341365814209, weights = tensor([5.8481, 3.0020], requires_grad=True)\n",
      "step №633: loss = 29.720165252685547, weights = tensor([5.8476, 3.0049], requires_grad=True)\n",
      "step №634: loss = 29.70621109008789, weights = tensor([5.8472, 3.0077], requires_grad=True)\n",
      "step №635: loss = 29.6922607421875, weights = tensor([5.8467, 3.0106], requires_grad=True)\n",
      "step №636: loss = 29.678325653076172, weights = tensor([5.8463, 3.0134], requires_grad=True)\n",
      "step №637: loss = 29.66440773010254, weights = tensor([5.8458, 3.0163], requires_grad=True)\n",
      "step №638: loss = 29.65048599243164, weights = tensor([5.8454, 3.0191], requires_grad=True)\n",
      "step №639: loss = 29.636577606201172, weights = tensor([5.8449, 3.0220], requires_grad=True)\n",
      "step №640: loss = 29.6226749420166, weights = tensor([5.8445, 3.0248], requires_grad=True)\n",
      "step №641: loss = 29.608789443969727, weights = tensor([5.8440, 3.0277], requires_grad=True)\n",
      "step №642: loss = 29.594905853271484, weights = tensor([5.8435, 3.0305], requires_grad=True)\n",
      "step №643: loss = 29.581035614013672, weights = tensor([5.8431, 3.0334], requires_grad=True)\n",
      "step №644: loss = 29.56717300415039, weights = tensor([5.8426, 3.0362], requires_grad=True)\n",
      "step №645: loss = 29.553319931030273, weights = tensor([5.8422, 3.0391], requires_grad=True)\n",
      "step №646: loss = 29.539478302001953, weights = tensor([5.8417, 3.0419], requires_grad=True)\n",
      "step №647: loss = 29.525646209716797, weights = tensor([5.8413, 3.0448], requires_grad=True)\n",
      "step №648: loss = 29.511816024780273, weights = tensor([5.8408, 3.0476], requires_grad=True)\n",
      "step №649: loss = 29.498004913330078, weights = tensor([5.8404, 3.0505], requires_grad=True)\n",
      "step №650: loss = 29.484201431274414, weights = tensor([5.8399, 3.0533], requires_grad=True)\n",
      "step №651: loss = 29.47040367126465, weights = tensor([5.8395, 3.0561], requires_grad=True)\n",
      "step №652: loss = 29.456613540649414, weights = tensor([5.8390, 3.0590], requires_grad=True)\n",
      "step №653: loss = 29.44284439086914, weights = tensor([5.8386, 3.0618], requires_grad=True)\n",
      "step №654: loss = 29.429073333740234, weights = tensor([5.8381, 3.0647], requires_grad=True)\n",
      "step №655: loss = 29.41530990600586, weights = tensor([5.8377, 3.0675], requires_grad=True)\n",
      "step №656: loss = 29.401561737060547, weights = tensor([5.8372, 3.0703], requires_grad=True)\n",
      "step №657: loss = 29.387821197509766, weights = tensor([5.8367, 3.0732], requires_grad=True)\n",
      "step №658: loss = 29.37409019470215, weights = tensor([5.8363, 3.0760], requires_grad=True)\n",
      "step №659: loss = 29.360366821289062, weights = tensor([5.8358, 3.0788], requires_grad=True)\n",
      "step №660: loss = 29.34665298461914, weights = tensor([5.8354, 3.0817], requires_grad=True)\n",
      "step №661: loss = 29.332950592041016, weights = tensor([5.8349, 3.0845], requires_grad=True)\n",
      "step №662: loss = 29.319255828857422, weights = tensor([5.8345, 3.0873], requires_grad=True)\n",
      "step №663: loss = 29.305572509765625, weights = tensor([5.8340, 3.0902], requires_grad=True)\n",
      "step №664: loss = 29.29189109802246, weights = tensor([5.8336, 3.0930], requires_grad=True)\n",
      "step №665: loss = 29.278223037719727, weights = tensor([5.8331, 3.0958], requires_grad=True)\n",
      "step №666: loss = 29.264575958251953, weights = tensor([5.8327, 3.0986], requires_grad=True)\n",
      "step №667: loss = 29.250919342041016, weights = tensor([5.8322, 3.1015], requires_grad=True)\n",
      "step №668: loss = 29.237285614013672, weights = tensor([5.8318, 3.1043], requires_grad=True)\n",
      "step №669: loss = 29.22365951538086, weights = tensor([5.8313, 3.1071], requires_grad=True)\n",
      "step №670: loss = 29.210037231445312, weights = tensor([5.8309, 3.1099], requires_grad=True)\n",
      "step №671: loss = 29.19643211364746, weights = tensor([5.8304, 3.1127], requires_grad=True)\n",
      "step №672: loss = 29.18282699584961, weights = tensor([5.8300, 3.1156], requires_grad=True)\n",
      "step №673: loss = 29.169235229492188, weights = tensor([5.8295, 3.1184], requires_grad=True)\n",
      "step №674: loss = 29.155654907226562, weights = tensor([5.8291, 3.1212], requires_grad=True)\n",
      "step №675: loss = 29.142080307006836, weights = tensor([5.8286, 3.1240], requires_grad=True)\n",
      "step №676: loss = 29.12851333618164, weights = tensor([5.8282, 3.1268], requires_grad=True)\n",
      "step №677: loss = 29.11496353149414, weights = tensor([5.8277, 3.1297], requires_grad=True)\n",
      "step №678: loss = 29.10141372680664, weights = tensor([5.8273, 3.1325], requires_grad=True)\n",
      "step №679: loss = 29.0878849029541, weights = tensor([5.8268, 3.1353], requires_grad=True)\n",
      "step №680: loss = 29.074352264404297, weights = tensor([5.8264, 3.1381], requires_grad=True)\n",
      "step №681: loss = 29.060836791992188, weights = tensor([5.8259, 3.1409], requires_grad=True)\n",
      "step №682: loss = 29.047327041625977, weights = tensor([5.8255, 3.1437], requires_grad=True)\n",
      "step №683: loss = 29.0338191986084, weights = tensor([5.8250, 3.1465], requires_grad=True)\n",
      "step №684: loss = 29.020336151123047, weights = tensor([5.8246, 3.1493], requires_grad=True)\n",
      "step №685: loss = 29.006847381591797, weights = tensor([5.8242, 3.1521], requires_grad=True)\n",
      "step №686: loss = 28.993377685546875, weights = tensor([5.8237, 3.1550], requires_grad=True)\n",
      "step №687: loss = 28.979909896850586, weights = tensor([5.8233, 3.1578], requires_grad=True)\n",
      "step №688: loss = 28.966466903686523, weights = tensor([5.8228, 3.1606], requires_grad=True)\n",
      "step №689: loss = 28.953014373779297, weights = tensor([5.8224, 3.1634], requires_grad=True)\n",
      "step №690: loss = 28.939584732055664, weights = tensor([5.8219, 3.1662], requires_grad=True)\n",
      "step №691: loss = 28.926151275634766, weights = tensor([5.8215, 3.1690], requires_grad=True)\n",
      "step №692: loss = 28.912731170654297, weights = tensor([5.8210, 3.1718], requires_grad=True)\n",
      "step №693: loss = 28.899328231811523, weights = tensor([5.8206, 3.1746], requires_grad=True)\n",
      "step №694: loss = 28.885929107666016, weights = tensor([5.8201, 3.1774], requires_grad=True)\n",
      "step №695: loss = 28.872539520263672, weights = tensor([5.8197, 3.1802], requires_grad=True)\n",
      "step №696: loss = 28.85915184020996, weights = tensor([5.8192, 3.1830], requires_grad=True)\n",
      "step №697: loss = 28.84578514099121, weights = tensor([5.8188, 3.1858], requires_grad=True)\n",
      "step №698: loss = 28.83241844177246, weights = tensor([5.8183, 3.1886], requires_grad=True)\n",
      "step №699: loss = 28.819067001342773, weights = tensor([5.8179, 3.1914], requires_grad=True)\n",
      "step №700: loss = 28.805715560913086, weights = tensor([5.8175, 3.1942], requires_grad=True)\n",
      "step №701: loss = 28.792383193969727, weights = tensor([5.8170, 3.1969], requires_grad=True)\n",
      "step №702: loss = 28.779052734375, weights = tensor([5.8166, 3.1997], requires_grad=True)\n",
      "step №703: loss = 28.765737533569336, weights = tensor([5.8161, 3.2025], requires_grad=True)\n",
      "step №704: loss = 28.75242042541504, weights = tensor([5.8157, 3.2053], requires_grad=True)\n",
      "step №705: loss = 28.7391300201416, weights = tensor([5.8152, 3.2081], requires_grad=True)\n",
      "step №706: loss = 28.725826263427734, weights = tensor([5.8148, 3.2109], requires_grad=True)\n",
      "step №707: loss = 28.712554931640625, weights = tensor([5.8143, 3.2137], requires_grad=True)\n",
      "step №708: loss = 28.699268341064453, weights = tensor([5.8139, 3.2165], requires_grad=True)\n",
      "step №709: loss = 28.68601417541504, weights = tensor([5.8135, 3.2193], requires_grad=True)\n",
      "step №710: loss = 28.672760009765625, weights = tensor([5.8130, 3.2220], requires_grad=True)\n",
      "step №711: loss = 28.659509658813477, weights = tensor([5.8126, 3.2248], requires_grad=True)\n",
      "step №712: loss = 28.64626693725586, weights = tensor([5.8121, 3.2276], requires_grad=True)\n",
      "step №713: loss = 28.633045196533203, weights = tensor([5.8117, 3.2304], requires_grad=True)\n",
      "step №714: loss = 28.619823455810547, weights = tensor([5.8112, 3.2332], requires_grad=True)\n",
      "step №715: loss = 28.606618881225586, weights = tensor([5.8108, 3.2359], requires_grad=True)\n",
      "step №716: loss = 28.59341049194336, weights = tensor([5.8103, 3.2387], requires_grad=True)\n",
      "step №717: loss = 28.58022689819336, weights = tensor([5.8099, 3.2415], requires_grad=True)\n",
      "step №718: loss = 28.567035675048828, weights = tensor([5.8095, 3.2443], requires_grad=True)\n",
      "step №719: loss = 28.553863525390625, weights = tensor([5.8090, 3.2471], requires_grad=True)\n",
      "step №720: loss = 28.540691375732422, weights = tensor([5.8086, 3.2498], requires_grad=True)\n",
      "step №721: loss = 28.52753257751465, weights = tensor([5.8081, 3.2526], requires_grad=True)\n",
      "step №722: loss = 28.514385223388672, weights = tensor([5.8077, 3.2554], requires_grad=True)\n",
      "step №723: loss = 28.50124740600586, weights = tensor([5.8072, 3.2581], requires_grad=True)\n",
      "step №724: loss = 28.48812484741211, weights = tensor([5.8068, 3.2609], requires_grad=True)\n",
      "step №725: loss = 28.475000381469727, weights = tensor([5.8064, 3.2637], requires_grad=True)\n",
      "step №726: loss = 28.461883544921875, weights = tensor([5.8059, 3.2665], requires_grad=True)\n",
      "step №727: loss = 28.448780059814453, weights = tensor([5.8055, 3.2692], requires_grad=True)\n",
      "step №728: loss = 28.435684204101562, weights = tensor([5.8050, 3.2720], requires_grad=True)\n",
      "step №729: loss = 28.4226016998291, weights = tensor([5.8046, 3.2748], requires_grad=True)\n",
      "step №730: loss = 28.40951919555664, weights = tensor([5.8042, 3.2775], requires_grad=True)\n",
      "step №731: loss = 28.396453857421875, weights = tensor([5.8037, 3.2803], requires_grad=True)\n",
      "step №732: loss = 28.38339614868164, weights = tensor([5.8033, 3.2831], requires_grad=True)\n",
      "step №733: loss = 28.37034034729004, weights = tensor([5.8028, 3.2858], requires_grad=True)\n",
      "step №734: loss = 28.357309341430664, weights = tensor([5.8024, 3.2886], requires_grad=True)\n",
      "step №735: loss = 28.34427833557129, weights = tensor([5.8020, 3.2913], requires_grad=True)\n",
      "step №736: loss = 28.331247329711914, weights = tensor([5.8015, 3.2941], requires_grad=True)\n",
      "step №737: loss = 28.3182315826416, weights = tensor([5.8011, 3.2969], requires_grad=True)\n",
      "step №738: loss = 28.305225372314453, weights = tensor([5.8006, 3.2996], requires_grad=True)\n",
      "step №739: loss = 28.292226791381836, weights = tensor([5.8002, 3.3024], requires_grad=True)\n",
      "step №740: loss = 28.279245376586914, weights = tensor([5.7998, 3.3051], requires_grad=True)\n",
      "step №741: loss = 28.266254425048828, weights = tensor([5.7993, 3.3079], requires_grad=True)\n",
      "step №742: loss = 28.253292083740234, weights = tensor([5.7989, 3.3106], requires_grad=True)\n",
      "step №743: loss = 28.240331649780273, weights = tensor([5.7984, 3.3134], requires_grad=True)\n",
      "step №744: loss = 28.227367401123047, weights = tensor([5.7980, 3.3161], requires_grad=True)\n",
      "step №745: loss = 28.214427947998047, weights = tensor([5.7976, 3.3189], requires_grad=True)\n",
      "step №746: loss = 28.201496124267578, weights = tensor([5.7971, 3.3216], requires_grad=True)\n",
      "step №747: loss = 28.188568115234375, weights = tensor([5.7967, 3.3244], requires_grad=True)\n",
      "step №748: loss = 28.175647735595703, weights = tensor([5.7962, 3.3271], requires_grad=True)\n",
      "step №749: loss = 28.162738800048828, weights = tensor([5.7958, 3.3299], requires_grad=True)\n",
      "step №750: loss = 28.149831771850586, weights = tensor([5.7954, 3.3326], requires_grad=True)\n",
      "step №751: loss = 28.13694190979004, weights = tensor([5.7949, 3.3354], requires_grad=True)\n",
      "step №752: loss = 28.124065399169922, weights = tensor([5.7945, 3.3381], requires_grad=True)\n",
      "step №753: loss = 28.111181259155273, weights = tensor([5.7941, 3.3409], requires_grad=True)\n",
      "step №754: loss = 28.09832191467285, weights = tensor([5.7936, 3.3436], requires_grad=True)\n",
      "step №755: loss = 28.085468292236328, weights = tensor([5.7932, 3.3464], requires_grad=True)\n",
      "step №756: loss = 28.07261085510254, weights = tensor([5.7927, 3.3491], requires_grad=True)\n",
      "step №757: loss = 28.059778213500977, weights = tensor([5.7923, 3.3518], requires_grad=True)\n",
      "step №758: loss = 28.046939849853516, weights = tensor([5.7919, 3.3546], requires_grad=True)\n",
      "step №759: loss = 28.034114837646484, weights = tensor([5.7914, 3.3573], requires_grad=True)\n",
      "step №760: loss = 28.02130126953125, weights = tensor([5.7910, 3.3601], requires_grad=True)\n",
      "step №761: loss = 28.008499145507812, weights = tensor([5.7906, 3.3628], requires_grad=True)\n",
      "step №762: loss = 27.995702743530273, weights = tensor([5.7901, 3.3655], requires_grad=True)\n",
      "step №763: loss = 27.982906341552734, weights = tensor([5.7897, 3.3683], requires_grad=True)\n",
      "step №764: loss = 27.97013282775879, weights = tensor([5.7893, 3.3710], requires_grad=True)\n",
      "step №765: loss = 27.957366943359375, weights = tensor([5.7888, 3.3737], requires_grad=True)\n",
      "step №766: loss = 27.94460678100586, weights = tensor([5.7884, 3.3765], requires_grad=True)\n",
      "step №767: loss = 27.931848526000977, weights = tensor([5.7879, 3.3792], requires_grad=True)\n",
      "step №768: loss = 27.91910743713379, weights = tensor([5.7875, 3.3819], requires_grad=True)\n",
      "step №769: loss = 27.9063720703125, weights = tensor([5.7871, 3.3846], requires_grad=True)\n",
      "step №770: loss = 27.89365005493164, weights = tensor([5.7866, 3.3874], requires_grad=True)\n",
      "step №771: loss = 27.880929946899414, weights = tensor([5.7862, 3.3901], requires_grad=True)\n",
      "step №772: loss = 27.86821937561035, weights = tensor([5.7858, 3.3928], requires_grad=True)\n",
      "step №773: loss = 27.855514526367188, weights = tensor([5.7853, 3.3956], requires_grad=True)\n",
      "step №774: loss = 27.842824935913086, weights = tensor([5.7849, 3.3983], requires_grad=True)\n",
      "step №775: loss = 27.83013916015625, weights = tensor([5.7845, 3.4010], requires_grad=True)\n",
      "step №776: loss = 27.817462921142578, weights = tensor([5.7840, 3.4037], requires_grad=True)\n",
      "step №777: loss = 27.804805755615234, weights = tensor([5.7836, 3.4064], requires_grad=True)\n",
      "step №778: loss = 27.792139053344727, weights = tensor([5.7832, 3.4092], requires_grad=True)\n",
      "step №779: loss = 27.779483795166016, weights = tensor([5.7827, 3.4119], requires_grad=True)\n",
      "step №780: loss = 27.7668399810791, weights = tensor([5.7823, 3.4146], requires_grad=True)\n",
      "step №781: loss = 27.75421714782715, weights = tensor([5.7819, 3.4173], requires_grad=True)\n",
      "step №782: loss = 27.741596221923828, weights = tensor([5.7814, 3.4200], requires_grad=True)\n",
      "step №783: loss = 27.72896957397461, weights = tensor([5.7810, 3.4228], requires_grad=True)\n",
      "step №784: loss = 27.716365814208984, weights = tensor([5.7806, 3.4255], requires_grad=True)\n",
      "step №785: loss = 27.70376968383789, weights = tensor([5.7801, 3.4282], requires_grad=True)\n",
      "step №786: loss = 27.691173553466797, weights = tensor([5.7797, 3.4309], requires_grad=True)\n",
      "step №787: loss = 27.678600311279297, weights = tensor([5.7793, 3.4336], requires_grad=True)\n",
      "step №788: loss = 27.666025161743164, weights = tensor([5.7788, 3.4363], requires_grad=True)\n",
      "step №789: loss = 27.65346336364746, weights = tensor([5.7784, 3.4390], requires_grad=True)\n",
      "step №790: loss = 27.640905380249023, weights = tensor([5.7780, 3.4417], requires_grad=True)\n",
      "step №791: loss = 27.628360748291016, weights = tensor([5.7775, 3.4445], requires_grad=True)\n",
      "step №792: loss = 27.61582374572754, weights = tensor([5.7771, 3.4472], requires_grad=True)\n",
      "step №793: loss = 27.603290557861328, weights = tensor([5.7767, 3.4499], requires_grad=True)\n",
      "step №794: loss = 27.590755462646484, weights = tensor([5.7762, 3.4526], requires_grad=True)\n",
      "step №795: loss = 27.5782527923584, weights = tensor([5.7758, 3.4553], requires_grad=True)\n",
      "step №796: loss = 27.565746307373047, weights = tensor([5.7754, 3.4580], requires_grad=True)\n",
      "step №797: loss = 27.553253173828125, weights = tensor([5.7749, 3.4607], requires_grad=True)\n",
      "step №798: loss = 27.540760040283203, weights = tensor([5.7745, 3.4634], requires_grad=True)\n",
      "step №799: loss = 27.52828025817871, weights = tensor([5.7741, 3.4661], requires_grad=True)\n",
      "step №800: loss = 27.515811920166016, weights = tensor([5.7737, 3.4688], requires_grad=True)\n",
      "step №801: loss = 27.50335121154785, weights = tensor([5.7732, 3.4715], requires_grad=True)\n",
      "step №802: loss = 27.490901947021484, weights = tensor([5.7728, 3.4742], requires_grad=True)\n",
      "step №803: loss = 27.478450775146484, weights = tensor([5.7724, 3.4769], requires_grad=True)\n",
      "step №804: loss = 27.46600914001465, weights = tensor([5.7719, 3.4796], requires_grad=True)\n",
      "step №805: loss = 27.453582763671875, weights = tensor([5.7715, 3.4823], requires_grad=True)\n",
      "step №806: loss = 27.441158294677734, weights = tensor([5.7711, 3.4850], requires_grad=True)\n",
      "step №807: loss = 27.428752899169922, weights = tensor([5.7706, 3.4877], requires_grad=True)\n",
      "step №808: loss = 27.416345596313477, weights = tensor([5.7702, 3.4904], requires_grad=True)\n",
      "step №809: loss = 27.403955459594727, weights = tensor([5.7698, 3.4931], requires_grad=True)\n",
      "step №810: loss = 27.39156723022461, weights = tensor([5.7694, 3.4958], requires_grad=True)\n",
      "step №811: loss = 27.37918472290039, weights = tensor([5.7689, 3.4984], requires_grad=True)\n",
      "step №812: loss = 27.3668212890625, weights = tensor([5.7685, 3.5011], requires_grad=True)\n",
      "step №813: loss = 27.35446548461914, weights = tensor([5.7681, 3.5038], requires_grad=True)\n",
      "step №814: loss = 27.34210205078125, weights = tensor([5.7676, 3.5065], requires_grad=True)\n",
      "step №815: loss = 27.329757690429688, weights = tensor([5.7672, 3.5092], requires_grad=True)\n",
      "step №816: loss = 27.317419052124023, weights = tensor([5.7668, 3.5119], requires_grad=True)\n",
      "step №817: loss = 27.305095672607422, weights = tensor([5.7664, 3.5146], requires_grad=True)\n",
      "step №818: loss = 27.292774200439453, weights = tensor([5.7659, 3.5173], requires_grad=True)\n",
      "step №819: loss = 27.28046226501465, weights = tensor([5.7655, 3.5199], requires_grad=True)\n",
      "step №820: loss = 27.268163681030273, weights = tensor([5.7651, 3.5226], requires_grad=True)\n",
      "step №821: loss = 27.2558650970459, weights = tensor([5.7646, 3.5253], requires_grad=True)\n",
      "step №822: loss = 27.243579864501953, weights = tensor([5.7642, 3.5280], requires_grad=True)\n",
      "step №823: loss = 27.231292724609375, weights = tensor([5.7638, 3.5307], requires_grad=True)\n",
      "step №824: loss = 27.21902847290039, weights = tensor([5.7634, 3.5333], requires_grad=True)\n",
      "step №825: loss = 27.206771850585938, weights = tensor([5.7629, 3.5360], requires_grad=True)\n",
      "step №826: loss = 27.194515228271484, weights = tensor([5.7625, 3.5387], requires_grad=True)\n",
      "step №827: loss = 27.182266235351562, weights = tensor([5.7621, 3.5414], requires_grad=True)\n",
      "step №828: loss = 27.170028686523438, weights = tensor([5.7617, 3.5440], requires_grad=True)\n",
      "step №829: loss = 27.157806396484375, weights = tensor([5.7612, 3.5467], requires_grad=True)\n",
      "step №830: loss = 27.145580291748047, weights = tensor([5.7608, 3.5494], requires_grad=True)\n",
      "step №831: loss = 27.133371353149414, weights = tensor([5.7604, 3.5521], requires_grad=True)\n",
      "step №832: loss = 27.121166229248047, weights = tensor([5.7600, 3.5547], requires_grad=True)\n",
      "step №833: loss = 27.10897445678711, weights = tensor([5.7595, 3.5574], requires_grad=True)\n",
      "step №834: loss = 27.096790313720703, weights = tensor([5.7591, 3.5601], requires_grad=True)\n",
      "step №835: loss = 27.084606170654297, weights = tensor([5.7587, 3.5627], requires_grad=True)\n",
      "step №836: loss = 27.07244300842285, weights = tensor([5.7582, 3.5654], requires_grad=True)\n",
      "step №837: loss = 27.06027603149414, weights = tensor([5.7578, 3.5681], requires_grad=True)\n",
      "step №838: loss = 27.048120498657227, weights = tensor([5.7574, 3.5707], requires_grad=True)\n",
      "step №839: loss = 27.03597640991211, weights = tensor([5.7570, 3.5734], requires_grad=True)\n",
      "step №840: loss = 27.02384376525879, weights = tensor([5.7565, 3.5761], requires_grad=True)\n",
      "step №841: loss = 27.0117130279541, weights = tensor([5.7561, 3.5787], requires_grad=True)\n",
      "step №842: loss = 26.999582290649414, weights = tensor([5.7557, 3.5814], requires_grad=True)\n",
      "step №843: loss = 26.987472534179688, weights = tensor([5.7553, 3.5841], requires_grad=True)\n",
      "step №844: loss = 26.975372314453125, weights = tensor([5.7548, 3.5867], requires_grad=True)\n",
      "step №845: loss = 26.963268280029297, weights = tensor([5.7544, 3.5894], requires_grad=True)\n",
      "step №846: loss = 26.951190948486328, weights = tensor([5.7540, 3.5920], requires_grad=True)\n",
      "step №847: loss = 26.93910789489746, weights = tensor([5.7536, 3.5947], requires_grad=True)\n",
      "step №848: loss = 26.92702865600586, weights = tensor([5.7532, 3.5974], requires_grad=True)\n",
      "step №849: loss = 26.914974212646484, weights = tensor([5.7527, 3.6000], requires_grad=True)\n",
      "step №850: loss = 26.902917861938477, weights = tensor([5.7523, 3.6027], requires_grad=True)\n",
      "step №851: loss = 26.890869140625, weights = tensor([5.7519, 3.6053], requires_grad=True)\n",
      "step №852: loss = 26.878833770751953, weights = tensor([5.7515, 3.6080], requires_grad=True)\n",
      "step №853: loss = 26.866802215576172, weights = tensor([5.7510, 3.6106], requires_grad=True)\n",
      "step №854: loss = 26.85477638244629, weights = tensor([5.7506, 3.6133], requires_grad=True)\n",
      "step №855: loss = 26.842763900756836, weights = tensor([5.7502, 3.6159], requires_grad=True)\n",
      "step №856: loss = 26.830753326416016, weights = tensor([5.7498, 3.6186], requires_grad=True)\n",
      "step №857: loss = 26.81876564025879, weights = tensor([5.7493, 3.6212], requires_grad=True)\n",
      "step №858: loss = 26.806766510009766, weights = tensor([5.7489, 3.6239], requires_grad=True)\n",
      "step №859: loss = 26.79477882385254, weights = tensor([5.7485, 3.6265], requires_grad=True)\n",
      "step №860: loss = 26.782806396484375, weights = tensor([5.7481, 3.6292], requires_grad=True)\n",
      "step №861: loss = 26.77083969116211, weights = tensor([5.7477, 3.6318], requires_grad=True)\n",
      "step №862: loss = 26.758880615234375, weights = tensor([5.7472, 3.6345], requires_grad=True)\n",
      "step №863: loss = 26.7469425201416, weights = tensor([5.7468, 3.6371], requires_grad=True)\n",
      "step №864: loss = 26.734996795654297, weights = tensor([5.7464, 3.6397], requires_grad=True)\n",
      "step №865: loss = 26.723058700561523, weights = tensor([5.7460, 3.6424], requires_grad=True)\n",
      "step №866: loss = 26.711132049560547, weights = tensor([5.7456, 3.6450], requires_grad=True)\n",
      "step №867: loss = 26.69921875, weights = tensor([5.7451, 3.6477], requires_grad=True)\n",
      "step №868: loss = 26.687307357788086, weights = tensor([5.7447, 3.6503], requires_grad=True)\n",
      "step №869: loss = 26.675405502319336, weights = tensor([5.7443, 3.6529], requires_grad=True)\n",
      "step №870: loss = 26.66351890563965, weights = tensor([5.7439, 3.6556], requires_grad=True)\n",
      "step №871: loss = 26.651630401611328, weights = tensor([5.7434, 3.6582], requires_grad=True)\n",
      "step №872: loss = 26.63974952697754, weights = tensor([5.7430, 3.6609], requires_grad=True)\n",
      "step №873: loss = 26.627883911132812, weights = tensor([5.7426, 3.6635], requires_grad=True)\n",
      "step №874: loss = 26.616024017333984, weights = tensor([5.7422, 3.6661], requires_grad=True)\n",
      "step №875: loss = 26.604167938232422, weights = tensor([5.7418, 3.6688], requires_grad=True)\n",
      "step №876: loss = 26.592321395874023, weights = tensor([5.7413, 3.6714], requires_grad=True)\n",
      "step №877: loss = 26.580490112304688, weights = tensor([5.7409, 3.6740], requires_grad=True)\n",
      "step №878: loss = 26.568660736083984, weights = tensor([5.7405, 3.6766], requires_grad=True)\n",
      "step №879: loss = 26.556833267211914, weights = tensor([5.7401, 3.6793], requires_grad=True)\n",
      "step №880: loss = 26.545028686523438, weights = tensor([5.7397, 3.6819], requires_grad=True)\n",
      "step №881: loss = 26.533227920532227, weights = tensor([5.7393, 3.6845], requires_grad=True)\n",
      "step №882: loss = 26.521419525146484, weights = tensor([5.7388, 3.6872], requires_grad=True)\n",
      "step №883: loss = 26.509634017944336, weights = tensor([5.7384, 3.6898], requires_grad=True)\n",
      "step №884: loss = 26.497852325439453, weights = tensor([5.7380, 3.6924], requires_grad=True)\n",
      "step №885: loss = 26.4860782623291, weights = tensor([5.7376, 3.6950], requires_grad=True)\n",
      "step №886: loss = 26.474315643310547, weights = tensor([5.7372, 3.6977], requires_grad=True)\n",
      "step №887: loss = 26.462560653686523, weights = tensor([5.7367, 3.7003], requires_grad=True)\n",
      "step №888: loss = 26.4508113861084, weights = tensor([5.7363, 3.7029], requires_grad=True)\n",
      "step №889: loss = 26.43906593322754, weights = tensor([5.7359, 3.7055], requires_grad=True)\n",
      "step №890: loss = 26.427331924438477, weights = tensor([5.7355, 3.7081], requires_grad=True)\n",
      "step №891: loss = 26.415607452392578, weights = tensor([5.7351, 3.7108], requires_grad=True)\n",
      "step №892: loss = 26.403888702392578, weights = tensor([5.7347, 3.7134], requires_grad=True)\n",
      "step №893: loss = 26.392187118530273, weights = tensor([5.7342, 3.7160], requires_grad=True)\n",
      "step №894: loss = 26.3804874420166, weights = tensor([5.7338, 3.7186], requires_grad=True)\n",
      "step №895: loss = 26.368785858154297, weights = tensor([5.7334, 3.7212], requires_grad=True)\n",
      "step №896: loss = 26.357101440429688, weights = tensor([5.7330, 3.7238], requires_grad=True)\n",
      "step №897: loss = 26.345422744750977, weights = tensor([5.7326, 3.7265], requires_grad=True)\n",
      "step №898: loss = 26.333751678466797, weights = tensor([5.7321, 3.7291], requires_grad=True)\n",
      "step №899: loss = 26.322093963623047, weights = tensor([5.7317, 3.7317], requires_grad=True)\n",
      "step №900: loss = 26.310449600219727, weights = tensor([5.7313, 3.7343], requires_grad=True)\n",
      "step №901: loss = 26.298791885375977, weights = tensor([5.7309, 3.7369], requires_grad=True)\n",
      "step №902: loss = 26.287158966064453, weights = tensor([5.7305, 3.7395], requires_grad=True)\n",
      "step №903: loss = 26.275527954101562, weights = tensor([5.7301, 3.7421], requires_grad=True)\n",
      "step №904: loss = 26.2639102935791, weights = tensor([5.7297, 3.7447], requires_grad=True)\n",
      "step №905: loss = 26.25229263305664, weights = tensor([5.7292, 3.7473], requires_grad=True)\n",
      "step №906: loss = 26.24068260192871, weights = tensor([5.7288, 3.7499], requires_grad=True)\n",
      "step №907: loss = 26.229084014892578, weights = tensor([5.7284, 3.7525], requires_grad=True)\n",
      "step №908: loss = 26.217493057250977, weights = tensor([5.7280, 3.7551], requires_grad=True)\n",
      "step №909: loss = 26.205917358398438, weights = tensor([5.7276, 3.7577], requires_grad=True)\n",
      "step №910: loss = 26.1943359375, weights = tensor([5.7272, 3.7603], requires_grad=True)\n",
      "step №911: loss = 26.18277359008789, weights = tensor([5.7267, 3.7629], requires_grad=True)\n",
      "step №912: loss = 26.171213150024414, weights = tensor([5.7263, 3.7655], requires_grad=True)\n",
      "step №913: loss = 26.159658432006836, weights = tensor([5.7259, 3.7681], requires_grad=True)\n",
      "step №914: loss = 26.148122787475586, weights = tensor([5.7255, 3.7707], requires_grad=True)\n",
      "step №915: loss = 26.136581420898438, weights = tensor([5.7251, 3.7733], requires_grad=True)\n",
      "step №916: loss = 26.12506103515625, weights = tensor([5.7247, 3.7759], requires_grad=True)\n",
      "step №917: loss = 26.113529205322266, weights = tensor([5.7243, 3.7785], requires_grad=True)\n",
      "step №918: loss = 26.102020263671875, weights = tensor([5.7238, 3.7811], requires_grad=True)\n",
      "step №919: loss = 26.09051513671875, weights = tensor([5.7234, 3.7837], requires_grad=True)\n",
      "step №920: loss = 26.07901954650879, weights = tensor([5.7230, 3.7863], requires_grad=True)\n",
      "step №921: loss = 26.067529678344727, weights = tensor([5.7226, 3.7889], requires_grad=True)\n",
      "step №922: loss = 26.05605125427246, weights = tensor([5.7222, 3.7915], requires_grad=True)\n",
      "step №923: loss = 26.044574737548828, weights = tensor([5.7218, 3.7941], requires_grad=True)\n",
      "step №924: loss = 26.03310203552246, weights = tensor([5.7214, 3.7967], requires_grad=True)\n",
      "step №925: loss = 26.02164649963379, weights = tensor([5.7210, 3.7993], requires_grad=True)\n",
      "step №926: loss = 26.010202407836914, weights = tensor([5.7205, 3.8019], requires_grad=True)\n",
      "step №927: loss = 25.99875831604004, weights = tensor([5.7201, 3.8044], requires_grad=True)\n",
      "step №928: loss = 25.98732566833496, weights = tensor([5.7197, 3.8070], requires_grad=True)\n",
      "step №929: loss = 25.97589683532715, weights = tensor([5.7193, 3.8096], requires_grad=True)\n",
      "step №930: loss = 25.964473724365234, weights = tensor([5.7189, 3.8122], requires_grad=True)\n",
      "step №931: loss = 25.953060150146484, weights = tensor([5.7185, 3.8148], requires_grad=True)\n",
      "step №932: loss = 25.941661834716797, weights = tensor([5.7181, 3.8174], requires_grad=True)\n",
      "step №933: loss = 25.93027114868164, weights = tensor([5.7177, 3.8199], requires_grad=True)\n",
      "step №934: loss = 25.918874740600586, weights = tensor([5.7172, 3.8225], requires_grad=True)\n",
      "step №935: loss = 25.90749740600586, weights = tensor([5.7168, 3.8251], requires_grad=True)\n",
      "step №936: loss = 25.8961238861084, weights = tensor([5.7164, 3.8277], requires_grad=True)\n",
      "step №937: loss = 25.8847599029541, weights = tensor([5.7160, 3.8303], requires_grad=True)\n",
      "step №938: loss = 25.873401641845703, weights = tensor([5.7156, 3.8328], requires_grad=True)\n",
      "step №939: loss = 25.8620548248291, weights = tensor([5.7152, 3.8354], requires_grad=True)\n",
      "step №940: loss = 25.8507137298584, weights = tensor([5.7148, 3.8380], requires_grad=True)\n",
      "step №941: loss = 25.83937644958496, weights = tensor([5.7144, 3.8406], requires_grad=True)\n",
      "step №942: loss = 25.828044891357422, weights = tensor([5.7140, 3.8431], requires_grad=True)\n",
      "step №943: loss = 25.816730499267578, weights = tensor([5.7135, 3.8457], requires_grad=True)\n",
      "step №944: loss = 25.805416107177734, weights = tensor([5.7131, 3.8483], requires_grad=True)\n",
      "step №945: loss = 25.79410743713379, weights = tensor([5.7127, 3.8509], requires_grad=True)\n",
      "step №946: loss = 25.782812118530273, weights = tensor([5.7123, 3.8534], requires_grad=True)\n",
      "step №947: loss = 25.771533966064453, weights = tensor([5.7119, 3.8560], requires_grad=True)\n",
      "step №948: loss = 25.760250091552734, weights = tensor([5.7115, 3.8586], requires_grad=True)\n",
      "step №949: loss = 25.748977661132812, weights = tensor([5.7111, 3.8611], requires_grad=True)\n",
      "step №950: loss = 25.737712860107422, weights = tensor([5.7107, 3.8637], requires_grad=True)\n",
      "step №951: loss = 25.726449966430664, weights = tensor([5.7103, 3.8663], requires_grad=True)\n",
      "step №952: loss = 25.715206146240234, weights = tensor([5.7099, 3.8688], requires_grad=True)\n",
      "step №953: loss = 25.70395851135254, weights = tensor([5.7095, 3.8714], requires_grad=True)\n",
      "step №954: loss = 25.69272804260254, weights = tensor([5.7090, 3.8740], requires_grad=True)\n",
      "step №955: loss = 25.681503295898438, weights = tensor([5.7086, 3.8765], requires_grad=True)\n",
      "step №956: loss = 25.670276641845703, weights = tensor([5.7082, 3.8791], requires_grad=True)\n",
      "step №957: loss = 25.6590633392334, weights = tensor([5.7078, 3.8816], requires_grad=True)\n",
      "step №958: loss = 25.647863388061523, weights = tensor([5.7074, 3.8842], requires_grad=True)\n",
      "step №959: loss = 25.636667251586914, weights = tensor([5.7070, 3.8868], requires_grad=True)\n",
      "step №960: loss = 25.625469207763672, weights = tensor([5.7066, 3.8893], requires_grad=True)\n",
      "step №961: loss = 25.614294052124023, weights = tensor([5.7062, 3.8919], requires_grad=True)\n",
      "step №962: loss = 25.603113174438477, weights = tensor([5.7058, 3.8944], requires_grad=True)\n",
      "step №963: loss = 25.591960906982422, weights = tensor([5.7054, 3.8970], requires_grad=True)\n",
      "step №964: loss = 25.580791473388672, weights = tensor([5.7050, 3.8995], requires_grad=True)\n",
      "step №965: loss = 25.569643020629883, weights = tensor([5.7046, 3.9021], requires_grad=True)\n",
      "step №966: loss = 25.558502197265625, weights = tensor([5.7041, 3.9046], requires_grad=True)\n",
      "step №967: loss = 25.547359466552734, weights = tensor([5.7037, 3.9072], requires_grad=True)\n",
      "step №968: loss = 25.53623390197754, weights = tensor([5.7033, 3.9098], requires_grad=True)\n",
      "step №969: loss = 25.52511978149414, weights = tensor([5.7029, 3.9123], requires_grad=True)\n",
      "step №970: loss = 25.513996124267578, weights = tensor([5.7025, 3.9148], requires_grad=True)\n",
      "step №971: loss = 25.502887725830078, weights = tensor([5.7021, 3.9174], requires_grad=True)\n",
      "step №972: loss = 25.49179458618164, weights = tensor([5.7017, 3.9199], requires_grad=True)\n",
      "step №973: loss = 25.480701446533203, weights = tensor([5.7013, 3.9225], requires_grad=True)\n",
      "step №974: loss = 25.469619750976562, weights = tensor([5.7009, 3.9250], requires_grad=True)\n",
      "step №975: loss = 25.458538055419922, weights = tensor([5.7005, 3.9276], requires_grad=True)\n",
      "step №976: loss = 25.44748306274414, weights = tensor([5.7001, 3.9301], requires_grad=True)\n",
      "step №977: loss = 25.436416625976562, weights = tensor([5.6997, 3.9327], requires_grad=True)\n",
      "step №978: loss = 25.425357818603516, weights = tensor([5.6993, 3.9352], requires_grad=True)\n",
      "step №979: loss = 25.41431427001953, weights = tensor([5.6989, 3.9378], requires_grad=True)\n",
      "step №980: loss = 25.403274536132812, weights = tensor([5.6985, 3.9403], requires_grad=True)\n",
      "step №981: loss = 25.39223861694336, weights = tensor([5.6981, 3.9428], requires_grad=True)\n",
      "step №982: loss = 25.381221771240234, weights = tensor([5.6977, 3.9454], requires_grad=True)\n",
      "step №983: loss = 25.37020492553711, weights = tensor([5.6972, 3.9479], requires_grad=True)\n",
      "step №984: loss = 25.359195709228516, weights = tensor([5.6968, 3.9505], requires_grad=True)\n",
      "step №985: loss = 25.348196029663086, weights = tensor([5.6964, 3.9530], requires_grad=True)\n",
      "step №986: loss = 25.337200164794922, weights = tensor([5.6960, 3.9555], requires_grad=True)\n",
      "step №987: loss = 25.326217651367188, weights = tensor([5.6956, 3.9581], requires_grad=True)\n",
      "step №988: loss = 25.31523323059082, weights = tensor([5.6952, 3.9606], requires_grad=True)\n",
      "step №989: loss = 25.304262161254883, weights = tensor([5.6948, 3.9631], requires_grad=True)\n",
      "step №990: loss = 25.293304443359375, weights = tensor([5.6944, 3.9657], requires_grad=True)\n",
      "step №991: loss = 25.282337188720703, weights = tensor([5.6940, 3.9682], requires_grad=True)\n",
      "step №992: loss = 25.27138900756836, weights = tensor([5.6936, 3.9707], requires_grad=True)\n",
      "step №993: loss = 25.26045036315918, weights = tensor([5.6932, 3.9732], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №994: loss = 25.249507904052734, weights = tensor([5.6928, 3.9758], requires_grad=True)\n",
      "step №995: loss = 25.238590240478516, weights = tensor([5.6924, 3.9783], requires_grad=True)\n",
      "step №996: loss = 25.2276668548584, weights = tensor([5.6920, 3.9808], requires_grad=True)\n",
      "step №997: loss = 25.216753005981445, weights = tensor([5.6916, 3.9834], requires_grad=True)\n",
      "step №998: loss = 25.205856323242188, weights = tensor([5.6912, 3.9859], requires_grad=True)\n",
      "step №999: loss = 25.194955825805664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#let's also use mse error and optimizer which are provided by Pytorch library  \n",
    "import torch.nn as nn \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float32, requires_grad = True)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05cac125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Everything is calculated by Pytorch. Slope = 5.690791130065918, intercept = 3.9884085655212402, loss = 25.194955825805664')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Everything is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3fa2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f8148147740>\n",
      "step №0: loss = 1367.7750244140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №1: loss = 1276.1319580078125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №2: loss = 1190.8009033203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №3: loss = 1111.346923828125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №4: loss = 1037.365234375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №5: loss = 968.4788208007812, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №6: loss = 904.3366088867188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №7: loss = 844.61181640625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №8: loss = 789.0, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №9: loss = 737.2178955078125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №10: loss = 689.0015869140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №11: loss = 644.1055297851562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №12: loss = 602.3009033203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №13: loss = 563.3748168945312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №14: loss = 527.1287841796875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №15: loss = 493.37841796875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №16: loss = 461.95159912109375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №17: loss = 432.68829345703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №18: loss = 405.43939208984375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №19: loss = 380.0662536621094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №20: loss = 356.4395751953125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №21: loss = 334.43902587890625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №22: loss = 313.9526062011719, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №23: loss = 294.87603759765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №24: loss = 277.1120910644531, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №25: loss = 260.5704345703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №26: loss = 245.1667938232422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №27: loss = 230.8228302001953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №28: loss = 217.46548461914062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №29: loss = 205.02682495117188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №30: loss = 193.44357299804688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №31: loss = 182.65676879882812, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №32: loss = 172.6114959716797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №33: loss = 163.25674438476562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №34: loss = 154.54493713378906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №35: loss = 146.4317626953125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №36: loss = 138.87599182128906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №37: loss = 131.83920288085938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №38: loss = 125.28562927246094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №39: loss = 119.1820297241211, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №40: loss = 113.49739074707031, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №41: loss = 108.20280456542969, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №42: loss = 103.271484375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №43: loss = 98.6783218383789, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №44: loss = 94.40007019042969, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №45: loss = 90.41500854492188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №46: loss = 86.7029800415039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №47: loss = 83.24513244628906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №48: loss = 80.02396392822266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №49: loss = 77.0231704711914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №50: loss = 74.22760009765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №51: loss = 71.6230697631836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №52: loss = 69.19646453857422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №53: loss = 66.93549346923828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №54: loss = 64.82876586914062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №55: loss = 62.86565399169922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №56: loss = 61.0362548828125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №57: loss = 59.33135223388672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №58: loss = 57.74238967895508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №59: loss = 56.261383056640625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №60: loss = 54.88086700439453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №61: loss = 53.59393310546875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №62: loss = 52.39415740966797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №63: loss = 51.2755012512207, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №64: loss = 50.23240280151367, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №65: loss = 49.2596549987793, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №66: loss = 48.352413177490234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №67: loss = 47.506134033203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №68: loss = 46.7166633605957, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №69: loss = 45.98005294799805, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №70: loss = 45.29267120361328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №71: loss = 44.651153564453125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №72: loss = 44.052310943603516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №73: loss = 43.49322509765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №74: loss = 42.97113800048828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №75: loss = 42.48351287841797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №76: loss = 42.027976989746094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №77: loss = 41.60231018066406, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №78: loss = 41.2044677734375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №79: loss = 40.832542419433594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №80: loss = 40.484737396240234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №81: loss = 40.15937423706055, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №82: loss = 39.85492706298828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №83: loss = 39.569969177246094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №84: loss = 39.303138732910156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №85: loss = 39.053199768066406, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №86: loss = 38.818973541259766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №87: loss = 38.599388122558594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №88: loss = 38.39344024658203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №89: loss = 38.2001953125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №90: loss = 38.01876449584961, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №91: loss = 37.84833908081055, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №92: loss = 37.68817901611328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №93: loss = 37.53754806518555, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №94: loss = 37.395816802978516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №95: loss = 37.262351989746094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №96: loss = 37.136592864990234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №97: loss = 37.018028259277344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №98: loss = 36.90614318847656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №99: loss = 36.800472259521484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №100: loss = 36.70061492919922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №101: loss = 36.60614013671875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №102: loss = 36.516700744628906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №103: loss = 36.43195343017578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №104: loss = 36.35155487060547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №105: loss = 36.27521514892578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №106: loss = 36.202667236328125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №107: loss = 36.13364028930664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №108: loss = 36.06788635253906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №109: loss = 36.00519943237305, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №110: loss = 35.94536590576172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №111: loss = 35.88816452026367, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №112: loss = 35.83344268798828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №113: loss = 35.781028747558594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №114: loss = 35.730751037597656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №115: loss = 35.682464599609375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №116: loss = 35.63605499267578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №117: loss = 35.59135818481445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №118: loss = 35.54828643798828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №119: loss = 35.5067138671875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №120: loss = 35.4665412902832, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №121: loss = 35.42768478393555, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №122: loss = 35.390037536621094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №123: loss = 35.35352325439453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №124: loss = 35.318077087402344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №125: loss = 35.28361511230469, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №126: loss = 35.250064849853516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №127: loss = 35.21736145019531, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №128: loss = 35.18547058105469, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №129: loss = 35.154327392578125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №130: loss = 35.12385940551758, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №131: loss = 35.09404754638672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №132: loss = 35.064849853515625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №133: loss = 35.03620910644531, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №134: loss = 35.00808334350586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №135: loss = 34.9804573059082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №136: loss = 34.95327377319336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №137: loss = 34.92653274536133, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №138: loss = 34.90018081665039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №139: loss = 34.87420654296875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №140: loss = 34.84857940673828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №141: loss = 34.823272705078125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №142: loss = 34.79827117919922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №143: loss = 34.7735595703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №144: loss = 34.74909973144531, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №145: loss = 34.724891662597656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №146: loss = 34.7009162902832, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №147: loss = 34.67715835571289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №148: loss = 34.65359115600586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №149: loss = 34.63023376464844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №150: loss = 34.607032775878906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №151: loss = 34.584007263183594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №152: loss = 34.5611457824707, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №153: loss = 34.538414001464844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №154: loss = 34.515830993652344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №155: loss = 34.493370056152344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №156: loss = 34.47102355957031, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №157: loss = 34.44879913330078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №158: loss = 34.426673889160156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №159: loss = 34.404659271240234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №160: loss = 34.38274002075195, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №161: loss = 34.36089324951172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №162: loss = 34.339141845703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №163: loss = 34.317466735839844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №164: loss = 34.295860290527344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №165: loss = 34.27433395385742, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №166: loss = 34.252864837646484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №167: loss = 34.231468200683594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №168: loss = 34.21011734008789, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №169: loss = 34.18882751464844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №170: loss = 34.16759490966797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №171: loss = 34.14641189575195, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №172: loss = 34.125274658203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №173: loss = 34.104190826416016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №174: loss = 34.08313751220703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №175: loss = 34.0621337890625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №176: loss = 34.041175842285156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №177: loss = 34.020233154296875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №178: loss = 33.99934387207031, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №179: loss = 33.97848892211914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №180: loss = 33.957664489746094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №181: loss = 33.93687057495117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №182: loss = 33.916107177734375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №183: loss = 33.89536666870117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №184: loss = 33.874656677246094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №185: loss = 33.85398864746094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №186: loss = 33.83333206176758, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №187: loss = 33.812705993652344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №188: loss = 33.79209518432617, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №189: loss = 33.77152633666992, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №190: loss = 33.75096893310547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №191: loss = 33.730438232421875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №192: loss = 33.709922790527344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №193: loss = 33.6894416809082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №194: loss = 33.668968200683594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №195: loss = 33.648529052734375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №196: loss = 33.62810516357422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №197: loss = 33.607696533203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №198: loss = 33.58729934692383, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №199: loss = 33.566932678222656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №200: loss = 33.54658889770508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №201: loss = 33.5262451171875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №202: loss = 33.50593948364258, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №203: loss = 33.485633850097656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №204: loss = 33.46535110473633, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №205: loss = 33.445091247558594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №206: loss = 33.42483901977539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №207: loss = 33.404605865478516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №208: loss = 33.38439178466797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №209: loss = 33.36418914794922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №210: loss = 33.3440055847168, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №211: loss = 33.32384490966797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №212: loss = 33.30369186401367, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №213: loss = 33.28355026245117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №214: loss = 33.263431549072266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №215: loss = 33.24332809448242, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №216: loss = 33.22323226928711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №217: loss = 33.20316696166992, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №218: loss = 33.18310546875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №219: loss = 33.163047790527344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №220: loss = 33.143028259277344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №221: loss = 33.123008728027344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №222: loss = 33.10300064086914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №223: loss = 33.08301544189453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №224: loss = 33.06304168701172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №225: loss = 33.043087005615234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №226: loss = 33.023136138916016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №227: loss = 33.00320053100586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №228: loss = 32.98328399658203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №229: loss = 32.963382720947266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №230: loss = 32.94348907470703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №231: loss = 32.923622131347656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №232: loss = 32.90376281738281, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №233: loss = 32.88391876220703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №234: loss = 32.86408233642578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №235: loss = 32.84426498413086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №236: loss = 32.824459075927734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №237: loss = 32.804664611816406, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №238: loss = 32.784889221191406, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №239: loss = 32.76512908935547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №240: loss = 32.7453727722168, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №241: loss = 32.72563552856445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №242: loss = 32.70591354370117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №243: loss = 32.686195373535156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №244: loss = 32.666507720947266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №245: loss = 32.64682388305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №246: loss = 32.62715530395508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №247: loss = 32.607505798339844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №248: loss = 32.587860107421875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №249: loss = 32.56822967529297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №250: loss = 32.54861068725586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №251: loss = 32.529014587402344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №252: loss = 32.509422302246094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №253: loss = 32.489845275878906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №254: loss = 32.47029113769531, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №255: loss = 32.45073699951172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №256: loss = 32.43120574951172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №257: loss = 32.41168212890625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №258: loss = 32.392173767089844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №259: loss = 32.3726806640625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №260: loss = 32.35319137573242, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №261: loss = 32.33373260498047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №262: loss = 32.314266204833984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №263: loss = 32.294830322265625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №264: loss = 32.27539825439453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №265: loss = 32.25597381591797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №266: loss = 32.23657989501953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №267: loss = 32.217185974121094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №268: loss = 32.19781494140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №269: loss = 32.17845153808594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №270: loss = 32.15910720825195, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №271: loss = 32.13976287841797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №272: loss = 32.12044143676758, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №273: loss = 32.101131439208984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №274: loss = 32.08184051513672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №275: loss = 32.06255340576172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №276: loss = 32.04328155517578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №277: loss = 32.024017333984375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №278: loss = 32.004764556884766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №279: loss = 31.98554039001465, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №280: loss = 31.966323852539062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №281: loss = 31.947118759155273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №282: loss = 31.927921295166016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №283: loss = 31.908742904663086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №284: loss = 31.889575958251953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №285: loss = 31.870418548583984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №286: loss = 31.851282119750977, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №287: loss = 31.832157135009766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №288: loss = 31.813039779663086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №289: loss = 31.793933868408203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №290: loss = 31.77484703063965, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №291: loss = 31.75576400756836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №292: loss = 31.736709594726562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №293: loss = 31.717662811279297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №294: loss = 31.698623657226562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №295: loss = 31.67959213256836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №296: loss = 31.66057777404785, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №297: loss = 31.641582489013672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №298: loss = 31.62259864807129, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №299: loss = 31.6036319732666, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №300: loss = 31.584667205810547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №301: loss = 31.56571388244629, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №302: loss = 31.546783447265625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №303: loss = 31.527868270874023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №304: loss = 31.508953094482422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №305: loss = 31.49005699157715, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №306: loss = 31.471172332763672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №307: loss = 31.452306747436523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №308: loss = 31.43345069885254, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №309: loss = 31.414592742919922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №310: loss = 31.395767211914062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №311: loss = 31.3769474029541, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №312: loss = 31.358142852783203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №313: loss = 31.339351654052734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №314: loss = 31.320566177368164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №315: loss = 31.301794052124023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №316: loss = 31.28304100036621, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №317: loss = 31.264293670654297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №318: loss = 31.24556541442871, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №319: loss = 31.226852416992188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №320: loss = 31.208154678344727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №321: loss = 31.1894588470459, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №322: loss = 31.170772552490234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №323: loss = 31.152109146118164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №324: loss = 31.13345718383789, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №325: loss = 31.114810943603516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №326: loss = 31.0961856842041, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №327: loss = 31.077566146850586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №328: loss = 31.0589542388916, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №329: loss = 31.04037094116211, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №330: loss = 31.021793365478516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №331: loss = 31.003231048583984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №332: loss = 30.984676361083984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №333: loss = 30.966135025024414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №334: loss = 30.94760513305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №335: loss = 30.929088592529297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №336: loss = 30.91058349609375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №337: loss = 30.8920955657959, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №338: loss = 30.87361717224121, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №339: loss = 30.855148315429688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №340: loss = 30.836700439453125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №341: loss = 30.818262100219727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №342: loss = 30.799829483032227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №343: loss = 30.781417846679688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №344: loss = 30.763011932373047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №345: loss = 30.744619369506836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №346: loss = 30.726245880126953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №347: loss = 30.707874298095703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №348: loss = 30.68951988220215, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №349: loss = 30.67118263244629, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №350: loss = 30.652851104736328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №351: loss = 30.634540557861328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №352: loss = 30.616235733032227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №353: loss = 30.597949981689453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №354: loss = 30.579662322998047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №355: loss = 30.561397552490234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №356: loss = 30.54314613342285, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №357: loss = 30.524911880493164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №358: loss = 30.50667381286621, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №359: loss = 30.48845863342285, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №360: loss = 30.470251083374023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №361: loss = 30.452068328857422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №362: loss = 30.433879852294922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №363: loss = 30.415719985961914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №364: loss = 30.39755630493164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №365: loss = 30.379413604736328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №366: loss = 30.361278533935547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №367: loss = 30.34316635131836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №368: loss = 30.325054168701172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №369: loss = 30.306964874267578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №370: loss = 30.288888931274414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №371: loss = 30.27081871032715, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №372: loss = 30.252761840820312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №373: loss = 30.23472023010254, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №374: loss = 30.216678619384766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №375: loss = 30.19866371154785, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №376: loss = 30.180654525756836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №377: loss = 30.162654876708984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №378: loss = 30.14467430114746, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №379: loss = 30.126705169677734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №380: loss = 30.108739852905273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №381: loss = 30.090789794921875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №382: loss = 30.072866439819336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №383: loss = 30.054941177368164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №384: loss = 30.037023544311523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №385: loss = 30.019128799438477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №386: loss = 30.001245498657227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №387: loss = 29.983367919921875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №388: loss = 29.965505599975586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №389: loss = 29.94765853881836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №390: loss = 29.929819107055664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №391: loss = 29.911991119384766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №392: loss = 29.894176483154297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №393: loss = 29.87637710571289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №394: loss = 29.858585357666016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №395: loss = 29.840805053710938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №396: loss = 29.823047637939453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №397: loss = 29.805288314819336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №398: loss = 29.787546157836914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №399: loss = 29.769826889038086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №400: loss = 29.75210189819336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №401: loss = 29.73439598083496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №402: loss = 29.716699600219727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №403: loss = 29.69900894165039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №404: loss = 29.68134117126465, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №405: loss = 29.663692474365234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №406: loss = 29.646038055419922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №407: loss = 29.628421783447266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №408: loss = 29.610782623291016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №409: loss = 29.593181610107422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №410: loss = 29.57558250427246, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №411: loss = 29.557994842529297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №412: loss = 29.540424346923828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №413: loss = 29.52286148071289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №414: loss = 29.50530433654785, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №415: loss = 29.487768173217773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №416: loss = 29.470245361328125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №417: loss = 29.45272445678711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №418: loss = 29.435222625732422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №419: loss = 29.417734146118164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №420: loss = 29.400259017944336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №421: loss = 29.38278579711914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №422: loss = 29.36533546447754, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №423: loss = 29.347890853881836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №424: loss = 29.330455780029297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №425: loss = 29.313039779663086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №426: loss = 29.29563331604004, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №427: loss = 29.27823257446289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №428: loss = 29.260848999023438, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №429: loss = 29.24347496032715, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №430: loss = 29.226110458374023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №431: loss = 29.20876693725586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №432: loss = 29.19142723083496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №433: loss = 29.17410659790039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №434: loss = 29.156784057617188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №435: loss = 29.139484405517578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №436: loss = 29.1221923828125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №437: loss = 29.104915618896484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №438: loss = 29.087650299072266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №439: loss = 29.070398330688477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №440: loss = 29.053146362304688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №441: loss = 29.035913467407227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №442: loss = 29.018701553344727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №443: loss = 29.00148582458496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №444: loss = 28.984294891357422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №445: loss = 28.967105865478516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №446: loss = 28.949941635131836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №447: loss = 28.932764053344727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №448: loss = 28.91562271118164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №449: loss = 28.898483276367188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №450: loss = 28.881357192993164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №451: loss = 28.864246368408203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №452: loss = 28.847137451171875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №453: loss = 28.830047607421875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №454: loss = 28.81296730041504, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №455: loss = 28.7958984375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №456: loss = 28.778844833374023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №457: loss = 28.761789321899414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №458: loss = 28.744760513305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №459: loss = 28.72772789001465, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №460: loss = 28.71072769165039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №461: loss = 28.693721771240234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №462: loss = 28.676733016967773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №463: loss = 28.659759521484375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №464: loss = 28.64278793334961, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №465: loss = 28.625843048095703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №466: loss = 28.608901977539062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №467: loss = 28.591964721679688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №468: loss = 28.575048446655273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №469: loss = 28.558141708374023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №470: loss = 28.541248321533203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №471: loss = 28.524362564086914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №472: loss = 28.50748062133789, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №473: loss = 28.490619659423828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №474: loss = 28.473770141601562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №475: loss = 28.456933975219727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №476: loss = 28.440109252929688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №477: loss = 28.423282623291016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №478: loss = 28.406478881835938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №479: loss = 28.389684677124023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №480: loss = 28.372894287109375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №481: loss = 28.356130599975586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №482: loss = 28.33937644958496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №483: loss = 28.322622299194336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №484: loss = 28.305883407592773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №485: loss = 28.289159774780273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №486: loss = 28.272441864013672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №487: loss = 28.255741119384766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №488: loss = 28.239059448242188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №489: loss = 28.222370147705078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №490: loss = 28.205707550048828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №491: loss = 28.189041137695312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №492: loss = 28.17239761352539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №493: loss = 28.155757904052734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №494: loss = 28.13913917541504, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №495: loss = 28.12252426147461, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №496: loss = 28.10591697692871, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №497: loss = 28.089330673217773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №498: loss = 28.0727596282959, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №499: loss = 28.05618667602539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №500: loss = 28.039630889892578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №501: loss = 28.023090362548828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №502: loss = 28.006555557250977, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №503: loss = 27.990026473999023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №504: loss = 27.9735164642334, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №505: loss = 27.957015991210938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №506: loss = 27.94052505493164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №507: loss = 27.924047470092773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №508: loss = 27.907581329345703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №509: loss = 27.891122817993164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №510: loss = 27.874685287475586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №511: loss = 27.85824966430664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №512: loss = 27.84181785583496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №513: loss = 27.82541847229004, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №514: loss = 27.809017181396484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №515: loss = 27.79262924194336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №516: loss = 27.7762451171875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №517: loss = 27.759878158569336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №518: loss = 27.7435302734375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №519: loss = 27.727182388305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №520: loss = 27.710845947265625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №521: loss = 27.694522857666016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №522: loss = 27.678211212158203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №523: loss = 27.661911010742188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №524: loss = 27.645618438720703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №525: loss = 27.629343032836914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №526: loss = 27.61307716369629, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №527: loss = 27.596811294555664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №528: loss = 27.580575942993164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №529: loss = 27.564346313476562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №530: loss = 27.548110961914062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №531: loss = 27.53190040588379, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №532: loss = 27.515701293945312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №533: loss = 27.499515533447266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №534: loss = 27.48333740234375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №535: loss = 27.4671630859375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №536: loss = 27.451007843017578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №537: loss = 27.434856414794922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №538: loss = 27.41872215270996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №539: loss = 27.40260887145996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №540: loss = 27.386493682861328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №541: loss = 27.37038230895996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №542: loss = 27.354290008544922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №543: loss = 27.338211059570312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №544: loss = 27.322139739990234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №545: loss = 27.306087493896484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №546: loss = 27.2900333404541, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №547: loss = 27.273998260498047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №548: loss = 27.257970809936523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №549: loss = 27.241954803466797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №550: loss = 27.2259521484375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №551: loss = 27.209957122802734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №552: loss = 27.1939697265625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №553: loss = 27.17800521850586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №554: loss = 27.162038803100586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №555: loss = 27.14609146118164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №556: loss = 27.13015365600586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №557: loss = 27.114215850830078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №558: loss = 27.098302841186523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №559: loss = 27.082393646240234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №560: loss = 27.06649398803711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №561: loss = 27.050613403320312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №562: loss = 27.034740447998047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №563: loss = 27.018878936767578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №564: loss = 27.00301742553711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №565: loss = 26.987171173095703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №566: loss = 26.971343994140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №567: loss = 26.955516815185547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №568: loss = 26.939712524414062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №569: loss = 26.923904418945312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №570: loss = 26.90812110900879, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №571: loss = 26.892343521118164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №572: loss = 26.876577377319336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №573: loss = 26.860815048217773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №574: loss = 26.845077514648438, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №575: loss = 26.829334259033203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №576: loss = 26.813613891601562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №577: loss = 26.797901153564453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №578: loss = 26.78218650817871, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №579: loss = 26.76650047302246, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №580: loss = 26.75081443786621, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №581: loss = 26.735143661499023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №582: loss = 26.7194766998291, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №583: loss = 26.703832626342773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №584: loss = 26.688190460205078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №585: loss = 26.672561645507812, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №586: loss = 26.656940460205078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №587: loss = 26.64133644104004, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №588: loss = 26.625741958618164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №589: loss = 26.61014747619629, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №590: loss = 26.59457778930664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №591: loss = 26.579004287719727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №592: loss = 26.563451766967773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №593: loss = 26.54791259765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №594: loss = 26.53236961364746, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №595: loss = 26.516845703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №596: loss = 26.5013370513916, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №597: loss = 26.485830307006836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №598: loss = 26.4703369140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №599: loss = 26.45485496520996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №600: loss = 26.439382553100586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №601: loss = 26.423919677734375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №602: loss = 26.408477783203125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №603: loss = 26.39303970336914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №604: loss = 26.37760353088379, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №605: loss = 26.362186431884766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №606: loss = 26.346776962280273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №607: loss = 26.331375122070312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №608: loss = 26.315994262695312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №609: loss = 26.300613403320312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №610: loss = 26.28524398803711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №611: loss = 26.269886016845703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №612: loss = 26.254547119140625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №613: loss = 26.239206314086914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №614: loss = 26.223880767822266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №615: loss = 26.208572387695312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №616: loss = 26.193256378173828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №617: loss = 26.1779727935791, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №618: loss = 26.162689208984375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №619: loss = 26.147409439086914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №620: loss = 26.13214683532715, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №621: loss = 26.116891860961914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №622: loss = 26.101648330688477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №623: loss = 26.0864200592041, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №624: loss = 26.071197509765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №625: loss = 26.055978775024414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №626: loss = 26.040782928466797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №627: loss = 26.02559471130371, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №628: loss = 26.010412216186523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №629: loss = 25.995243072509766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №630: loss = 25.980083465576172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №631: loss = 25.964929580688477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №632: loss = 25.94978904724121, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №633: loss = 25.934661865234375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №634: loss = 25.919546127319336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №635: loss = 25.904430389404297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №636: loss = 25.889331817626953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №637: loss = 25.874242782592773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №638: loss = 25.85917091369629, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №639: loss = 25.84409523010254, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №640: loss = 25.82904624938965, weights = tensor([5.6908, 3.9884], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №641: loss = 25.81399154663086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №642: loss = 25.798961639404297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №643: loss = 25.783939361572266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №644: loss = 25.768917083740234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №645: loss = 25.75390625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №646: loss = 25.738910675048828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №647: loss = 25.723918914794922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №648: loss = 25.708948135375977, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №649: loss = 25.693981170654297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №650: loss = 25.679027557373047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №651: loss = 25.664081573486328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №652: loss = 25.649145126342773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №653: loss = 25.63421630859375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №654: loss = 25.61929702758789, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №655: loss = 25.604400634765625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №656: loss = 25.589506149291992, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №657: loss = 25.57461929321289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №658: loss = 25.55974769592285, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №659: loss = 25.544878005981445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №660: loss = 25.530025482177734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №661: loss = 25.515178680419922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №662: loss = 25.500341415405273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №663: loss = 25.485523223876953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №664: loss = 25.470699310302734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №665: loss = 25.455900192260742, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №666: loss = 25.441099166870117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №667: loss = 25.426319122314453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №668: loss = 25.411550521850586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №669: loss = 25.396780014038086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №670: loss = 25.382028579711914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №671: loss = 25.367284774780273, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №672: loss = 25.352542877197266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №673: loss = 25.337825775146484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №674: loss = 25.323104858398438, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №675: loss = 25.308399200439453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №676: loss = 25.29370880126953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №677: loss = 25.27901840209961, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №678: loss = 25.264354705810547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №679: loss = 25.24968910217285, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №680: loss = 25.235027313232422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №681: loss = 25.220388412475586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №682: loss = 25.205747604370117, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №683: loss = 25.191120147705078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №684: loss = 25.176509857177734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №685: loss = 25.161901473999023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №686: loss = 25.147308349609375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №687: loss = 25.132722854614258, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №688: loss = 25.118148803710938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №689: loss = 25.10358428955078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №690: loss = 25.089021682739258, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №691: loss = 25.074481964111328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №692: loss = 25.059940338134766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №693: loss = 25.04541778564453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №694: loss = 25.030902862548828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №695: loss = 25.016399383544922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №696: loss = 25.001893997192383, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №697: loss = 24.987403869628906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №698: loss = 24.972929000854492, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №699: loss = 24.958457946777344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №700: loss = 24.944005966186523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №701: loss = 24.929561614990234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №702: loss = 24.915122985839844, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №703: loss = 24.90069580078125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №704: loss = 24.886266708374023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №705: loss = 24.871856689453125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №706: loss = 24.857460021972656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №707: loss = 24.843076705932617, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №708: loss = 24.82870101928711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №709: loss = 24.8143310546875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №710: loss = 24.799968719482422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №711: loss = 24.785612106323242, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №712: loss = 24.771276473999023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №713: loss = 24.756946563720703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №714: loss = 24.74262237548828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №715: loss = 24.728313446044922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №716: loss = 24.714008331298828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №717: loss = 24.69972038269043, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №718: loss = 24.68543815612793, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №719: loss = 24.671167373657227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №720: loss = 24.656902313232422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №721: loss = 24.64264488220215, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №722: loss = 24.628406524658203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №723: loss = 24.61417007446289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №724: loss = 24.59994888305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №725: loss = 24.58572769165039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №726: loss = 24.5715274810791, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №727: loss = 24.55733299255371, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №728: loss = 24.543148040771484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №729: loss = 24.528968811035156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №730: loss = 24.51479721069336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №731: loss = 24.500648498535156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №732: loss = 24.486492156982422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №733: loss = 24.472352981567383, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №734: loss = 24.458227157592773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №735: loss = 24.444114685058594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №736: loss = 24.430004119873047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №737: loss = 24.41590118408203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №738: loss = 24.401817321777344, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №739: loss = 24.387731552124023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №740: loss = 24.373655319213867, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №741: loss = 24.359600067138672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №742: loss = 24.345550537109375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №743: loss = 24.331510543823242, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №744: loss = 24.317472457885742, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №745: loss = 24.303447723388672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №746: loss = 24.289440155029297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №747: loss = 24.275432586669922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №748: loss = 24.26143455505371, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №749: loss = 24.24744987487793, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №750: loss = 24.233474731445312, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №751: loss = 24.219507217407227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №752: loss = 24.205549240112305, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №753: loss = 24.191600799560547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №754: loss = 24.177669525146484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №755: loss = 24.16374397277832, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №756: loss = 24.149816513061523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №757: loss = 24.135913848876953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №758: loss = 24.122005462646484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №759: loss = 24.108118057250977, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №760: loss = 24.094236373901367, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №761: loss = 24.080364227294922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №762: loss = 24.06650161743164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №763: loss = 24.052648544311523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №764: loss = 24.038806915283203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №765: loss = 24.02497100830078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №766: loss = 24.011146545410156, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №767: loss = 23.997325897216797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №768: loss = 23.983524322509766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №769: loss = 23.9697265625, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №770: loss = 23.955942153930664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №771: loss = 23.94215965270996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №772: loss = 23.928390502929688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №773: loss = 23.914628982543945, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №774: loss = 23.90088653564453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №775: loss = 23.88714027404785, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №776: loss = 23.8734073638916, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №777: loss = 23.859683990478516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №778: loss = 23.845975875854492, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №779: loss = 23.832271575927734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №780: loss = 23.818572998046875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №781: loss = 23.804889678955078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №782: loss = 23.79121208190918, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №783: loss = 23.777542114257812, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №784: loss = 23.763887405395508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №785: loss = 23.7502384185791, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №786: loss = 23.736600875854492, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №787: loss = 23.722970962524414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №788: loss = 23.709348678588867, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №789: loss = 23.695735931396484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №790: loss = 23.682140350341797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №791: loss = 23.66854476928711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №792: loss = 23.654956817626953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №793: loss = 23.64138412475586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №794: loss = 23.627819061279297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №795: loss = 23.6142635345459, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №796: loss = 23.600711822509766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №797: loss = 23.587173461914062, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №798: loss = 23.573650360107422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №799: loss = 23.56012535095215, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №800: loss = 23.546619415283203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №801: loss = 23.53311538696289, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №802: loss = 23.51962661743164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №803: loss = 23.506141662597656, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №804: loss = 23.492664337158203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №805: loss = 23.47920036315918, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №806: loss = 23.465747833251953, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №807: loss = 23.452302932739258, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №808: loss = 23.438861846923828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №809: loss = 23.425432205200195, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №810: loss = 23.41201400756836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №811: loss = 23.398605346679688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №812: loss = 23.385204315185547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №813: loss = 23.37181282043457, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №814: loss = 23.35843276977539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №815: loss = 23.34505271911621, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №816: loss = 23.331687927246094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №817: loss = 23.318334579467773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №818: loss = 23.30499267578125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №819: loss = 23.29165267944336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №820: loss = 23.278322219848633, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №821: loss = 23.265005111694336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №822: loss = 23.251699447631836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №823: loss = 23.238393783569336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №824: loss = 23.225099563598633, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №825: loss = 23.211814880371094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №826: loss = 23.198545455932617, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №827: loss = 23.18527603149414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №828: loss = 23.172021865844727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №829: loss = 23.158777236938477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №830: loss = 23.14554214477539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №831: loss = 23.132312774658203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №832: loss = 23.119089126586914, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №833: loss = 23.105878829956055, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №834: loss = 23.09267807006836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №835: loss = 23.079477310180664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №836: loss = 23.066295623779297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №837: loss = 23.053117752075195, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №838: loss = 23.039953231811523, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №839: loss = 23.026798248291016, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №840: loss = 23.013652801513672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №841: loss = 23.00050926208496, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №842: loss = 22.98737144470215, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №843: loss = 22.974258422851562, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №844: loss = 22.961139678955078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №845: loss = 22.948034286499023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №846: loss = 22.934946060180664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №847: loss = 22.921852111816406, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №848: loss = 22.908777236938477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №849: loss = 22.895708084106445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №850: loss = 22.88265037536621, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №851: loss = 22.86959457397461, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №852: loss = 22.856557846069336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №853: loss = 22.843524932861328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №854: loss = 22.83049201965332, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №855: loss = 22.81747817993164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №856: loss = 22.804475784301758, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №857: loss = 22.791473388671875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №858: loss = 22.778484344482422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №859: loss = 22.7655029296875, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №860: loss = 22.75253677368164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №861: loss = 22.739572525024414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №862: loss = 22.726613998413086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №863: loss = 22.71367073059082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №864: loss = 22.700733184814453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №865: loss = 22.68780517578125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №866: loss = 22.674884796142578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №867: loss = 22.661983489990234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №868: loss = 22.64907455444336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №869: loss = 22.636180877685547, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №870: loss = 22.623294830322266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №871: loss = 22.610424041748047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №872: loss = 22.59756088256836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №873: loss = 22.584699630737305, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №874: loss = 22.57185173034668, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №875: loss = 22.559005737304688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №876: loss = 22.546178817749023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №877: loss = 22.53335189819336, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №878: loss = 22.52054214477539, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №879: loss = 22.50773048400879, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №880: loss = 22.494937896728516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №881: loss = 22.482145309448242, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №882: loss = 22.46937370300293, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №883: loss = 22.45659637451172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №884: loss = 22.443832397460938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №885: loss = 22.43107795715332, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №886: loss = 22.418331146240234, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №887: loss = 22.405603408813477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №888: loss = 22.39286994934082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №889: loss = 22.38014793395996, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №890: loss = 22.367443084716797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №891: loss = 22.354736328125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №892: loss = 22.342042922973633, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №893: loss = 22.329364776611328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №894: loss = 22.31668472290039, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №895: loss = 22.304014205932617, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №896: loss = 22.29135513305664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №897: loss = 22.278705596923828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №898: loss = 22.26605987548828, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №899: loss = 22.25343132019043, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №900: loss = 22.24081039428711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №901: loss = 22.22818946838379, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №902: loss = 22.215585708618164, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №903: loss = 22.202983856201172, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №904: loss = 22.190385818481445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №905: loss = 22.177812576293945, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №906: loss = 22.165239334106445, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №907: loss = 22.152673721313477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №908: loss = 22.140111923217773, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №909: loss = 22.1275691986084, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №910: loss = 22.115026473999023, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №911: loss = 22.10249900817871, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №912: loss = 22.0899715423584, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №913: loss = 22.077465057373047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №914: loss = 22.064952850341797, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №915: loss = 22.052453994750977, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №916: loss = 22.03997039794922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №917: loss = 22.02748680114746, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №918: loss = 22.015018463134766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №919: loss = 22.002552032470703, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №920: loss = 21.990095138549805, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №921: loss = 21.97764778137207, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №922: loss = 21.965211868286133, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №923: loss = 21.952787399291992, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №924: loss = 21.940359115600586, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №925: loss = 21.927949905395508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №926: loss = 21.915544509887695, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №927: loss = 21.903146743774414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №928: loss = 21.890758514404297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №929: loss = 21.87838363647461, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №930: loss = 21.86601448059082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №931: loss = 21.853649139404297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №932: loss = 21.841297149658203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №933: loss = 21.82895278930664, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №934: loss = 21.816608428955078, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №935: loss = 21.80428695678711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №936: loss = 21.791963577270508, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №937: loss = 21.7796573638916, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №938: loss = 21.76734733581543, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №939: loss = 21.755046844482422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №940: loss = 21.74276351928711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №941: loss = 21.730487823486328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №942: loss = 21.71821403503418, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №943: loss = 21.705957412719727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №944: loss = 21.693702697753906, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №945: loss = 21.681453704833984, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №946: loss = 21.669219970703125, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №947: loss = 21.656986236572266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №948: loss = 21.644765853881836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №949: loss = 21.632553100585938, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №950: loss = 21.620349884033203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №951: loss = 21.608152389526367, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №952: loss = 21.595966339111328, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №953: loss = 21.583786010742188, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №954: loss = 21.571619033813477, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №955: loss = 21.559452056884766, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №956: loss = 21.54730224609375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №957: loss = 21.535152435302734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №958: loss = 21.523012161254883, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №959: loss = 21.510887145996094, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №960: loss = 21.498760223388672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №961: loss = 21.48664665222168, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №962: loss = 21.47454071044922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №963: loss = 21.46245002746582, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №964: loss = 21.450359344482422, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №965: loss = 21.438274383544922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №966: loss = 21.426204681396484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №967: loss = 21.414142608642578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №968: loss = 21.402088165283203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №969: loss = 21.390039443969727, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №970: loss = 21.377994537353516, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №971: loss = 21.365962982177734, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №972: loss = 21.35393524169922, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №973: loss = 21.341930389404297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №974: loss = 21.329914093017578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №975: loss = 21.317920684814453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №976: loss = 21.305931091308594, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №977: loss = 21.293949127197266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №978: loss = 21.28197479248047, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №979: loss = 21.270000457763672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №980: loss = 21.258045196533203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №981: loss = 21.24609375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №982: loss = 21.234155654907227, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №983: loss = 21.222219467163086, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №984: loss = 21.210296630859375, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №985: loss = 21.198375701904297, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №986: loss = 21.186464309692383, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №987: loss = 21.174564361572266, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №988: loss = 21.162668228149414, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №989: loss = 21.15078353881836, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №990: loss = 21.138904571533203, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №991: loss = 21.12704086303711, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №992: loss = 21.115169525146484, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №993: loss = 21.10331916809082, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №994: loss = 21.091476440429688, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №995: loss = 21.079639434814453, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №996: loss = 21.067811965942383, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №997: loss = 21.055988311767578, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №998: loss = 21.044170379638672, weights = tensor([5.6908, 3.9884], requires_grad=True)\n",
      "step №999: loss = 21.03236961364746, weights = tensor([5.6908, 3.9884], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#it's time to replace prediction calculations too\n",
    "x = np.arange(10)\n",
    "y = x*5 + 10 + np.random.randint(-5, 5, 10)\n",
    "x = torch.tensor([[item] for item in x], dtype = torch.float32)\n",
    "y = torch.tensor([[item] for item in y], dtype = torch.float32)\n",
    "model = nn.Linear(1, 1)\n",
    "print(model.parameters())\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = model(x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c93033fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Now absolutely verything is calculated by Pytorch. Slope = 5.690791130065918, intercept = 3.9884085655212402, loss = 21.03236961364746')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y])\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y_pred])\n",
    "plt.title(f'Now absolutely verything is calculated by Pytorch. Slope = {}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cf3c364",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ty/zhdd1dkj36j5r796hbm4035c0000gn/T/ipykernel_13661/1621924350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3cf3e529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6271],\n",
       "        [ 9.5790],\n",
       "        [15.5308],\n",
       "        [21.4826],\n",
       "        [27.4345],\n",
       "        [33.3863],\n",
       "        [39.3381],\n",
       "        [45.2899],\n",
       "        [51.2418],\n",
       "        [57.1936]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14b2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
