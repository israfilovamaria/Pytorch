{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a412bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9611c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "75cbea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float16)\n",
      "tensor([13., 10., 21., 26., 32., 36., 38., 49., 45., 53.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# creating some data to train Linear Regression \n",
    "x = torch.tensor(np.arange(10), dtype = torch.float16)\n",
    "y = x*5 + 10 + torch.randint(low = -5, high = 5, size = (10,), dtype = torch.float16)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2f397c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "114c029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# initializing weights\n",
    "# we have only one feature, so we need to find two coefficients: slope and intercept \n",
    "# let's initialize them as zeros \n",
    "\n",
    "w = torch.zeros(2, dtype = torch.float16, )\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5a72d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def predict(w, x):\n",
    "    y_pred = w[0]*x + w[1]\n",
    "    return y_pred \n",
    "\n",
    "def mseerror(y, y_pred):\n",
    "    loss_val = ((y_pred - y)**2).mean()\n",
    "    return loss_val \n",
    "\n",
    "# derivative from mse with respect to beta1: 2x*(y - y_pred)\n",
    "def beta1gradient(x, y, y_pred):\n",
    "    direction = np.dot(2*x, y_pred - y).mean()\n",
    "    return direction \n",
    "# derivative from mse with respect to beta0: 2x*(y - y_pred)\n",
    "def beta0gradient(x, y, y_pred):\n",
    "    direction = np.dot(2, y_pred - y).mean()\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d15b1d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1238., dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# let's check our predictions before training \n",
    "y_pred = predict(w, x)\n",
    "error = mseerror(y, y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fde8a22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8da7043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1238.0, weights = tensor([2.2148, 0.0388], dtype=torch.float16)\n",
      "step №1: loss = 559.0, weights = tensor([3.6699, 0.0655], dtype=torch.float16)\n",
      "step №2: loss = 265.5, weights = tensor([4.6250, 0.0844], dtype=torch.float16)\n",
      "step №3: loss = 138.625, weights = tensor([5.2539, 0.0980], dtype=torch.float16)\n",
      "step №4: loss = 83.75, weights = tensor([5.6680, 0.1083], dtype=torch.float16)\n",
      "step №5: loss = 60.0625, weights = tensor([5.9414, 0.1163], dtype=torch.float16)\n",
      "step №6: loss = 49.71875, weights = tensor([6.1172, 0.1229], dtype=torch.float16)\n",
      "step №7: loss = 45.34375, weights = tensor([6.2344, 0.1284], dtype=torch.float16)\n",
      "step №8: loss = 43.46875, weights = tensor([6.3125, 0.1334], dtype=torch.float16)\n",
      "step №9: loss = 42.53125, weights = tensor([6.3633, 0.1379], dtype=torch.float16)\n",
      "step №10: loss = 42.1875, weights = tensor([6.3945, 0.1422], dtype=torch.float16)\n",
      "step №11: loss = 42.0625, weights = tensor([6.4141, 0.1462], dtype=torch.float16)\n",
      "step №12: loss = 41.9375, weights = tensor([6.4258, 0.1501], dtype=torch.float16)\n",
      "step №13: loss = 41.90625, weights = tensor([6.4336, 0.1541], dtype=torch.float16)\n",
      "step №14: loss = 41.875, weights = tensor([6.4414, 0.1578], dtype=torch.float16)\n",
      "step №15: loss = 41.8125, weights = tensor([6.4453, 0.1616], dtype=torch.float16)\n",
      "step №16: loss = 41.78125, weights = tensor([6.4492, 0.1654], dtype=torch.float16)\n",
      "step №17: loss = 41.71875, weights = tensor([6.4492, 0.1692], dtype=torch.float16)\n",
      "step №18: loss = 41.6875, weights = tensor([6.4492, 0.1730], dtype=torch.float16)\n",
      "step №19: loss = 41.71875, weights = tensor([6.4492, 0.1766], dtype=torch.float16)\n",
      "step №20: loss = 41.6875, weights = tensor([6.4492, 0.1803], dtype=torch.float16)\n",
      "step №21: loss = 41.65625, weights = tensor([6.4492, 0.1840], dtype=torch.float16)\n",
      "step №22: loss = 41.625, weights = tensor([6.4492, 0.1876], dtype=torch.float16)\n",
      "step №23: loss = 41.625, weights = tensor([6.4492, 0.1913], dtype=torch.float16)\n",
      "step №24: loss = 41.59375, weights = tensor([6.4492, 0.1949], dtype=torch.float16)\n",
      "step №25: loss = 41.5625, weights = tensor([6.4492, 0.1986], dtype=torch.float16)\n",
      "step №26: loss = 41.5625, weights = tensor([6.4492, 0.2023], dtype=torch.float16)\n",
      "step №27: loss = 41.5, weights = tensor([6.4492, 0.2059], dtype=torch.float16)\n",
      "step №28: loss = 41.53125, weights = tensor([6.4453, 0.2096], dtype=torch.float16)\n",
      "step №29: loss = 41.5, weights = tensor([6.4453, 0.2133], dtype=torch.float16)\n",
      "step №30: loss = 41.4375, weights = tensor([6.4453, 0.2169], dtype=torch.float16)\n",
      "step №31: loss = 41.4375, weights = tensor([6.4453, 0.2206], dtype=torch.float16)\n",
      "step №32: loss = 41.4375, weights = tensor([6.4453, 0.2242], dtype=torch.float16)\n",
      "step №33: loss = 41.40625, weights = tensor([6.4453, 0.2279], dtype=torch.float16)\n",
      "step №34: loss = 41.34375, weights = tensor([6.4453, 0.2316], dtype=torch.float16)\n",
      "step №35: loss = 41.3125, weights = tensor([6.4453, 0.2352], dtype=torch.float16)\n",
      "step №36: loss = 41.34375, weights = tensor([6.4414, 0.2389], dtype=torch.float16)\n",
      "step №37: loss = 41.3125, weights = tensor([6.4414, 0.2426], dtype=torch.float16)\n",
      "step №38: loss = 41.28125, weights = tensor([6.4414, 0.2462], dtype=torch.float16)\n",
      "step №39: loss = 41.21875, weights = tensor([6.4414, 0.2499], dtype=torch.float16)\n",
      "step №40: loss = 41.21875, weights = tensor([6.4414, 0.2537], dtype=torch.float16)\n",
      "step №41: loss = 41.21875, weights = tensor([6.4414, 0.2573], dtype=torch.float16)\n",
      "step №42: loss = 41.1875, weights = tensor([6.4414, 0.2610], dtype=torch.float16)\n",
      "step №43: loss = 41.15625, weights = tensor([6.4414, 0.2646], dtype=torch.float16)\n",
      "step №44: loss = 41.125, weights = tensor([6.4414, 0.2683], dtype=torch.float16)\n",
      "step №45: loss = 41.15625, weights = tensor([6.4375, 0.2720], dtype=torch.float16)\n",
      "step №46: loss = 41.15625, weights = tensor([6.4375, 0.2756], dtype=torch.float16)\n",
      "step №47: loss = 41.09375, weights = tensor([6.4375, 0.2793], dtype=torch.float16)\n",
      "step №48: loss = 41.0625, weights = tensor([6.4375, 0.2830], dtype=torch.float16)\n",
      "step №49: loss = 41.0625, weights = tensor([6.4375, 0.2866], dtype=torch.float16)\n",
      "step №50: loss = 41.03125, weights = tensor([6.4375, 0.2903], dtype=torch.float16)\n",
      "step №51: loss = 41.0, weights = tensor([6.4375, 0.2939], dtype=torch.float16)\n",
      "step №52: loss = 40.9375, weights = tensor([6.4375, 0.2976], dtype=torch.float16)\n",
      "step №53: loss = 40.96875, weights = tensor([6.4336, 0.3013], dtype=torch.float16)\n",
      "step №54: loss = 40.9375, weights = tensor([6.4336, 0.3049], dtype=torch.float16)\n",
      "step №55: loss = 40.9375, weights = tensor([6.4336, 0.3086], dtype=torch.float16)\n",
      "step №56: loss = 40.90625, weights = tensor([6.4336, 0.3123], dtype=torch.float16)\n",
      "step №57: loss = 40.90625, weights = tensor([6.4336, 0.3159], dtype=torch.float16)\n",
      "step №58: loss = 40.90625, weights = tensor([6.4336, 0.3196], dtype=torch.float16)\n",
      "step №59: loss = 40.84375, weights = tensor([6.4336, 0.3232], dtype=torch.float16)\n",
      "step №60: loss = 40.8125, weights = tensor([6.4297, 0.3269], dtype=torch.float16)\n",
      "step №61: loss = 40.78125, weights = tensor([6.4297, 0.3306], dtype=torch.float16)\n",
      "step №62: loss = 40.8125, weights = tensor([6.4258, 0.3342], dtype=torch.float16)\n",
      "step №63: loss = 40.78125, weights = tensor([6.4258, 0.3379], dtype=torch.float16)\n",
      "step №64: loss = 40.71875, weights = tensor([6.4258, 0.3416], dtype=torch.float16)\n",
      "step №65: loss = 40.6875, weights = tensor([6.4258, 0.3452], dtype=torch.float16)\n",
      "step №66: loss = 40.6875, weights = tensor([6.4258, 0.3489], dtype=torch.float16)\n",
      "step №67: loss = 40.65625, weights = tensor([6.4258, 0.3525], dtype=torch.float16)\n",
      "step №68: loss = 40.625, weights = tensor([6.4258, 0.3562], dtype=torch.float16)\n",
      "step №69: loss = 40.5625, weights = tensor([6.4258, 0.3599], dtype=torch.float16)\n",
      "step №70: loss = 40.625, weights = tensor([6.4219, 0.3635], dtype=torch.float16)\n",
      "step №71: loss = 40.5625, weights = tensor([6.4219, 0.3672], dtype=torch.float16)\n",
      "step №72: loss = 40.5625, weights = tensor([6.4219, 0.3708], dtype=torch.float16)\n",
      "step №73: loss = 40.5625, weights = tensor([6.4219, 0.3745], dtype=torch.float16)\n",
      "step №74: loss = 40.5, weights = tensor([6.4219, 0.3782], dtype=torch.float16)\n",
      "step №75: loss = 40.5, weights = tensor([6.4219, 0.3818], dtype=torch.float16)\n",
      "step №76: loss = 40.46875, weights = tensor([6.4219, 0.3855], dtype=torch.float16)\n",
      "step №77: loss = 40.4375, weights = tensor([6.4219, 0.3892], dtype=torch.float16)\n",
      "step №78: loss = 40.40625, weights = tensor([6.4219, 0.3928], dtype=torch.float16)\n",
      "step №79: loss = 40.4375, weights = tensor([6.4180, 0.3965], dtype=torch.float16)\n",
      "step №80: loss = 40.34375, weights = tensor([6.4180, 0.4001], dtype=torch.float16)\n",
      "step №81: loss = 40.3125, weights = tensor([6.4180, 0.4038], dtype=torch.float16)\n",
      "step №82: loss = 40.28125, weights = tensor([6.4180, 0.4075], dtype=torch.float16)\n",
      "step №83: loss = 40.28125, weights = tensor([6.4180, 0.4111], dtype=torch.float16)\n",
      "step №84: loss = 40.25, weights = tensor([6.4180, 0.4148], dtype=torch.float16)\n",
      "step №85: loss = 40.1875, weights = tensor([6.4180, 0.4185], dtype=torch.float16)\n",
      "step №86: loss = 40.1875, weights = tensor([6.4180, 0.4221], dtype=torch.float16)\n",
      "step №87: loss = 40.1875, weights = tensor([6.4141, 0.4258], dtype=torch.float16)\n",
      "step №88: loss = 40.1875, weights = tensor([6.4141, 0.4294], dtype=torch.float16)\n",
      "step №89: loss = 40.1875, weights = tensor([6.4141, 0.4331], dtype=torch.float16)\n",
      "step №90: loss = 40.125, weights = tensor([6.4102, 0.4368], dtype=torch.float16)\n",
      "step №91: loss = 40.09375, weights = tensor([6.4102, 0.4404], dtype=torch.float16)\n",
      "step №92: loss = 40.09375, weights = tensor([6.4102, 0.4441], dtype=torch.float16)\n",
      "step №93: loss = 40.0625, weights = tensor([6.4102, 0.4478], dtype=torch.float16)\n",
      "step №94: loss = 40.0, weights = tensor([6.4102, 0.4514], dtype=torch.float16)\n",
      "step №95: loss = 39.96875, weights = tensor([6.4102, 0.4551], dtype=torch.float16)\n",
      "step №96: loss = 40.0, weights = tensor([6.4062, 0.4587], dtype=torch.float16)\n",
      "step №97: loss = 40.0, weights = tensor([6.4062, 0.4624], dtype=torch.float16)\n",
      "step №98: loss = 39.9375, weights = tensor([6.4062, 0.4661], dtype=torch.float16)\n",
      "step №99: loss = 39.9375, weights = tensor([6.4062, 0.4697], dtype=torch.float16)\n",
      "step №100: loss = 39.9375, weights = tensor([6.4062, 0.4734], dtype=torch.float16)\n",
      "step №101: loss = 39.90625, weights = tensor([6.4062, 0.4771], dtype=torch.float16)\n",
      "step №102: loss = 39.84375, weights = tensor([6.4062, 0.4807], dtype=torch.float16)\n",
      "step №103: loss = 39.8125, weights = tensor([6.4062, 0.4844], dtype=torch.float16)\n",
      "step №104: loss = 39.84375, weights = tensor([6.4062, 0.4880], dtype=torch.float16)\n",
      "step №105: loss = 39.84375, weights = tensor([6.4023, 0.4917], dtype=torch.float16)\n",
      "step №106: loss = 39.8125, weights = tensor([6.4023, 0.4954], dtype=torch.float16)\n",
      "step №107: loss = 39.8125, weights = tensor([6.4023, 0.4990], dtype=torch.float16)\n",
      "step №108: loss = 39.75, weights = tensor([6.4023, 0.5024], dtype=torch.float16)\n",
      "step №109: loss = 39.75, weights = tensor([6.4023, 0.5059], dtype=torch.float16)\n",
      "step №110: loss = 39.6875, weights = tensor([6.4023, 0.5093], dtype=torch.float16)\n",
      "step №111: loss = 39.6875, weights = tensor([6.4023, 0.5127], dtype=torch.float16)\n",
      "step №112: loss = 39.65625, weights = tensor([6.4023, 0.5161], dtype=torch.float16)\n",
      "step №113: loss = 39.6875, weights = tensor([6.3984, 0.5195], dtype=torch.float16)\n",
      "step №114: loss = 39.6875, weights = tensor([6.3984, 0.5229], dtype=torch.float16)\n",
      "step №115: loss = 39.625, weights = tensor([6.3984, 0.5264], dtype=torch.float16)\n",
      "step №116: loss = 39.59375, weights = tensor([6.3984, 0.5298], dtype=torch.float16)\n",
      "step №117: loss = 39.5625, weights = tensor([6.3984, 0.5332], dtype=torch.float16)\n",
      "step №118: loss = 39.5625, weights = tensor([6.3984, 0.5366], dtype=torch.float16)\n",
      "step №119: loss = 39.53125, weights = tensor([6.3984, 0.5400], dtype=torch.float16)\n",
      "step №120: loss = 39.46875, weights = tensor([6.3984, 0.5435], dtype=torch.float16)\n",
      "step №121: loss = 39.4375, weights = tensor([6.3984, 0.5469], dtype=torch.float16)\n",
      "step №122: loss = 39.46875, weights = tensor([6.3945, 0.5503], dtype=torch.float16)\n",
      "step №123: loss = 39.5, weights = tensor([6.3906, 0.5537], dtype=torch.float16)\n",
      "step №124: loss = 39.40625, weights = tensor([6.3906, 0.5571], dtype=torch.float16)\n",
      "step №125: loss = 39.375, weights = tensor([6.3906, 0.5605], dtype=torch.float16)\n",
      "step №126: loss = 39.34375, weights = tensor([6.3906, 0.5640], dtype=torch.float16)\n",
      "step №127: loss = 39.3125, weights = tensor([6.3906, 0.5674], dtype=torch.float16)\n",
      "step №128: loss = 39.28125, weights = tensor([6.3906, 0.5708], dtype=torch.float16)\n",
      "step №129: loss = 39.25, weights = tensor([6.3906, 0.5742], dtype=torch.float16)\n",
      "step №130: loss = 39.1875, weights = tensor([6.3906, 0.5776], dtype=torch.float16)\n",
      "step №131: loss = 39.1875, weights = tensor([6.3906, 0.5811], dtype=torch.float16)\n",
      "step №132: loss = 39.25, weights = tensor([6.3867, 0.5845], dtype=torch.float16)\n",
      "step №133: loss = 39.21875, weights = tensor([6.3867, 0.5879], dtype=torch.float16)\n",
      "step №134: loss = 39.1875, weights = tensor([6.3867, 0.5913], dtype=torch.float16)\n",
      "step №135: loss = 39.15625, weights = tensor([6.3867, 0.5947], dtype=torch.float16)\n",
      "step №136: loss = 39.15625, weights = tensor([6.3867, 0.5981], dtype=torch.float16)\n",
      "step №137: loss = 39.125, weights = tensor([6.3867, 0.6016], dtype=torch.float16)\n",
      "step №138: loss = 39.09375, weights = tensor([6.3867, 0.6050], dtype=torch.float16)\n",
      "step №139: loss = 39.0625, weights = tensor([6.3867, 0.6084], dtype=torch.float16)\n",
      "step №140: loss = 39.0625, weights = tensor([6.3867, 0.6118], dtype=torch.float16)\n",
      "step №141: loss = 39.0625, weights = tensor([6.3828, 0.6152], dtype=torch.float16)\n",
      "step №142: loss = 39.0625, weights = tensor([6.3828, 0.6187], dtype=torch.float16)\n",
      "step №143: loss = 39.0, weights = tensor([6.3828, 0.6221], dtype=torch.float16)\n",
      "step №144: loss = 38.96875, weights = tensor([6.3828, 0.6255], dtype=torch.float16)\n",
      "step №145: loss = 38.96875, weights = tensor([6.3828, 0.6289], dtype=torch.float16)\n",
      "step №146: loss = 38.96875, weights = tensor([6.3828, 0.6323], dtype=torch.float16)\n",
      "step №147: loss = 38.9375, weights = tensor([6.3828, 0.6357], dtype=torch.float16)\n",
      "step №148: loss = 38.90625, weights = tensor([6.3828, 0.6392], dtype=torch.float16)\n",
      "step №149: loss = 38.84375, weights = tensor([6.3828, 0.6426], dtype=torch.float16)\n",
      "step №150: loss = 38.90625, weights = tensor([6.3789, 0.6460], dtype=torch.float16)\n",
      "step №151: loss = 38.875, weights = tensor([6.3789, 0.6494], dtype=torch.float16)\n",
      "step №152: loss = 38.8125, weights = tensor([6.3789, 0.6528], dtype=torch.float16)\n",
      "step №153: loss = 38.8125, weights = tensor([6.3789, 0.6562], dtype=torch.float16)\n",
      "step №154: loss = 38.78125, weights = tensor([6.3750, 0.6597], dtype=torch.float16)\n",
      "step №155: loss = 38.8125, weights = tensor([6.3750, 0.6631], dtype=torch.float16)\n",
      "step №156: loss = 38.75, weights = tensor([6.3750, 0.6665], dtype=torch.float16)\n",
      "step №157: loss = 38.71875, weights = tensor([6.3750, 0.6699], dtype=torch.float16)\n",
      "step №158: loss = 38.6875, weights = tensor([6.3750, 0.6733], dtype=torch.float16)\n",
      "step №159: loss = 38.71875, weights = tensor([6.3711, 0.6768], dtype=torch.float16)\n",
      "step №160: loss = 38.6875, weights = tensor([6.3711, 0.6802], dtype=torch.float16)\n",
      "step №161: loss = 38.6875, weights = tensor([6.3711, 0.6836], dtype=torch.float16)\n",
      "step №162: loss = 38.65625, weights = tensor([6.3711, 0.6870], dtype=torch.float16)\n",
      "step №163: loss = 38.625, weights = tensor([6.3711, 0.6904], dtype=torch.float16)\n",
      "step №164: loss = 38.59375, weights = tensor([6.3711, 0.6938], dtype=torch.float16)\n",
      "step №165: loss = 38.5625, weights = tensor([6.3711, 0.6973], dtype=torch.float16)\n",
      "step №166: loss = 38.53125, weights = tensor([6.3711, 0.7007], dtype=torch.float16)\n",
      "step №167: loss = 38.5, weights = tensor([6.3711, 0.7041], dtype=torch.float16)\n",
      "step №168: loss = 38.5625, weights = tensor([6.3672, 0.7075], dtype=torch.float16)\n",
      "step №169: loss = 38.53125, weights = tensor([6.3672, 0.7109], dtype=torch.float16)\n",
      "step №170: loss = 38.46875, weights = tensor([6.3672, 0.7144], dtype=torch.float16)\n",
      "step №171: loss = 38.46875, weights = tensor([6.3672, 0.7178], dtype=torch.float16)\n",
      "step №172: loss = 38.4375, weights = tensor([6.3672, 0.7212], dtype=torch.float16)\n",
      "step №173: loss = 38.4375, weights = tensor([6.3672, 0.7246], dtype=torch.float16)\n",
      "step №174: loss = 38.40625, weights = tensor([6.3672, 0.7280], dtype=torch.float16)\n",
      "step №175: loss = 38.375, weights = tensor([6.3672, 0.7314], dtype=torch.float16)\n",
      "step №176: loss = 38.34375, weights = tensor([6.3672, 0.7349], dtype=torch.float16)\n",
      "step №177: loss = 38.375, weights = tensor([6.3633, 0.7383], dtype=torch.float16)\n",
      "step №178: loss = 38.375, weights = tensor([6.3633, 0.7417], dtype=torch.float16)\n",
      "step №179: loss = 38.3125, weights = tensor([6.3633, 0.7451], dtype=torch.float16)\n",
      "step №180: loss = 38.3125, weights = tensor([6.3633, 0.7485], dtype=torch.float16)\n",
      "step №181: loss = 38.25, weights = tensor([6.3633, 0.7520], dtype=torch.float16)\n",
      "step №182: loss = 38.25, weights = tensor([6.3633, 0.7554], dtype=torch.float16)\n",
      "step №183: loss = 38.21875, weights = tensor([6.3633, 0.7588], dtype=torch.float16)\n",
      "step №184: loss = 38.1875, weights = tensor([6.3633, 0.7622], dtype=torch.float16)\n",
      "step №185: loss = 38.15625, weights = tensor([6.3633, 0.7656], dtype=torch.float16)\n",
      "step №186: loss = 38.1875, weights = tensor([6.3594, 0.7690], dtype=torch.float16)\n",
      "step №187: loss = 38.1875, weights = tensor([6.3594, 0.7725], dtype=torch.float16)\n",
      "step №188: loss = 38.1875, weights = tensor([6.3594, 0.7759], dtype=torch.float16)\n",
      "step №189: loss = 38.125, weights = tensor([6.3594, 0.7793], dtype=torch.float16)\n",
      "step №190: loss = 38.09375, weights = tensor([6.3594, 0.7827], dtype=torch.float16)\n",
      "step №191: loss = 38.0625, weights = tensor([6.3555, 0.7861], dtype=torch.float16)\n",
      "step №192: loss = 38.0, weights = tensor([6.3555, 0.7896], dtype=torch.float16)\n",
      "step №193: loss = 37.96875, weights = tensor([6.3555, 0.7930], dtype=torch.float16)\n",
      "step №194: loss = 37.9375, weights = tensor([6.3555, 0.7964], dtype=torch.float16)\n",
      "step №195: loss = 37.9375, weights = tensor([6.3555, 0.7998], dtype=torch.float16)\n",
      "step №196: loss = 37.96875, weights = tensor([6.3516, 0.8032], dtype=torch.float16)\n",
      "step №197: loss = 37.9375, weights = tensor([6.3516, 0.8066], dtype=torch.float16)\n",
      "step №198: loss = 37.90625, weights = tensor([6.3516, 0.8101], dtype=torch.float16)\n",
      "step №199: loss = 37.84375, weights = tensor([6.3516, 0.8135], dtype=torch.float16)\n",
      "step №200: loss = 37.84375, weights = tensor([6.3516, 0.8169], dtype=torch.float16)\n",
      "step №201: loss = 37.8125, weights = tensor([6.3516, 0.8203], dtype=torch.float16)\n",
      "step №202: loss = 37.8125, weights = tensor([6.3516, 0.8237], dtype=torch.float16)\n",
      "step №203: loss = 37.8125, weights = tensor([6.3516, 0.8271], dtype=torch.float16)\n",
      "step №204: loss = 37.75, weights = tensor([6.3516, 0.8306], dtype=torch.float16)\n",
      "step №205: loss = 37.78125, weights = tensor([6.3477, 0.8340], dtype=torch.float16)\n",
      "step №206: loss = 37.78125, weights = tensor([6.3477, 0.8374], dtype=torch.float16)\n",
      "step №207: loss = 37.71875, weights = tensor([6.3477, 0.8408], dtype=torch.float16)\n",
      "step №208: loss = 37.6875, weights = tensor([6.3477, 0.8442], dtype=torch.float16)\n",
      "step №209: loss = 37.6875, weights = tensor([6.3477, 0.8477], dtype=torch.float16)\n",
      "step №210: loss = 37.65625, weights = tensor([6.3477, 0.8511], dtype=torch.float16)\n",
      "step №211: loss = 37.65625, weights = tensor([6.3477, 0.8545], dtype=torch.float16)\n",
      "step №212: loss = 37.59375, weights = tensor([6.3477, 0.8579], dtype=torch.float16)\n",
      "step №213: loss = 37.5625, weights = tensor([6.3477, 0.8613], dtype=torch.float16)\n",
      "step №214: loss = 37.625, weights = tensor([6.3438, 0.8647], dtype=torch.float16)\n",
      "step №215: loss = 37.59375, weights = tensor([6.3438, 0.8682], dtype=torch.float16)\n",
      "step №216: loss = 37.5625, weights = tensor([6.3438, 0.8716], dtype=torch.float16)\n",
      "step №217: loss = 37.53125, weights = tensor([6.3438, 0.8750], dtype=torch.float16)\n",
      "step №218: loss = 37.5, weights = tensor([6.3438, 0.8784], dtype=torch.float16)\n",
      "step №219: loss = 37.5, weights = tensor([6.3438, 0.8818], dtype=torch.float16)\n",
      "step №220: loss = 37.46875, weights = tensor([6.3438, 0.8853], dtype=torch.float16)\n",
      "step №221: loss = 37.4375, weights = tensor([6.3438, 0.8887], dtype=torch.float16)\n",
      "step №222: loss = 37.40625, weights = tensor([6.3438, 0.8921], dtype=torch.float16)\n",
      "step №223: loss = 37.4375, weights = tensor([6.3398, 0.8955], dtype=torch.float16)\n",
      "step №224: loss = 37.4375, weights = tensor([6.3398, 0.8989], dtype=torch.float16)\n",
      "step №225: loss = 37.40625, weights = tensor([6.3398, 0.9023], dtype=torch.float16)\n",
      "step №226: loss = 37.375, weights = tensor([6.3398, 0.9058], dtype=torch.float16)\n",
      "step №227: loss = 37.34375, weights = tensor([6.3398, 0.9092], dtype=torch.float16)\n",
      "step №228: loss = 37.3125, weights = tensor([6.3359, 0.9126], dtype=torch.float16)\n",
      "step №229: loss = 37.3125, weights = tensor([6.3359, 0.9160], dtype=torch.float16)\n",
      "step №230: loss = 37.28125, weights = tensor([6.3359, 0.9194], dtype=torch.float16)\n",
      "step №231: loss = 37.25, weights = tensor([6.3359, 0.9229], dtype=torch.float16)\n",
      "step №232: loss = 37.28125, weights = tensor([6.3320, 0.9263], dtype=torch.float16)\n",
      "step №233: loss = 37.25, weights = tensor([6.3320, 0.9297], dtype=torch.float16)\n",
      "step №234: loss = 37.21875, weights = tensor([6.3320, 0.9331], dtype=torch.float16)\n",
      "step №235: loss = 37.1875, weights = tensor([6.3320, 0.9365], dtype=torch.float16)\n",
      "step №236: loss = 37.1875, weights = tensor([6.3320, 0.9399], dtype=torch.float16)\n",
      "step №237: loss = 37.1875, weights = tensor([6.3320, 0.9434], dtype=torch.float16)\n",
      "step №238: loss = 37.15625, weights = tensor([6.3320, 0.9468], dtype=torch.float16)\n",
      "step №239: loss = 37.09375, weights = tensor([6.3320, 0.9502], dtype=torch.float16)\n",
      "step №240: loss = 37.0625, weights = tensor([6.3320, 0.9536], dtype=torch.float16)\n",
      "step №241: loss = 37.09375, weights = tensor([6.3281, 0.9570], dtype=torch.float16)\n",
      "step №242: loss = 37.0625, weights = tensor([6.3281, 0.9604], dtype=torch.float16)\n",
      "step №243: loss = 37.03125, weights = tensor([6.3281, 0.9639], dtype=torch.float16)\n",
      "step №244: loss = 37.0, weights = tensor([6.3281, 0.9673], dtype=torch.float16)\n",
      "step №245: loss = 36.9375, weights = tensor([6.3281, 0.9707], dtype=torch.float16)\n",
      "step №246: loss = 36.9375, weights = tensor([6.3281, 0.9741], dtype=torch.float16)\n",
      "step №247: loss = 36.90625, weights = tensor([6.3281, 0.9775], dtype=torch.float16)\n",
      "step №248: loss = 36.875, weights = tensor([6.3281, 0.9810], dtype=torch.float16)\n",
      "step №249: loss = 36.8125, weights = tensor([6.3281, 0.9844], dtype=torch.float16)\n",
      "step №250: loss = 36.875, weights = tensor([6.3242, 0.9878], dtype=torch.float16)\n",
      "step №251: loss = 36.875, weights = tensor([6.3242, 0.9912], dtype=torch.float16)\n",
      "step №252: loss = 36.84375, weights = tensor([6.3242, 0.9946], dtype=torch.float16)\n",
      "step №253: loss = 36.8125, weights = tensor([6.3242, 0.9980], dtype=torch.float16)\n",
      "step №254: loss = 36.78125, weights = tensor([6.3242, 1.0010], dtype=torch.float16)\n",
      "step №255: loss = 36.78125, weights = tensor([6.3242, 1.0039], dtype=torch.float16)\n",
      "step №256: loss = 36.78125, weights = tensor([6.3242, 1.0068], dtype=torch.float16)\n",
      "step №257: loss = 36.75, weights = tensor([6.3242, 1.0098], dtype=torch.float16)\n",
      "step №258: loss = 36.6875, weights = tensor([6.3242, 1.0127], dtype=torch.float16)\n",
      "step №259: loss = 36.6875, weights = tensor([6.3242, 1.0156], dtype=torch.float16)\n",
      "step №260: loss = 36.71875, weights = tensor([6.3203, 1.0186], dtype=torch.float16)\n",
      "step №261: loss = 36.6875, weights = tensor([6.3203, 1.0215], dtype=torch.float16)\n",
      "step №262: loss = 36.6875, weights = tensor([6.3203, 1.0244], dtype=torch.float16)\n",
      "step №263: loss = 36.65625, weights = tensor([6.3164, 1.0273], dtype=torch.float16)\n",
      "step №264: loss = 36.625, weights = tensor([6.3164, 1.0303], dtype=torch.float16)\n",
      "step №265: loss = 36.625, weights = tensor([6.3164, 1.0332], dtype=torch.float16)\n",
      "step №266: loss = 36.59375, weights = tensor([6.3164, 1.0361], dtype=torch.float16)\n",
      "step №267: loss = 36.5625, weights = tensor([6.3164, 1.0391], dtype=torch.float16)\n",
      "step №268: loss = 36.53125, weights = tensor([6.3164, 1.0420], dtype=torch.float16)\n",
      "step №269: loss = 36.53125, weights = tensor([6.3164, 1.0449], dtype=torch.float16)\n",
      "step №270: loss = 36.5, weights = tensor([6.3164, 1.0479], dtype=torch.float16)\n",
      "step №271: loss = 36.5625, weights = tensor([6.3125, 1.0508], dtype=torch.float16)\n",
      "step №272: loss = 36.5625, weights = tensor([6.3125, 1.0537], dtype=torch.float16)\n",
      "step №273: loss = 36.53125, weights = tensor([6.3125, 1.0566], dtype=torch.float16)\n",
      "step №274: loss = 36.46875, weights = tensor([6.3125, 1.0596], dtype=torch.float16)\n",
      "step №275: loss = 36.4375, weights = tensor([6.3125, 1.0625], dtype=torch.float16)\n",
      "step №276: loss = 36.4375, weights = tensor([6.3125, 1.0654], dtype=torch.float16)\n",
      "step №277: loss = 36.4375, weights = tensor([6.3125, 1.0684], dtype=torch.float16)\n",
      "step №278: loss = 36.40625, weights = tensor([6.3125, 1.0713], dtype=torch.float16)\n",
      "step №279: loss = 36.375, weights = tensor([6.3125, 1.0742], dtype=torch.float16)\n",
      "step №280: loss = 36.3125, weights = tensor([6.3125, 1.0771], dtype=torch.float16)\n",
      "step №281: loss = 36.3125, weights = tensor([6.3125, 1.0801], dtype=torch.float16)\n",
      "step №282: loss = 36.375, weights = tensor([6.3086, 1.0830], dtype=torch.float16)\n",
      "step №283: loss = 36.34375, weights = tensor([6.3086, 1.0859], dtype=torch.float16)\n",
      "step №284: loss = 36.34375, weights = tensor([6.3086, 1.0889], dtype=torch.float16)\n",
      "step №285: loss = 36.3125, weights = tensor([6.3086, 1.0918], dtype=torch.float16)\n",
      "step №286: loss = 36.28125, weights = tensor([6.3086, 1.0947], dtype=torch.float16)\n",
      "step №287: loss = 36.25, weights = tensor([6.3086, 1.0977], dtype=torch.float16)\n",
      "step №288: loss = 36.25, weights = tensor([6.3086, 1.1006], dtype=torch.float16)\n",
      "step №289: loss = 36.21875, weights = tensor([6.3086, 1.1035], dtype=torch.float16)\n",
      "step №290: loss = 36.1875, weights = tensor([6.3086, 1.1064], dtype=torch.float16)\n",
      "step №291: loss = 36.15625, weights = tensor([6.3086, 1.1094], dtype=torch.float16)\n",
      "step №292: loss = 36.15625, weights = tensor([6.3086, 1.1123], dtype=torch.float16)\n",
      "step №293: loss = 36.1875, weights = tensor([6.3047, 1.1152], dtype=torch.float16)\n",
      "step №294: loss = 36.1875, weights = tensor([6.3047, 1.1182], dtype=torch.float16)\n",
      "step №295: loss = 36.15625, weights = tensor([6.3047, 1.1211], dtype=torch.float16)\n",
      "step №296: loss = 36.09375, weights = tensor([6.3047, 1.1240], dtype=torch.float16)\n",
      "step №297: loss = 36.09375, weights = tensor([6.3047, 1.1270], dtype=torch.float16)\n",
      "step №298: loss = 36.09375, weights = tensor([6.3047, 1.1299], dtype=torch.float16)\n",
      "step №299: loss = 36.0625, weights = tensor([6.3047, 1.1328], dtype=torch.float16)\n",
      "step №300: loss = 36.0625, weights = tensor([6.3047, 1.1357], dtype=torch.float16)\n",
      "step №301: loss = 36.03125, weights = tensor([6.3047, 1.1387], dtype=torch.float16)\n",
      "step №302: loss = 36.0, weights = tensor([6.3047, 1.1416], dtype=torch.float16)\n",
      "step №303: loss = 36.03125, weights = tensor([6.3008, 1.1445], dtype=torch.float16)\n",
      "step №304: loss = 36.03125, weights = tensor([6.2969, 1.1475], dtype=torch.float16)\n",
      "step №305: loss = 36.03125, weights = tensor([6.2969, 1.1504], dtype=torch.float16)\n",
      "step №306: loss = 35.96875, weights = tensor([6.2969, 1.1533], dtype=torch.float16)\n",
      "step №307: loss = 35.9375, weights = tensor([6.2969, 1.1562], dtype=torch.float16)\n",
      "step №308: loss = 35.9375, weights = tensor([6.2969, 1.1592], dtype=torch.float16)\n",
      "step №309: loss = 35.9375, weights = tensor([6.2969, 1.1621], dtype=torch.float16)\n",
      "step №310: loss = 35.90625, weights = tensor([6.2969, 1.1650], dtype=torch.float16)\n",
      "step №311: loss = 35.84375, weights = tensor([6.2969, 1.1680], dtype=torch.float16)\n",
      "step №312: loss = 35.8125, weights = tensor([6.2969, 1.1709], dtype=torch.float16)\n",
      "step №313: loss = 35.8125, weights = tensor([6.2969, 1.1738], dtype=torch.float16)\n",
      "step №314: loss = 35.875, weights = tensor([6.2930, 1.1768], dtype=torch.float16)\n",
      "step №315: loss = 35.78125, weights = tensor([6.2930, 1.1797], dtype=torch.float16)\n",
      "step №316: loss = 35.78125, weights = tensor([6.2930, 1.1826], dtype=torch.float16)\n",
      "step №317: loss = 35.75, weights = tensor([6.2930, 1.1855], dtype=torch.float16)\n",
      "step №318: loss = 35.6875, weights = tensor([6.2930, 1.1885], dtype=torch.float16)\n",
      "step №319: loss = 35.6875, weights = tensor([6.2930, 1.1914], dtype=torch.float16)\n",
      "step №320: loss = 35.6875, weights = tensor([6.2930, 1.1943], dtype=torch.float16)\n",
      "step №321: loss = 35.6875, weights = tensor([6.2930, 1.1973], dtype=torch.float16)\n",
      "step №322: loss = 35.65625, weights = tensor([6.2930, 1.2002], dtype=torch.float16)\n",
      "step №323: loss = 35.59375, weights = tensor([6.2930, 1.2031], dtype=torch.float16)\n",
      "step №324: loss = 35.625, weights = tensor([6.2930, 1.2061], dtype=torch.float16)\n",
      "step №325: loss = 35.625, weights = tensor([6.2891, 1.2090], dtype=torch.float16)\n",
      "step №326: loss = 35.59375, weights = tensor([6.2891, 1.2119], dtype=torch.float16)\n",
      "step №327: loss = 35.5625, weights = tensor([6.2891, 1.2148], dtype=torch.float16)\n",
      "step №328: loss = 35.53125, weights = tensor([6.2891, 1.2178], dtype=torch.float16)\n",
      "step №329: loss = 35.53125, weights = tensor([6.2891, 1.2207], dtype=torch.float16)\n",
      "step №330: loss = 35.53125, weights = tensor([6.2891, 1.2236], dtype=torch.float16)\n",
      "step №331: loss = 35.5, weights = tensor([6.2891, 1.2266], dtype=torch.float16)\n",
      "step №332: loss = 35.5, weights = tensor([6.2891, 1.2295], dtype=torch.float16)\n",
      "step №333: loss = 35.46875, weights = tensor([6.2891, 1.2324], dtype=torch.float16)\n",
      "step №334: loss = 35.4375, weights = tensor([6.2891, 1.2354], dtype=torch.float16)\n",
      "step №335: loss = 35.46875, weights = tensor([6.2852, 1.2383], dtype=torch.float16)\n",
      "step №336: loss = 35.46875, weights = tensor([6.2852, 1.2412], dtype=torch.float16)\n",
      "step №337: loss = 35.4375, weights = tensor([6.2852, 1.2441], dtype=torch.float16)\n",
      "step №338: loss = 35.4375, weights = tensor([6.2852, 1.2471], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №339: loss = 35.375, weights = tensor([6.2852, 1.2500], dtype=torch.float16)\n",
      "step №340: loss = 35.34375, weights = tensor([6.2812, 1.2529], dtype=torch.float16)\n",
      "step №341: loss = 35.375, weights = tensor([6.2812, 1.2559], dtype=torch.float16)\n",
      "step №342: loss = 35.34375, weights = tensor([6.2812, 1.2588], dtype=torch.float16)\n",
      "step №343: loss = 35.3125, weights = tensor([6.2812, 1.2617], dtype=torch.float16)\n",
      "step №344: loss = 35.28125, weights = tensor([6.2812, 1.2646], dtype=torch.float16)\n",
      "step №345: loss = 35.28125, weights = tensor([6.2812, 1.2676], dtype=torch.float16)\n",
      "step №346: loss = 35.3125, weights = tensor([6.2773, 1.2705], dtype=torch.float16)\n",
      "step №347: loss = 35.3125, weights = tensor([6.2773, 1.2734], dtype=torch.float16)\n",
      "step №348: loss = 35.3125, weights = tensor([6.2773, 1.2764], dtype=torch.float16)\n",
      "step №349: loss = 35.25, weights = tensor([6.2773, 1.2793], dtype=torch.float16)\n",
      "step №350: loss = 35.21875, weights = tensor([6.2773, 1.2822], dtype=torch.float16)\n",
      "step №351: loss = 35.1875, weights = tensor([6.2773, 1.2852], dtype=torch.float16)\n",
      "step №352: loss = 35.1875, weights = tensor([6.2773, 1.2881], dtype=torch.float16)\n",
      "step №353: loss = 35.1875, weights = tensor([6.2773, 1.2910], dtype=torch.float16)\n",
      "step №354: loss = 35.125, weights = tensor([6.2773, 1.2939], dtype=torch.float16)\n",
      "step №355: loss = 35.09375, weights = tensor([6.2773, 1.2969], dtype=torch.float16)\n",
      "step №356: loss = 35.09375, weights = tensor([6.2773, 1.2998], dtype=torch.float16)\n",
      "step №357: loss = 35.15625, weights = tensor([6.2734, 1.3027], dtype=torch.float16)\n",
      "step №358: loss = 35.09375, weights = tensor([6.2734, 1.3057], dtype=torch.float16)\n",
      "step №359: loss = 35.0625, weights = tensor([6.2734, 1.3086], dtype=torch.float16)\n",
      "step №360: loss = 35.0625, weights = tensor([6.2734, 1.3115], dtype=torch.float16)\n",
      "step №361: loss = 35.0625, weights = tensor([6.2734, 1.3145], dtype=torch.float16)\n",
      "step №362: loss = 35.0625, weights = tensor([6.2734, 1.3174], dtype=torch.float16)\n",
      "step №363: loss = 35.0, weights = tensor([6.2734, 1.3203], dtype=torch.float16)\n",
      "step №364: loss = 35.0, weights = tensor([6.2734, 1.3232], dtype=torch.float16)\n",
      "step №365: loss = 34.96875, weights = tensor([6.2734, 1.3262], dtype=torch.float16)\n",
      "step №366: loss = 34.9375, weights = tensor([6.2734, 1.3291], dtype=torch.float16)\n",
      "step №367: loss = 34.96875, weights = tensor([6.2695, 1.3320], dtype=torch.float16)\n",
      "step №368: loss = 34.96875, weights = tensor([6.2695, 1.3350], dtype=torch.float16)\n",
      "step №369: loss = 34.9375, weights = tensor([6.2695, 1.3379], dtype=torch.float16)\n",
      "step №370: loss = 34.90625, weights = tensor([6.2695, 1.3408], dtype=torch.float16)\n",
      "step №371: loss = 34.875, weights = tensor([6.2695, 1.3438], dtype=torch.float16)\n",
      "step №372: loss = 34.875, weights = tensor([6.2695, 1.3467], dtype=torch.float16)\n",
      "step №373: loss = 34.875, weights = tensor([6.2695, 1.3496], dtype=torch.float16)\n",
      "step №374: loss = 34.84375, weights = tensor([6.2695, 1.3525], dtype=torch.float16)\n",
      "step №375: loss = 34.8125, weights = tensor([6.2695, 1.3555], dtype=torch.float16)\n",
      "step №376: loss = 34.8125, weights = tensor([6.2695, 1.3584], dtype=torch.float16)\n",
      "step №377: loss = 34.78125, weights = tensor([6.2695, 1.3613], dtype=torch.float16)\n",
      "step №378: loss = 34.8125, weights = tensor([6.2656, 1.3643], dtype=torch.float16)\n",
      "step №379: loss = 34.75, weights = tensor([6.2656, 1.3672], dtype=torch.float16)\n",
      "step №380: loss = 34.71875, weights = tensor([6.2656, 1.3701], dtype=torch.float16)\n",
      "step №381: loss = 34.6875, weights = tensor([6.2656, 1.3730], dtype=torch.float16)\n",
      "step №382: loss = 34.6875, weights = tensor([6.2656, 1.3760], dtype=torch.float16)\n",
      "step №383: loss = 34.65625, weights = tensor([6.2656, 1.3789], dtype=torch.float16)\n",
      "step №384: loss = 34.65625, weights = tensor([6.2617, 1.3818], dtype=torch.float16)\n",
      "step №385: loss = 34.65625, weights = tensor([6.2617, 1.3848], dtype=torch.float16)\n",
      "step №386: loss = 34.59375, weights = tensor([6.2617, 1.3877], dtype=torch.float16)\n",
      "step №387: loss = 34.5625, weights = tensor([6.2617, 1.3906], dtype=torch.float16)\n",
      "step №388: loss = 34.59375, weights = tensor([6.2617, 1.3936], dtype=torch.float16)\n",
      "step №389: loss = 34.59375, weights = tensor([6.2578, 1.3965], dtype=torch.float16)\n",
      "step №390: loss = 34.5625, weights = tensor([6.2578, 1.3994], dtype=torch.float16)\n",
      "step №391: loss = 34.53125, weights = tensor([6.2578, 1.4023], dtype=torch.float16)\n",
      "step №392: loss = 34.5, weights = tensor([6.2578, 1.4053], dtype=torch.float16)\n",
      "step №393: loss = 34.5, weights = tensor([6.2578, 1.4082], dtype=torch.float16)\n",
      "step №394: loss = 34.5, weights = tensor([6.2578, 1.4111], dtype=torch.float16)\n",
      "step №395: loss = 34.46875, weights = tensor([6.2578, 1.4141], dtype=torch.float16)\n",
      "step №396: loss = 34.46875, weights = tensor([6.2578, 1.4170], dtype=torch.float16)\n",
      "step №397: loss = 34.4375, weights = tensor([6.2578, 1.4199], dtype=torch.float16)\n",
      "step №398: loss = 34.40625, weights = tensor([6.2578, 1.4229], dtype=torch.float16)\n",
      "step №399: loss = 34.4375, weights = tensor([6.2539, 1.4258], dtype=torch.float16)\n",
      "step №400: loss = 34.4375, weights = tensor([6.2539, 1.4287], dtype=torch.float16)\n",
      "step №401: loss = 34.4375, weights = tensor([6.2539, 1.4316], dtype=torch.float16)\n",
      "step №402: loss = 34.375, weights = tensor([6.2539, 1.4346], dtype=torch.float16)\n",
      "step №403: loss = 34.34375, weights = tensor([6.2539, 1.4375], dtype=torch.float16)\n",
      "step №404: loss = 34.34375, weights = tensor([6.2539, 1.4404], dtype=torch.float16)\n",
      "step №405: loss = 34.3125, weights = tensor([6.2539, 1.4434], dtype=torch.float16)\n",
      "step №406: loss = 34.3125, weights = tensor([6.2539, 1.4463], dtype=torch.float16)\n",
      "step №407: loss = 34.25, weights = tensor([6.2539, 1.4492], dtype=torch.float16)\n",
      "step №408: loss = 34.25, weights = tensor([6.2539, 1.4521], dtype=torch.float16)\n",
      "step №409: loss = 34.21875, weights = tensor([6.2539, 1.4551], dtype=torch.float16)\n",
      "step №410: loss = 34.28125, weights = tensor([6.2500, 1.4580], dtype=torch.float16)\n",
      "step №411: loss = 34.25, weights = tensor([6.2500, 1.4609], dtype=torch.float16)\n",
      "step №412: loss = 34.1875, weights = tensor([6.2500, 1.4639], dtype=torch.float16)\n",
      "step №413: loss = 34.1875, weights = tensor([6.2500, 1.4668], dtype=torch.float16)\n",
      "step №414: loss = 34.1875, weights = tensor([6.2500, 1.4697], dtype=torch.float16)\n",
      "step №415: loss = 34.1875, weights = tensor([6.2500, 1.4727], dtype=torch.float16)\n",
      "step №416: loss = 34.1875, weights = tensor([6.2500, 1.4756], dtype=torch.float16)\n",
      "step №417: loss = 34.15625, weights = tensor([6.2500, 1.4785], dtype=torch.float16)\n",
      "step №418: loss = 34.09375, weights = tensor([6.2500, 1.4814], dtype=torch.float16)\n",
      "step №419: loss = 34.0625, weights = tensor([6.2500, 1.4844], dtype=torch.float16)\n",
      "step №420: loss = 34.09375, weights = tensor([6.2461, 1.4873], dtype=torch.float16)\n",
      "step №421: loss = 34.125, weights = tensor([6.2461, 1.4902], dtype=torch.float16)\n",
      "step №422: loss = 34.0625, weights = tensor([6.2461, 1.4932], dtype=torch.float16)\n",
      "step №423: loss = 34.0625, weights = tensor([6.2461, 1.4961], dtype=torch.float16)\n",
      "step №424: loss = 34.03125, weights = tensor([6.2461, 1.4990], dtype=torch.float16)\n",
      "step №425: loss = 34.03125, weights = tensor([6.2461, 1.5020], dtype=torch.float16)\n",
      "step №426: loss = 34.0, weights = tensor([6.2422, 1.5049], dtype=torch.float16)\n",
      "step №427: loss = 34.0, weights = tensor([6.2422, 1.5078], dtype=torch.float16)\n",
      "step №428: loss = 34.0, weights = tensor([6.2422, 1.5107], dtype=torch.float16)\n",
      "step №429: loss = 33.9375, weights = tensor([6.2422, 1.5137], dtype=torch.float16)\n",
      "step №430: loss = 33.9375, weights = tensor([6.2422, 1.5166], dtype=torch.float16)\n",
      "step №431: loss = 33.9375, weights = tensor([6.2383, 1.5195], dtype=torch.float16)\n",
      "step №432: loss = 33.9375, weights = tensor([6.2383, 1.5225], dtype=torch.float16)\n",
      "step №433: loss = 33.9375, weights = tensor([6.2383, 1.5254], dtype=torch.float16)\n",
      "step №434: loss = 33.875, weights = tensor([6.2383, 1.5283], dtype=torch.float16)\n",
      "step №435: loss = 33.84375, weights = tensor([6.2383, 1.5312], dtype=torch.float16)\n",
      "step №436: loss = 33.84375, weights = tensor([6.2383, 1.5342], dtype=torch.float16)\n",
      "step №437: loss = 33.84375, weights = tensor([6.2383, 1.5371], dtype=torch.float16)\n",
      "step №438: loss = 33.8125, weights = tensor([6.2383, 1.5400], dtype=torch.float16)\n",
      "step №439: loss = 33.78125, weights = tensor([6.2383, 1.5430], dtype=torch.float16)\n",
      "step №440: loss = 33.75, weights = tensor([6.2383, 1.5459], dtype=torch.float16)\n",
      "step №441: loss = 33.75, weights = tensor([6.2383, 1.5488], dtype=torch.float16)\n",
      "step №442: loss = 33.78125, weights = tensor([6.2344, 1.5518], dtype=torch.float16)\n",
      "step №443: loss = 33.78125, weights = tensor([6.2344, 1.5547], dtype=torch.float16)\n",
      "step №444: loss = 33.75, weights = tensor([6.2344, 1.5576], dtype=torch.float16)\n",
      "step №445: loss = 33.71875, weights = tensor([6.2344, 1.5605], dtype=torch.float16)\n",
      "step №446: loss = 33.6875, weights = tensor([6.2344, 1.5635], dtype=torch.float16)\n",
      "step №447: loss = 33.6875, weights = tensor([6.2344, 1.5664], dtype=torch.float16)\n",
      "step №448: loss = 33.6875, weights = tensor([6.2344, 1.5693], dtype=torch.float16)\n",
      "step №449: loss = 33.65625, weights = tensor([6.2344, 1.5723], dtype=torch.float16)\n",
      "step №450: loss = 33.625, weights = tensor([6.2344, 1.5752], dtype=torch.float16)\n",
      "step №451: loss = 33.59375, weights = tensor([6.2344, 1.5781], dtype=torch.float16)\n",
      "step №452: loss = 33.59375, weights = tensor([6.2344, 1.5811], dtype=torch.float16)\n",
      "step №453: loss = 33.65625, weights = tensor([6.2305, 1.5840], dtype=torch.float16)\n",
      "step №454: loss = 33.5625, weights = tensor([6.2305, 1.5869], dtype=torch.float16)\n",
      "step №455: loss = 33.5, weights = tensor([6.2305, 1.5898], dtype=torch.float16)\n",
      "step №456: loss = 33.5, weights = tensor([6.2305, 1.5928], dtype=torch.float16)\n",
      "step №457: loss = 33.46875, weights = tensor([6.2305, 1.5957], dtype=torch.float16)\n",
      "step №458: loss = 33.46875, weights = tensor([6.2305, 1.5986], dtype=torch.float16)\n",
      "step №459: loss = 33.4375, weights = tensor([6.2305, 1.6016], dtype=torch.float16)\n",
      "step №460: loss = 33.4375, weights = tensor([6.2305, 1.6045], dtype=torch.float16)\n",
      "step №461: loss = 33.40625, weights = tensor([6.2305, 1.6074], dtype=torch.float16)\n",
      "step №462: loss = 33.375, weights = tensor([6.2305, 1.6104], dtype=torch.float16)\n",
      "step №463: loss = 33.40625, weights = tensor([6.2266, 1.6133], dtype=torch.float16)\n",
      "step №464: loss = 33.4375, weights = tensor([6.2266, 1.6162], dtype=torch.float16)\n",
      "step №465: loss = 33.40625, weights = tensor([6.2266, 1.6191], dtype=torch.float16)\n",
      "step №466: loss = 33.34375, weights = tensor([6.2227, 1.6221], dtype=torch.float16)\n",
      "step №467: loss = 33.3125, weights = tensor([6.2227, 1.6250], dtype=torch.float16)\n",
      "step №468: loss = 33.3125, weights = tensor([6.2227, 1.6279], dtype=torch.float16)\n",
      "step №469: loss = 33.3125, weights = tensor([6.2227, 1.6309], dtype=torch.float16)\n",
      "step №470: loss = 33.28125, weights = tensor([6.2227, 1.6338], dtype=torch.float16)\n",
      "step №471: loss = 33.25, weights = tensor([6.2227, 1.6367], dtype=torch.float16)\n",
      "step №472: loss = 33.21875, weights = tensor([6.2227, 1.6396], dtype=torch.float16)\n",
      "step №473: loss = 33.1875, weights = tensor([6.2227, 1.6426], dtype=torch.float16)\n",
      "step №474: loss = 33.25, weights = tensor([6.2188, 1.6455], dtype=torch.float16)\n",
      "step №475: loss = 33.21875, weights = tensor([6.2188, 1.6484], dtype=torch.float16)\n",
      "step №476: loss = 33.1875, weights = tensor([6.2188, 1.6514], dtype=torch.float16)\n",
      "step №477: loss = 33.1875, weights = tensor([6.2188, 1.6543], dtype=torch.float16)\n",
      "step №478: loss = 33.15625, weights = tensor([6.2188, 1.6572], dtype=torch.float16)\n",
      "step №479: loss = 33.15625, weights = tensor([6.2188, 1.6602], dtype=torch.float16)\n",
      "step №480: loss = 33.15625, weights = tensor([6.2188, 1.6631], dtype=torch.float16)\n",
      "step №481: loss = 33.125, weights = tensor([6.2188, 1.6660], dtype=torch.float16)\n",
      "step №482: loss = 33.09375, weights = tensor([6.2188, 1.6689], dtype=torch.float16)\n",
      "step №483: loss = 33.0625, weights = tensor([6.2188, 1.6719], dtype=torch.float16)\n",
      "step №484: loss = 33.09375, weights = tensor([6.2188, 1.6748], dtype=torch.float16)\n",
      "step №485: loss = 33.09375, weights = tensor([6.2148, 1.6777], dtype=torch.float16)\n",
      "step №486: loss = 33.0625, weights = tensor([6.2148, 1.6807], dtype=torch.float16)\n",
      "step №487: loss = 33.03125, weights = tensor([6.2148, 1.6836], dtype=torch.float16)\n",
      "step №488: loss = 33.03125, weights = tensor([6.2148, 1.6865], dtype=torch.float16)\n",
      "step №489: loss = 33.0, weights = tensor([6.2148, 1.6895], dtype=torch.float16)\n",
      "step №490: loss = 33.0, weights = tensor([6.2148, 1.6924], dtype=torch.float16)\n",
      "step №491: loss = 32.9375, weights = tensor([6.2148, 1.6953], dtype=torch.float16)\n",
      "step №492: loss = 32.9375, weights = tensor([6.2148, 1.6982], dtype=torch.float16)\n",
      "step №493: loss = 32.9375, weights = tensor([6.2148, 1.7012], dtype=torch.float16)\n",
      "step №494: loss = 32.90625, weights = tensor([6.2148, 1.7041], dtype=torch.float16)\n",
      "step №495: loss = 32.9375, weights = tensor([6.2109, 1.7070], dtype=torch.float16)\n",
      "step №496: loss = 32.9375, weights = tensor([6.2109, 1.7100], dtype=torch.float16)\n",
      "step №497: loss = 32.90625, weights = tensor([6.2109, 1.7129], dtype=torch.float16)\n",
      "step №498: loss = 32.875, weights = tensor([6.2109, 1.7158], dtype=torch.float16)\n",
      "step №499: loss = 32.84375, weights = tensor([6.2109, 1.7188], dtype=torch.float16)\n",
      "step №500: loss = 32.84375, weights = tensor([6.2109, 1.7217], dtype=torch.float16)\n",
      "step №501: loss = 32.84375, weights = tensor([6.2109, 1.7246], dtype=torch.float16)\n",
      "step №502: loss = 32.8125, weights = tensor([6.2109, 1.7275], dtype=torch.float16)\n",
      "step №503: loss = 32.78125, weights = tensor([6.2109, 1.7305], dtype=torch.float16)\n",
      "step №504: loss = 32.75, weights = tensor([6.2109, 1.7334], dtype=torch.float16)\n",
      "step №505: loss = 32.75, weights = tensor([6.2109, 1.7363], dtype=torch.float16)\n",
      "step №506: loss = 32.78125, weights = tensor([6.2070, 1.7393], dtype=torch.float16)\n",
      "step №507: loss = 32.75, weights = tensor([6.2031, 1.7422], dtype=torch.float16)\n",
      "step №508: loss = 32.6875, weights = tensor([6.2031, 1.7451], dtype=torch.float16)\n",
      "step №509: loss = 32.6875, weights = tensor([6.2031, 1.7480], dtype=torch.float16)\n",
      "step №510: loss = 32.65625, weights = tensor([6.2031, 1.7510], dtype=torch.float16)\n",
      "step №511: loss = 32.625, weights = tensor([6.2031, 1.7539], dtype=torch.float16)\n",
      "step №512: loss = 32.625, weights = tensor([6.2031, 1.7568], dtype=torch.float16)\n",
      "step №513: loss = 32.59375, weights = tensor([6.2031, 1.7598], dtype=torch.float16)\n",
      "step №514: loss = 32.5625, weights = tensor([6.2031, 1.7627], dtype=torch.float16)\n",
      "step №515: loss = 32.53125, weights = tensor([6.2031, 1.7656], dtype=torch.float16)\n",
      "step №516: loss = 32.53125, weights = tensor([6.2031, 1.7686], dtype=torch.float16)\n",
      "step №517: loss = 32.5625, weights = tensor([6.1992, 1.7715], dtype=torch.float16)\n",
      "step №518: loss = 32.5625, weights = tensor([6.1992, 1.7744], dtype=torch.float16)\n",
      "step №519: loss = 32.5, weights = tensor([6.1992, 1.7773], dtype=torch.float16)\n",
      "step №520: loss = 32.5, weights = tensor([6.1992, 1.7803], dtype=torch.float16)\n",
      "step №521: loss = 32.46875, weights = tensor([6.1992, 1.7832], dtype=torch.float16)\n",
      "step №522: loss = 32.46875, weights = tensor([6.1992, 1.7861], dtype=torch.float16)\n",
      "step №523: loss = 32.4375, weights = tensor([6.1992, 1.7891], dtype=torch.float16)\n",
      "step №524: loss = 32.4375, weights = tensor([6.1992, 1.7920], dtype=torch.float16)\n",
      "step №525: loss = 32.40625, weights = tensor([6.1992, 1.7949], dtype=torch.float16)\n",
      "step №526: loss = 32.375, weights = tensor([6.1992, 1.7979], dtype=torch.float16)\n",
      "step №527: loss = 32.40625, weights = tensor([6.1953, 1.8008], dtype=torch.float16)\n",
      "step №528: loss = 32.40625, weights = tensor([6.1953, 1.8037], dtype=torch.float16)\n",
      "step №529: loss = 32.375, weights = tensor([6.1953, 1.8066], dtype=torch.float16)\n",
      "step №530: loss = 32.34375, weights = tensor([6.1953, 1.8096], dtype=torch.float16)\n",
      "step №531: loss = 32.3125, weights = tensor([6.1953, 1.8125], dtype=torch.float16)\n",
      "step №532: loss = 32.3125, weights = tensor([6.1953, 1.8154], dtype=torch.float16)\n",
      "step №533: loss = 32.3125, weights = tensor([6.1953, 1.8184], dtype=torch.float16)\n",
      "step №534: loss = 32.3125, weights = tensor([6.1953, 1.8213], dtype=torch.float16)\n",
      "step №535: loss = 32.25, weights = tensor([6.1953, 1.8242], dtype=torch.float16)\n",
      "step №536: loss = 32.21875, weights = tensor([6.1953, 1.8271], dtype=torch.float16)\n",
      "step №537: loss = 32.21875, weights = tensor([6.1953, 1.8301], dtype=torch.float16)\n",
      "step №538: loss = 32.25, weights = tensor([6.1914, 1.8330], dtype=torch.float16)\n",
      "step №539: loss = 32.25, weights = tensor([6.1914, 1.8359], dtype=torch.float16)\n",
      "step №540: loss = 32.21875, weights = tensor([6.1914, 1.8389], dtype=torch.float16)\n",
      "step №541: loss = 32.1875, weights = tensor([6.1914, 1.8418], dtype=torch.float16)\n",
      "step №542: loss = 32.1875, weights = tensor([6.1914, 1.8447], dtype=torch.float16)\n",
      "step №543: loss = 32.15625, weights = tensor([6.1875, 1.8477], dtype=torch.float16)\n",
      "step №544: loss = 32.1875, weights = tensor([6.1875, 1.8506], dtype=torch.float16)\n",
      "step №545: loss = 32.15625, weights = tensor([6.1875, 1.8535], dtype=torch.float16)\n",
      "step №546: loss = 32.09375, weights = tensor([6.1875, 1.8564], dtype=torch.float16)\n",
      "step №547: loss = 32.0625, weights = tensor([6.1875, 1.8594], dtype=torch.float16)\n",
      "step №548: loss = 32.09375, weights = tensor([6.1836, 1.8623], dtype=torch.float16)\n",
      "step №549: loss = 32.125, weights = tensor([6.1836, 1.8652], dtype=torch.float16)\n",
      "step №550: loss = 32.09375, weights = tensor([6.1836, 1.8682], dtype=torch.float16)\n",
      "step №551: loss = 32.0625, weights = tensor([6.1836, 1.8711], dtype=torch.float16)\n",
      "step №552: loss = 32.03125, weights = tensor([6.1836, 1.8740], dtype=torch.float16)\n",
      "step №553: loss = 32.03125, weights = tensor([6.1836, 1.8770], dtype=torch.float16)\n",
      "step №554: loss = 32.0, weights = tensor([6.1836, 1.8799], dtype=torch.float16)\n",
      "step №555: loss = 31.96875, weights = tensor([6.1836, 1.8828], dtype=torch.float16)\n",
      "step №556: loss = 31.921875, weights = tensor([6.1836, 1.8857], dtype=torch.float16)\n",
      "step №557: loss = 31.921875, weights = tensor([6.1836, 1.8887], dtype=torch.float16)\n",
      "step №558: loss = 31.90625, weights = tensor([6.1836, 1.8916], dtype=torch.float16)\n",
      "step №559: loss = 31.953125, weights = tensor([6.1797, 1.8945], dtype=torch.float16)\n",
      "step №560: loss = 31.953125, weights = tensor([6.1797, 1.8975], dtype=torch.float16)\n",
      "step №561: loss = 31.921875, weights = tensor([6.1797, 1.9004], dtype=torch.float16)\n",
      "step №562: loss = 31.90625, weights = tensor([6.1797, 1.9033], dtype=torch.float16)\n",
      "step №563: loss = 31.84375, weights = tensor([6.1797, 1.9062], dtype=torch.float16)\n",
      "step №564: loss = 31.84375, weights = tensor([6.1797, 1.9092], dtype=torch.float16)\n",
      "step №565: loss = 31.84375, weights = tensor([6.1797, 1.9121], dtype=torch.float16)\n",
      "step №566: loss = 31.828125, weights = tensor([6.1797, 1.9150], dtype=torch.float16)\n",
      "step №567: loss = 31.796875, weights = tensor([6.1797, 1.9180], dtype=torch.float16)\n",
      "step №568: loss = 31.78125, weights = tensor([6.1797, 1.9209], dtype=torch.float16)\n",
      "step №569: loss = 31.78125, weights = tensor([6.1797, 1.9238], dtype=torch.float16)\n",
      "step №570: loss = 31.796875, weights = tensor([6.1758, 1.9268], dtype=torch.float16)\n",
      "step №571: loss = 31.78125, weights = tensor([6.1758, 1.9297], dtype=torch.float16)\n",
      "step №572: loss = 31.75, weights = tensor([6.1758, 1.9326], dtype=torch.float16)\n",
      "step №573: loss = 31.71875, weights = tensor([6.1758, 1.9355], dtype=torch.float16)\n",
      "step №574: loss = 31.703125, weights = tensor([6.1758, 1.9385], dtype=torch.float16)\n",
      "step №575: loss = 31.703125, weights = tensor([6.1758, 1.9414], dtype=torch.float16)\n",
      "step №576: loss = 31.703125, weights = tensor([6.1758, 1.9443], dtype=torch.float16)\n",
      "step №577: loss = 31.671875, weights = tensor([6.1758, 1.9473], dtype=torch.float16)\n",
      "step №578: loss = 31.625, weights = tensor([6.1758, 1.9502], dtype=torch.float16)\n",
      "step №579: loss = 31.59375, weights = tensor([6.1758, 1.9531], dtype=torch.float16)\n",
      "step №580: loss = 31.65625, weights = tensor([6.1719, 1.9561], dtype=torch.float16)\n",
      "step №581: loss = 31.65625, weights = tensor([6.1719, 1.9590], dtype=torch.float16)\n",
      "step №582: loss = 31.625, weights = tensor([6.1719, 1.9619], dtype=torch.float16)\n",
      "step №583: loss = 31.578125, weights = tensor([6.1719, 1.9648], dtype=torch.float16)\n",
      "step №584: loss = 31.546875, weights = tensor([6.1719, 1.9678], dtype=torch.float16)\n",
      "step №585: loss = 31.546875, weights = tensor([6.1719, 1.9707], dtype=torch.float16)\n",
      "step №586: loss = 31.53125, weights = tensor([6.1680, 1.9736], dtype=torch.float16)\n",
      "step №587: loss = 31.46875, weights = tensor([6.1680, 1.9766], dtype=torch.float16)\n",
      "step №588: loss = 31.453125, weights = tensor([6.1680, 1.9795], dtype=torch.float16)\n",
      "step №589: loss = 31.421875, weights = tensor([6.1680, 1.9824], dtype=torch.float16)\n",
      "step №590: loss = 31.40625, weights = tensor([6.1680, 1.9854], dtype=torch.float16)\n",
      "step №591: loss = 31.453125, weights = tensor([6.1641, 1.9883], dtype=torch.float16)\n",
      "step №592: loss = 31.453125, weights = tensor([6.1641, 1.9912], dtype=torch.float16)\n",
      "step №593: loss = 31.40625, weights = tensor([6.1641, 1.9941], dtype=torch.float16)\n",
      "step №594: loss = 31.375, weights = tensor([6.1641, 1.9971], dtype=torch.float16)\n",
      "step №595: loss = 31.34375, weights = tensor([6.1641, 2.0000], dtype=torch.float16)\n",
      "step №596: loss = 31.34375, weights = tensor([6.1641, 2.0039], dtype=torch.float16)\n",
      "step №597: loss = 31.34375, weights = tensor([6.1641, 2.0078], dtype=torch.float16)\n",
      "step №598: loss = 31.296875, weights = tensor([6.1641, 2.0117], dtype=torch.float16)\n",
      "step №599: loss = 31.25, weights = tensor([6.1641, 2.0156], dtype=torch.float16)\n",
      "step №600: loss = 31.25, weights = tensor([6.1641, 2.0195], dtype=torch.float16)\n",
      "step №601: loss = 31.28125, weights = tensor([6.1602, 2.0234], dtype=torch.float16)\n",
      "step №602: loss = 31.28125, weights = tensor([6.1602, 2.0273], dtype=torch.float16)\n",
      "step №603: loss = 31.203125, weights = tensor([6.1602, 2.0312], dtype=torch.float16)\n",
      "step №604: loss = 31.203125, weights = tensor([6.1602, 2.0352], dtype=torch.float16)\n",
      "step №605: loss = 31.171875, weights = tensor([6.1602, 2.0391], dtype=torch.float16)\n",
      "step №606: loss = 31.09375, weights = tensor([6.1602, 2.0430], dtype=torch.float16)\n",
      "step №607: loss = 31.09375, weights = tensor([6.1602, 2.0469], dtype=torch.float16)\n",
      "step №608: loss = 31.09375, weights = tensor([6.1562, 2.0508], dtype=torch.float16)\n",
      "step №609: loss = 31.125, weights = tensor([6.1562, 2.0547], dtype=torch.float16)\n",
      "step №610: loss = 31.078125, weights = tensor([6.1562, 2.0586], dtype=torch.float16)\n",
      "step №611: loss = 31.03125, weights = tensor([6.1562, 2.0625], dtype=torch.float16)\n",
      "step №612: loss = 31.03125, weights = tensor([6.1562, 2.0664], dtype=torch.float16)\n",
      "step №613: loss = 31.03125, weights = tensor([6.1562, 2.0703], dtype=torch.float16)\n",
      "step №614: loss = 31.0, weights = tensor([6.1562, 2.0742], dtype=torch.float16)\n",
      "step №615: loss = 30.953125, weights = tensor([6.1562, 2.0781], dtype=torch.float16)\n",
      "step №616: loss = 30.921875, weights = tensor([6.1523, 2.0820], dtype=torch.float16)\n",
      "step №617: loss = 30.96875, weights = tensor([6.1523, 2.0859], dtype=torch.float16)\n",
      "step №618: loss = 30.953125, weights = tensor([6.1523, 2.0898], dtype=torch.float16)\n",
      "step №619: loss = 30.90625, weights = tensor([6.1523, 2.0938], dtype=torch.float16)\n",
      "step №620: loss = 30.875, weights = tensor([6.1484, 2.0977], dtype=torch.float16)\n",
      "step №621: loss = 30.90625, weights = tensor([6.1484, 2.1016], dtype=torch.float16)\n",
      "step №622: loss = 30.84375, weights = tensor([6.1484, 2.1055], dtype=torch.float16)\n",
      "step №623: loss = 30.796875, weights = tensor([6.1484, 2.1094], dtype=torch.float16)\n",
      "step №624: loss = 30.828125, weights = tensor([6.1484, 2.1133], dtype=torch.float16)\n",
      "step №625: loss = 30.828125, weights = tensor([6.1445, 2.1172], dtype=torch.float16)\n",
      "step №626: loss = 30.78125, weights = tensor([6.1445, 2.1211], dtype=torch.float16)\n",
      "step №627: loss = 30.75, weights = tensor([6.1445, 2.1250], dtype=torch.float16)\n",
      "step №628: loss = 30.75, weights = tensor([6.1445, 2.1289], dtype=torch.float16)\n",
      "step №629: loss = 30.71875, weights = tensor([6.1445, 2.1328], dtype=torch.float16)\n",
      "step №630: loss = 30.703125, weights = tensor([6.1445, 2.1367], dtype=torch.float16)\n",
      "step №631: loss = 30.65625, weights = tensor([6.1445, 2.1406], dtype=torch.float16)\n",
      "step №632: loss = 30.671875, weights = tensor([6.1445, 2.1445], dtype=torch.float16)\n",
      "step №633: loss = 30.65625, weights = tensor([6.1406, 2.1484], dtype=torch.float16)\n",
      "step №634: loss = 30.59375, weights = tensor([6.1406, 2.1523], dtype=torch.float16)\n",
      "step №635: loss = 30.546875, weights = tensor([6.1406, 2.1562], dtype=torch.float16)\n",
      "step №636: loss = 30.53125, weights = tensor([6.1406, 2.1602], dtype=torch.float16)\n",
      "step №637: loss = 30.53125, weights = tensor([6.1406, 2.1641], dtype=torch.float16)\n",
      "step №638: loss = 30.46875, weights = tensor([6.1406, 2.1680], dtype=torch.float16)\n",
      "step №639: loss = 30.421875, weights = tensor([6.1406, 2.1719], dtype=torch.float16)\n",
      "step №640: loss = 30.46875, weights = tensor([6.1367, 2.1758], dtype=torch.float16)\n",
      "step №641: loss = 30.46875, weights = tensor([6.1367, 2.1797], dtype=torch.float16)\n",
      "step №642: loss = 30.421875, weights = tensor([6.1367, 2.1836], dtype=torch.float16)\n",
      "step №643: loss = 30.40625, weights = tensor([6.1367, 2.1875], dtype=torch.float16)\n",
      "step №644: loss = 30.40625, weights = tensor([6.1367, 2.1914], dtype=torch.float16)\n",
      "step №645: loss = 30.375, weights = tensor([6.1367, 2.1953], dtype=torch.float16)\n",
      "step №646: loss = 30.34375, weights = tensor([6.1367, 2.1992], dtype=torch.float16)\n",
      "step №647: loss = 30.296875, weights = tensor([6.1367, 2.2031], dtype=torch.float16)\n",
      "step №648: loss = 30.328125, weights = tensor([6.1328, 2.2070], dtype=torch.float16)\n",
      "step №649: loss = 30.328125, weights = tensor([6.1328, 2.2109], dtype=torch.float16)\n",
      "step №650: loss = 30.28125, weights = tensor([6.1289, 2.2148], dtype=torch.float16)\n",
      "step №651: loss = 30.25, weights = tensor([6.1289, 2.2188], dtype=torch.float16)\n",
      "step №652: loss = 30.21875, weights = tensor([6.1289, 2.2227], dtype=torch.float16)\n",
      "step №653: loss = 30.21875, weights = tensor([6.1289, 2.2266], dtype=torch.float16)\n",
      "step №654: loss = 30.171875, weights = tensor([6.1289, 2.2305], dtype=torch.float16)\n",
      "step №655: loss = 30.15625, weights = tensor([6.1289, 2.2344], dtype=torch.float16)\n",
      "step №656: loss = 30.125, weights = tensor([6.1289, 2.2383], dtype=torch.float16)\n",
      "step №657: loss = 30.171875, weights = tensor([6.1250, 2.2422], dtype=torch.float16)\n",
      "step №658: loss = 30.125, weights = tensor([6.1250, 2.2461], dtype=torch.float16)\n",
      "step №659: loss = 30.09375, weights = tensor([6.1250, 2.2500], dtype=torch.float16)\n",
      "step №660: loss = 30.09375, weights = tensor([6.1250, 2.2539], dtype=torch.float16)\n",
      "step №661: loss = 30.09375, weights = tensor([6.1250, 2.2578], dtype=torch.float16)\n",
      "step №662: loss = 30.046875, weights = tensor([6.1250, 2.2617], dtype=torch.float16)\n",
      "step №663: loss = 30.0, weights = tensor([6.1250, 2.2656], dtype=torch.float16)\n",
      "step №664: loss = 30.0, weights = tensor([6.1250, 2.2695], dtype=torch.float16)\n",
      "step №665: loss = 30.03125, weights = tensor([6.1211, 2.2734], dtype=torch.float16)\n",
      "step №666: loss = 30.03125, weights = tensor([6.1211, 2.2773], dtype=torch.float16)\n",
      "step №667: loss = 29.953125, weights = tensor([6.1211, 2.2812], dtype=torch.float16)\n",
      "step №668: loss = 29.953125, weights = tensor([6.1211, 2.2852], dtype=torch.float16)\n",
      "step №669: loss = 29.921875, weights = tensor([6.1211, 2.2891], dtype=torch.float16)\n",
      "step №670: loss = 29.875, weights = tensor([6.1211, 2.2930], dtype=torch.float16)\n",
      "step №671: loss = 29.84375, weights = tensor([6.1211, 2.2969], dtype=torch.float16)\n",
      "step №672: loss = 29.84375, weights = tensor([6.1211, 2.3008], dtype=torch.float16)\n",
      "step №673: loss = 29.875, weights = tensor([6.1172, 2.3047], dtype=torch.float16)\n",
      "step №674: loss = 29.828125, weights = tensor([6.1172, 2.3086], dtype=torch.float16)\n",
      "step №675: loss = 29.796875, weights = tensor([6.1172, 2.3125], dtype=torch.float16)\n",
      "step №676: loss = 29.796875, weights = tensor([6.1172, 2.3164], dtype=torch.float16)\n",
      "step №677: loss = 29.796875, weights = tensor([6.1172, 2.3203], dtype=torch.float16)\n",
      "step №678: loss = 29.75, weights = tensor([6.1172, 2.3242], dtype=torch.float16)\n",
      "step №679: loss = 29.703125, weights = tensor([6.1172, 2.3281], dtype=torch.float16)\n",
      "step №680: loss = 29.671875, weights = tensor([6.1133, 2.3320], dtype=torch.float16)\n",
      "step №681: loss = 29.71875, weights = tensor([6.1094, 2.3359], dtype=torch.float16)\n",
      "step №682: loss = 29.703125, weights = tensor([6.1094, 2.3398], dtype=torch.float16)\n",
      "step №683: loss = 29.65625, weights = tensor([6.1094, 2.3438], dtype=torch.float16)\n",
      "step №684: loss = 29.65625, weights = tensor([6.1094, 2.3477], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №685: loss = 29.65625, weights = tensor([6.1094, 2.3516], dtype=torch.float16)\n",
      "step №686: loss = 29.59375, weights = tensor([6.1094, 2.3555], dtype=torch.float16)\n",
      "step №687: loss = 29.546875, weights = tensor([6.1094, 2.3594], dtype=torch.float16)\n",
      "step №688: loss = 29.59375, weights = tensor([6.1055, 2.3633], dtype=torch.float16)\n",
      "step №689: loss = 29.53125, weights = tensor([6.1055, 2.3672], dtype=torch.float16)\n",
      "step №690: loss = 29.5, weights = tensor([6.1055, 2.3711], dtype=torch.float16)\n",
      "step №691: loss = 29.453125, weights = tensor([6.1055, 2.3750], dtype=torch.float16)\n",
      "step №692: loss = 29.453125, weights = tensor([6.1055, 2.3789], dtype=torch.float16)\n",
      "step №693: loss = 29.421875, weights = tensor([6.1055, 2.3828], dtype=torch.float16)\n",
      "step №694: loss = 29.40625, weights = tensor([6.1055, 2.3867], dtype=torch.float16)\n",
      "step №695: loss = 29.375, weights = tensor([6.1055, 2.3906], dtype=torch.float16)\n",
      "step №696: loss = 29.375, weights = tensor([6.1055, 2.3926], dtype=torch.float16)\n",
      "step №697: loss = 29.375, weights = tensor([6.1016, 2.3945], dtype=torch.float16)\n",
      "step №698: loss = 29.40625, weights = tensor([6.1016, 2.3984], dtype=torch.float16)\n",
      "step №699: loss = 29.328125, weights = tensor([6.1016, 2.4004], dtype=torch.float16)\n",
      "step №700: loss = 29.328125, weights = tensor([6.1016, 2.4023], dtype=torch.float16)\n",
      "step №701: loss = 29.296875, weights = tensor([6.1016, 2.4043], dtype=torch.float16)\n",
      "step №702: loss = 29.296875, weights = tensor([6.1016, 2.4062], dtype=torch.float16)\n",
      "step №703: loss = 29.296875, weights = tensor([6.1016, 2.4082], dtype=torch.float16)\n",
      "step №704: loss = 29.296875, weights = tensor([6.1016, 2.4102], dtype=torch.float16)\n",
      "step №705: loss = 29.296875, weights = tensor([6.1016, 2.4121], dtype=torch.float16)\n",
      "step №706: loss = 29.28125, weights = tensor([6.1016, 2.4141], dtype=torch.float16)\n",
      "step №707: loss = 29.28125, weights = tensor([6.1016, 2.4160], dtype=torch.float16)\n",
      "step №708: loss = 29.25, weights = tensor([6.1016, 2.4180], dtype=torch.float16)\n",
      "step №709: loss = 29.203125, weights = tensor([6.1016, 2.4199], dtype=torch.float16)\n",
      "step №710: loss = 29.203125, weights = tensor([6.1016, 2.4219], dtype=torch.float16)\n",
      "step №711: loss = 29.21875, weights = tensor([6.0977, 2.4238], dtype=torch.float16)\n",
      "step №712: loss = 29.25, weights = tensor([6.0977, 2.4258], dtype=torch.float16)\n",
      "step №713: loss = 29.25, weights = tensor([6.0977, 2.4277], dtype=torch.float16)\n",
      "step №714: loss = 29.21875, weights = tensor([6.0977, 2.4297], dtype=torch.float16)\n",
      "step №715: loss = 29.21875, weights = tensor([6.0977, 2.4316], dtype=torch.float16)\n",
      "step №716: loss = 29.203125, weights = tensor([6.0977, 2.4336], dtype=torch.float16)\n",
      "step №717: loss = 29.171875, weights = tensor([6.0977, 2.4355], dtype=torch.float16)\n",
      "step №718: loss = 29.171875, weights = tensor([6.0977, 2.4375], dtype=torch.float16)\n",
      "step №719: loss = 29.15625, weights = tensor([6.0977, 2.4395], dtype=torch.float16)\n",
      "step №720: loss = 29.15625, weights = tensor([6.0938, 2.4414], dtype=torch.float16)\n",
      "step №721: loss = 29.15625, weights = tensor([6.0938, 2.4434], dtype=torch.float16)\n",
      "step №722: loss = 29.125, weights = tensor([6.0938, 2.4453], dtype=torch.float16)\n",
      "step №723: loss = 29.125, weights = tensor([6.0938, 2.4473], dtype=torch.float16)\n",
      "step №724: loss = 29.09375, weights = tensor([6.0938, 2.4492], dtype=torch.float16)\n",
      "step №725: loss = 29.078125, weights = tensor([6.0938, 2.4512], dtype=torch.float16)\n",
      "step №726: loss = 29.078125, weights = tensor([6.0938, 2.4531], dtype=torch.float16)\n",
      "step №727: loss = 29.046875, weights = tensor([6.0938, 2.4551], dtype=torch.float16)\n",
      "step №728: loss = 29.09375, weights = tensor([6.0898, 2.4570], dtype=torch.float16)\n",
      "step №729: loss = 29.09375, weights = tensor([6.0898, 2.4590], dtype=torch.float16)\n",
      "step №730: loss = 29.078125, weights = tensor([6.0898, 2.4609], dtype=torch.float16)\n",
      "step №731: loss = 29.078125, weights = tensor([6.0898, 2.4629], dtype=torch.float16)\n",
      "step №732: loss = 29.046875, weights = tensor([6.0898, 2.4648], dtype=torch.float16)\n",
      "step №733: loss = 29.046875, weights = tensor([6.0898, 2.4668], dtype=torch.float16)\n",
      "step №734: loss = 29.03125, weights = tensor([6.0898, 2.4688], dtype=torch.float16)\n",
      "step №735: loss = 29.0, weights = tensor([6.0898, 2.4707], dtype=torch.float16)\n",
      "step №736: loss = 29.0, weights = tensor([6.0898, 2.4727], dtype=torch.float16)\n",
      "step №737: loss = 29.0, weights = tensor([6.0898, 2.4746], dtype=torch.float16)\n",
      "step №738: loss = 28.96875, weights = tensor([6.0898, 2.4766], dtype=torch.float16)\n",
      "step №739: loss = 28.953125, weights = tensor([6.0898, 2.4785], dtype=torch.float16)\n",
      "step №740: loss = 28.953125, weights = tensor([6.0898, 2.4805], dtype=torch.float16)\n",
      "step №741: loss = 28.921875, weights = tensor([6.0898, 2.4824], dtype=torch.float16)\n",
      "step №742: loss = 28.921875, weights = tensor([6.0898, 2.4844], dtype=torch.float16)\n",
      "step №743: loss = 28.921875, weights = tensor([6.0898, 2.4863], dtype=torch.float16)\n",
      "step №744: loss = 28.96875, weights = tensor([6.0859, 2.4883], dtype=torch.float16)\n",
      "step №745: loss = 28.953125, weights = tensor([6.0859, 2.4902], dtype=torch.float16)\n",
      "step №746: loss = 28.921875, weights = tensor([6.0859, 2.4922], dtype=torch.float16)\n",
      "step №747: loss = 28.90625, weights = tensor([6.0859, 2.4941], dtype=torch.float16)\n",
      "step №748: loss = 28.90625, weights = tensor([6.0859, 2.4961], dtype=torch.float16)\n",
      "step №749: loss = 28.875, weights = tensor([6.0859, 2.4980], dtype=torch.float16)\n",
      "step №750: loss = 28.875, weights = tensor([6.0859, 2.5000], dtype=torch.float16)\n",
      "step №751: loss = 28.875, weights = tensor([6.0859, 2.5020], dtype=torch.float16)\n",
      "step №752: loss = 28.875, weights = tensor([6.0859, 2.5039], dtype=torch.float16)\n",
      "step №753: loss = 28.875, weights = tensor([6.0859, 2.5059], dtype=torch.float16)\n",
      "step №754: loss = 28.84375, weights = tensor([6.0859, 2.5078], dtype=torch.float16)\n",
      "step №755: loss = 28.84375, weights = tensor([6.0859, 2.5098], dtype=torch.float16)\n",
      "step №756: loss = 28.796875, weights = tensor([6.0859, 2.5117], dtype=torch.float16)\n",
      "step №757: loss = 28.78125, weights = tensor([6.0859, 2.5137], dtype=torch.float16)\n",
      "step №758: loss = 28.78125, weights = tensor([6.0859, 2.5156], dtype=torch.float16)\n",
      "step №759: loss = 28.78125, weights = tensor([6.0859, 2.5176], dtype=torch.float16)\n",
      "step №760: loss = 28.796875, weights = tensor([6.0820, 2.5195], dtype=torch.float16)\n",
      "step №761: loss = 28.796875, weights = tensor([6.0820, 2.5215], dtype=torch.float16)\n",
      "step №762: loss = 28.78125, weights = tensor([6.0820, 2.5234], dtype=torch.float16)\n",
      "step №763: loss = 28.78125, weights = tensor([6.0820, 2.5254], dtype=torch.float16)\n",
      "step №764: loss = 28.75, weights = tensor([6.0820, 2.5273], dtype=torch.float16)\n",
      "step №765: loss = 28.71875, weights = tensor([6.0820, 2.5293], dtype=torch.float16)\n",
      "step №766: loss = 28.71875, weights = tensor([6.0820, 2.5312], dtype=torch.float16)\n",
      "step №767: loss = 28.71875, weights = tensor([6.0820, 2.5332], dtype=torch.float16)\n",
      "step №768: loss = 28.71875, weights = tensor([6.0820, 2.5352], dtype=torch.float16)\n",
      "step №769: loss = 28.71875, weights = tensor([6.0820, 2.5371], dtype=torch.float16)\n",
      "step №770: loss = 28.703125, weights = tensor([6.0820, 2.5391], dtype=torch.float16)\n",
      "step №771: loss = 28.671875, weights = tensor([6.0820, 2.5410], dtype=torch.float16)\n",
      "step №772: loss = 28.671875, weights = tensor([6.0820, 2.5430], dtype=torch.float16)\n",
      "step №773: loss = 28.65625, weights = tensor([6.0820, 2.5449], dtype=torch.float16)\n",
      "step №774: loss = 28.625, weights = tensor([6.0820, 2.5469], dtype=torch.float16)\n",
      "step №775: loss = 28.65625, weights = tensor([6.0781, 2.5488], dtype=torch.float16)\n",
      "step №776: loss = 28.625, weights = tensor([6.0781, 2.5508], dtype=torch.float16)\n",
      "step №777: loss = 28.625, weights = tensor([6.0781, 2.5527], dtype=torch.float16)\n",
      "step №778: loss = 28.59375, weights = tensor([6.0781, 2.5547], dtype=torch.float16)\n",
      "step №779: loss = 28.578125, weights = tensor([6.0781, 2.5566], dtype=torch.float16)\n",
      "step №780: loss = 28.578125, weights = tensor([6.0781, 2.5586], dtype=torch.float16)\n",
      "step №781: loss = 28.546875, weights = tensor([6.0781, 2.5605], dtype=torch.float16)\n",
      "step №782: loss = 28.546875, weights = tensor([6.0781, 2.5625], dtype=torch.float16)\n",
      "step №783: loss = 28.546875, weights = tensor([6.0781, 2.5645], dtype=torch.float16)\n",
      "step №784: loss = 28.53125, weights = tensor([6.0781, 2.5664], dtype=torch.float16)\n",
      "step №785: loss = 28.53125, weights = tensor([6.0781, 2.5684], dtype=torch.float16)\n",
      "step №786: loss = 28.5, weights = tensor([6.0742, 2.5703], dtype=torch.float16)\n",
      "step №787: loss = 28.5, weights = tensor([6.0742, 2.5723], dtype=torch.float16)\n",
      "step №788: loss = 28.46875, weights = tensor([6.0742, 2.5742], dtype=torch.float16)\n",
      "step №789: loss = 28.453125, weights = tensor([6.0742, 2.5762], dtype=torch.float16)\n",
      "step №790: loss = 28.453125, weights = tensor([6.0742, 2.5781], dtype=torch.float16)\n",
      "step №791: loss = 28.46875, weights = tensor([6.0742, 2.5801], dtype=torch.float16)\n",
      "step №792: loss = 28.46875, weights = tensor([6.0703, 2.5820], dtype=torch.float16)\n",
      "step №793: loss = 28.46875, weights = tensor([6.0703, 2.5840], dtype=torch.float16)\n",
      "step №794: loss = 28.453125, weights = tensor([6.0703, 2.5859], dtype=torch.float16)\n",
      "step №795: loss = 28.421875, weights = tensor([6.0703, 2.5879], dtype=torch.float16)\n",
      "step №796: loss = 28.421875, weights = tensor([6.0703, 2.5898], dtype=torch.float16)\n",
      "step №797: loss = 28.40625, weights = tensor([6.0703, 2.5918], dtype=torch.float16)\n",
      "step №798: loss = 28.40625, weights = tensor([6.0703, 2.5938], dtype=torch.float16)\n",
      "step №799: loss = 28.40625, weights = tensor([6.0703, 2.5957], dtype=torch.float16)\n",
      "step №800: loss = 28.40625, weights = tensor([6.0703, 2.5977], dtype=torch.float16)\n",
      "step №801: loss = 28.40625, weights = tensor([6.0703, 2.5996], dtype=torch.float16)\n",
      "step №802: loss = 28.34375, weights = tensor([6.0703, 2.6016], dtype=torch.float16)\n",
      "step №803: loss = 28.34375, weights = tensor([6.0703, 2.6035], dtype=torch.float16)\n",
      "step №804: loss = 28.328125, weights = tensor([6.0703, 2.6055], dtype=torch.float16)\n",
      "step №805: loss = 28.296875, weights = tensor([6.0703, 2.6074], dtype=torch.float16)\n",
      "step №806: loss = 28.296875, weights = tensor([6.0703, 2.6094], dtype=torch.float16)\n",
      "step №807: loss = 28.34375, weights = tensor([6.0664, 2.6113], dtype=torch.float16)\n",
      "step №808: loss = 28.34375, weights = tensor([6.0664, 2.6133], dtype=torch.float16)\n",
      "step №809: loss = 28.328125, weights = tensor([6.0664, 2.6152], dtype=torch.float16)\n",
      "step №810: loss = 28.328125, weights = tensor([6.0664, 2.6172], dtype=torch.float16)\n",
      "step №811: loss = 28.328125, weights = tensor([6.0664, 2.6191], dtype=torch.float16)\n",
      "step №812: loss = 28.28125, weights = tensor([6.0664, 2.6211], dtype=torch.float16)\n",
      "step №813: loss = 28.28125, weights = tensor([6.0664, 2.6230], dtype=torch.float16)\n",
      "step №814: loss = 28.25, weights = tensor([6.0664, 2.6250], dtype=torch.float16)\n",
      "step №815: loss = 28.25, weights = tensor([6.0664, 2.6270], dtype=torch.float16)\n",
      "step №816: loss = 28.21875, weights = tensor([6.0664, 2.6289], dtype=torch.float16)\n",
      "step №817: loss = 28.21875, weights = tensor([6.0664, 2.6309], dtype=torch.float16)\n",
      "step №818: loss = 28.21875, weights = tensor([6.0664, 2.6328], dtype=torch.float16)\n",
      "step №819: loss = 28.171875, weights = tensor([6.0664, 2.6348], dtype=torch.float16)\n",
      "step №820: loss = 28.171875, weights = tensor([6.0664, 2.6367], dtype=torch.float16)\n",
      "step №821: loss = 28.171875, weights = tensor([6.0664, 2.6387], dtype=torch.float16)\n",
      "step №822: loss = 28.15625, weights = tensor([6.0664, 2.6406], dtype=torch.float16)\n",
      "step №823: loss = 28.203125, weights = tensor([6.0625, 2.6426], dtype=torch.float16)\n",
      "step №824: loss = 28.203125, weights = tensor([6.0625, 2.6445], dtype=torch.float16)\n",
      "step №825: loss = 28.203125, weights = tensor([6.0625, 2.6465], dtype=torch.float16)\n",
      "step №826: loss = 28.171875, weights = tensor([6.0625, 2.6484], dtype=torch.float16)\n",
      "step №827: loss = 28.125, weights = tensor([6.0625, 2.6504], dtype=torch.float16)\n",
      "step №828: loss = 28.125, weights = tensor([6.0625, 2.6523], dtype=torch.float16)\n",
      "step №829: loss = 28.09375, weights = tensor([6.0625, 2.6543], dtype=torch.float16)\n",
      "step №830: loss = 28.09375, weights = tensor([6.0625, 2.6562], dtype=torch.float16)\n",
      "step №831: loss = 28.09375, weights = tensor([6.0625, 2.6582], dtype=torch.float16)\n",
      "step №832: loss = 28.09375, weights = tensor([6.0625, 2.6602], dtype=torch.float16)\n",
      "step №833: loss = 28.09375, weights = tensor([6.0625, 2.6621], dtype=torch.float16)\n",
      "step №834: loss = 28.078125, weights = tensor([6.0625, 2.6641], dtype=torch.float16)\n",
      "step №835: loss = 28.078125, weights = tensor([6.0625, 2.6660], dtype=torch.float16)\n",
      "step №836: loss = 28.046875, weights = tensor([6.0625, 2.6680], dtype=torch.float16)\n",
      "step №837: loss = 28.03125, weights = tensor([6.0625, 2.6699], dtype=torch.float16)\n",
      "step №838: loss = 28.03125, weights = tensor([6.0625, 2.6719], dtype=torch.float16)\n",
      "step №839: loss = 28.046875, weights = tensor([6.0586, 2.6738], dtype=torch.float16)\n",
      "step №840: loss = 28.046875, weights = tensor([6.0586, 2.6758], dtype=torch.float16)\n",
      "step №841: loss = 28.046875, weights = tensor([6.0586, 2.6777], dtype=torch.float16)\n",
      "step №842: loss = 28.03125, weights = tensor([6.0586, 2.6797], dtype=torch.float16)\n",
      "step №843: loss = 28.03125, weights = tensor([6.0586, 2.6816], dtype=torch.float16)\n",
      "step №844: loss = 28.0, weights = tensor([6.0586, 2.6836], dtype=torch.float16)\n",
      "step №845: loss = 27.96875, weights = tensor([6.0586, 2.6855], dtype=torch.float16)\n",
      "step №846: loss = 27.96875, weights = tensor([6.0586, 2.6875], dtype=torch.float16)\n",
      "step №847: loss = 27.953125, weights = tensor([6.0547, 2.6895], dtype=torch.float16)\n",
      "step №848: loss = 27.96875, weights = tensor([6.0547, 2.6914], dtype=torch.float16)\n",
      "step №849: loss = 27.96875, weights = tensor([6.0547, 2.6934], dtype=torch.float16)\n",
      "step №850: loss = 27.953125, weights = tensor([6.0547, 2.6953], dtype=torch.float16)\n",
      "step №851: loss = 27.953125, weights = tensor([6.0547, 2.6973], dtype=torch.float16)\n",
      "step №852: loss = 27.90625, weights = tensor([6.0547, 2.6992], dtype=torch.float16)\n",
      "step №853: loss = 27.875, weights = tensor([6.0547, 2.7012], dtype=torch.float16)\n",
      "step №854: loss = 27.875, weights = tensor([6.0547, 2.7031], dtype=torch.float16)\n",
      "step №855: loss = 27.84375, weights = tensor([6.0547, 2.7051], dtype=torch.float16)\n",
      "step №856: loss = 27.90625, weights = tensor([6.0508, 2.7070], dtype=torch.float16)\n",
      "step №857: loss = 27.90625, weights = tensor([6.0508, 2.7090], dtype=torch.float16)\n",
      "step №858: loss = 27.875, weights = tensor([6.0508, 2.7109], dtype=torch.float16)\n",
      "step №859: loss = 27.875, weights = tensor([6.0508, 2.7129], dtype=torch.float16)\n",
      "step №860: loss = 27.84375, weights = tensor([6.0508, 2.7148], dtype=torch.float16)\n",
      "step №861: loss = 27.828125, weights = tensor([6.0508, 2.7168], dtype=torch.float16)\n",
      "step №862: loss = 27.828125, weights = tensor([6.0508, 2.7188], dtype=torch.float16)\n",
      "step №863: loss = 27.828125, weights = tensor([6.0508, 2.7207], dtype=torch.float16)\n",
      "step №864: loss = 27.828125, weights = tensor([6.0508, 2.7227], dtype=torch.float16)\n",
      "step №865: loss = 27.796875, weights = tensor([6.0508, 2.7246], dtype=torch.float16)\n",
      "step №866: loss = 27.796875, weights = tensor([6.0508, 2.7266], dtype=torch.float16)\n",
      "step №867: loss = 27.78125, weights = tensor([6.0508, 2.7285], dtype=torch.float16)\n",
      "step №868: loss = 27.78125, weights = tensor([6.0508, 2.7305], dtype=torch.float16)\n",
      "step №869: loss = 27.75, weights = tensor([6.0508, 2.7324], dtype=torch.float16)\n",
      "step №870: loss = 27.75, weights = tensor([6.0508, 2.7344], dtype=torch.float16)\n",
      "step №871: loss = 27.71875, weights = tensor([6.0508, 2.7363], dtype=torch.float16)\n",
      "step №872: loss = 27.78125, weights = tensor([6.0469, 2.7383], dtype=torch.float16)\n",
      "step №873: loss = 27.78125, weights = tensor([6.0469, 2.7402], dtype=torch.float16)\n",
      "step №874: loss = 27.75, weights = tensor([6.0469, 2.7422], dtype=torch.float16)\n",
      "step №875: loss = 27.75, weights = tensor([6.0469, 2.7441], dtype=torch.float16)\n",
      "step №876: loss = 27.71875, weights = tensor([6.0469, 2.7461], dtype=torch.float16)\n",
      "step №877: loss = 27.703125, weights = tensor([6.0469, 2.7480], dtype=torch.float16)\n",
      "step №878: loss = 27.703125, weights = tensor([6.0469, 2.7500], dtype=torch.float16)\n",
      "step №879: loss = 27.671875, weights = tensor([6.0469, 2.7520], dtype=torch.float16)\n",
      "step №880: loss = 27.671875, weights = tensor([6.0469, 2.7539], dtype=torch.float16)\n",
      "step №881: loss = 27.671875, weights = tensor([6.0469, 2.7559], dtype=torch.float16)\n",
      "step №882: loss = 27.65625, weights = tensor([6.0469, 2.7578], dtype=torch.float16)\n",
      "step №883: loss = 27.625, weights = tensor([6.0469, 2.7598], dtype=torch.float16)\n",
      "step №884: loss = 27.625, weights = tensor([6.0469, 2.7617], dtype=torch.float16)\n",
      "step №885: loss = 27.59375, weights = tensor([6.0469, 2.7637], dtype=torch.float16)\n",
      "step №886: loss = 27.59375, weights = tensor([6.0469, 2.7656], dtype=torch.float16)\n",
      "step №887: loss = 27.59375, weights = tensor([6.0469, 2.7676], dtype=torch.float16)\n",
      "step №888: loss = 27.65625, weights = tensor([6.0430, 2.7695], dtype=torch.float16)\n",
      "step №889: loss = 27.578125, weights = tensor([6.0430, 2.7715], dtype=torch.float16)\n",
      "step №890: loss = 27.546875, weights = tensor([6.0430, 2.7734], dtype=torch.float16)\n",
      "step №891: loss = 27.53125, weights = tensor([6.0430, 2.7754], dtype=torch.float16)\n",
      "step №892: loss = 27.53125, weights = tensor([6.0430, 2.7773], dtype=torch.float16)\n",
      "step №893: loss = 27.5, weights = tensor([6.0430, 2.7793], dtype=torch.float16)\n",
      "step №894: loss = 27.5, weights = tensor([6.0430, 2.7812], dtype=torch.float16)\n",
      "step №895: loss = 27.5, weights = tensor([6.0430, 2.7832], dtype=torch.float16)\n",
      "step №896: loss = 27.5, weights = tensor([6.0430, 2.7852], dtype=torch.float16)\n",
      "step №897: loss = 27.46875, weights = tensor([6.0430, 2.7871], dtype=torch.float16)\n",
      "step №898: loss = 27.46875, weights = tensor([6.0430, 2.7891], dtype=torch.float16)\n",
      "step №899: loss = 27.453125, weights = tensor([6.0430, 2.7910], dtype=torch.float16)\n",
      "step №900: loss = 27.421875, weights = tensor([6.0430, 2.7930], dtype=torch.float16)\n",
      "step №901: loss = 27.421875, weights = tensor([6.0430, 2.7949], dtype=torch.float16)\n",
      "step №902: loss = 27.40625, weights = tensor([6.0430, 2.7969], dtype=torch.float16)\n",
      "step №903: loss = 27.421875, weights = tensor([6.0391, 2.7988], dtype=torch.float16)\n",
      "step №904: loss = 27.453125, weights = tensor([6.0391, 2.8008], dtype=torch.float16)\n",
      "step №905: loss = 27.453125, weights = tensor([6.0391, 2.8027], dtype=torch.float16)\n",
      "step №906: loss = 27.40625, weights = tensor([6.0391, 2.8047], dtype=torch.float16)\n",
      "step №907: loss = 27.375, weights = tensor([6.0352, 2.8066], dtype=torch.float16)\n",
      "step №908: loss = 27.40625, weights = tensor([6.0352, 2.8086], dtype=torch.float16)\n",
      "step №909: loss = 27.375, weights = tensor([6.0352, 2.8105], dtype=torch.float16)\n",
      "step №910: loss = 27.375, weights = tensor([6.0352, 2.8125], dtype=torch.float16)\n",
      "step №911: loss = 27.34375, weights = tensor([6.0352, 2.8145], dtype=torch.float16)\n",
      "step №912: loss = 27.34375, weights = tensor([6.0352, 2.8164], dtype=torch.float16)\n",
      "step №913: loss = 27.34375, weights = tensor([6.0352, 2.8184], dtype=torch.float16)\n",
      "step №914: loss = 27.328125, weights = tensor([6.0352, 2.8203], dtype=torch.float16)\n",
      "step №915: loss = 27.296875, weights = tensor([6.0352, 2.8223], dtype=torch.float16)\n",
      "step №916: loss = 27.296875, weights = tensor([6.0352, 2.8242], dtype=torch.float16)\n",
      "step №917: loss = 27.28125, weights = tensor([6.0352, 2.8262], dtype=torch.float16)\n",
      "step №918: loss = 27.25, weights = tensor([6.0352, 2.8281], dtype=torch.float16)\n",
      "step №919: loss = 27.296875, weights = tensor([6.0352, 2.8301], dtype=torch.float16)\n",
      "step №920: loss = 27.296875, weights = tensor([6.0312, 2.8320], dtype=torch.float16)\n",
      "step №921: loss = 27.296875, weights = tensor([6.0312, 2.8340], dtype=torch.float16)\n",
      "step №922: loss = 27.28125, weights = tensor([6.0312, 2.8359], dtype=torch.float16)\n",
      "step №923: loss = 27.25, weights = tensor([6.0312, 2.8379], dtype=torch.float16)\n",
      "step №924: loss = 27.25, weights = tensor([6.0312, 2.8398], dtype=torch.float16)\n",
      "step №925: loss = 27.21875, weights = tensor([6.0312, 2.8418], dtype=torch.float16)\n",
      "step №926: loss = 27.21875, weights = tensor([6.0312, 2.8438], dtype=torch.float16)\n",
      "step №927: loss = 27.21875, weights = tensor([6.0312, 2.8457], dtype=torch.float16)\n",
      "step №928: loss = 27.21875, weights = tensor([6.0312, 2.8477], dtype=torch.float16)\n",
      "step №929: loss = 27.21875, weights = tensor([6.0312, 2.8496], dtype=torch.float16)\n",
      "step №930: loss = 27.203125, weights = tensor([6.0312, 2.8516], dtype=torch.float16)\n",
      "step №931: loss = 27.203125, weights = tensor([6.0312, 2.8535], dtype=torch.float16)\n",
      "step №932: loss = 27.15625, weights = tensor([6.0312, 2.8555], dtype=torch.float16)\n",
      "step №933: loss = 27.125, weights = tensor([6.0312, 2.8574], dtype=torch.float16)\n",
      "step №934: loss = 27.125, weights = tensor([6.0312, 2.8594], dtype=torch.float16)\n",
      "step №935: loss = 27.171875, weights = tensor([6.0312, 2.8613], dtype=torch.float16)\n",
      "step №936: loss = 27.15625, weights = tensor([6.0273, 2.8633], dtype=torch.float16)\n",
      "step №937: loss = 27.15625, weights = tensor([6.0273, 2.8652], dtype=torch.float16)\n",
      "step №938: loss = 27.15625, weights = tensor([6.0273, 2.8672], dtype=torch.float16)\n",
      "step №939: loss = 27.15625, weights = tensor([6.0273, 2.8691], dtype=torch.float16)\n",
      "step №940: loss = 27.125, weights = tensor([6.0273, 2.8711], dtype=torch.float16)\n",
      "step №941: loss = 27.09375, weights = tensor([6.0273, 2.8730], dtype=torch.float16)\n",
      "step №942: loss = 27.09375, weights = tensor([6.0273, 2.8750], dtype=torch.float16)\n",
      "step №943: loss = 27.078125, weights = tensor([6.0273, 2.8770], dtype=torch.float16)\n",
      "step №944: loss = 27.078125, weights = tensor([6.0273, 2.8789], dtype=torch.float16)\n",
      "step №945: loss = 27.046875, weights = tensor([6.0273, 2.8809], dtype=torch.float16)\n",
      "step №946: loss = 27.046875, weights = tensor([6.0273, 2.8828], dtype=torch.float16)\n",
      "step №947: loss = 27.03125, weights = tensor([6.0273, 2.8848], dtype=torch.float16)\n",
      "step №948: loss = 27.03125, weights = tensor([6.0273, 2.8867], dtype=torch.float16)\n",
      "step №949: loss = 27.0, weights = tensor([6.0273, 2.8887], dtype=torch.float16)\n",
      "step №950: loss = 26.96875, weights = tensor([6.0273, 2.8906], dtype=torch.float16)\n",
      "step №951: loss = 27.03125, weights = tensor([6.0234, 2.8926], dtype=torch.float16)\n",
      "step №952: loss = 27.03125, weights = tensor([6.0234, 2.8945], dtype=torch.float16)\n",
      "step №953: loss = 27.03125, weights = tensor([6.0234, 2.8965], dtype=torch.float16)\n",
      "step №954: loss = 27.0, weights = tensor([6.0234, 2.8984], dtype=torch.float16)\n",
      "step №955: loss = 26.96875, weights = tensor([6.0234, 2.9004], dtype=torch.float16)\n",
      "step №956: loss = 26.96875, weights = tensor([6.0234, 2.9023], dtype=torch.float16)\n",
      "step №957: loss = 26.953125, weights = tensor([6.0234, 2.9043], dtype=torch.float16)\n",
      "step №958: loss = 26.953125, weights = tensor([6.0234, 2.9062], dtype=torch.float16)\n",
      "step №959: loss = 26.953125, weights = tensor([6.0234, 2.9082], dtype=torch.float16)\n",
      "step №960: loss = 26.953125, weights = tensor([6.0234, 2.9102], dtype=torch.float16)\n",
      "step №961: loss = 26.953125, weights = tensor([6.0234, 2.9121], dtype=torch.float16)\n",
      "step №962: loss = 26.921875, weights = tensor([6.0234, 2.9141], dtype=torch.float16)\n",
      "step №963: loss = 26.921875, weights = tensor([6.0234, 2.9160], dtype=torch.float16)\n",
      "step №964: loss = 26.875, weights = tensor([6.0234, 2.9180], dtype=torch.float16)\n",
      "step №965: loss = 26.84375, weights = tensor([6.0234, 2.9199], dtype=torch.float16)\n",
      "step №966: loss = 26.84375, weights = tensor([6.0234, 2.9219], dtype=torch.float16)\n",
      "step №967: loss = 26.875, weights = tensor([6.0195, 2.9238], dtype=torch.float16)\n",
      "step №968: loss = 26.90625, weights = tensor([6.0156, 2.9258], dtype=torch.float16)\n",
      "step №969: loss = 26.84375, weights = tensor([6.0156, 2.9277], dtype=torch.float16)\n",
      "step №970: loss = 26.828125, weights = tensor([6.0156, 2.9297], dtype=torch.float16)\n",
      "step №971: loss = 26.828125, weights = tensor([6.0156, 2.9316], dtype=torch.float16)\n",
      "step №972: loss = 26.796875, weights = tensor([6.0156, 2.9336], dtype=torch.float16)\n",
      "step №973: loss = 26.78125, weights = tensor([6.0156, 2.9355], dtype=torch.float16)\n",
      "step №974: loss = 26.78125, weights = tensor([6.0156, 2.9375], dtype=torch.float16)\n",
      "step №975: loss = 26.78125, weights = tensor([6.0156, 2.9395], dtype=torch.float16)\n",
      "step №976: loss = 26.75, weights = tensor([6.0156, 2.9414], dtype=torch.float16)\n",
      "step №977: loss = 26.75, weights = tensor([6.0156, 2.9434], dtype=torch.float16)\n",
      "step №978: loss = 26.71875, weights = tensor([6.0156, 2.9453], dtype=torch.float16)\n",
      "step №979: loss = 26.703125, weights = tensor([6.0156, 2.9473], dtype=torch.float16)\n",
      "step №980: loss = 26.703125, weights = tensor([6.0156, 2.9492], dtype=torch.float16)\n",
      "step №981: loss = 26.671875, weights = tensor([6.0156, 2.9512], dtype=torch.float16)\n",
      "step №982: loss = 26.671875, weights = tensor([6.0156, 2.9531], dtype=torch.float16)\n",
      "step №983: loss = 26.671875, weights = tensor([6.0156, 2.9551], dtype=torch.float16)\n",
      "step №984: loss = 26.703125, weights = tensor([6.0117, 2.9570], dtype=torch.float16)\n",
      "step №985: loss = 26.703125, weights = tensor([6.0117, 2.9590], dtype=torch.float16)\n",
      "step №986: loss = 26.671875, weights = tensor([6.0117, 2.9609], dtype=torch.float16)\n",
      "step №987: loss = 26.671875, weights = tensor([6.0117, 2.9629], dtype=torch.float16)\n",
      "step №988: loss = 26.65625, weights = tensor([6.0117, 2.9648], dtype=torch.float16)\n",
      "step №989: loss = 26.625, weights = tensor([6.0117, 2.9668], dtype=torch.float16)\n",
      "step №990: loss = 26.625, weights = tensor([6.0117, 2.9688], dtype=torch.float16)\n",
      "step №991: loss = 26.625, weights = tensor([6.0117, 2.9707], dtype=torch.float16)\n",
      "step №992: loss = 26.625, weights = tensor([6.0117, 2.9727], dtype=torch.float16)\n",
      "step №993: loss = 26.625, weights = tensor([6.0117, 2.9746], dtype=torch.float16)\n",
      "step №994: loss = 26.59375, weights = tensor([6.0117, 2.9766], dtype=torch.float16)\n",
      "step №995: loss = 26.578125, weights = tensor([6.0117, 2.9785], dtype=torch.float16)\n",
      "step №996: loss = 26.578125, weights = tensor([6.0117, 2.9805], dtype=torch.float16)\n",
      "step №997: loss = 26.546875, weights = tensor([6.0117, 2.9824], dtype=torch.float16)\n",
      "step №998: loss = 26.546875, weights = tensor([6.0117, 2.9844], dtype=torch.float16)\n",
      "step №999: loss = 26.53125, weights = tensor([6.0117, 2.9863], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#first let's train our model with a stopping criteria as numbers of steps \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d61e4419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "98c4e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 293.5\n",
      "did one more step, loss reduced by 126.875\n",
      "did one more step, loss reduced by 54.875\n",
      "did one more step, loss reduced by 23.6875\n",
      "did one more step, loss reduced by 10.34375\n",
      "did one more step, loss reduced by 4.375\n",
      "did one more step, loss reduced by 1.875\n",
      "did one more step, loss reduced by 0.9375\n",
      "did one more step, loss reduced by 0.34375\n",
      "did one more step, loss reduced by 0.125\n",
      "did one more step, loss reduced by 0.125\n",
      "did one more step, loss reduced by 0.03125\n",
      "did one more step, loss reduced by 0.03125\n",
      "did one more step, loss reduced by 0.0625\n",
      "did one more step, loss reduced by 0.03125\n",
      "did one more step, loss reduced by 0.0625\n",
      "did one more step, loss reduced by 0.03125\n",
      "did one more step, loss reduced by -0.03125\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#now stopping criteria is difference between errors  \n",
    "learning_rait = 0.0006 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "y_pred = predict(w, x)\n",
    "previous_error = mseerror(y, y_pred)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "error = mseerror(y, y_pred)\n",
    "y_pred = predict(w, x)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "previous_error = error\n",
    "error = mseerror(y, y_pred)\n",
    "while previous_error - error > 0.0000000005:\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    previous_error = error \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'did one more step, loss reduced by {previous_error - error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c9e7ae96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "83a08790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1238.0, weights = tensor([0.2213, 0.0387], dtype=torch.float16, requires_grad=True)\n",
      "step №1: loss = 1155.0, weights = tensor([0.4348, 0.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №2: loss = 1078.0, weights = tensor([0.6411, 0.1125], dtype=torch.float16, requires_grad=True)\n",
      "step №3: loss = 1007.0, weights = tensor([0.8398, 0.1477], dtype=torch.float16, requires_grad=True)\n",
      "step №4: loss = 940.0, weights = tensor([1.0312, 0.1818], dtype=torch.float16, requires_grad=True)\n",
      "step №5: loss = 877.5, weights = tensor([1.2168, 0.2147], dtype=torch.float16, requires_grad=True)\n",
      "step №6: loss = 820.0, weights = tensor([1.3955, 0.2466], dtype=torch.float16, requires_grad=True)\n",
      "step №7: loss = 766.0, weights = tensor([1.5684, 0.2776], dtype=torch.float16, requires_grad=True)\n",
      "step №8: loss = 715.5, weights = tensor([1.7344, 0.3076], dtype=torch.float16, requires_grad=True)\n",
      "step №9: loss = 669.0, weights = tensor([1.8945, 0.3367], dtype=torch.float16, requires_grad=True)\n",
      "step №10: loss = 625.5, weights = tensor([2.0488, 0.3647], dtype=torch.float16, requires_grad=True)\n",
      "step №11: loss = 585.0, weights = tensor([2.1992, 0.3921], dtype=torch.float16, requires_grad=True)\n",
      "step №12: loss = 547.0, weights = tensor([2.3438, 0.4185], dtype=torch.float16, requires_grad=True)\n",
      "step №13: loss = 511.5, weights = tensor([2.4824, 0.4441], dtype=torch.float16, requires_grad=True)\n",
      "step №14: loss = 479.25, weights = tensor([2.6172, 0.4690], dtype=torch.float16, requires_grad=True)\n",
      "step №15: loss = 448.5, weights = tensor([2.7461, 0.4932], dtype=torch.float16, requires_grad=True)\n",
      "step №16: loss = 420.0, weights = tensor([2.8711, 0.5166], dtype=torch.float16, requires_grad=True)\n",
      "step №17: loss = 393.75, weights = tensor([2.9922, 0.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №18: loss = 369.0, weights = tensor([3.1094, 0.5610], dtype=torch.float16, requires_grad=True)\n",
      "step №19: loss = 345.75, weights = tensor([3.2207, 0.5825], dtype=torch.float16, requires_grad=True)\n",
      "step №20: loss = 324.5, weights = tensor([3.3281, 0.6030], dtype=torch.float16, requires_grad=True)\n",
      "step №21: loss = 304.75, weights = tensor([3.4316, 0.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №22: loss = 286.5, weights = tensor([3.5332, 0.6426], dtype=torch.float16, requires_grad=True)\n",
      "step №23: loss = 269.25, weights = tensor([3.6309, 0.6616], dtype=torch.float16, requires_grad=True)\n",
      "step №24: loss = 253.0, weights = tensor([3.7246, 0.6802], dtype=torch.float16, requires_grad=True)\n",
      "step №25: loss = 237.75, weights = tensor([3.8145, 0.6982], dtype=torch.float16, requires_grad=True)\n",
      "step №26: loss = 224.25, weights = tensor([3.9023, 0.7153], dtype=torch.float16, requires_grad=True)\n",
      "step №27: loss = 211.25, weights = tensor([3.9863, 0.7319], dtype=torch.float16, requires_grad=True)\n",
      "step №28: loss = 199.25, weights = tensor([4.0664, 0.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №29: loss = 188.0, weights = tensor([4.1445, 0.7642], dtype=torch.float16, requires_grad=True)\n",
      "step №30: loss = 177.625, weights = tensor([4.2188, 0.7798], dtype=torch.float16, requires_grad=True)\n",
      "step №31: loss = 168.0, weights = tensor([4.2930, 0.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №32: loss = 158.875, weights = tensor([4.3633, 0.8096], dtype=torch.float16, requires_grad=True)\n",
      "step №33: loss = 150.25, weights = tensor([4.4297, 0.8237], dtype=torch.float16, requires_grad=True)\n",
      "step №34: loss = 142.625, weights = tensor([4.4961, 0.8374], dtype=torch.float16, requires_grad=True)\n",
      "step №35: loss = 135.125, weights = tensor([4.5586, 0.8511], dtype=torch.float16, requires_grad=True)\n",
      "step №36: loss = 128.5, weights = tensor([4.6211, 0.8643], dtype=torch.float16, requires_grad=True)\n",
      "step №37: loss = 121.8125, weights = tensor([4.6797, 0.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №38: loss = 116.0, weights = tensor([4.7383, 0.8892], dtype=torch.float16, requires_grad=True)\n",
      "step №39: loss = 110.3125, weights = tensor([4.7930, 0.9014], dtype=torch.float16, requires_grad=True)\n",
      "step №40: loss = 105.125, weights = tensor([4.8438, 0.9131], dtype=torch.float16, requires_grad=True)\n",
      "step №41: loss = 100.625, weights = tensor([4.8945, 0.9248], dtype=torch.float16, requires_grad=True)\n",
      "step №42: loss = 96.0, weights = tensor([4.9453, 0.9360], dtype=torch.float16, requires_grad=True)\n",
      "step №43: loss = 91.75, weights = tensor([4.9922, 0.9468], dtype=torch.float16, requires_grad=True)\n",
      "step №44: loss = 87.9375, weights = tensor([5.0391, 0.9575], dtype=torch.float16, requires_grad=True)\n",
      "step №45: loss = 84.1875, weights = tensor([5.0820, 0.9678], dtype=torch.float16, requires_grad=True)\n",
      "step №46: loss = 80.9375, weights = tensor([5.1250, 0.9780], dtype=torch.float16, requires_grad=True)\n",
      "step №47: loss = 77.8125, weights = tensor([5.1641, 0.9878], dtype=torch.float16, requires_grad=True)\n",
      "step №48: loss = 75.0, weights = tensor([5.2031, 0.9976], dtype=torch.float16, requires_grad=True)\n",
      "step №49: loss = 72.3125, weights = tensor([5.2422, 1.0068], dtype=torch.float16, requires_grad=True)\n",
      "step №50: loss = 69.8125, weights = tensor([5.2773, 1.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №51: loss = 67.5625, weights = tensor([5.3125, 1.0244], dtype=torch.float16, requires_grad=True)\n",
      "step №52: loss = 65.25, weights = tensor([5.3477, 1.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №53: loss = 63.25, weights = tensor([5.3789, 1.0420], dtype=torch.float16, requires_grad=True)\n",
      "step №54: loss = 61.4375, weights = tensor([5.4102, 1.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №55: loss = 59.59375, weights = tensor([5.4414, 1.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №56: loss = 57.90625, weights = tensor([5.4727, 1.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №57: loss = 56.34375, weights = tensor([5.5000, 1.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №58: loss = 54.9375, weights = tensor([5.5273, 1.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №59: loss = 53.65625, weights = tensor([5.5547, 1.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №60: loss = 52.3125, weights = tensor([5.5820, 1.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №61: loss = 51.125, weights = tensor([5.6055, 1.1045], dtype=torch.float16, requires_grad=True)\n",
      "step №62: loss = 50.0625, weights = tensor([5.6289, 1.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №63: loss = 49.03125, weights = tensor([5.6523, 1.1182], dtype=torch.float16, requires_grad=True)\n",
      "step №64: loss = 48.0625, weights = tensor([5.6758, 1.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №65: loss = 47.1875, weights = tensor([5.6953, 1.1318], dtype=torch.float16, requires_grad=True)\n",
      "step №66: loss = 46.4375, weights = tensor([5.7148, 1.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №67: loss = 45.6875, weights = tensor([5.7344, 1.1455], dtype=torch.float16, requires_grad=True)\n",
      "step №68: loss = 44.96875, weights = tensor([5.7539, 1.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №69: loss = 44.25, weights = tensor([5.7734, 1.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №70: loss = 43.6875, weights = tensor([5.7930, 1.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №71: loss = 43.0, weights = tensor([5.8086, 1.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №72: loss = 42.53125, weights = tensor([5.8242, 1.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №73: loss = 42.0, weights = tensor([5.8398, 1.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №74: loss = 41.5625, weights = tensor([5.8555, 1.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №75: loss = 41.0625, weights = tensor([5.8711, 1.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №76: loss = 40.6875, weights = tensor([5.8867, 1.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №77: loss = 40.21875, weights = tensor([5.9023, 1.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №78: loss = 39.84375, weights = tensor([5.9141, 1.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №79: loss = 39.46875, weights = tensor([5.9258, 1.2158], dtype=torch.float16, requires_grad=True)\n",
      "step №80: loss = 39.25, weights = tensor([5.9375, 1.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №81: loss = 39.0, weights = tensor([5.9492, 1.2256], dtype=torch.float16, requires_grad=True)\n",
      "step №82: loss = 38.6875, weights = tensor([5.9609, 1.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №83: loss = 38.4375, weights = tensor([5.9727, 1.2354], dtype=torch.float16, requires_grad=True)\n",
      "step №84: loss = 38.1875, weights = tensor([5.9844, 1.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №85: loss = 37.96875, weights = tensor([5.9961, 1.2451], dtype=torch.float16, requires_grad=True)\n",
      "step №86: loss = 37.71875, weights = tensor([6.0039, 1.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №87: loss = 37.53125, weights = tensor([6.0117, 1.2549], dtype=torch.float16, requires_grad=True)\n",
      "step №88: loss = 37.375, weights = tensor([6.0195, 1.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №89: loss = 37.21875, weights = tensor([6.0273, 1.2646], dtype=torch.float16, requires_grad=True)\n",
      "step №90: loss = 37.0625, weights = tensor([6.0352, 1.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №91: loss = 36.9375, weights = tensor([6.0430, 1.2744], dtype=torch.float16, requires_grad=True)\n",
      "step №92: loss = 36.75, weights = tensor([6.0508, 1.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №93: loss = 36.6875, weights = tensor([6.0586, 1.2842], dtype=torch.float16, requires_grad=True)\n",
      "step №94: loss = 36.5625, weights = tensor([6.0664, 1.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №95: loss = 36.34375, weights = tensor([6.0742, 1.2939], dtype=torch.float16, requires_grad=True)\n",
      "step №96: loss = 36.21875, weights = tensor([6.0820, 1.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №97: loss = 36.1875, weights = tensor([6.0898, 1.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №98: loss = 36.0625, weights = tensor([6.0977, 1.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №99: loss = 35.90625, weights = tensor([6.1055, 1.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №100: loss = 35.8125, weights = tensor([6.1094, 1.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №101: loss = 35.8125, weights = tensor([6.1133, 1.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №102: loss = 35.71875, weights = tensor([6.1172, 1.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №103: loss = 35.625, weights = tensor([6.1211, 1.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №104: loss = 35.5625, weights = tensor([6.1250, 1.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №105: loss = 35.5625, weights = tensor([6.1289, 1.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №106: loss = 35.46875, weights = tensor([6.1328, 1.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №107: loss = 35.375, weights = tensor([6.1367, 1.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №108: loss = 35.3125, weights = tensor([6.1406, 1.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №109: loss = 35.25, weights = tensor([6.1445, 1.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №110: loss = 35.25, weights = tensor([6.1484, 1.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №111: loss = 35.1875, weights = tensor([6.1523, 1.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №112: loss = 35.125, weights = tensor([6.1562, 1.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №113: loss = 35.09375, weights = tensor([6.1602, 1.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №114: loss = 35.0625, weights = tensor([6.1641, 1.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №115: loss = 34.96875, weights = tensor([6.1680, 1.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №116: loss = 34.90625, weights = tensor([6.1719, 1.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №117: loss = 34.9375, weights = tensor([6.1758, 1.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №118: loss = 34.875, weights = tensor([6.1797, 1.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №119: loss = 34.8125, weights = tensor([6.1836, 1.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №120: loss = 34.75, weights = tensor([6.1875, 1.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №121: loss = 34.75, weights = tensor([6.1914, 1.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №122: loss = 34.6875, weights = tensor([6.1953, 1.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №123: loss = 34.65625, weights = tensor([6.1992, 1.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №124: loss = 34.5625, weights = tensor([6.1992, 1.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №125: loss = 34.5625, weights = tensor([6.1992, 1.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №126: loss = 34.5625, weights = tensor([6.1992, 1.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №127: loss = 34.53125, weights = tensor([6.1992, 1.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №128: loss = 34.46875, weights = tensor([6.1992, 1.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №129: loss = 34.5, weights = tensor([6.1992, 1.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №130: loss = 34.46875, weights = tensor([6.1992, 1.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №131: loss = 34.4375, weights = tensor([6.1992, 1.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №132: loss = 34.40625, weights = tensor([6.1992, 1.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №133: loss = 34.40625, weights = tensor([6.1992, 1.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №134: loss = 34.375, weights = tensor([6.1992, 1.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №135: loss = 34.3125, weights = tensor([6.1992, 1.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №136: loss = 34.3125, weights = tensor([6.1992, 1.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №137: loss = 34.3125, weights = tensor([6.1992, 1.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №138: loss = 34.28125, weights = tensor([6.1992, 1.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №139: loss = 34.25, weights = tensor([6.1992, 1.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №140: loss = 34.21875, weights = tensor([6.1992, 1.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №141: loss = 34.21875, weights = tensor([6.1992, 1.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №142: loss = 34.1875, weights = tensor([6.1992, 1.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №143: loss = 34.15625, weights = tensor([6.1992, 1.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №144: loss = 34.125, weights = tensor([6.1992, 1.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №145: loss = 34.125, weights = tensor([6.1992, 1.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №146: loss = 34.09375, weights = tensor([6.1992, 1.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №147: loss = 34.0625, weights = tensor([6.1992, 1.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №148: loss = 34.03125, weights = tensor([6.1992, 1.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №149: loss = 34.03125, weights = tensor([6.1992, 1.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №150: loss = 34.0, weights = tensor([6.1992, 1.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №151: loss = 33.96875, weights = tensor([6.1992, 1.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №152: loss = 33.9375, weights = tensor([6.1992, 1.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №153: loss = 33.9375, weights = tensor([6.1992, 1.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №154: loss = 33.9375, weights = tensor([6.1992, 1.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №155: loss = 33.875, weights = tensor([6.1992, 1.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №156: loss = 33.84375, weights = tensor([6.1992, 1.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №157: loss = 33.84375, weights = tensor([6.1992, 1.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №158: loss = 33.8125, weights = tensor([6.1992, 1.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №159: loss = 33.78125, weights = tensor([6.1992, 1.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №160: loss = 33.75, weights = tensor([6.1992, 1.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №161: loss = 33.78125, weights = tensor([6.1992, 1.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №162: loss = 33.75, weights = tensor([6.1992, 1.5557], dtype=torch.float16, requires_grad=True)\n",
      "step №163: loss = 33.6875, weights = tensor([6.1992, 1.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №164: loss = 33.6875, weights = tensor([6.1992, 1.5615], dtype=torch.float16, requires_grad=True)\n",
      "step №165: loss = 33.6875, weights = tensor([6.1992, 1.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №166: loss = 33.6875, weights = tensor([6.1992, 1.5674], dtype=torch.float16, requires_grad=True)\n",
      "step №167: loss = 33.65625, weights = tensor([6.1992, 1.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №168: loss = 33.625, weights = tensor([6.1992, 1.5732], dtype=torch.float16, requires_grad=True)\n",
      "step №169: loss = 33.59375, weights = tensor([6.1992, 1.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №170: loss = 33.5625, weights = tensor([6.1992, 1.5791], dtype=torch.float16, requires_grad=True)\n",
      "step №171: loss = 33.59375, weights = tensor([6.1992, 1.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №172: loss = 33.59375, weights = tensor([6.1992, 1.5850], dtype=torch.float16, requires_grad=True)\n",
      "step №173: loss = 33.5625, weights = tensor([6.1992, 1.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №174: loss = 33.53125, weights = tensor([6.1992, 1.5908], dtype=torch.float16, requires_grad=True)\n",
      "step №175: loss = 33.5, weights = tensor([6.1992, 1.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №176: loss = 33.5, weights = tensor([6.1992, 1.5967], dtype=torch.float16, requires_grad=True)\n",
      "step №177: loss = 33.5, weights = tensor([6.1992, 1.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №178: loss = 33.4375, weights = tensor([6.1992, 1.6025], dtype=torch.float16, requires_grad=True)\n",
      "step №179: loss = 33.4375, weights = tensor([6.1992, 1.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №180: loss = 33.40625, weights = tensor([6.1992, 1.6084], dtype=torch.float16, requires_grad=True)\n",
      "step №181: loss = 33.40625, weights = tensor([6.1992, 1.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №182: loss = 33.4375, weights = tensor([6.1992, 1.6143], dtype=torch.float16, requires_grad=True)\n",
      "step №183: loss = 33.40625, weights = tensor([6.1992, 1.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №184: loss = 33.375, weights = tensor([6.1992, 1.6201], dtype=torch.float16, requires_grad=True)\n",
      "step №185: loss = 33.34375, weights = tensor([6.1992, 1.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №186: loss = 33.3125, weights = tensor([6.1992, 1.6260], dtype=torch.float16, requires_grad=True)\n",
      "step №187: loss = 33.3125, weights = tensor([6.1992, 1.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №188: loss = 33.3125, weights = tensor([6.1992, 1.6318], dtype=torch.float16, requires_grad=True)\n",
      "step №189: loss = 33.3125, weights = tensor([6.1992, 1.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №190: loss = 33.25, weights = tensor([6.1992, 1.6377], dtype=torch.float16, requires_grad=True)\n",
      "step №191: loss = 33.21875, weights = tensor([6.1992, 1.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №192: loss = 33.25, weights = tensor([6.1992, 1.6436], dtype=torch.float16, requires_grad=True)\n",
      "step №193: loss = 33.25, weights = tensor([6.1992, 1.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №194: loss = 33.21875, weights = tensor([6.1992, 1.6494], dtype=torch.float16, requires_grad=True)\n",
      "step №195: loss = 33.1875, weights = tensor([6.1992, 1.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №196: loss = 33.15625, weights = tensor([6.1992, 1.6553], dtype=torch.float16, requires_grad=True)\n",
      "step №197: loss = 33.15625, weights = tensor([6.1992, 1.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №198: loss = 33.15625, weights = tensor([6.1992, 1.6611], dtype=torch.float16, requires_grad=True)\n",
      "step №199: loss = 33.125, weights = tensor([6.1992, 1.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №200: loss = 33.09375, weights = tensor([6.1992, 1.6670], dtype=torch.float16, requires_grad=True)\n",
      "step №201: loss = 33.0625, weights = tensor([6.1992, 1.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №202: loss = 33.0625, weights = tensor([6.1992, 1.6729], dtype=torch.float16, requires_grad=True)\n",
      "step №203: loss = 33.0625, weights = tensor([6.1992, 1.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №204: loss = 33.0625, weights = tensor([6.1992, 1.6787], dtype=torch.float16, requires_grad=True)\n",
      "step №205: loss = 33.0625, weights = tensor([6.1992, 1.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №206: loss = 33.0, weights = tensor([6.1992, 1.6846], dtype=torch.float16, requires_grad=True)\n",
      "step №207: loss = 32.96875, weights = tensor([6.1992, 1.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №208: loss = 32.96875, weights = tensor([6.1992, 1.6904], dtype=torch.float16, requires_grad=True)\n",
      "step №209: loss = 32.96875, weights = tensor([6.1992, 1.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №210: loss = 32.9375, weights = tensor([6.1992, 1.6963], dtype=torch.float16, requires_grad=True)\n",
      "step №211: loss = 32.9375, weights = tensor([6.1992, 1.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №212: loss = 32.90625, weights = tensor([6.1992, 1.7021], dtype=torch.float16, requires_grad=True)\n",
      "step №213: loss = 32.875, weights = tensor([6.1992, 1.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №214: loss = 32.90625, weights = tensor([6.1992, 1.7080], dtype=torch.float16, requires_grad=True)\n",
      "step №215: loss = 32.875, weights = tensor([6.1992, 1.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №216: loss = 32.84375, weights = tensor([6.1992, 1.7139], dtype=torch.float16, requires_grad=True)\n",
      "step №217: loss = 32.84375, weights = tensor([6.1992, 1.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №218: loss = 32.8125, weights = tensor([6.1992, 1.7197], dtype=torch.float16, requires_grad=True)\n",
      "step №219: loss = 32.8125, weights = tensor([6.1992, 1.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №220: loss = 32.8125, weights = tensor([6.1992, 1.7256], dtype=torch.float16, requires_grad=True)\n",
      "step №221: loss = 32.78125, weights = tensor([6.1992, 1.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №222: loss = 32.71875, weights = tensor([6.1992, 1.7314], dtype=torch.float16, requires_grad=True)\n",
      "step №223: loss = 32.71875, weights = tensor([6.1992, 1.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №224: loss = 32.6875, weights = tensor([6.1992, 1.7373], dtype=torch.float16, requires_grad=True)\n",
      "step №225: loss = 32.75, weights = tensor([6.1992, 1.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №226: loss = 32.6875, weights = tensor([6.1992, 1.7432], dtype=torch.float16, requires_grad=True)\n",
      "step №227: loss = 32.6875, weights = tensor([6.1992, 1.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №228: loss = 32.65625, weights = tensor([6.1992, 1.7490], dtype=torch.float16, requires_grad=True)\n",
      "step №229: loss = 32.65625, weights = tensor([6.1992, 1.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №230: loss = 32.65625, weights = tensor([6.1992, 1.7549], dtype=torch.float16, requires_grad=True)\n",
      "step №231: loss = 32.625, weights = tensor([6.1992, 1.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №232: loss = 32.59375, weights = tensor([6.1992, 1.7607], dtype=torch.float16, requires_grad=True)\n",
      "step №233: loss = 32.5625, weights = tensor([6.1992, 1.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №234: loss = 32.5625, weights = tensor([6.1992, 1.7666], dtype=torch.float16, requires_grad=True)\n",
      "step №235: loss = 32.5625, weights = tensor([6.1992, 1.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №236: loss = 32.5625, weights = tensor([6.1992, 1.7725], dtype=torch.float16, requires_grad=True)\n",
      "step №237: loss = 32.5625, weights = tensor([6.1992, 1.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №238: loss = 32.5, weights = tensor([6.1992, 1.7783], dtype=torch.float16, requires_grad=True)\n",
      "step №239: loss = 32.46875, weights = tensor([6.1992, 1.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №240: loss = 32.46875, weights = tensor([6.1992, 1.7842], dtype=torch.float16, requires_grad=True)\n",
      "step №241: loss = 32.46875, weights = tensor([6.1992, 1.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №242: loss = 32.4375, weights = tensor([6.1992, 1.7900], dtype=torch.float16, requires_grad=True)\n",
      "step №243: loss = 32.40625, weights = tensor([6.1992, 1.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №244: loss = 32.40625, weights = tensor([6.1992, 1.7959], dtype=torch.float16, requires_grad=True)\n",
      "step №245: loss = 32.375, weights = tensor([6.1992, 1.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №246: loss = 32.40625, weights = tensor([6.1992, 1.8018], dtype=torch.float16, requires_grad=True)\n",
      "step №247: loss = 32.375, weights = tensor([6.1992, 1.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №248: loss = 32.34375, weights = tensor([6.1992, 1.8076], dtype=torch.float16, requires_grad=True)\n",
      "step №249: loss = 32.34375, weights = tensor([6.1992, 1.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №250: loss = 32.3125, weights = tensor([6.1992, 1.8135], dtype=torch.float16, requires_grad=True)\n",
      "step №251: loss = 32.3125, weights = tensor([6.1992, 1.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №252: loss = 32.3125, weights = tensor([6.1992, 1.8193], dtype=torch.float16, requires_grad=True)\n",
      "step №253: loss = 32.3125, weights = tensor([6.1992, 1.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №254: loss = 32.25, weights = tensor([6.1992, 1.8252], dtype=torch.float16, requires_grad=True)\n",
      "step №255: loss = 32.21875, weights = tensor([6.1992, 1.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №256: loss = 32.25, weights = tensor([6.1992, 1.8311], dtype=torch.float16, requires_grad=True)\n",
      "step №257: loss = 32.25, weights = tensor([6.1992, 1.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №258: loss = 32.21875, weights = tensor([6.1992, 1.8369], dtype=torch.float16, requires_grad=True)\n",
      "step №259: loss = 32.1875, weights = tensor([6.1992, 1.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №260: loss = 32.1875, weights = tensor([6.1992, 1.8428], dtype=torch.float16, requires_grad=True)\n",
      "step №261: loss = 32.15625, weights = tensor([6.1992, 1.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №262: loss = 32.15625, weights = tensor([6.1992, 1.8486], dtype=torch.float16, requires_grad=True)\n",
      "step №263: loss = 32.125, weights = tensor([6.1992, 1.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №264: loss = 32.09375, weights = tensor([6.1992, 1.8545], dtype=torch.float16, requires_grad=True)\n",
      "step №265: loss = 32.09375, weights = tensor([6.1992, 1.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №266: loss = 32.0625, weights = tensor([6.1992, 1.8604], dtype=torch.float16, requires_grad=True)\n",
      "step №267: loss = 32.09375, weights = tensor([6.1992, 1.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №268: loss = 32.0625, weights = tensor([6.1992, 1.8662], dtype=torch.float16, requires_grad=True)\n",
      "step №269: loss = 32.0625, weights = tensor([6.1992, 1.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №270: loss = 32.03125, weights = tensor([6.1992, 1.8721], dtype=torch.float16, requires_grad=True)\n",
      "step №271: loss = 32.0, weights = tensor([6.1992, 1.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №272: loss = 32.0, weights = tensor([6.1992, 1.8779], dtype=torch.float16, requires_grad=True)\n",
      "step №273: loss = 32.0, weights = tensor([6.1992, 1.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №274: loss = 31.96875, weights = tensor([6.1992, 1.8838], dtype=torch.float16, requires_grad=True)\n",
      "step №275: loss = 31.921875, weights = tensor([6.1992, 1.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №276: loss = 31.921875, weights = tensor([6.1992, 1.8896], dtype=torch.float16, requires_grad=True)\n",
      "step №277: loss = 31.90625, weights = tensor([6.1992, 1.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №278: loss = 31.921875, weights = tensor([6.1992, 1.8955], dtype=torch.float16, requires_grad=True)\n",
      "step №279: loss = 31.90625, weights = tensor([6.1992, 1.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №280: loss = 31.875, weights = tensor([6.1992, 1.9014], dtype=torch.float16, requires_grad=True)\n",
      "step №281: loss = 31.875, weights = tensor([6.1992, 1.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №282: loss = 31.828125, weights = tensor([6.1992, 1.9072], dtype=torch.float16, requires_grad=True)\n",
      "step №283: loss = 31.828125, weights = tensor([6.1992, 1.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №284: loss = 31.828125, weights = tensor([6.1992, 1.9131], dtype=torch.float16, requires_grad=True)\n",
      "step №285: loss = 31.796875, weights = tensor([6.1992, 1.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №286: loss = 31.78125, weights = tensor([6.1992, 1.9189], dtype=torch.float16, requires_grad=True)\n",
      "step №287: loss = 31.75, weights = tensor([6.1992, 1.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №288: loss = 31.71875, weights = tensor([6.1992, 1.9248], dtype=torch.float16, requires_grad=True)\n",
      "step №289: loss = 31.78125, weights = tensor([6.1992, 1.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №290: loss = 31.75, weights = tensor([6.1992, 1.9307], dtype=torch.float16, requires_grad=True)\n",
      "step №291: loss = 31.703125, weights = tensor([6.1992, 1.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №292: loss = 31.703125, weights = tensor([6.1992, 1.9365], dtype=torch.float16, requires_grad=True)\n",
      "step №293: loss = 31.671875, weights = tensor([6.1992, 1.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №294: loss = 31.671875, weights = tensor([6.1992, 1.9424], dtype=torch.float16, requires_grad=True)\n",
      "step №295: loss = 31.65625, weights = tensor([6.1992, 1.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №296: loss = 31.625, weights = tensor([6.1992, 1.9482], dtype=torch.float16, requires_grad=True)\n",
      "step №297: loss = 31.625, weights = tensor([6.1992, 1.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №298: loss = 31.59375, weights = tensor([6.1992, 1.9541], dtype=torch.float16, requires_grad=True)\n",
      "step №299: loss = 31.625, weights = tensor([6.1992, 1.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №300: loss = 31.625, weights = tensor([6.1992, 1.9600], dtype=torch.float16, requires_grad=True)\n",
      "step №301: loss = 31.59375, weights = tensor([6.1992, 1.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №302: loss = 31.546875, weights = tensor([6.1992, 1.9658], dtype=torch.float16, requires_grad=True)\n",
      "step №303: loss = 31.53125, weights = tensor([6.1992, 1.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №304: loss = 31.53125, weights = tensor([6.1992, 1.9717], dtype=torch.float16, requires_grad=True)\n",
      "step №305: loss = 31.53125, weights = tensor([6.1992, 1.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №306: loss = 31.5, weights = tensor([6.1992, 1.9775], dtype=torch.float16, requires_grad=True)\n",
      "step №307: loss = 31.453125, weights = tensor([6.1992, 1.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №308: loss = 31.453125, weights = tensor([6.1992, 1.9834], dtype=torch.float16, requires_grad=True)\n",
      "step №309: loss = 31.453125, weights = tensor([6.1992, 1.9863], dtype=torch.float16, requires_grad=True)\n",
      "step №310: loss = 31.46875, weights = tensor([6.1992, 1.9893], dtype=torch.float16, requires_grad=True)\n",
      "step №311: loss = 31.453125, weights = tensor([6.1992, 1.9922], dtype=torch.float16, requires_grad=True)\n",
      "step №312: loss = 31.421875, weights = tensor([6.1992, 1.9951], dtype=torch.float16, requires_grad=True)\n",
      "step №313: loss = 31.40625, weights = tensor([6.1992, 1.9980], dtype=torch.float16, requires_grad=True)\n",
      "step №314: loss = 31.375, weights = tensor([6.1992, 2.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №315: loss = 31.375, weights = tensor([6.1992, 2.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №316: loss = 31.375, weights = tensor([6.1992, 2.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №317: loss = 31.375, weights = tensor([6.1992, 2.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №318: loss = 31.34375, weights = tensor([6.1992, 2.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №319: loss = 31.328125, weights = tensor([6.1992, 2.0098], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №320: loss = 31.296875, weights = tensor([6.1992, 2.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №321: loss = 31.296875, weights = tensor([6.1992, 2.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №322: loss = 31.296875, weights = tensor([6.1992, 2.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №323: loss = 31.328125, weights = tensor([6.1992, 2.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №324: loss = 31.328125, weights = tensor([6.1992, 2.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №325: loss = 31.328125, weights = tensor([6.1992, 2.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №326: loss = 31.296875, weights = tensor([6.1992, 2.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №327: loss = 31.28125, weights = tensor([6.1992, 2.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №328: loss = 31.25, weights = tensor([6.1992, 2.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №329: loss = 31.25, weights = tensor([6.1992, 2.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №330: loss = 31.21875, weights = tensor([6.1992, 2.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №331: loss = 31.21875, weights = tensor([6.1992, 2.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №332: loss = 31.21875, weights = tensor([6.1992, 2.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №333: loss = 31.21875, weights = tensor([6.1992, 2.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №334: loss = 31.203125, weights = tensor([6.1992, 2.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №335: loss = 31.171875, weights = tensor([6.1992, 2.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №336: loss = 31.15625, weights = tensor([6.1992, 2.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №337: loss = 31.15625, weights = tensor([6.1992, 2.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №338: loss = 31.15625, weights = tensor([6.1992, 2.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №339: loss = 31.125, weights = tensor([6.1992, 2.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №340: loss = 31.171875, weights = tensor([6.1992, 2.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №341: loss = 31.171875, weights = tensor([6.1992, 2.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №342: loss = 31.15625, weights = tensor([6.1992, 2.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №343: loss = 31.125, weights = tensor([6.1992, 2.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №344: loss = 31.125, weights = tensor([6.1992, 2.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №345: loss = 31.09375, weights = tensor([6.1992, 2.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №346: loss = 31.078125, weights = tensor([6.1992, 2.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №347: loss = 31.078125, weights = tensor([6.1992, 2.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №348: loss = 31.078125, weights = tensor([6.1992, 2.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №349: loss = 31.078125, weights = tensor([6.1992, 2.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №350: loss = 31.046875, weights = tensor([6.1992, 2.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №351: loss = 31.03125, weights = tensor([6.1992, 2.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №352: loss = 31.03125, weights = tensor([6.1992, 2.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №353: loss = 31.0, weights = tensor([6.1992, 2.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №354: loss = 31.0, weights = tensor([6.1992, 2.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №355: loss = 31.046875, weights = tensor([6.1992, 2.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №356: loss = 31.03125, weights = tensor([6.1992, 2.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №357: loss = 31.03125, weights = tensor([6.1992, 2.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №358: loss = 31.0, weights = tensor([6.1992, 2.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №359: loss = 30.96875, weights = tensor([6.1992, 2.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №360: loss = 30.96875, weights = tensor([6.1992, 2.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №361: loss = 30.953125, weights = tensor([6.1992, 2.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №362: loss = 30.921875, weights = tensor([6.1992, 2.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №363: loss = 30.921875, weights = tensor([6.1992, 2.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №364: loss = 30.921875, weights = tensor([6.1992, 2.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №365: loss = 30.921875, weights = tensor([6.1992, 2.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №366: loss = 30.90625, weights = tensor([6.1992, 2.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №367: loss = 30.90625, weights = tensor([6.1992, 2.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №368: loss = 30.875, weights = tensor([6.1992, 2.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №369: loss = 30.84375, weights = tensor([6.1992, 2.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №370: loss = 30.84375, weights = tensor([6.1992, 2.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №371: loss = 30.828125, weights = tensor([6.1992, 2.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №372: loss = 30.90625, weights = tensor([6.1992, 2.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №373: loss = 30.875, weights = tensor([6.1992, 2.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №374: loss = 30.84375, weights = tensor([6.1992, 2.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №375: loss = 30.84375, weights = tensor([6.1992, 2.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №376: loss = 30.828125, weights = tensor([6.1953, 2.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №377: loss = 30.796875, weights = tensor([6.1953, 2.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №378: loss = 30.796875, weights = tensor([6.1953, 2.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №379: loss = 30.796875, weights = tensor([6.1953, 2.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №380: loss = 30.796875, weights = tensor([6.1953, 2.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №381: loss = 30.78125, weights = tensor([6.1953, 2.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №382: loss = 30.75, weights = tensor([6.1953, 2.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №383: loss = 30.75, weights = tensor([6.1953, 2.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №384: loss = 30.71875, weights = tensor([6.1953, 2.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №385: loss = 30.703125, weights = tensor([6.1953, 2.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №386: loss = 30.703125, weights = tensor([6.1953, 2.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №387: loss = 30.671875, weights = tensor([6.1953, 2.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №388: loss = 30.75, weights = tensor([6.1914, 2.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №389: loss = 30.71875, weights = tensor([6.1914, 2.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №390: loss = 30.703125, weights = tensor([6.1914, 2.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №391: loss = 30.703125, weights = tensor([6.1914, 2.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №392: loss = 30.671875, weights = tensor([6.1914, 2.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №393: loss = 30.65625, weights = tensor([6.1914, 2.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №394: loss = 30.65625, weights = tensor([6.1914, 2.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №395: loss = 30.625, weights = tensor([6.1914, 2.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №396: loss = 30.625, weights = tensor([6.1914, 2.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №397: loss = 30.625, weights = tensor([6.1914, 2.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №398: loss = 30.59375, weights = tensor([6.1914, 2.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №399: loss = 30.578125, weights = tensor([6.1914, 2.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №400: loss = 30.578125, weights = tensor([6.1914, 2.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №401: loss = 30.546875, weights = tensor([6.1914, 2.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №402: loss = 30.546875, weights = tensor([6.1914, 2.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №403: loss = 30.546875, weights = tensor([6.1914, 2.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №404: loss = 30.59375, weights = tensor([6.1875, 2.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №405: loss = 30.578125, weights = tensor([6.1875, 2.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №406: loss = 30.546875, weights = tensor([6.1875, 2.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №407: loss = 30.53125, weights = tensor([6.1875, 2.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №408: loss = 30.53125, weights = tensor([6.1875, 2.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №409: loss = 30.5, weights = tensor([6.1875, 2.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №410: loss = 30.5, weights = tensor([6.1875, 2.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №411: loss = 30.5, weights = tensor([6.1875, 2.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №412: loss = 30.5, weights = tensor([6.1875, 2.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №413: loss = 30.5, weights = tensor([6.1875, 2.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №414: loss = 30.453125, weights = tensor([6.1875, 2.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №415: loss = 30.453125, weights = tensor([6.1875, 2.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №416: loss = 30.421875, weights = tensor([6.1875, 2.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №417: loss = 30.40625, weights = tensor([6.1875, 2.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №418: loss = 30.40625, weights = tensor([6.1875, 2.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №419: loss = 30.40625, weights = tensor([6.1875, 2.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №420: loss = 30.453125, weights = tensor([6.1836, 2.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №421: loss = 30.421875, weights = tensor([6.1836, 2.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №422: loss = 30.40625, weights = tensor([6.1836, 2.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №423: loss = 30.40625, weights = tensor([6.1836, 2.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №424: loss = 30.375, weights = tensor([6.1836, 2.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №425: loss = 30.375, weights = tensor([6.1836, 2.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №426: loss = 30.34375, weights = tensor([6.1836, 2.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №427: loss = 30.34375, weights = tensor([6.1836, 2.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №428: loss = 30.328125, weights = tensor([6.1836, 2.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №429: loss = 30.328125, weights = tensor([6.1836, 2.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №430: loss = 30.296875, weights = tensor([6.1836, 2.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №431: loss = 30.28125, weights = tensor([6.1797, 2.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №432: loss = 30.28125, weights = tensor([6.1797, 2.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №433: loss = 30.25, weights = tensor([6.1797, 2.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №434: loss = 30.25, weights = tensor([6.1797, 2.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №435: loss = 30.296875, weights = tensor([6.1797, 2.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №436: loss = 30.28125, weights = tensor([6.1797, 2.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №437: loss = 30.28125, weights = tensor([6.1797, 2.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №438: loss = 30.25, weights = tensor([6.1797, 2.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №439: loss = 30.21875, weights = tensor([6.1758, 2.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №440: loss = 30.21875, weights = tensor([6.1758, 2.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №441: loss = 30.203125, weights = tensor([6.1758, 2.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №442: loss = 30.203125, weights = tensor([6.1758, 2.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №443: loss = 30.203125, weights = tensor([6.1758, 2.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №444: loss = 30.203125, weights = tensor([6.1758, 2.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №445: loss = 30.171875, weights = tensor([6.1758, 2.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №446: loss = 30.171875, weights = tensor([6.1758, 2.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №447: loss = 30.15625, weights = tensor([6.1758, 2.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №448: loss = 30.125, weights = tensor([6.1758, 2.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №449: loss = 30.125, weights = tensor([6.1758, 2.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №450: loss = 30.09375, weights = tensor([6.1758, 2.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №451: loss = 30.15625, weights = tensor([6.1719, 2.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №452: loss = 30.15625, weights = tensor([6.1719, 2.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №453: loss = 30.15625, weights = tensor([6.1719, 2.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №454: loss = 30.125, weights = tensor([6.1719, 2.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №455: loss = 30.09375, weights = tensor([6.1719, 2.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №456: loss = 30.078125, weights = tensor([6.1719, 2.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №457: loss = 30.046875, weights = tensor([6.1719, 2.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №458: loss = 30.046875, weights = tensor([6.1719, 2.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №459: loss = 30.046875, weights = tensor([6.1719, 2.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №460: loss = 30.03125, weights = tensor([6.1719, 2.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №461: loss = 30.03125, weights = tensor([6.1719, 2.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №462: loss = 30.0, weights = tensor([6.1719, 2.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №463: loss = 30.0, weights = tensor([6.1719, 2.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №464: loss = 29.96875, weights = tensor([6.1719, 2.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №465: loss = 29.953125, weights = tensor([6.1719, 2.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №466: loss = 29.953125, weights = tensor([6.1719, 2.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №467: loss = 30.0, weights = tensor([6.1680, 2.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №468: loss = 29.921875, weights = tensor([6.1680, 2.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №469: loss = 29.921875, weights = tensor([6.1680, 2.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №470: loss = 29.90625, weights = tensor([6.1680, 2.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №471: loss = 29.90625, weights = tensor([6.1680, 2.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №472: loss = 29.875, weights = tensor([6.1680, 2.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №473: loss = 29.84375, weights = tensor([6.1680, 2.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №474: loss = 29.84375, weights = tensor([6.1680, 2.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №475: loss = 29.84375, weights = tensor([6.1680, 2.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №476: loss = 29.84375, weights = tensor([6.1680, 2.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №477: loss = 29.84375, weights = tensor([6.1680, 2.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №478: loss = 29.828125, weights = tensor([6.1680, 2.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №479: loss = 29.796875, weights = tensor([6.1680, 2.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №480: loss = 29.78125, weights = tensor([6.1680, 2.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №481: loss = 29.78125, weights = tensor([6.1680, 2.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №482: loss = 29.75, weights = tensor([6.1680, 2.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №483: loss = 29.78125, weights = tensor([6.1641, 2.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №484: loss = 29.796875, weights = tensor([6.1641, 2.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №485: loss = 29.796875, weights = tensor([6.1641, 2.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №486: loss = 29.78125, weights = tensor([6.1641, 2.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №487: loss = 29.71875, weights = tensor([6.1602, 2.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №488: loss = 29.71875, weights = tensor([6.1602, 2.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №489: loss = 29.71875, weights = tensor([6.1602, 2.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №490: loss = 29.703125, weights = tensor([6.1602, 2.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №491: loss = 29.703125, weights = tensor([6.1602, 2.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №492: loss = 29.671875, weights = tensor([6.1602, 2.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №493: loss = 29.671875, weights = tensor([6.1602, 2.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №494: loss = 29.65625, weights = tensor([6.1602, 2.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №495: loss = 29.625, weights = tensor([6.1602, 2.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №496: loss = 29.625, weights = tensor([6.1602, 2.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №497: loss = 29.59375, weights = tensor([6.1602, 2.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №498: loss = 29.59375, weights = tensor([6.1602, 2.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №499: loss = 29.59375, weights = tensor([6.1602, 2.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №500: loss = 29.65625, weights = tensor([6.1562, 2.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №501: loss = 29.65625, weights = tensor([6.1562, 2.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №502: loss = 29.59375, weights = tensor([6.1562, 2.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №503: loss = 29.578125, weights = tensor([6.1562, 2.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №504: loss = 29.578125, weights = tensor([6.1562, 2.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №505: loss = 29.546875, weights = tensor([6.1562, 2.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №506: loss = 29.546875, weights = tensor([6.1562, 2.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №507: loss = 29.546875, weights = tensor([6.1562, 2.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №508: loss = 29.546875, weights = tensor([6.1562, 2.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №509: loss = 29.546875, weights = tensor([6.1562, 2.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №510: loss = 29.53125, weights = tensor([6.1562, 2.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №511: loss = 29.53125, weights = tensor([6.1562, 2.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №512: loss = 29.5, weights = tensor([6.1562, 2.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №513: loss = 29.453125, weights = tensor([6.1562, 2.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №514: loss = 29.453125, weights = tensor([6.1562, 2.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №515: loss = 29.453125, weights = tensor([6.1562, 2.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №516: loss = 29.5, weights = tensor([6.1523, 2.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №517: loss = 29.5, weights = tensor([6.1523, 2.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №518: loss = 29.46875, weights = tensor([6.1523, 2.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №519: loss = 29.46875, weights = tensor([6.1523, 2.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №520: loss = 29.453125, weights = tensor([6.1523, 2.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №521: loss = 29.421875, weights = tensor([6.1523, 2.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №522: loss = 29.421875, weights = tensor([6.1523, 2.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №523: loss = 29.40625, weights = tensor([6.1523, 2.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №524: loss = 29.40625, weights = tensor([6.1523, 2.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №525: loss = 29.40625, weights = tensor([6.1523, 2.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №526: loss = 29.375, weights = tensor([6.1523, 2.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №527: loss = 29.34375, weights = tensor([6.1523, 2.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №528: loss = 29.328125, weights = tensor([6.1523, 2.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №529: loss = 29.328125, weights = tensor([6.1523, 2.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №530: loss = 29.296875, weights = tensor([6.1523, 2.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №531: loss = 29.296875, weights = tensor([6.1484, 2.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №532: loss = 29.34375, weights = tensor([6.1484, 2.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №533: loss = 29.34375, weights = tensor([6.1484, 2.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №534: loss = 29.328125, weights = tensor([6.1484, 2.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №535: loss = 29.296875, weights = tensor([6.1484, 2.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №536: loss = 29.296875, weights = tensor([6.1484, 2.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №537: loss = 29.25, weights = tensor([6.1484, 2.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №538: loss = 29.25, weights = tensor([6.1484, 2.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №539: loss = 29.25, weights = tensor([6.1484, 2.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №540: loss = 29.25, weights = tensor([6.1484, 2.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №541: loss = 29.25, weights = tensor([6.1484, 2.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №542: loss = 29.21875, weights = tensor([6.1484, 2.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №543: loss = 29.21875, weights = tensor([6.1484, 2.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №544: loss = 29.203125, weights = tensor([6.1484, 2.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №545: loss = 29.171875, weights = tensor([6.1484, 2.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №546: loss = 29.171875, weights = tensor([6.1484, 2.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №547: loss = 29.171875, weights = tensor([6.1445, 2.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №548: loss = 29.203125, weights = tensor([6.1445, 2.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №549: loss = 29.203125, weights = tensor([6.1445, 2.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №550: loss = 29.171875, weights = tensor([6.1445, 2.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №551: loss = 29.15625, weights = tensor([6.1445, 2.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №552: loss = 29.15625, weights = tensor([6.1406, 2.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №553: loss = 29.078125, weights = tensor([6.1406, 2.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №554: loss = 29.078125, weights = tensor([6.1406, 2.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №555: loss = 29.046875, weights = tensor([6.1406, 2.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №556: loss = 29.046875, weights = tensor([6.1406, 2.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №557: loss = 29.046875, weights = tensor([6.1406, 2.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №558: loss = 29.03125, weights = tensor([6.1406, 2.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №559: loss = 29.0, weights = tensor([6.1406, 2.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №560: loss = 29.0, weights = tensor([6.1406, 2.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №561: loss = 28.96875, weights = tensor([6.1406, 2.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №562: loss = 28.96875, weights = tensor([6.1406, 2.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №563: loss = 29.03125, weights = tensor([6.1406, 2.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №564: loss = 29.03125, weights = tensor([6.1367, 2.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №565: loss = 29.0, weights = tensor([6.1367, 2.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №566: loss = 28.96875, weights = tensor([6.1367, 2.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №567: loss = 28.96875, weights = tensor([6.1367, 2.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №568: loss = 28.953125, weights = tensor([6.1367, 2.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №569: loss = 28.921875, weights = tensor([6.1367, 2.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №570: loss = 28.921875, weights = tensor([6.1367, 2.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №571: loss = 28.921875, weights = tensor([6.1367, 2.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №572: loss = 28.921875, weights = tensor([6.1367, 2.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №573: loss = 28.90625, weights = tensor([6.1367, 2.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №574: loss = 28.90625, weights = tensor([6.1367, 2.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №575: loss = 28.875, weights = tensor([6.1367, 2.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №576: loss = 28.84375, weights = tensor([6.1367, 2.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №577: loss = 28.84375, weights = tensor([6.1367, 2.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №578: loss = 28.828125, weights = tensor([6.1367, 2.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №579: loss = 28.875, weights = tensor([6.1367, 2.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №580: loss = 28.875, weights = tensor([6.1328, 2.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №581: loss = 28.875, weights = tensor([6.1328, 2.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №582: loss = 28.828125, weights = tensor([6.1328, 2.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №583: loss = 28.796875, weights = tensor([6.1328, 2.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №584: loss = 28.796875, weights = tensor([6.1328, 2.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №585: loss = 28.78125, weights = tensor([6.1328, 2.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №586: loss = 28.78125, weights = tensor([6.1328, 2.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №587: loss = 28.78125, weights = tensor([6.1328, 2.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №588: loss = 28.78125, weights = tensor([6.1328, 2.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №589: loss = 28.78125, weights = tensor([6.1328, 2.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №590: loss = 28.75, weights = tensor([6.1328, 2.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №591: loss = 28.75, weights = tensor([6.1328, 2.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №592: loss = 28.71875, weights = tensor([6.1328, 2.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №593: loss = 28.703125, weights = tensor([6.1328, 2.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №594: loss = 28.703125, weights = tensor([6.1328, 2.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №595: loss = 28.75, weights = tensor([6.1289, 2.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №596: loss = 28.71875, weights = tensor([6.1289, 2.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №597: loss = 28.71875, weights = tensor([6.1289, 2.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №598: loss = 28.703125, weights = tensor([6.1289, 2.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №599: loss = 28.703125, weights = tensor([6.1289, 2.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №600: loss = 28.671875, weights = tensor([6.1289, 2.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №601: loss = 28.65625, weights = tensor([6.1289, 2.5605], dtype=torch.float16, requires_grad=True)\n",
      "step №602: loss = 28.65625, weights = tensor([6.1289, 2.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №603: loss = 28.65625, weights = tensor([6.1289, 2.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №604: loss = 28.625, weights = tensor([6.1289, 2.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №605: loss = 28.625, weights = tensor([6.1289, 2.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №606: loss = 28.59375, weights = tensor([6.1289, 2.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №607: loss = 28.578125, weights = tensor([6.1250, 2.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №608: loss = 28.578125, weights = tensor([6.1250, 2.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №609: loss = 28.546875, weights = tensor([6.1250, 2.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №610: loss = 28.546875, weights = tensor([6.1250, 2.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №611: loss = 28.546875, weights = tensor([6.1250, 2.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №612: loss = 28.578125, weights = tensor([6.1250, 2.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №613: loss = 28.578125, weights = tensor([6.1250, 2.5840], dtype=torch.float16, requires_grad=True)\n",
      "step №614: loss = 28.546875, weights = tensor([6.1250, 2.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №615: loss = 28.53125, weights = tensor([6.1211, 2.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №616: loss = 28.53125, weights = tensor([6.1211, 2.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №617: loss = 28.5, weights = tensor([6.1211, 2.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №618: loss = 28.5, weights = tensor([6.1211, 2.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №619: loss = 28.5, weights = tensor([6.1211, 2.5957], dtype=torch.float16, requires_grad=True)\n",
      "step №620: loss = 28.46875, weights = tensor([6.1211, 2.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №621: loss = 28.46875, weights = tensor([6.1211, 2.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №622: loss = 28.453125, weights = tensor([6.1211, 2.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №623: loss = 28.421875, weights = tensor([6.1211, 2.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №624: loss = 28.421875, weights = tensor([6.1211, 2.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №625: loss = 28.40625, weights = tensor([6.1211, 2.6074], dtype=torch.float16, requires_grad=True)\n",
      "step №626: loss = 28.40625, weights = tensor([6.1211, 2.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №627: loss = 28.40625, weights = tensor([6.1211, 2.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №628: loss = 28.453125, weights = tensor([6.1172, 2.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №629: loss = 28.421875, weights = tensor([6.1172, 2.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №630: loss = 28.40625, weights = tensor([6.1172, 2.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №631: loss = 28.375, weights = tensor([6.1172, 2.6191], dtype=torch.float16, requires_grad=True)\n",
      "step №632: loss = 28.375, weights = tensor([6.1172, 2.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №633: loss = 28.34375, weights = tensor([6.1172, 2.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №634: loss = 28.34375, weights = tensor([6.1172, 2.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №635: loss = 28.34375, weights = tensor([6.1172, 2.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №636: loss = 28.34375, weights = tensor([6.1172, 2.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №637: loss = 28.34375, weights = tensor([6.1172, 2.6309], dtype=torch.float16, requires_grad=True)\n",
      "step №638: loss = 28.328125, weights = tensor([6.1172, 2.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №639: loss = 28.328125, weights = tensor([6.1172, 2.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №640: loss = 28.296875, weights = tensor([6.1172, 2.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №641: loss = 28.28125, weights = tensor([6.1172, 2.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №642: loss = 28.25, weights = tensor([6.1172, 2.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №643: loss = 28.25, weights = tensor([6.1172, 2.6426], dtype=torch.float16, requires_grad=True)\n",
      "step №644: loss = 28.296875, weights = tensor([6.1133, 2.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №645: loss = 28.28125, weights = tensor([6.1133, 2.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №646: loss = 28.28125, weights = tensor([6.1133, 2.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №647: loss = 28.25, weights = tensor([6.1133, 2.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №648: loss = 28.21875, weights = tensor([6.1133, 2.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №649: loss = 28.21875, weights = tensor([6.1133, 2.6543], dtype=torch.float16, requires_grad=True)\n",
      "step №650: loss = 28.203125, weights = tensor([6.1133, 2.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №651: loss = 28.203125, weights = tensor([6.1133, 2.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №652: loss = 28.203125, weights = tensor([6.1133, 2.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №653: loss = 28.203125, weights = tensor([6.1133, 2.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №654: loss = 28.171875, weights = tensor([6.1133, 2.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №655: loss = 28.171875, weights = tensor([6.1133, 2.6660], dtype=torch.float16, requires_grad=True)\n",
      "step №656: loss = 28.15625, weights = tensor([6.1133, 2.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №657: loss = 28.125, weights = tensor([6.1133, 2.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №658: loss = 28.125, weights = tensor([6.1133, 2.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №659: loss = 28.125, weights = tensor([6.1094, 2.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №660: loss = 28.171875, weights = tensor([6.1094, 2.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №661: loss = 28.171875, weights = tensor([6.1094, 2.6777], dtype=torch.float16, requires_grad=True)\n",
      "step №662: loss = 28.125, weights = tensor([6.1094, 2.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №663: loss = 28.125, weights = tensor([6.1094, 2.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №664: loss = 28.09375, weights = tensor([6.1094, 2.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №665: loss = 28.078125, weights = tensor([6.1094, 2.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №666: loss = 28.078125, weights = tensor([6.1094, 2.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №667: loss = 28.078125, weights = tensor([6.1094, 2.6895], dtype=torch.float16, requires_grad=True)\n",
      "step №668: loss = 28.078125, weights = tensor([6.1094, 2.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №669: loss = 28.078125, weights = tensor([6.1094, 2.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №670: loss = 28.03125, weights = tensor([6.1094, 2.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №671: loss = 28.03125, weights = tensor([6.1055, 2.6973], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №672: loss = 27.953125, weights = tensor([6.1055, 2.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №673: loss = 27.921875, weights = tensor([6.1055, 2.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №674: loss = 27.921875, weights = tensor([6.1055, 2.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №675: loss = 27.953125, weights = tensor([6.1055, 2.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №676: loss = 27.96875, weights = tensor([6.1055, 2.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №677: loss = 27.953125, weights = tensor([6.1055, 2.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №678: loss = 27.953125, weights = tensor([6.1055, 2.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №679: loss = 27.921875, weights = tensor([6.1016, 2.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №680: loss = 27.90625, weights = tensor([6.1016, 2.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №681: loss = 27.875, weights = tensor([6.1016, 2.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №682: loss = 27.875, weights = tensor([6.1016, 2.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №683: loss = 27.875, weights = tensor([6.1016, 2.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №684: loss = 27.875, weights = tensor([6.1016, 2.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №685: loss = 27.875, weights = tensor([6.1016, 2.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №686: loss = 27.84375, weights = tensor([6.1016, 2.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №687: loss = 27.84375, weights = tensor([6.1016, 2.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №688: loss = 27.828125, weights = tensor([6.1016, 2.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №689: loss = 27.796875, weights = tensor([6.1016, 2.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №690: loss = 27.796875, weights = tensor([6.1016, 2.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №691: loss = 27.828125, weights = tensor([6.1016, 2.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №692: loss = 27.828125, weights = tensor([6.0977, 2.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №693: loss = 27.828125, weights = tensor([6.0977, 2.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №694: loss = 27.796875, weights = tensor([6.0977, 2.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №695: loss = 27.796875, weights = tensor([6.0977, 2.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №696: loss = 27.78125, weights = tensor([6.0977, 2.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №697: loss = 27.75, weights = tensor([6.0977, 2.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №698: loss = 27.75, weights = tensor([6.0977, 2.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №699: loss = 27.71875, weights = tensor([6.0977, 2.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №700: loss = 27.71875, weights = tensor([6.0977, 2.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №701: loss = 27.71875, weights = tensor([6.0977, 2.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №702: loss = 27.703125, weights = tensor([6.0977, 2.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №703: loss = 27.671875, weights = tensor([6.0977, 2.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №704: loss = 27.671875, weights = tensor([6.0977, 2.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №705: loss = 27.65625, weights = tensor([6.0977, 2.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №706: loss = 27.65625, weights = tensor([6.0977, 2.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №707: loss = 27.703125, weights = tensor([6.0977, 2.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №708: loss = 27.703125, weights = tensor([6.0938, 2.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №709: loss = 27.671875, weights = tensor([6.0938, 2.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №710: loss = 27.65625, weights = tensor([6.0938, 2.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №711: loss = 27.625, weights = tensor([6.0938, 2.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №712: loss = 27.625, weights = tensor([6.0938, 2.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №713: loss = 27.59375, weights = tensor([6.0938, 2.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №714: loss = 27.59375, weights = tensor([6.0938, 2.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №715: loss = 27.59375, weights = tensor([6.0938, 2.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №716: loss = 27.59375, weights = tensor([6.0938, 2.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №717: loss = 27.59375, weights = tensor([6.0938, 2.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №718: loss = 27.578125, weights = tensor([6.0938, 2.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №719: loss = 27.578125, weights = tensor([6.0938, 2.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №720: loss = 27.546875, weights = tensor([6.0938, 2.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №721: loss = 27.53125, weights = tensor([6.0938, 2.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №722: loss = 27.53125, weights = tensor([6.0938, 2.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №723: loss = 27.578125, weights = tensor([6.0898, 2.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №724: loss = 27.546875, weights = tensor([6.0898, 2.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №725: loss = 27.546875, weights = tensor([6.0898, 2.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №726: loss = 27.53125, weights = tensor([6.0898, 2.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №727: loss = 27.53125, weights = tensor([6.0898, 2.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №728: loss = 27.5, weights = tensor([6.0898, 2.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №729: loss = 27.46875, weights = tensor([6.0898, 2.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №730: loss = 27.46875, weights = tensor([6.0898, 2.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №731: loss = 27.46875, weights = tensor([6.0898, 2.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №732: loss = 27.453125, weights = tensor([6.0898, 2.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №733: loss = 27.453125, weights = tensor([6.0898, 2.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №734: loss = 27.421875, weights = tensor([6.0898, 2.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №735: loss = 27.40625, weights = tensor([6.0859, 2.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №736: loss = 27.40625, weights = tensor([6.0859, 2.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №737: loss = 27.375, weights = tensor([6.0859, 2.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №738: loss = 27.375, weights = tensor([6.0859, 2.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №739: loss = 27.375, weights = tensor([6.0859, 2.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №740: loss = 27.40625, weights = tensor([6.0859, 2.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №741: loss = 27.40625, weights = tensor([6.0859, 2.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №742: loss = 27.375, weights = tensor([6.0859, 2.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №743: loss = 27.34375, weights = tensor([6.0820, 2.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №744: loss = 27.34375, weights = tensor([6.0820, 2.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №745: loss = 27.328125, weights = tensor([6.0820, 2.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №746: loss = 27.328125, weights = tensor([6.0820, 2.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №747: loss = 27.328125, weights = tensor([6.0820, 2.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №748: loss = 27.328125, weights = tensor([6.0820, 2.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №749: loss = 27.296875, weights = tensor([6.0820, 2.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №750: loss = 27.296875, weights = tensor([6.0820, 2.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №751: loss = 27.28125, weights = tensor([6.0820, 2.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №752: loss = 27.28125, weights = tensor([6.0820, 2.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №753: loss = 27.25, weights = tensor([6.0820, 2.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №754: loss = 27.25, weights = tensor([6.0820, 2.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №755: loss = 27.25, weights = tensor([6.0820, 2.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №756: loss = 27.28125, weights = tensor([6.0781, 2.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №757: loss = 27.21875, weights = tensor([6.0781, 2.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №758: loss = 27.203125, weights = tensor([6.0781, 2.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №759: loss = 27.171875, weights = tensor([6.0781, 2.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №760: loss = 27.171875, weights = tensor([6.0781, 2.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №761: loss = 27.15625, weights = tensor([6.0781, 2.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №762: loss = 27.15625, weights = tensor([6.0781, 2.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №763: loss = 27.15625, weights = tensor([6.0781, 2.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №764: loss = 27.125, weights = tensor([6.0781, 2.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №765: loss = 27.125, weights = tensor([6.0781, 2.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №766: loss = 27.09375, weights = tensor([6.0781, 2.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №767: loss = 27.078125, weights = tensor([6.0781, 2.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №768: loss = 27.078125, weights = tensor([6.0781, 2.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №769: loss = 27.046875, weights = tensor([6.0781, 2.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №770: loss = 27.046875, weights = tensor([6.0781, 2.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №771: loss = 27.046875, weights = tensor([6.0781, 2.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №772: loss = 27.09375, weights = tensor([6.0742, 2.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №773: loss = 27.078125, weights = tensor([6.0742, 2.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №774: loss = 27.046875, weights = tensor([6.0742, 2.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №775: loss = 27.046875, weights = tensor([6.0742, 2.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №776: loss = 27.03125, weights = tensor([6.0742, 2.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №777: loss = 27.0, weights = tensor([6.0742, 2.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №778: loss = 27.0, weights = tensor([6.0742, 2.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №779: loss = 27.0, weights = tensor([6.0742, 2.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №780: loss = 27.0, weights = tensor([6.0742, 2.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №781: loss = 27.0, weights = tensor([6.0742, 2.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №782: loss = 26.96875, weights = tensor([6.0742, 2.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №783: loss = 26.953125, weights = tensor([6.0742, 2.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №784: loss = 26.953125, weights = tensor([6.0742, 2.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №785: loss = 26.921875, weights = tensor([6.0742, 2.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №786: loss = 26.921875, weights = tensor([6.0742, 2.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №787: loss = 26.90625, weights = tensor([6.0703, 2.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №788: loss = 26.953125, weights = tensor([6.0703, 2.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №789: loss = 26.953125, weights = tensor([6.0703, 2.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №790: loss = 26.921875, weights = tensor([6.0703, 2.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №791: loss = 26.90625, weights = tensor([6.0703, 2.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №792: loss = 26.90625, weights = tensor([6.0703, 2.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №793: loss = 26.875, weights = tensor([6.0703, 2.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №794: loss = 26.875, weights = tensor([6.0703, 2.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №795: loss = 26.875, weights = tensor([6.0703, 2.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №796: loss = 26.875, weights = tensor([6.0703, 2.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №797: loss = 26.875, weights = tensor([6.0664, 2.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №798: loss = 26.828125, weights = tensor([6.0664, 2.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №799: loss = 26.796875, weights = tensor([6.0664, 2.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №800: loss = 26.796875, weights = tensor([6.0664, 2.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №801: loss = 26.78125, weights = tensor([6.0664, 2.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №802: loss = 26.78125, weights = tensor([6.0664, 2.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №803: loss = 26.828125, weights = tensor([6.0625, 2.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №804: loss = 26.796875, weights = tensor([6.0625, 2.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №805: loss = 26.796875, weights = tensor([6.0625, 2.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №806: loss = 26.78125, weights = tensor([6.0625, 2.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №807: loss = 26.75, weights = tensor([6.0625, 2.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №808: loss = 26.75, weights = tensor([6.0625, 2.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №809: loss = 26.71875, weights = tensor([6.0625, 2.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №810: loss = 26.71875, weights = tensor([6.0625, 2.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №811: loss = 26.71875, weights = tensor([6.0625, 2.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №812: loss = 26.71875, weights = tensor([6.0625, 2.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №813: loss = 26.71875, weights = tensor([6.0625, 2.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №814: loss = 26.703125, weights = tensor([6.0625, 2.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №815: loss = 26.703125, weights = tensor([6.0625, 2.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №816: loss = 26.671875, weights = tensor([6.0625, 2.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №817: loss = 26.65625, weights = tensor([6.0625, 2.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №818: loss = 26.65625, weights = tensor([6.0625, 2.9844], dtype=torch.float16, requires_grad=True)\n",
      "step №819: loss = 26.671875, weights = tensor([6.0586, 2.9863], dtype=torch.float16, requires_grad=True)\n",
      "step №820: loss = 26.671875, weights = tensor([6.0586, 2.9883], dtype=torch.float16, requires_grad=True)\n",
      "step №821: loss = 26.671875, weights = tensor([6.0586, 2.9902], dtype=torch.float16, requires_grad=True)\n",
      "step №822: loss = 26.65625, weights = tensor([6.0586, 2.9922], dtype=torch.float16, requires_grad=True)\n",
      "step №823: loss = 26.65625, weights = tensor([6.0586, 2.9941], dtype=torch.float16, requires_grad=True)\n",
      "step №824: loss = 26.625, weights = tensor([6.0586, 2.9961], dtype=torch.float16, requires_grad=True)\n",
      "step №825: loss = 26.59375, weights = tensor([6.0586, 2.9980], dtype=torch.float16, requires_grad=True)\n",
      "step №826: loss = 26.59375, weights = tensor([6.0586, 3.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №827: loss = 26.578125, weights = tensor([6.0586, 3.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №828: loss = 26.578125, weights = tensor([6.0586, 3.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №829: loss = 26.578125, weights = tensor([6.0586, 3.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №830: loss = 26.546875, weights = tensor([6.0586, 3.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №831: loss = 26.53125, weights = tensor([6.0586, 3.0098], dtype=torch.float16, requires_grad=True)\n",
      "step №832: loss = 26.53125, weights = tensor([6.0586, 3.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №833: loss = 26.5, weights = tensor([6.0586, 3.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №834: loss = 26.5, weights = tensor([6.0586, 3.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №835: loss = 26.546875, weights = tensor([6.0547, 3.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №836: loss = 26.546875, weights = tensor([6.0547, 3.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №837: loss = 26.546875, weights = tensor([6.0547, 3.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №838: loss = 26.53125, weights = tensor([6.0547, 3.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №839: loss = 26.5, weights = tensor([6.0547, 3.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №840: loss = 26.5, weights = tensor([6.0547, 3.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №841: loss = 26.453125, weights = tensor([6.0547, 3.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №842: loss = 26.453125, weights = tensor([6.0547, 3.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №843: loss = 26.453125, weights = tensor([6.0547, 3.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №844: loss = 26.453125, weights = tensor([6.0547, 3.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №845: loss = 26.453125, weights = tensor([6.0547, 3.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №846: loss = 26.421875, weights = tensor([6.0547, 3.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №847: loss = 26.421875, weights = tensor([6.0547, 3.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №848: loss = 26.40625, weights = tensor([6.0547, 3.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №849: loss = 26.375, weights = tensor([6.0547, 3.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №850: loss = 26.375, weights = tensor([6.0547, 3.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №851: loss = 26.421875, weights = tensor([6.0508, 3.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №852: loss = 26.40625, weights = tensor([6.0508, 3.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №853: loss = 26.40625, weights = tensor([6.0508, 3.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №854: loss = 26.40625, weights = tensor([6.0508, 3.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №855: loss = 26.375, weights = tensor([6.0508, 3.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №856: loss = 26.34375, weights = tensor([6.0469, 3.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №857: loss = 26.328125, weights = tensor([6.0469, 3.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №858: loss = 26.328125, weights = tensor([6.0469, 3.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №859: loss = 26.328125, weights = tensor([6.0469, 3.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №860: loss = 26.328125, weights = tensor([6.0469, 3.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №861: loss = 26.328125, weights = tensor([6.0469, 3.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №862: loss = 26.296875, weights = tensor([6.0469, 3.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №863: loss = 26.28125, weights = tensor([6.0469, 3.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №864: loss = 26.25, weights = tensor([6.0469, 3.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №865: loss = 26.21875, weights = tensor([6.0469, 3.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №866: loss = 26.21875, weights = tensor([6.0469, 3.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №867: loss = 26.25, weights = tensor([6.0469, 3.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №868: loss = 26.28125, weights = tensor([6.0430, 3.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №869: loss = 26.203125, weights = tensor([6.0430, 3.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №870: loss = 26.203125, weights = tensor([6.0430, 3.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №871: loss = 26.171875, weights = tensor([6.0430, 3.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №872: loss = 26.171875, weights = tensor([6.0430, 3.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №873: loss = 26.15625, weights = tensor([6.0430, 3.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №874: loss = 26.15625, weights = tensor([6.0430, 3.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №875: loss = 26.125, weights = tensor([6.0430, 3.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №876: loss = 26.125, weights = tensor([6.0430, 3.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №877: loss = 26.125, weights = tensor([6.0430, 3.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №878: loss = 26.125, weights = tensor([6.0430, 3.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №879: loss = 26.09375, weights = tensor([6.0430, 3.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №880: loss = 26.078125, weights = tensor([6.0430, 3.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №881: loss = 26.078125, weights = tensor([6.0430, 3.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №882: loss = 26.046875, weights = tensor([6.0430, 3.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №883: loss = 26.078125, weights = tensor([6.0430, 3.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №884: loss = 26.09375, weights = tensor([6.0391, 3.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №885: loss = 26.09375, weights = tensor([6.0391, 3.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №886: loss = 26.078125, weights = tensor([6.0391, 3.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №887: loss = 26.03125, weights = tensor([6.0391, 3.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №888: loss = 26.03125, weights = tensor([6.0391, 3.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №889: loss = 26.0, weights = tensor([6.0391, 3.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №890: loss = 26.0, weights = tensor([6.0391, 3.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №891: loss = 26.0, weights = tensor([6.0391, 3.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №892: loss = 26.0, weights = tensor([6.0391, 3.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №893: loss = 26.0, weights = tensor([6.0391, 3.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №894: loss = 25.96875, weights = tensor([6.0391, 3.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №895: loss = 25.96875, weights = tensor([6.0391, 3.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №896: loss = 25.953125, weights = tensor([6.0391, 3.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №897: loss = 25.921875, weights = tensor([6.0391, 3.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №898: loss = 25.921875, weights = tensor([6.0391, 3.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №899: loss = 25.921875, weights = tensor([6.0352, 3.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №900: loss = 25.953125, weights = tensor([6.0352, 3.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №901: loss = 25.953125, weights = tensor([6.0352, 3.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №902: loss = 25.953125, weights = tensor([6.0352, 3.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №903: loss = 25.953125, weights = tensor([6.0352, 3.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №904: loss = 25.921875, weights = tensor([6.0352, 3.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №905: loss = 25.90625, weights = tensor([6.0352, 3.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №906: loss = 25.875, weights = tensor([6.0352, 3.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №907: loss = 25.875, weights = tensor([6.0352, 3.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №908: loss = 25.875, weights = tensor([6.0352, 3.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №909: loss = 25.84375, weights = tensor([6.0352, 3.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №910: loss = 25.84375, weights = tensor([6.0352, 3.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №911: loss = 25.828125, weights = tensor([6.0352, 3.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №912: loss = 25.828125, weights = tensor([6.0352, 3.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №913: loss = 25.796875, weights = tensor([6.0352, 3.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №914: loss = 25.78125, weights = tensor([6.0312, 3.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №915: loss = 25.828125, weights = tensor([6.0312, 3.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №916: loss = 25.828125, weights = tensor([6.0312, 3.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №917: loss = 25.828125, weights = tensor([6.0312, 3.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №918: loss = 25.796875, weights = tensor([6.0312, 3.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №919: loss = 25.78125, weights = tensor([6.0273, 3.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №920: loss = 25.78125, weights = tensor([6.0273, 3.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №921: loss = 25.75, weights = tensor([6.0273, 3.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №922: loss = 25.75, weights = tensor([6.0273, 3.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №923: loss = 25.75, weights = tensor([6.0273, 3.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №924: loss = 25.71875, weights = tensor([6.0273, 3.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №925: loss = 25.71875, weights = tensor([6.0273, 3.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №926: loss = 25.703125, weights = tensor([6.0273, 3.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №927: loss = 25.671875, weights = tensor([6.0273, 3.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №928: loss = 25.671875, weights = tensor([6.0273, 3.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №929: loss = 25.65625, weights = tensor([6.0273, 3.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №930: loss = 25.65625, weights = tensor([6.0273, 3.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №931: loss = 25.703125, weights = tensor([6.0273, 3.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №932: loss = 25.703125, weights = tensor([6.0234, 3.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №933: loss = 25.703125, weights = tensor([6.0234, 3.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №934: loss = 25.671875, weights = tensor([6.0234, 3.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №935: loss = 25.625, weights = tensor([6.0234, 3.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №936: loss = 25.625, weights = tensor([6.0234, 3.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №937: loss = 25.59375, weights = tensor([6.0234, 3.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №938: loss = 25.59375, weights = tensor([6.0234, 3.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №939: loss = 25.59375, weights = tensor([6.0234, 3.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №940: loss = 25.59375, weights = tensor([6.0234, 3.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №941: loss = 25.59375, weights = tensor([6.0234, 3.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №942: loss = 25.578125, weights = tensor([6.0234, 3.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №943: loss = 25.578125, weights = tensor([6.0234, 3.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №944: loss = 25.546875, weights = tensor([6.0234, 3.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №945: loss = 25.53125, weights = tensor([6.0234, 3.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №946: loss = 25.53125, weights = tensor([6.0234, 3.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №947: loss = 25.5625, weights = tensor([6.0234, 3.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №948: loss = 25.578125, weights = tensor([6.0195, 3.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №949: loss = 25.546875, weights = tensor([6.0195, 3.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №950: loss = 25.53125, weights = tensor([6.0195, 3.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №951: loss = 25.53125, weights = tensor([6.0195, 3.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №952: loss = 25.5, weights = tensor([6.0195, 3.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №953: loss = 25.484375, weights = tensor([6.0195, 3.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №954: loss = 25.484375, weights = tensor([6.0195, 3.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №955: loss = 25.484375, weights = tensor([6.0195, 3.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №956: loss = 25.484375, weights = tensor([6.0195, 3.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №957: loss = 25.46875, weights = tensor([6.0195, 3.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №958: loss = 25.46875, weights = tensor([6.0195, 3.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №959: loss = 25.453125, weights = tensor([6.0195, 3.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №960: loss = 25.421875, weights = tensor([6.0195, 3.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №961: loss = 25.40625, weights = tensor([6.0195, 3.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №962: loss = 25.40625, weights = tensor([6.0195, 3.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №963: loss = 25.421875, weights = tensor([6.0156, 3.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №964: loss = 25.390625, weights = tensor([6.0156, 3.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №965: loss = 25.390625, weights = tensor([6.0156, 3.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №966: loss = 25.359375, weights = tensor([6.0156, 3.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №967: loss = 25.34375, weights = tensor([6.0156, 3.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №968: loss = 25.34375, weights = tensor([6.0156, 3.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №969: loss = 25.3125, weights = tensor([6.0156, 3.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №970: loss = 25.3125, weights = tensor([6.0156, 3.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №971: loss = 25.296875, weights = tensor([6.0156, 3.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №972: loss = 25.296875, weights = tensor([6.0156, 3.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №973: loss = 25.296875, weights = tensor([6.0156, 3.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №974: loss = 25.28125, weights = tensor([6.0156, 3.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №975: loss = 25.265625, weights = tensor([6.0156, 3.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №976: loss = 25.234375, weights = tensor([6.0117, 3.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №977: loss = 25.21875, weights = tensor([6.0117, 3.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №978: loss = 25.21875, weights = tensor([6.0117, 3.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №979: loss = 25.203125, weights = tensor([6.0117, 3.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №980: loss = 25.25, weights = tensor([6.0117, 3.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №981: loss = 25.234375, weights = tensor([6.0117, 3.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №982: loss = 25.21875, weights = tensor([6.0117, 3.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №983: loss = 25.21875, weights = tensor([6.0117, 3.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №984: loss = 25.203125, weights = tensor([6.0078, 3.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №985: loss = 25.171875, weights = tensor([6.0078, 3.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №986: loss = 25.171875, weights = tensor([6.0078, 3.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №987: loss = 25.171875, weights = tensor([6.0078, 3.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №988: loss = 25.171875, weights = tensor([6.0078, 3.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №989: loss = 25.171875, weights = tensor([6.0078, 3.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №990: loss = 25.15625, weights = tensor([6.0078, 3.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №991: loss = 25.15625, weights = tensor([6.0078, 3.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №992: loss = 25.109375, weights = tensor([6.0078, 3.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №993: loss = 25.09375, weights = tensor([6.0078, 3.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №994: loss = 25.09375, weights = tensor([6.0078, 3.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №995: loss = 25.078125, weights = tensor([6.0078, 3.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №996: loss = 25.125, weights = tensor([6.0039, 3.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №997: loss = 25.125, weights = tensor([6.0039, 3.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №998: loss = 25.09375, weights = tensor([6.0039, 3.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №999: loss = 25.09375, weights = tensor([6.0039, 3.3379], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#now let's use PyTorch gradient calculation. We don't need our gradient function. \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16, requires_grad = True) #it should be mentioned that in the future \n",
    "                                                            #you will need a derivative with respect to these values\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = mseerror(y, y_pred)\n",
    "    error.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rait*w.grad\n",
    "    w.grad.zero_()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "07a35c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453961a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
