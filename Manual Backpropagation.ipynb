{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a412bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18610bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41035715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eef8bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([14., 16., 15., 27., 25., 39., 39., 49., 47., 59.])\n"
     ]
    }
   ],
   "source": [
    "# creating some data to train Linear Regression \n",
    "x = torch.tensor(np.arange(10), dtype = torch.float32)\n",
    "y = x*5 + 10 + torch.randint(low = -5, high = 5, size = (10,), dtype = torch.float32)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2d386c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cbe7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# initializing weights\n",
    "# we have only one feature, so we need to find two coefficients: slope and intercept \n",
    "# let's initialize them as zeros \n",
    "\n",
    "w = torch.zeros(2, dtype = torch.float16, )\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d8f3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def predict(w, x):\n",
    "    y_pred = w[0]*x + w[1]\n",
    "    return y_pred \n",
    "\n",
    "def mseerror(y, y_pred):\n",
    "    loss_val = ((y_pred - y)**2).mean()\n",
    "    return loss_val \n",
    "\n",
    "# derivative from mse with respect to slope: 2x*(y - y_pred)\n",
    "def beta1gradient(x, y, y_pred):\n",
    "    direction = np.dot(2*x, y_pred - y).mean()\n",
    "    return direction \n",
    "# derivative from mse with respect to intercept: 2x*(y - y_pred)\n",
    "def beta0gradient(x, y, y_pred):\n",
    "    direction = np.dot(2, y_pred - y).mean()\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e2992c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1316.4000)\n"
     ]
    }
   ],
   "source": [
    "# let's check our predictions before training \n",
    "y_pred = predict(w, x)\n",
    "error = mseerror(y, y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b345c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "174bbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1316.4000244140625, weights = tensor([2.2871, 0.0396], dtype=torch.float16)\n",
      "step №1: loss = 591.8363037109375, weights = tensor([3.7891, 0.0668], dtype=torch.float16)\n",
      "step №2: loss = 279.05633544921875, weights = tensor([4.7773, 0.0859], dtype=torch.float16)\n",
      "step №3: loss = 143.7643280029297, weights = tensor([5.4258, 0.0995], dtype=torch.float16)\n",
      "step №4: loss = 85.40746307373047, weights = tensor([5.8516, 0.1097], dtype=torch.float16)\n",
      "step №5: loss = 60.19525146484375, weights = tensor([6.1328, 0.1176], dtype=torch.float16)\n",
      "step №6: loss = 49.23950958251953, weights = tensor([6.3164, 0.1240], dtype=torch.float16)\n",
      "step №7: loss = 44.53091049194336, weights = tensor([6.4375, 0.1293], dtype=torch.float16)\n",
      "step №8: loss = 42.479530334472656, weights = tensor([6.5156, 0.1339], dtype=torch.float16)\n",
      "step №9: loss = 41.597198486328125, weights = tensor([6.5664, 0.1382], dtype=torch.float16)\n",
      "step №10: loss = 41.2054328918457, weights = tensor([6.6016, 0.1422], dtype=torch.float16)\n",
      "step №11: loss = 41.01588439941406, weights = tensor([6.6250, 0.1460], dtype=torch.float16)\n",
      "step №12: loss = 40.92339324951172, weights = tensor([6.6367, 0.1497], dtype=torch.float16)\n",
      "step №13: loss = 40.878944396972656, weights = tensor([6.6445, 0.1532], dtype=torch.float16)\n",
      "step №14: loss = 40.847618103027344, weights = tensor([6.6523, 0.1567], dtype=torch.float16)\n",
      "step №15: loss = 40.82030487060547, weights = tensor([6.6562, 0.1603], dtype=torch.float16)\n",
      "step №16: loss = 40.79792022705078, weights = tensor([6.6602, 0.1637], dtype=torch.float16)\n",
      "step №17: loss = 40.777381896972656, weights = tensor([6.6602, 0.1671], dtype=torch.float16)\n",
      "step №18: loss = 40.75780487060547, weights = tensor([6.6602, 0.1705], dtype=torch.float16)\n",
      "step №19: loss = 40.738250732421875, weights = tensor([6.6602, 0.1740], dtype=torch.float16)\n",
      "step №20: loss = 40.71871566772461, weights = tensor([6.6602, 0.1774], dtype=torch.float16)\n",
      "step №21: loss = 40.69921112060547, weights = tensor([6.6602, 0.1808], dtype=torch.float16)\n",
      "step №22: loss = 40.679725646972656, weights = tensor([6.6602, 0.1842], dtype=torch.float16)\n",
      "step №23: loss = 40.6602668762207, weights = tensor([6.6602, 0.1876], dtype=torch.float16)\n",
      "step №24: loss = 40.640830993652344, weights = tensor([6.6602, 0.1910], dtype=torch.float16)\n",
      "step №25: loss = 40.62141799926758, weights = tensor([6.6602, 0.1945], dtype=torch.float16)\n",
      "step №26: loss = 40.602027893066406, weights = tensor([6.6602, 0.1979], dtype=torch.float16)\n",
      "step №27: loss = 40.58266067504883, weights = tensor([6.6602, 0.2013], dtype=torch.float16)\n",
      "step №28: loss = 40.563316345214844, weights = tensor([6.6602, 0.2047], dtype=torch.float16)\n",
      "step №29: loss = 40.54399490356445, weights = tensor([6.6602, 0.2081], dtype=torch.float16)\n",
      "step №30: loss = 40.52470016479492, weights = tensor([6.6602, 0.2115], dtype=torch.float16)\n",
      "step №31: loss = 40.50542449951172, weights = tensor([6.6562, 0.2150], dtype=torch.float16)\n",
      "step №32: loss = 40.485191345214844, weights = tensor([6.6562, 0.2184], dtype=torch.float16)\n",
      "step №33: loss = 40.46583938598633, weights = tensor([6.6562, 0.2218], dtype=torch.float16)\n",
      "step №34: loss = 40.44651794433594, weights = tensor([6.6562, 0.2252], dtype=torch.float16)\n",
      "step №35: loss = 40.42721939086914, weights = tensor([6.6562, 0.2286], dtype=torch.float16)\n",
      "step №36: loss = 40.407936096191406, weights = tensor([6.6562, 0.2321], dtype=torch.float16)\n",
      "step №37: loss = 40.38868713378906, weights = tensor([6.6562, 0.2355], dtype=torch.float16)\n",
      "step №38: loss = 40.36945343017578, weights = tensor([6.6562, 0.2389], dtype=torch.float16)\n",
      "step №39: loss = 40.350250244140625, weights = tensor([6.6523, 0.2423], dtype=torch.float16)\n",
      "step №40: loss = 40.32999038696289, weights = tensor([6.6523, 0.2457], dtype=torch.float16)\n",
      "step №41: loss = 40.31071090698242, weights = tensor([6.6523, 0.2491], dtype=torch.float16)\n",
      "step №42: loss = 40.29145431518555, weights = tensor([6.6523, 0.2524], dtype=torch.float16)\n",
      "step №43: loss = 40.27290344238281, weights = tensor([6.6523, 0.2559], dtype=torch.float16)\n",
      "step №44: loss = 40.25368881225586, weights = tensor([6.6523, 0.2593], dtype=torch.float16)\n",
      "step №45: loss = 40.23450469970703, weights = tensor([6.6523, 0.2627], dtype=torch.float16)\n",
      "step №46: loss = 40.21533966064453, weights = tensor([6.6484, 0.2661], dtype=torch.float16)\n",
      "step №47: loss = 40.19515609741211, weights = tensor([6.6484, 0.2695], dtype=torch.float16)\n",
      "step №48: loss = 40.17591094970703, weights = tensor([6.6484, 0.2729], dtype=torch.float16)\n",
      "step №49: loss = 40.15670394897461, weights = tensor([6.6484, 0.2764], dtype=torch.float16)\n",
      "step №50: loss = 40.13751220703125, weights = tensor([6.6484, 0.2798], dtype=torch.float16)\n",
      "step №51: loss = 40.11834716796875, weights = tensor([6.6484, 0.2832], dtype=torch.float16)\n",
      "step №52: loss = 40.09919738769531, weights = tensor([6.6484, 0.2866], dtype=torch.float16)\n",
      "step №53: loss = 40.080078125, weights = tensor([6.6445, 0.2900], dtype=torch.float16)\n",
      "step №54: loss = 40.059967041015625, weights = tensor([6.6445, 0.2935], dtype=torch.float16)\n",
      "step №55: loss = 40.040771484375, weights = tensor([6.6445, 0.2969], dtype=torch.float16)\n",
      "step №56: loss = 40.02159881591797, weights = tensor([6.6445, 0.3003], dtype=torch.float16)\n",
      "step №57: loss = 40.0024528503418, weights = tensor([6.6445, 0.3037], dtype=torch.float16)\n",
      "step №58: loss = 39.98332977294922, weights = tensor([6.6445, 0.3071], dtype=torch.float16)\n",
      "step №59: loss = 39.96422576904297, weights = tensor([6.6445, 0.3105], dtype=torch.float16)\n",
      "step №60: loss = 39.94514846801758, weights = tensor([6.6406, 0.3140], dtype=torch.float16)\n",
      "step №61: loss = 39.92511749267578, weights = tensor([6.6406, 0.3174], dtype=torch.float16)\n",
      "step №62: loss = 39.90596389770508, weights = tensor([6.6406, 0.3208], dtype=torch.float16)\n",
      "step №63: loss = 39.88683319091797, weights = tensor([6.6406, 0.3242], dtype=torch.float16)\n",
      "step №64: loss = 39.86771774291992, weights = tensor([6.6406, 0.3276], dtype=torch.float16)\n",
      "step №65: loss = 39.84865188598633, weights = tensor([6.6406, 0.3311], dtype=torch.float16)\n",
      "step №66: loss = 39.82958221435547, weights = tensor([6.6406, 0.3345], dtype=torch.float16)\n",
      "step №67: loss = 39.81055450439453, weights = tensor([6.6367, 0.3379], dtype=torch.float16)\n",
      "step №68: loss = 39.79058074951172, weights = tensor([6.6367, 0.3413], dtype=torch.float16)\n",
      "step №69: loss = 39.77147674560547, weights = tensor([6.6367, 0.3447], dtype=torch.float16)\n",
      "step №70: loss = 39.75239562988281, weights = tensor([6.6367, 0.3481], dtype=torch.float16)\n",
      "step №71: loss = 39.73332977294922, weights = tensor([6.6367, 0.3516], dtype=torch.float16)\n",
      "step №72: loss = 39.71429443359375, weights = tensor([6.6367, 0.3550], dtype=torch.float16)\n",
      "step №73: loss = 39.695281982421875, weights = tensor([6.6367, 0.3584], dtype=torch.float16)\n",
      "step №74: loss = 39.676292419433594, weights = tensor([6.6367, 0.3618], dtype=torch.float16)\n",
      "step №75: loss = 39.657325744628906, weights = tensor([6.6328, 0.3652], dtype=torch.float16)\n",
      "step №76: loss = 39.637325286865234, weights = tensor([6.6328, 0.3687], dtype=torch.float16)\n",
      "step №77: loss = 39.61829376220703, weights = tensor([6.6328, 0.3721], dtype=torch.float16)\n",
      "step №78: loss = 39.59926986694336, weights = tensor([6.6328, 0.3755], dtype=torch.float16)\n",
      "step №79: loss = 39.58028030395508, weights = tensor([6.6328, 0.3789], dtype=torch.float16)\n",
      "step №80: loss = 39.56129837036133, weights = tensor([6.6328, 0.3823], dtype=torch.float16)\n",
      "step №81: loss = 39.5423583984375, weights = tensor([6.6328, 0.3857], dtype=torch.float16)\n",
      "step №82: loss = 39.523433685302734, weights = tensor([6.6289, 0.3892], dtype=torch.float16)\n",
      "step №83: loss = 39.50351333618164, weights = tensor([6.6289, 0.3926], dtype=torch.float16)\n",
      "step №84: loss = 39.48451232910156, weights = tensor([6.6289, 0.3960], dtype=torch.float16)\n",
      "step №85: loss = 39.465545654296875, weights = tensor([6.6289, 0.3994], dtype=torch.float16)\n",
      "step №86: loss = 39.446590423583984, weights = tensor([6.6289, 0.4028], dtype=torch.float16)\n",
      "step №87: loss = 39.42766571044922, weights = tensor([6.6289, 0.4062], dtype=torch.float16)\n",
      "step №88: loss = 39.40876007080078, weights = tensor([6.6289, 0.4097], dtype=torch.float16)\n",
      "step №89: loss = 39.3898811340332, weights = tensor([6.6250, 0.4131], dtype=torch.float16)\n",
      "step №90: loss = 39.37003707885742, weights = tensor([6.6250, 0.4165], dtype=torch.float16)\n",
      "step №91: loss = 39.35106658935547, weights = tensor([6.6250, 0.4199], dtype=torch.float16)\n",
      "step №92: loss = 39.3321418762207, weights = tensor([6.6250, 0.4233], dtype=torch.float16)\n",
      "step №93: loss = 39.3132438659668, weights = tensor([6.6250, 0.4268], dtype=torch.float16)\n",
      "step №94: loss = 39.29435729980469, weights = tensor([6.6250, 0.4302], dtype=torch.float16)\n",
      "step №95: loss = 39.27549743652344, weights = tensor([6.6250, 0.4336], dtype=torch.float16)\n",
      "step №96: loss = 39.25664520263672, weights = tensor([6.6211, 0.4370], dtype=torch.float16)\n",
      "step №97: loss = 39.23687744140625, weights = tensor([6.6211, 0.4404], dtype=torch.float16)\n",
      "step №98: loss = 39.21796798706055, weights = tensor([6.6211, 0.4438], dtype=torch.float16)\n",
      "step №99: loss = 39.19908142089844, weights = tensor([6.6211, 0.4473], dtype=torch.float16)\n",
      "step №100: loss = 39.180213928222656, weights = tensor([6.6211, 0.4507], dtype=torch.float16)\n",
      "step №101: loss = 39.161373138427734, weights = tensor([6.6211, 0.4541], dtype=torch.float16)\n",
      "step №102: loss = 39.14255905151367, weights = tensor([6.6211, 0.4575], dtype=torch.float16)\n",
      "step №103: loss = 39.12376403808594, weights = tensor([6.6211, 0.4609], dtype=torch.float16)\n",
      "step №104: loss = 39.1049919128418, weights = tensor([6.6172, 0.4641], dtype=torch.float16)\n",
      "step №105: loss = 39.08654022216797, weights = tensor([6.6172, 0.4675], dtype=torch.float16)\n",
      "step №106: loss = 39.067691802978516, weights = tensor([6.6172, 0.4709], dtype=torch.float16)\n",
      "step №107: loss = 39.048866271972656, weights = tensor([6.6172, 0.4744], dtype=torch.float16)\n",
      "step №108: loss = 39.030067443847656, weights = tensor([6.6172, 0.4778], dtype=torch.float16)\n",
      "step №109: loss = 39.011295318603516, weights = tensor([6.6172, 0.4810], dtype=torch.float16)\n",
      "step №110: loss = 38.99388122558594, weights = tensor([6.6172, 0.4841], dtype=torch.float16)\n",
      "step №111: loss = 38.97649383544922, weights = tensor([6.6133, 0.4873], dtype=torch.float16)\n",
      "step №112: loss = 38.95812225341797, weights = tensor([6.6133, 0.4907], dtype=torch.float16)\n",
      "step №113: loss = 38.9393196105957, weights = tensor([6.6133, 0.4941], dtype=torch.float16)\n",
      "step №114: loss = 38.920528411865234, weights = tensor([6.6133, 0.4973], dtype=torch.float16)\n",
      "step №115: loss = 38.90311050415039, weights = tensor([6.6133, 0.5005], dtype=torch.float16)\n",
      "step №116: loss = 38.885711669921875, weights = tensor([6.6133, 0.5039], dtype=torch.float16)\n",
      "step №117: loss = 38.86699295043945, weights = tensor([6.6133, 0.5073], dtype=torch.float16)\n",
      "step №118: loss = 38.84830093383789, weights = tensor([6.6133, 0.5107], dtype=torch.float16)\n",
      "step №119: loss = 38.829627990722656, weights = tensor([6.6094, 0.5142], dtype=torch.float16)\n",
      "step №120: loss = 38.80991744995117, weights = tensor([6.6094, 0.5176], dtype=torch.float16)\n",
      "step №121: loss = 38.791168212890625, weights = tensor([6.6094, 0.5210], dtype=torch.float16)\n",
      "step №122: loss = 38.7724494934082, weights = tensor([6.6094, 0.5244], dtype=torch.float16)\n",
      "step №123: loss = 38.75375747680664, weights = tensor([6.6094, 0.5278], dtype=torch.float16)\n",
      "step №124: loss = 38.73508071899414, weights = tensor([6.6094, 0.5312], dtype=torch.float16)\n",
      "step №125: loss = 38.7164306640625, weights = tensor([6.6094, 0.5347], dtype=torch.float16)\n",
      "step №126: loss = 38.69780349731445, weights = tensor([6.6055, 0.5381], dtype=torch.float16)\n",
      "step №127: loss = 38.67816162109375, weights = tensor([6.6055, 0.5415], dtype=torch.float16)\n",
      "step №128: loss = 38.659461975097656, weights = tensor([6.6055, 0.5449], dtype=torch.float16)\n",
      "step №129: loss = 38.64078140258789, weights = tensor([6.6055, 0.5483], dtype=torch.float16)\n",
      "step №130: loss = 38.62213134765625, weights = tensor([6.6055, 0.5518], dtype=torch.float16)\n",
      "step №131: loss = 38.60350036621094, weights = tensor([6.6055, 0.5552], dtype=torch.float16)\n",
      "step №132: loss = 38.58489227294922, weights = tensor([6.6055, 0.5586], dtype=torch.float16)\n",
      "step №133: loss = 38.566307067871094, weights = tensor([6.6016, 0.5620], dtype=torch.float16)\n",
      "step №134: loss = 38.54674530029297, weights = tensor([6.6016, 0.5654], dtype=torch.float16)\n",
      "step №135: loss = 38.52808380126953, weights = tensor([6.6016, 0.5688], dtype=torch.float16)\n",
      "step №136: loss = 38.50945281982422, weights = tensor([6.6016, 0.5723], dtype=torch.float16)\n",
      "step №137: loss = 38.49083709716797, weights = tensor([6.6016, 0.5757], dtype=torch.float16)\n",
      "step №138: loss = 38.472251892089844, weights = tensor([6.6016, 0.5791], dtype=torch.float16)\n",
      "step №139: loss = 38.45368957519531, weights = tensor([6.6016, 0.5825], dtype=torch.float16)\n",
      "step №140: loss = 38.435150146484375, weights = tensor([6.5977, 0.5859], dtype=torch.float16)\n",
      "step №141: loss = 38.4156494140625, weights = tensor([6.5977, 0.5894], dtype=torch.float16)\n",
      "step №142: loss = 38.39703369140625, weights = tensor([6.5977, 0.5928], dtype=torch.float16)\n",
      "step №143: loss = 38.378448486328125, weights = tensor([6.5977, 0.5962], dtype=torch.float16)\n",
      "step №144: loss = 38.35988235473633, weights = tensor([6.5977, 0.5996], dtype=torch.float16)\n",
      "step №145: loss = 38.341331481933594, weights = tensor([6.5977, 0.6030], dtype=torch.float16)\n",
      "step №146: loss = 38.32281494140625, weights = tensor([6.5977, 0.6064], dtype=torch.float16)\n",
      "step №147: loss = 38.3043212890625, weights = tensor([6.5977, 0.6099], dtype=torch.float16)\n",
      "step №148: loss = 38.28584671020508, weights = tensor([6.5938, 0.6133], dtype=torch.float16)\n",
      "step №149: loss = 38.26631546020508, weights = tensor([6.5938, 0.6167], dtype=torch.float16)\n",
      "step №150: loss = 38.247772216796875, weights = tensor([6.5938, 0.6201], dtype=torch.float16)\n",
      "step №151: loss = 38.22925567626953, weights = tensor([6.5938, 0.6235], dtype=torch.float16)\n",
      "step №152: loss = 38.210750579833984, weights = tensor([6.5938, 0.6270], dtype=torch.float16)\n",
      "step №153: loss = 38.1922721862793, weights = tensor([6.5938, 0.6304], dtype=torch.float16)\n",
      "step №154: loss = 38.173824310302734, weights = tensor([6.5938, 0.6338], dtype=torch.float16)\n",
      "step №155: loss = 38.15540313720703, weights = tensor([6.5898, 0.6372], dtype=torch.float16)\n",
      "step №156: loss = 38.13594436645508, weights = tensor([6.5898, 0.6406], dtype=torch.float16)\n",
      "step №157: loss = 38.11743927001953, weights = tensor([6.5898, 0.6440], dtype=torch.float16)\n",
      "step №158: loss = 38.098960876464844, weights = tensor([6.5898, 0.6475], dtype=torch.float16)\n",
      "step №159: loss = 38.080501556396484, weights = tensor([6.5898, 0.6509], dtype=torch.float16)\n",
      "step №160: loss = 38.062068939208984, weights = tensor([6.5898, 0.6543], dtype=torch.float16)\n",
      "step №161: loss = 38.04365539550781, weights = tensor([6.5898, 0.6577], dtype=torch.float16)\n",
      "step №162: loss = 38.02527618408203, weights = tensor([6.5859, 0.6611], dtype=torch.float16)\n",
      "step №163: loss = 38.00589370727539, weights = tensor([6.5859, 0.6646], dtype=torch.float16)\n",
      "step №164: loss = 37.9874382019043, weights = tensor([6.5859, 0.6680], dtype=torch.float16)\n",
      "step №165: loss = 37.96898651123047, weights = tensor([6.5859, 0.6714], dtype=torch.float16)\n",
      "step №166: loss = 37.950584411621094, weights = tensor([6.5859, 0.6748], dtype=torch.float16)\n",
      "step №167: loss = 37.932193756103516, weights = tensor([6.5859, 0.6782], dtype=torch.float16)\n",
      "step №168: loss = 37.91382598876953, weights = tensor([6.5859, 0.6816], dtype=torch.float16)\n",
      "step №169: loss = 37.89548110961914, weights = tensor([6.5820, 0.6851], dtype=torch.float16)\n",
      "step №170: loss = 37.87617874145508, weights = tensor([6.5820, 0.6885], dtype=torch.float16)\n",
      "step №171: loss = 37.857765197753906, weights = tensor([6.5820, 0.6919], dtype=torch.float16)\n",
      "step №172: loss = 37.8393669128418, weights = tensor([6.5820, 0.6953], dtype=torch.float16)\n",
      "step №173: loss = 37.82099533081055, weights = tensor([6.5820, 0.6987], dtype=torch.float16)\n",
      "step №174: loss = 37.802650451660156, weights = tensor([6.5820, 0.7021], dtype=torch.float16)\n",
      "step №175: loss = 37.78432846069336, weights = tensor([6.5820, 0.7056], dtype=torch.float16)\n",
      "step №176: loss = 37.766029357910156, weights = tensor([6.5781, 0.7090], dtype=torch.float16)\n",
      "step №177: loss = 37.746788024902344, weights = tensor([6.5781, 0.7124], dtype=torch.float16)\n",
      "step №178: loss = 37.72842025756836, weights = tensor([6.5781, 0.7158], dtype=torch.float16)\n",
      "step №179: loss = 37.71006393432617, weights = tensor([6.5781, 0.7192], dtype=torch.float16)\n",
      "step №180: loss = 37.691741943359375, weights = tensor([6.5781, 0.7227], dtype=torch.float16)\n",
      "step №181: loss = 37.673431396484375, weights = tensor([6.5781, 0.7261], dtype=torch.float16)\n",
      "step №182: loss = 37.65515899658203, weights = tensor([6.5781, 0.7295], dtype=torch.float16)\n",
      "step №183: loss = 37.63690948486328, weights = tensor([6.5781, 0.7329], dtype=torch.float16)\n",
      "step №184: loss = 37.618675231933594, weights = tensor([6.5742, 0.7363], dtype=torch.float16)\n",
      "step №185: loss = 37.59940719604492, weights = tensor([6.5742, 0.7397], dtype=torch.float16)\n",
      "step №186: loss = 37.58110809326172, weights = tensor([6.5742, 0.7432], dtype=torch.float16)\n",
      "step №187: loss = 37.56282424926758, weights = tensor([6.5742, 0.7466], dtype=torch.float16)\n",
      "step №188: loss = 37.54456329345703, weights = tensor([6.5742, 0.7500], dtype=torch.float16)\n",
      "step №189: loss = 37.52632522583008, weights = tensor([6.5742, 0.7534], dtype=torch.float16)\n",
      "step №190: loss = 37.508113861083984, weights = tensor([6.5742, 0.7568], dtype=torch.float16)\n",
      "step №191: loss = 37.48992156982422, weights = tensor([6.5703, 0.7603], dtype=torch.float16)\n",
      "step №192: loss = 37.47073745727539, weights = tensor([6.5703, 0.7637], dtype=torch.float16)\n",
      "step №193: loss = 37.45246887207031, weights = tensor([6.5703, 0.7671], dtype=torch.float16)\n",
      "step №194: loss = 37.434234619140625, weights = tensor([6.5703, 0.7705], dtype=torch.float16)\n",
      "step №195: loss = 37.416015625, weights = tensor([6.5703, 0.7739], dtype=torch.float16)\n",
      "step №196: loss = 37.39781951904297, weights = tensor([6.5703, 0.7773], dtype=torch.float16)\n",
      "step №197: loss = 37.37964630126953, weights = tensor([6.5703, 0.7808], dtype=torch.float16)\n",
      "step №198: loss = 37.36151123046875, weights = tensor([6.5664, 0.7842], dtype=torch.float16)\n",
      "step №199: loss = 37.34239196777344, weights = tensor([6.5664, 0.7876], dtype=torch.float16)\n",
      "step №200: loss = 37.32417678833008, weights = tensor([6.5664, 0.7910], dtype=torch.float16)\n",
      "step №201: loss = 37.305973052978516, weights = tensor([6.5664, 0.7944], dtype=torch.float16)\n",
      "step №202: loss = 37.28779983520508, weights = tensor([6.5664, 0.7979], dtype=torch.float16)\n",
      "step №203: loss = 37.269657135009766, weights = tensor([6.5664, 0.8013], dtype=torch.float16)\n",
      "step №204: loss = 37.25152587890625, weights = tensor([6.5664, 0.8047], dtype=torch.float16)\n",
      "step №205: loss = 37.233421325683594, weights = tensor([6.5625, 0.8081], dtype=torch.float16)\n",
      "step №206: loss = 37.21438980102539, weights = tensor([6.5625, 0.8115], dtype=torch.float16)\n",
      "step №207: loss = 37.196205139160156, weights = tensor([6.5625, 0.8149], dtype=torch.float16)\n",
      "step №208: loss = 37.17804718017578, weights = tensor([6.5625, 0.8184], dtype=torch.float16)\n",
      "step №209: loss = 37.15991973876953, weights = tensor([6.5625, 0.8218], dtype=torch.float16)\n",
      "step №210: loss = 37.141822814941406, weights = tensor([6.5625, 0.8252], dtype=torch.float16)\n",
      "step №211: loss = 37.12372589111328, weights = tensor([6.5625, 0.8281], dtype=torch.float16)\n",
      "step №212: loss = 37.108253479003906, weights = tensor([6.5625, 0.8311], dtype=torch.float16)\n",
      "step №213: loss = 37.09278106689453, weights = tensor([6.5586, 0.8340], dtype=torch.float16)\n",
      "step №214: loss = 37.076332092285156, weights = tensor([6.5586, 0.8374], dtype=torch.float16)\n",
      "step №215: loss = 37.05821990966797, weights = tensor([6.5586, 0.8408], dtype=torch.float16)\n",
      "step №216: loss = 37.040122985839844, weights = tensor([6.5586, 0.8442], dtype=torch.float16)\n",
      "step №217: loss = 37.02205276489258, weights = tensor([6.5586, 0.8472], dtype=torch.float16)\n",
      "step №218: loss = 37.0065803527832, weights = tensor([6.5586, 0.8501], dtype=torch.float16)\n",
      "step №219: loss = 36.99112319946289, weights = tensor([6.5586, 0.8530], dtype=torch.float16)\n",
      "step №220: loss = 36.975685119628906, weights = tensor([6.5586, 0.8560], dtype=torch.float16)\n",
      "step №221: loss = 36.96025848388672, weights = tensor([6.5547, 0.8589], dtype=torch.float16)\n",
      "step №222: loss = 36.94384765625, weights = tensor([6.5547, 0.8623], dtype=torch.float16)\n",
      "step №223: loss = 36.925777435302734, weights = tensor([6.5547, 0.8652], dtype=torch.float16)\n",
      "step №224: loss = 36.91030502319336, weights = tensor([6.5547, 0.8682], dtype=torch.float16)\n",
      "step №225: loss = 36.894859313964844, weights = tensor([6.5547, 0.8711], dtype=torch.float16)\n",
      "step №226: loss = 36.87941360473633, weights = tensor([6.5547, 0.8740], dtype=torch.float16)\n",
      "step №227: loss = 36.86400604248047, weights = tensor([6.5547, 0.8770], dtype=torch.float16)\n",
      "step №228: loss = 36.84859848022461, weights = tensor([6.5547, 0.8799], dtype=torch.float16)\n",
      "step №229: loss = 36.83321762084961, weights = tensor([6.5508, 0.8828], dtype=torch.float16)\n",
      "step №230: loss = 36.816871643066406, weights = tensor([6.5508, 0.8857], dtype=torch.float16)\n",
      "step №231: loss = 36.80141830444336, weights = tensor([6.5508, 0.8887], dtype=torch.float16)\n",
      "step №232: loss = 36.785980224609375, weights = tensor([6.5508, 0.8916], dtype=torch.float16)\n",
      "step №233: loss = 36.77056884765625, weights = tensor([6.5508, 0.8945], dtype=torch.float16)\n",
      "step №234: loss = 36.755165100097656, weights = tensor([6.5508, 0.8975], dtype=torch.float16)\n",
      "step №235: loss = 36.739784240722656, weights = tensor([6.5508, 0.9004], dtype=torch.float16)\n",
      "step №236: loss = 36.72441101074219, weights = tensor([6.5508, 0.9033], dtype=torch.float16)\n",
      "step №237: loss = 36.709068298339844, weights = tensor([6.5508, 0.9062], dtype=torch.float16)\n",
      "step №238: loss = 36.69373321533203, weights = tensor([6.5469, 0.9092], dtype=torch.float16)\n",
      "step №239: loss = 36.67737579345703, weights = tensor([6.5469, 0.9121], dtype=torch.float16)\n",
      "step №240: loss = 36.6619758605957, weights = tensor([6.5469, 0.9150], dtype=torch.float16)\n",
      "step №241: loss = 36.64659881591797, weights = tensor([6.5469, 0.9180], dtype=torch.float16)\n",
      "step №242: loss = 36.63121795654297, weights = tensor([6.5469, 0.9209], dtype=torch.float16)\n",
      "step №243: loss = 36.61587905883789, weights = tensor([6.5469, 0.9238], dtype=torch.float16)\n",
      "step №244: loss = 36.60054397583008, weights = tensor([6.5469, 0.9268], dtype=torch.float16)\n",
      "step №245: loss = 36.585235595703125, weights = tensor([6.5469, 0.9297], dtype=torch.float16)\n",
      "step №246: loss = 36.5699348449707, weights = tensor([6.5430, 0.9326], dtype=torch.float16)\n",
      "step №247: loss = 36.55365753173828, weights = tensor([6.5430, 0.9355], dtype=torch.float16)\n",
      "step №248: loss = 36.538291931152344, weights = tensor([6.5430, 0.9385], dtype=torch.float16)\n",
      "step №249: loss = 36.522945404052734, weights = tensor([6.5430, 0.9414], dtype=torch.float16)\n",
      "step №250: loss = 36.50761032104492, weights = tensor([6.5430, 0.9443], dtype=torch.float16)\n",
      "step №251: loss = 36.4922981262207, weights = tensor([6.5430, 0.9473], dtype=torch.float16)\n",
      "step №252: loss = 36.47699737548828, weights = tensor([6.5430, 0.9502], dtype=torch.float16)\n",
      "step №253: loss = 36.46171951293945, weights = tensor([6.5430, 0.9531], dtype=torch.float16)\n",
      "step №254: loss = 36.44645309448242, weights = tensor([6.5391, 0.9561], dtype=torch.float16)\n",
      "step №255: loss = 36.430259704589844, weights = tensor([6.5391, 0.9590], dtype=torch.float16)\n",
      "step №256: loss = 36.41492462158203, weights = tensor([6.5391, 0.9619], dtype=torch.float16)\n",
      "step №257: loss = 36.399620056152344, weights = tensor([6.5391, 0.9648], dtype=torch.float16)\n",
      "step №258: loss = 36.384307861328125, weights = tensor([6.5391, 0.9678], dtype=torch.float16)\n",
      "step №259: loss = 36.36903762817383, weights = tensor([6.5391, 0.9707], dtype=torch.float16)\n",
      "step №260: loss = 36.35376739501953, weights = tensor([6.5391, 0.9736], dtype=torch.float16)\n",
      "step №261: loss = 36.338523864746094, weights = tensor([6.5391, 0.9766], dtype=torch.float16)\n",
      "step №262: loss = 36.32329559326172, weights = tensor([6.5391, 0.9795], dtype=torch.float16)\n",
      "step №263: loss = 36.308082580566406, weights = tensor([6.5352, 0.9824], dtype=torch.float16)\n",
      "step №264: loss = 36.2918815612793, weights = tensor([6.5352, 0.9854], dtype=torch.float16)\n",
      "step №265: loss = 36.276607513427734, weights = tensor([6.5352, 0.9883], dtype=torch.float16)\n",
      "step №266: loss = 36.2613410949707, weights = tensor([6.5352, 0.9912], dtype=torch.float16)\n",
      "step №267: loss = 36.246097564697266, weights = tensor([6.5352, 0.9941], dtype=torch.float16)\n",
      "step №268: loss = 36.23086166381836, weights = tensor([6.5352, 0.9971], dtype=torch.float16)\n",
      "step №269: loss = 36.21565628051758, weights = tensor([6.5352, 1.0000], dtype=torch.float16)\n",
      "step №270: loss = 36.20045852661133, weights = tensor([6.5352, 1.0029], dtype=torch.float16)\n",
      "step №271: loss = 36.185279846191406, weights = tensor([6.5312, 1.0059], dtype=torch.float16)\n",
      "step №272: loss = 36.169158935546875, weights = tensor([6.5312, 1.0088], dtype=torch.float16)\n",
      "step №273: loss = 36.15392303466797, weights = tensor([6.5312, 1.0117], dtype=torch.float16)\n",
      "step №274: loss = 36.13867950439453, weights = tensor([6.5312, 1.0146], dtype=torch.float16)\n",
      "step №275: loss = 36.12347412109375, weights = tensor([6.5312, 1.0176], dtype=torch.float16)\n",
      "step №276: loss = 36.1082763671875, weights = tensor([6.5312, 1.0205], dtype=torch.float16)\n",
      "step №277: loss = 36.09310531616211, weights = tensor([6.5312, 1.0234], dtype=torch.float16)\n",
      "step №278: loss = 36.07794189453125, weights = tensor([6.5312, 1.0264], dtype=torch.float16)\n",
      "step №279: loss = 36.062801361083984, weights = tensor([6.5312, 1.0293], dtype=torch.float16)\n",
      "step №280: loss = 36.04766845703125, weights = tensor([6.5273, 1.0322], dtype=torch.float16)\n",
      "step №281: loss = 36.03154754638672, weights = tensor([6.5273, 1.0352], dtype=torch.float16)\n",
      "step №282: loss = 36.01634979248047, weights = tensor([6.5273, 1.0381], dtype=torch.float16)\n",
      "step №283: loss = 36.00117492675781, weights = tensor([6.5273, 1.0410], dtype=torch.float16)\n",
      "step №284: loss = 35.98601150512695, weights = tensor([6.5273, 1.0439], dtype=torch.float16)\n",
      "step №285: loss = 35.97087097167969, weights = tensor([6.5273, 1.0469], dtype=torch.float16)\n",
      "step №286: loss = 35.95574188232422, weights = tensor([6.5273, 1.0498], dtype=torch.float16)\n",
      "step №287: loss = 35.940635681152344, weights = tensor([6.5273, 1.0527], dtype=torch.float16)\n",
      "step №288: loss = 35.925540924072266, weights = tensor([6.5234, 1.0557], dtype=torch.float16)\n",
      "step №289: loss = 35.90950393676758, weights = tensor([6.5234, 1.0586], dtype=torch.float16)\n",
      "step №290: loss = 35.89432907104492, weights = tensor([6.5234, 1.0615], dtype=torch.float16)\n",
      "step №291: loss = 35.87919616699219, weights = tensor([6.5234, 1.0645], dtype=torch.float16)\n",
      "step №292: loss = 35.86406707763672, weights = tensor([6.5234, 1.0674], dtype=torch.float16)\n",
      "step №293: loss = 35.848960876464844, weights = tensor([6.5234, 1.0703], dtype=torch.float16)\n",
      "step №294: loss = 35.83386993408203, weights = tensor([6.5234, 1.0732], dtype=torch.float16)\n",
      "step №295: loss = 35.81879425048828, weights = tensor([6.5234, 1.0762], dtype=torch.float16)\n",
      "step №296: loss = 35.803733825683594, weights = tensor([6.5234, 1.0791], dtype=torch.float16)\n",
      "step №297: loss = 35.7886962890625, weights = tensor([6.5195, 1.0820], dtype=torch.float16)\n",
      "step №298: loss = 35.77264404296875, weights = tensor([6.5195, 1.0850], dtype=torch.float16)\n",
      "step №299: loss = 35.757537841796875, weights = tensor([6.5195, 1.0879], dtype=torch.float16)\n",
      "step №300: loss = 35.74243927001953, weights = tensor([6.5195, 1.0908], dtype=torch.float16)\n",
      "step №301: loss = 35.72737503051758, weights = tensor([6.5195, 1.0938], dtype=torch.float16)\n",
      "step №302: loss = 35.712310791015625, weights = tensor([6.5195, 1.0967], dtype=torch.float16)\n",
      "step №303: loss = 35.69727325439453, weights = tensor([6.5195, 1.0996], dtype=torch.float16)\n",
      "step №304: loss = 35.68224334716797, weights = tensor([6.5195, 1.1025], dtype=torch.float16)\n",
      "step №305: loss = 35.66724395751953, weights = tensor([6.5156, 1.1055], dtype=torch.float16)\n",
      "step №306: loss = 35.65126419067383, weights = tensor([6.5156, 1.1084], dtype=torch.float16)\n",
      "step №307: loss = 35.63619613647461, weights = tensor([6.5156, 1.1113], dtype=torch.float16)\n",
      "step №308: loss = 35.62113571166992, weights = tensor([6.5156, 1.1143], dtype=torch.float16)\n",
      "step №309: loss = 35.606101989746094, weights = tensor([6.5156, 1.1172], dtype=torch.float16)\n",
      "step №310: loss = 35.5910758972168, weights = tensor([6.5156, 1.1201], dtype=torch.float16)\n",
      "step №311: loss = 35.576072692871094, weights = tensor([6.5156, 1.1230], dtype=torch.float16)\n",
      "step №312: loss = 35.56107711791992, weights = tensor([6.5156, 1.1260], dtype=torch.float16)\n",
      "step №313: loss = 35.54610824584961, weights = tensor([6.5156, 1.1289], dtype=torch.float16)\n",
      "step №314: loss = 35.53114700317383, weights = tensor([6.5117, 1.1318], dtype=torch.float16)\n",
      "step №315: loss = 35.51517868041992, weights = tensor([6.5117, 1.1348], dtype=torch.float16)\n",
      "step №316: loss = 35.500152587890625, weights = tensor([6.5117, 1.1377], dtype=torch.float16)\n",
      "step №317: loss = 35.48514938354492, weights = tensor([6.5117, 1.1406], dtype=torch.float16)\n",
      "step №318: loss = 35.47016143798828, weights = tensor([6.5117, 1.1436], dtype=torch.float16)\n",
      "step №319: loss = 35.45519256591797, weights = tensor([6.5117, 1.1465], dtype=torch.float16)\n",
      "step №320: loss = 35.44023132324219, weights = tensor([6.5117, 1.1494], dtype=torch.float16)\n",
      "step №321: loss = 35.42530059814453, weights = tensor([6.5117, 1.1523], dtype=torch.float16)\n",
      "step №322: loss = 35.410377502441406, weights = tensor([6.5078, 1.1553], dtype=torch.float16)\n",
      "step №323: loss = 35.39448165893555, weights = tensor([6.5078, 1.1582], dtype=torch.float16)\n",
      "step №324: loss = 35.37948989868164, weights = tensor([6.5078, 1.1611], dtype=torch.float16)\n",
      "step №325: loss = 35.36452102661133, weights = tensor([6.5078, 1.1641], dtype=torch.float16)\n",
      "step №326: loss = 35.34956741333008, weights = tensor([6.5078, 1.1670], dtype=torch.float16)\n",
      "step №327: loss = 35.33462905883789, weights = tensor([6.5078, 1.1699], dtype=torch.float16)\n",
      "step №328: loss = 35.319705963134766, weights = tensor([6.5078, 1.1729], dtype=torch.float16)\n",
      "step №329: loss = 35.304805755615234, weights = tensor([6.5078, 1.1758], dtype=torch.float16)\n",
      "step №330: loss = 35.28990936279297, weights = tensor([6.5039, 1.1787], dtype=torch.float16)\n",
      "step №331: loss = 35.274105072021484, weights = tensor([6.5039, 1.1816], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №332: loss = 35.25914764404297, weights = tensor([6.5039, 1.1846], dtype=torch.float16)\n",
      "step №333: loss = 35.24421310424805, weights = tensor([6.5039, 1.1875], dtype=torch.float16)\n",
      "step №334: loss = 35.22929000854492, weights = tensor([6.5039, 1.1904], dtype=torch.float16)\n",
      "step №335: loss = 35.21438980102539, weights = tensor([6.5039, 1.1934], dtype=torch.float16)\n",
      "step №336: loss = 35.199501037597656, weights = tensor([6.5039, 1.1963], dtype=torch.float16)\n",
      "step №337: loss = 35.184635162353516, weights = tensor([6.5039, 1.1992], dtype=torch.float16)\n",
      "step №338: loss = 35.16978073120117, weights = tensor([6.5039, 1.2021], dtype=torch.float16)\n",
      "step №339: loss = 35.15494918823242, weights = tensor([6.5000, 1.2051], dtype=torch.float16)\n",
      "step №340: loss = 35.139122009277344, weights = tensor([6.5000, 1.2080], dtype=torch.float16)\n",
      "step №341: loss = 35.124229431152344, weights = tensor([6.5000, 1.2109], dtype=torch.float16)\n",
      "step №342: loss = 35.109336853027344, weights = tensor([6.5000, 1.2139], dtype=torch.float16)\n",
      "step №343: loss = 35.09447479248047, weights = tensor([6.5000, 1.2168], dtype=torch.float16)\n",
      "step №344: loss = 35.079612731933594, weights = tensor([6.5000, 1.2197], dtype=torch.float16)\n",
      "step №345: loss = 35.064781188964844, weights = tensor([6.5000, 1.2227], dtype=torch.float16)\n",
      "step №346: loss = 35.049957275390625, weights = tensor([6.5000, 1.2256], dtype=torch.float16)\n",
      "step №347: loss = 35.03517532348633, weights = tensor([6.4961, 1.2285], dtype=torch.float16)\n",
      "step №348: loss = 35.0194206237793, weights = tensor([6.4961, 1.2314], dtype=torch.float16)\n",
      "step №349: loss = 35.004554748535156, weights = tensor([6.4961, 1.2344], dtype=torch.float16)\n",
      "step №350: loss = 34.98970413208008, weights = tensor([6.4961, 1.2373], dtype=torch.float16)\n",
      "step №351: loss = 34.97487258911133, weights = tensor([6.4961, 1.2402], dtype=torch.float16)\n",
      "step №352: loss = 34.96004867553711, weights = tensor([6.4961, 1.2432], dtype=torch.float16)\n",
      "step №353: loss = 34.945255279541016, weights = tensor([6.4961, 1.2461], dtype=torch.float16)\n",
      "step №354: loss = 34.93046951293945, weights = tensor([6.4961, 1.2490], dtype=torch.float16)\n",
      "step №355: loss = 34.915706634521484, weights = tensor([6.4961, 1.2520], dtype=torch.float16)\n",
      "step №356: loss = 34.90095138549805, weights = tensor([6.4922, 1.2549], dtype=torch.float16)\n",
      "step №357: loss = 34.88520812988281, weights = tensor([6.4922, 1.2578], dtype=torch.float16)\n",
      "step №358: loss = 34.870391845703125, weights = tensor([6.4922, 1.2607], dtype=torch.float16)\n",
      "step №359: loss = 34.8555908203125, weights = tensor([6.4922, 1.2637], dtype=torch.float16)\n",
      "step №360: loss = 34.84080505371094, weights = tensor([6.4922, 1.2666], dtype=torch.float16)\n",
      "step №361: loss = 34.82604217529297, weights = tensor([6.4922, 1.2695], dtype=torch.float16)\n",
      "step №362: loss = 34.81128692626953, weights = tensor([6.4922, 1.2725], dtype=torch.float16)\n",
      "step №363: loss = 34.796566009521484, weights = tensor([6.4922, 1.2754], dtype=torch.float16)\n",
      "step №364: loss = 34.78184509277344, weights = tensor([6.4883, 1.2783], dtype=torch.float16)\n",
      "step №365: loss = 34.76618576049805, weights = tensor([6.4883, 1.2812], dtype=torch.float16)\n",
      "step №366: loss = 34.75139617919922, weights = tensor([6.4883, 1.2842], dtype=torch.float16)\n",
      "step №367: loss = 34.73663330078125, weights = tensor([6.4883, 1.2871], dtype=torch.float16)\n",
      "step №368: loss = 34.72188186645508, weights = tensor([6.4883, 1.2900], dtype=torch.float16)\n",
      "step №369: loss = 34.7071533203125, weights = tensor([6.4883, 1.2930], dtype=torch.float16)\n",
      "step №370: loss = 34.69243621826172, weights = tensor([6.4883, 1.2959], dtype=torch.float16)\n",
      "step №371: loss = 34.67774200439453, weights = tensor([6.4883, 1.2988], dtype=torch.float16)\n",
      "step №372: loss = 34.66305923461914, weights = tensor([6.4883, 1.3018], dtype=torch.float16)\n",
      "step №373: loss = 34.648399353027344, weights = tensor([6.4844, 1.3047], dtype=torch.float16)\n",
      "step №374: loss = 34.632728576660156, weights = tensor([6.4844, 1.3076], dtype=torch.float16)\n",
      "step №375: loss = 34.61800003051758, weights = tensor([6.4844, 1.3105], dtype=torch.float16)\n",
      "step №376: loss = 34.60327911376953, weights = tensor([6.4844, 1.3135], dtype=torch.float16)\n",
      "step №377: loss = 34.588584899902344, weights = tensor([6.4844, 1.3164], dtype=torch.float16)\n",
      "step №378: loss = 34.57389450073242, weights = tensor([6.4844, 1.3193], dtype=torch.float16)\n",
      "step №379: loss = 34.559242248535156, weights = tensor([6.4844, 1.3223], dtype=torch.float16)\n",
      "step №380: loss = 34.544593811035156, weights = tensor([6.4844, 1.3252], dtype=torch.float16)\n",
      "step №381: loss = 34.52996063232422, weights = tensor([6.4805, 1.3281], dtype=torch.float16)\n",
      "step №382: loss = 34.514373779296875, weights = tensor([6.4805, 1.3311], dtype=torch.float16)\n",
      "step №383: loss = 34.49968338012695, weights = tensor([6.4805, 1.3340], dtype=torch.float16)\n",
      "step №384: loss = 34.48499298095703, weights = tensor([6.4805, 1.3369], dtype=torch.float16)\n",
      "step №385: loss = 34.4703369140625, weights = tensor([6.4805, 1.3398], dtype=torch.float16)\n",
      "step №386: loss = 34.4556884765625, weights = tensor([6.4805, 1.3428], dtype=torch.float16)\n",
      "step №387: loss = 34.441062927246094, weights = tensor([6.4805, 1.3457], dtype=torch.float16)\n",
      "step №388: loss = 34.42644500732422, weights = tensor([6.4805, 1.3486], dtype=torch.float16)\n",
      "step №389: loss = 34.41185760498047, weights = tensor([6.4805, 1.3516], dtype=torch.float16)\n",
      "step №390: loss = 34.39727783203125, weights = tensor([6.4766, 1.3545], dtype=torch.float16)\n",
      "step №391: loss = 34.38167953491211, weights = tensor([6.4766, 1.3574], dtype=torch.float16)\n",
      "step №392: loss = 34.36703109741211, weights = tensor([6.4766, 1.3604], dtype=torch.float16)\n",
      "step №393: loss = 34.35240936279297, weights = tensor([6.4766, 1.3633], dtype=torch.float16)\n",
      "step №394: loss = 34.33778762817383, weights = tensor([6.4766, 1.3662], dtype=torch.float16)\n",
      "step №395: loss = 34.323204040527344, weights = tensor([6.4766, 1.3691], dtype=torch.float16)\n",
      "step №396: loss = 34.30862045288086, weights = tensor([6.4766, 1.3721], dtype=torch.float16)\n",
      "step №397: loss = 34.294063568115234, weights = tensor([6.4766, 1.3750], dtype=torch.float16)\n",
      "step №398: loss = 34.27952194213867, weights = tensor([6.4727, 1.3779], dtype=torch.float16)\n",
      "step №399: loss = 34.26400375366211, weights = tensor([6.4727, 1.3809], dtype=torch.float16)\n",
      "step №400: loss = 34.2493896484375, weights = tensor([6.4727, 1.3838], dtype=torch.float16)\n",
      "step №401: loss = 34.234798431396484, weights = tensor([6.4727, 1.3867], dtype=torch.float16)\n",
      "step №402: loss = 34.22022247314453, weights = tensor([6.4727, 1.3896], dtype=torch.float16)\n",
      "step №403: loss = 34.205665588378906, weights = tensor([6.4727, 1.3926], dtype=torch.float16)\n",
      "step №404: loss = 34.19111633300781, weights = tensor([6.4727, 1.3955], dtype=torch.float16)\n",
      "step №405: loss = 34.17659378051758, weights = tensor([6.4727, 1.3984], dtype=torch.float16)\n",
      "step №406: loss = 34.162086486816406, weights = tensor([6.4688, 1.4014], dtype=torch.float16)\n",
      "step №407: loss = 34.14665222167969, weights = tensor([6.4688, 1.4043], dtype=torch.float16)\n",
      "step №408: loss = 34.1320686340332, weights = tensor([6.4688, 1.4072], dtype=torch.float16)\n",
      "step №409: loss = 34.11751174926758, weights = tensor([6.4688, 1.4102], dtype=torch.float16)\n",
      "step №410: loss = 34.10295867919922, weights = tensor([6.4688, 1.4131], dtype=torch.float16)\n",
      "step №411: loss = 34.08845138549805, weights = tensor([6.4688, 1.4160], dtype=torch.float16)\n",
      "step №412: loss = 34.07393264770508, weights = tensor([6.4688, 1.4189], dtype=torch.float16)\n",
      "step №413: loss = 34.05944061279297, weights = tensor([6.4688, 1.4219], dtype=torch.float16)\n",
      "step №414: loss = 34.04497146606445, weights = tensor([6.4688, 1.4248], dtype=torch.float16)\n",
      "step №415: loss = 34.03050994873047, weights = tensor([6.4648, 1.4277], dtype=torch.float16)\n",
      "step №416: loss = 34.01506805419922, weights = tensor([6.4648, 1.4307], dtype=torch.float16)\n",
      "step №417: loss = 34.000545501708984, weights = tensor([6.4648, 1.4336], dtype=torch.float16)\n",
      "step №418: loss = 33.98603439331055, weights = tensor([6.4648, 1.4365], dtype=torch.float16)\n",
      "step №419: loss = 33.97154998779297, weights = tensor([6.4648, 1.4395], dtype=torch.float16)\n",
      "step №420: loss = 33.957069396972656, weights = tensor([6.4648, 1.4424], dtype=torch.float16)\n",
      "step №421: loss = 33.94261932373047, weights = tensor([6.4648, 1.4453], dtype=torch.float16)\n",
      "step №422: loss = 33.92817306518555, weights = tensor([6.4648, 1.4482], dtype=torch.float16)\n",
      "step №423: loss = 33.91374969482422, weights = tensor([6.4609, 1.4512], dtype=torch.float16)\n",
      "step №424: loss = 33.89838409423828, weights = tensor([6.4609, 1.4541], dtype=torch.float16)\n",
      "step №425: loss = 33.8838996887207, weights = tensor([6.4609, 1.4570], dtype=torch.float16)\n",
      "step №426: loss = 33.869415283203125, weights = tensor([6.4609, 1.4600], dtype=torch.float16)\n",
      "step №427: loss = 33.85497283935547, weights = tensor([6.4609, 1.4629], dtype=torch.float16)\n",
      "step №428: loss = 33.84052276611328, weights = tensor([6.4609, 1.4658], dtype=torch.float16)\n",
      "step №429: loss = 33.82610321044922, weights = tensor([6.4609, 1.4688], dtype=torch.float16)\n",
      "step №430: loss = 33.81169891357422, weights = tensor([6.4609, 1.4717], dtype=torch.float16)\n",
      "step №431: loss = 33.79730987548828, weights = tensor([6.4609, 1.4746], dtype=torch.float16)\n",
      "step №432: loss = 33.782936096191406, weights = tensor([6.4570, 1.4775], dtype=torch.float16)\n",
      "step №433: loss = 33.76757049560547, weights = tensor([6.4570, 1.4805], dtype=torch.float16)\n",
      "step №434: loss = 33.75313186645508, weights = tensor([6.4570, 1.4834], dtype=torch.float16)\n",
      "step №435: loss = 33.738712310791016, weights = tensor([6.4570, 1.4863], dtype=torch.float16)\n",
      "step №436: loss = 33.724300384521484, weights = tensor([6.4570, 1.4893], dtype=torch.float16)\n",
      "step №437: loss = 33.70991516113281, weights = tensor([6.4570, 1.4922], dtype=torch.float16)\n",
      "step №438: loss = 33.6955451965332, weights = tensor([6.4570, 1.4951], dtype=torch.float16)\n",
      "step №439: loss = 33.681190490722656, weights = tensor([6.4570, 1.4980], dtype=torch.float16)\n",
      "step №440: loss = 33.66685104370117, weights = tensor([6.4531, 1.5010], dtype=torch.float16)\n",
      "step №441: loss = 33.65156555175781, weights = tensor([6.4531, 1.5039], dtype=torch.float16)\n",
      "step №442: loss = 33.63715362548828, weights = tensor([6.4531, 1.5068], dtype=torch.float16)\n",
      "step №443: loss = 33.62277603149414, weights = tensor([6.4531, 1.5098], dtype=torch.float16)\n",
      "step №444: loss = 33.6083984375, weights = tensor([6.4531, 1.5127], dtype=torch.float16)\n",
      "step №445: loss = 33.59404373168945, weights = tensor([6.4531, 1.5156], dtype=torch.float16)\n",
      "step №446: loss = 33.5797119140625, weights = tensor([6.4531, 1.5186], dtype=torch.float16)\n",
      "step №447: loss = 33.56538772583008, weights = tensor([6.4531, 1.5215], dtype=torch.float16)\n",
      "step №448: loss = 33.55108642578125, weights = tensor([6.4531, 1.5244], dtype=torch.float16)\n",
      "step №449: loss = 33.536808013916016, weights = tensor([6.4492, 1.5273], dtype=torch.float16)\n",
      "step №450: loss = 33.521507263183594, weights = tensor([6.4492, 1.5303], dtype=torch.float16)\n",
      "step №451: loss = 33.50716018676758, weights = tensor([6.4492, 1.5332], dtype=torch.float16)\n",
      "step №452: loss = 33.49281692504883, weights = tensor([6.4492, 1.5361], dtype=torch.float16)\n",
      "step №453: loss = 33.47850036621094, weights = tensor([6.4492, 1.5391], dtype=torch.float16)\n",
      "step №454: loss = 33.464195251464844, weights = tensor([6.4492, 1.5420], dtype=torch.float16)\n",
      "step №455: loss = 33.449913024902344, weights = tensor([6.4492, 1.5449], dtype=torch.float16)\n",
      "step №456: loss = 33.43564224243164, weights = tensor([6.4492, 1.5479], dtype=torch.float16)\n",
      "step №457: loss = 33.42139434814453, weights = tensor([6.4453, 1.5508], dtype=torch.float16)\n",
      "step №458: loss = 33.40616989135742, weights = tensor([6.4453, 1.5537], dtype=torch.float16)\n",
      "step №459: loss = 33.39186477661133, weights = tensor([6.4453, 1.5566], dtype=torch.float16)\n",
      "step №460: loss = 33.37755584716797, weights = tensor([6.4453, 1.5596], dtype=torch.float16)\n",
      "step №461: loss = 33.36327362060547, weights = tensor([6.4453, 1.5625], dtype=torch.float16)\n",
      "step №462: loss = 33.34900665283203, weights = tensor([6.4453, 1.5654], dtype=torch.float16)\n",
      "step №463: loss = 33.334754943847656, weights = tensor([6.4453, 1.5684], dtype=torch.float16)\n",
      "step №464: loss = 33.320518493652344, weights = tensor([6.4453, 1.5713], dtype=torch.float16)\n",
      "step №465: loss = 33.30630874633789, weights = tensor([6.4453, 1.5742], dtype=torch.float16)\n",
      "step №466: loss = 33.29209518432617, weights = tensor([6.4414, 1.5771], dtype=torch.float16)\n",
      "step №467: loss = 33.27688217163086, weights = tensor([6.4414, 1.5801], dtype=torch.float16)\n",
      "step №468: loss = 33.262611389160156, weights = tensor([6.4414, 1.5830], dtype=torch.float16)\n",
      "step №469: loss = 33.24836349487305, weights = tensor([6.4414, 1.5859], dtype=torch.float16)\n",
      "step №470: loss = 33.234130859375, weights = tensor([6.4414, 1.5889], dtype=torch.float16)\n",
      "step №471: loss = 33.21991729736328, weights = tensor([6.4414, 1.5918], dtype=torch.float16)\n",
      "step №472: loss = 33.205711364746094, weights = tensor([6.4414, 1.5947], dtype=torch.float16)\n",
      "step №473: loss = 33.19153594970703, weights = tensor([6.4414, 1.5977], dtype=torch.float16)\n",
      "step №474: loss = 33.1773681640625, weights = tensor([6.4375, 1.6006], dtype=torch.float16)\n",
      "step №475: loss = 33.1622314453125, weights = tensor([6.4375, 1.6035], dtype=torch.float16)\n",
      "step №476: loss = 33.14799118041992, weights = tensor([6.4375, 1.6064], dtype=torch.float16)\n",
      "step №477: loss = 33.1337776184082, weights = tensor([6.4375, 1.6094], dtype=torch.float16)\n",
      "step №478: loss = 33.11957931518555, weights = tensor([6.4375, 1.6123], dtype=torch.float16)\n",
      "step №479: loss = 33.10539627075195, weights = tensor([6.4375, 1.6152], dtype=torch.float16)\n",
      "step №480: loss = 33.09122848510742, weights = tensor([6.4375, 1.6182], dtype=torch.float16)\n",
      "step №481: loss = 33.07708740234375, weights = tensor([6.4375, 1.6211], dtype=torch.float16)\n",
      "step №482: loss = 33.062950134277344, weights = tensor([6.4336, 1.6240], dtype=torch.float16)\n",
      "step №483: loss = 33.04789733886719, weights = tensor([6.4336, 1.6270], dtype=torch.float16)\n",
      "step №484: loss = 33.03369140625, weights = tensor([6.4336, 1.6299], dtype=torch.float16)\n",
      "step №485: loss = 33.01951599121094, weights = tensor([6.4336, 1.6328], dtype=torch.float16)\n",
      "step №486: loss = 33.005348205566406, weights = tensor([6.4336, 1.6357], dtype=torch.float16)\n",
      "step №487: loss = 32.9911994934082, weights = tensor([6.4336, 1.6387], dtype=torch.float16)\n",
      "step №488: loss = 32.97706604003906, weights = tensor([6.4336, 1.6416], dtype=torch.float16)\n",
      "step №489: loss = 32.962955474853516, weights = tensor([6.4336, 1.6445], dtype=torch.float16)\n",
      "step №490: loss = 32.94886016845703, weights = tensor([6.4336, 1.6475], dtype=torch.float16)\n",
      "step №491: loss = 32.93478012084961, weights = tensor([6.4297, 1.6504], dtype=torch.float16)\n",
      "step №492: loss = 32.91971206665039, weights = tensor([6.4297, 1.6533], dtype=torch.float16)\n",
      "step №493: loss = 32.90556716918945, weights = tensor([6.4297, 1.6562], dtype=torch.float16)\n",
      "step №494: loss = 32.89143753051758, weights = tensor([6.4297, 1.6592], dtype=torch.float16)\n",
      "step №495: loss = 32.877323150634766, weights = tensor([6.4297, 1.6621], dtype=torch.float16)\n",
      "step №496: loss = 32.863224029541016, weights = tensor([6.4297, 1.6650], dtype=torch.float16)\n",
      "step №497: loss = 32.849151611328125, weights = tensor([6.4297, 1.6680], dtype=torch.float16)\n",
      "step №498: loss = 32.8350830078125, weights = tensor([6.4297, 1.6709], dtype=torch.float16)\n",
      "step №499: loss = 32.821041107177734, weights = tensor([6.4258, 1.6738], dtype=torch.float16)\n",
      "step №500: loss = 32.806053161621094, weights = tensor([6.4258, 1.6768], dtype=torch.float16)\n",
      "step №501: loss = 32.79193878173828, weights = tensor([6.4258, 1.6797], dtype=torch.float16)\n",
      "step №502: loss = 32.7778434753418, weights = tensor([6.4258, 1.6826], dtype=torch.float16)\n",
      "step №503: loss = 32.763771057128906, weights = tensor([6.4258, 1.6855], dtype=torch.float16)\n",
      "step №504: loss = 32.74970245361328, weights = tensor([6.4258, 1.6885], dtype=torch.float16)\n",
      "step №505: loss = 32.73566436767578, weights = tensor([6.4258, 1.6914], dtype=torch.float16)\n",
      "step №506: loss = 32.72163009643555, weights = tensor([6.4258, 1.6943], dtype=torch.float16)\n",
      "step №507: loss = 32.70762252807617, weights = tensor([6.4258, 1.6973], dtype=torch.float16)\n",
      "step №508: loss = 32.693626403808594, weights = tensor([6.4219, 1.7002], dtype=torch.float16)\n",
      "step №509: loss = 32.67863464355469, weights = tensor([6.4219, 1.7031], dtype=torch.float16)\n",
      "step №510: loss = 32.664573669433594, weights = tensor([6.4219, 1.7061], dtype=torch.float16)\n",
      "step №511: loss = 32.65052795410156, weights = tensor([6.4219, 1.7090], dtype=torch.float16)\n",
      "step №512: loss = 32.636497497558594, weights = tensor([6.4219, 1.7119], dtype=torch.float16)\n",
      "step №513: loss = 32.622493743896484, weights = tensor([6.4219, 1.7148], dtype=torch.float16)\n",
      "step №514: loss = 32.608497619628906, weights = tensor([6.4219, 1.7178], dtype=torch.float16)\n",
      "step №515: loss = 32.594520568847656, weights = tensor([6.4219, 1.7207], dtype=torch.float16)\n",
      "step №516: loss = 32.58055877685547, weights = tensor([6.4180, 1.7236], dtype=torch.float16)\n",
      "step №517: loss = 32.56565475463867, weights = tensor([6.4180, 1.7266], dtype=torch.float16)\n",
      "step №518: loss = 32.5516242980957, weights = tensor([6.4180, 1.7295], dtype=torch.float16)\n",
      "step №519: loss = 32.53761291503906, weights = tensor([6.4180, 1.7324], dtype=torch.float16)\n",
      "step №520: loss = 32.523616790771484, weights = tensor([6.4180, 1.7354], dtype=torch.float16)\n",
      "step №521: loss = 32.5096435546875, weights = tensor([6.4180, 1.7383], dtype=torch.float16)\n",
      "step №522: loss = 32.49568557739258, weights = tensor([6.4180, 1.7412], dtype=torch.float16)\n",
      "step №523: loss = 32.48174285888672, weights = tensor([6.4180, 1.7441], dtype=torch.float16)\n",
      "step №524: loss = 32.46781539916992, weights = tensor([6.4180, 1.7471], dtype=torch.float16)\n",
      "step №525: loss = 32.45391082763672, weights = tensor([6.4141, 1.7500], dtype=torch.float16)\n",
      "step №526: loss = 32.438995361328125, weights = tensor([6.4141, 1.7529], dtype=torch.float16)\n",
      "step №527: loss = 32.425018310546875, weights = tensor([6.4141, 1.7559], dtype=torch.float16)\n",
      "step №528: loss = 32.41105651855469, weights = tensor([6.4141, 1.7588], dtype=torch.float16)\n",
      "step №529: loss = 32.397117614746094, weights = tensor([6.4141, 1.7617], dtype=torch.float16)\n",
      "step №530: loss = 32.3831901550293, weights = tensor([6.4141, 1.7646], dtype=torch.float16)\n",
      "step №531: loss = 32.369285583496094, weights = tensor([6.4141, 1.7676], dtype=torch.float16)\n",
      "step №532: loss = 32.35539245605469, weights = tensor([6.4141, 1.7705], dtype=torch.float16)\n",
      "step №533: loss = 32.341522216796875, weights = tensor([6.4102, 1.7734], dtype=torch.float16)\n",
      "step №534: loss = 32.326683044433594, weights = tensor([6.4102, 1.7764], dtype=torch.float16)\n",
      "step №535: loss = 32.312747955322266, weights = tensor([6.4102, 1.7793], dtype=torch.float16)\n",
      "step №536: loss = 32.2988166809082, weights = tensor([6.4102, 1.7822], dtype=torch.float16)\n",
      "step №537: loss = 32.284915924072266, weights = tensor([6.4102, 1.7852], dtype=torch.float16)\n",
      "step №538: loss = 32.271018981933594, weights = tensor([6.4102, 1.7881], dtype=torch.float16)\n",
      "step №539: loss = 32.25714874267578, weights = tensor([6.4102, 1.7910], dtype=torch.float16)\n",
      "step №540: loss = 32.243289947509766, weights = tensor([6.4102, 1.7939], dtype=torch.float16)\n",
      "step №541: loss = 32.229454040527344, weights = tensor([6.4102, 1.7969], dtype=torch.float16)\n",
      "step №542: loss = 32.21562957763672, weights = tensor([6.4062, 1.7998], dtype=torch.float16)\n",
      "step №543: loss = 32.20078659057617, weights = tensor([6.4062, 1.8027], dtype=torch.float16)\n",
      "step №544: loss = 32.18689727783203, weights = tensor([6.4062, 1.8057], dtype=torch.float16)\n",
      "step №545: loss = 32.17302703857422, weights = tensor([6.4062, 1.8086], dtype=torch.float16)\n",
      "step №546: loss = 32.1591682434082, weights = tensor([6.4062, 1.8115], dtype=torch.float16)\n",
      "step №547: loss = 32.14533233642578, weights = tensor([6.4062, 1.8145], dtype=torch.float16)\n",
      "step №548: loss = 32.131507873535156, weights = tensor([6.4062, 1.8174], dtype=torch.float16)\n",
      "step №549: loss = 32.11771011352539, weights = tensor([6.4062, 1.8203], dtype=torch.float16)\n",
      "step №550: loss = 32.103919982910156, weights = tensor([6.4023, 1.8232], dtype=torch.float16)\n",
      "step №551: loss = 32.08915328979492, weights = tensor([6.4023, 1.8262], dtype=torch.float16)\n",
      "step №552: loss = 32.075294494628906, weights = tensor([6.4023, 1.8291], dtype=torch.float16)\n",
      "step №553: loss = 32.061458587646484, weights = tensor([6.4023, 1.8320], dtype=torch.float16)\n",
      "step №554: loss = 32.047637939453125, weights = tensor([6.4023, 1.8350], dtype=torch.float16)\n",
      "step №555: loss = 32.033836364746094, weights = tensor([6.4023, 1.8379], dtype=torch.float16)\n",
      "step №556: loss = 32.020042419433594, weights = tensor([6.4023, 1.8408], dtype=torch.float16)\n",
      "step №557: loss = 32.00627899169922, weights = tensor([6.4023, 1.8438], dtype=torch.float16)\n",
      "step №558: loss = 31.992523193359375, weights = tensor([6.3984, 1.8467], dtype=torch.float16)\n",
      "step №559: loss = 31.977840423583984, weights = tensor([6.3984, 1.8496], dtype=torch.float16)\n",
      "step №560: loss = 31.96401596069336, weights = tensor([6.3984, 1.8525], dtype=torch.float16)\n",
      "step №561: loss = 31.950220108032227, weights = tensor([6.3984, 1.8555], dtype=torch.float16)\n",
      "step №562: loss = 31.936426162719727, weights = tensor([6.3984, 1.8584], dtype=torch.float16)\n",
      "step №563: loss = 31.922658920288086, weights = tensor([6.3984, 1.8613], dtype=torch.float16)\n",
      "step №564: loss = 31.90890121459961, weights = tensor([6.3984, 1.8643], dtype=torch.float16)\n",
      "step №565: loss = 31.89516830444336, weights = tensor([6.3984, 1.8672], dtype=torch.float16)\n",
      "step №566: loss = 31.881450653076172, weights = tensor([6.3984, 1.8701], dtype=torch.float16)\n",
      "step №567: loss = 31.867748260498047, weights = tensor([6.3945, 1.8730], dtype=torch.float16)\n",
      "step №568: loss = 31.853057861328125, weights = tensor([6.3945, 1.8760], dtype=torch.float16)\n",
      "step №569: loss = 31.83929443359375, weights = tensor([6.3945, 1.8789], dtype=torch.float16)\n",
      "step №570: loss = 31.825536727905273, weights = tensor([6.3945, 1.8818], dtype=torch.float16)\n",
      "step №571: loss = 31.811803817749023, weights = tensor([6.3945, 1.8848], dtype=torch.float16)\n",
      "step №572: loss = 31.798080444335938, weights = tensor([6.3945, 1.8877], dtype=torch.float16)\n",
      "step №573: loss = 31.784387588500977, weights = tensor([6.3945, 1.8906], dtype=torch.float16)\n",
      "step №574: loss = 31.77069664001465, weights = tensor([6.3945, 1.8936], dtype=torch.float16)\n",
      "step №575: loss = 31.757030487060547, weights = tensor([6.3906, 1.8965], dtype=torch.float16)\n",
      "step №576: loss = 31.742420196533203, weights = tensor([6.3906, 1.8994], dtype=torch.float16)\n",
      "step №577: loss = 31.728689193725586, weights = tensor([6.3906, 1.9023], dtype=torch.float16)\n",
      "step №578: loss = 31.7149658203125, weights = tensor([6.3906, 1.9053], dtype=torch.float16)\n",
      "step №579: loss = 31.70126724243164, weights = tensor([6.3906, 1.9082], dtype=torch.float16)\n",
      "step №580: loss = 31.687580108642578, weights = tensor([6.3906, 1.9111], dtype=torch.float16)\n",
      "step №581: loss = 31.673919677734375, weights = tensor([6.3906, 1.9141], dtype=torch.float16)\n",
      "step №582: loss = 31.660266876220703, weights = tensor([6.3906, 1.9170], dtype=torch.float16)\n",
      "step №583: loss = 31.646636962890625, weights = tensor([6.3906, 1.9199], dtype=torch.float16)\n",
      "step №584: loss = 31.633014678955078, weights = tensor([6.3867, 1.9229], dtype=torch.float16)\n",
      "step №585: loss = 31.6184024810791, weights = tensor([6.3867, 1.9258], dtype=torch.float16)\n",
      "step №586: loss = 31.604717254638672, weights = tensor([6.3867, 1.9287], dtype=torch.float16)\n",
      "step №587: loss = 31.591053009033203, weights = tensor([6.3867, 1.9316], dtype=torch.float16)\n",
      "step №588: loss = 31.5773983001709, weights = tensor([6.3867, 1.9346], dtype=torch.float16)\n",
      "step №589: loss = 31.563770294189453, weights = tensor([6.3867, 1.9375], dtype=torch.float16)\n",
      "step №590: loss = 31.550151824951172, weights = tensor([6.3867, 1.9404], dtype=torch.float16)\n",
      "step №591: loss = 31.53656005859375, weights = tensor([6.3867, 1.9434], dtype=torch.float16)\n",
      "step №592: loss = 31.52297019958496, weights = tensor([6.3828, 1.9463], dtype=torch.float16)\n",
      "step №593: loss = 31.508441925048828, weights = tensor([6.3828, 1.9492], dtype=torch.float16)\n",
      "step №594: loss = 31.494787216186523, weights = tensor([6.3828, 1.9521], dtype=torch.float16)\n",
      "step №595: loss = 31.481159210205078, weights = tensor([6.3828, 1.9551], dtype=torch.float16)\n",
      "step №596: loss = 31.467538833618164, weights = tensor([6.3828, 1.9580], dtype=torch.float16)\n",
      "step №597: loss = 31.453943252563477, weights = tensor([6.3828, 1.9609], dtype=torch.float16)\n",
      "step №598: loss = 31.44036293029785, weights = tensor([6.3828, 1.9639], dtype=torch.float16)\n",
      "step №599: loss = 31.42679786682129, weights = tensor([6.3828, 1.9668], dtype=torch.float16)\n",
      "step №600: loss = 31.41324806213379, weights = tensor([6.3828, 1.9697], dtype=torch.float16)\n",
      "step №601: loss = 31.399723052978516, weights = tensor([6.3789, 1.9727], dtype=torch.float16)\n",
      "step №602: loss = 31.385181427001953, weights = tensor([6.3789, 1.9756], dtype=torch.float16)\n",
      "step №603: loss = 31.371585845947266, weights = tensor([6.3789, 1.9785], dtype=torch.float16)\n",
      "step №604: loss = 31.35799789428711, weights = tensor([6.3789, 1.9814], dtype=torch.float16)\n",
      "step №605: loss = 31.344440460205078, weights = tensor([6.3789, 1.9844], dtype=torch.float16)\n",
      "step №606: loss = 31.330890655517578, weights = tensor([6.3789, 1.9873], dtype=torch.float16)\n",
      "step №607: loss = 31.31736183166504, weights = tensor([6.3789, 1.9902], dtype=torch.float16)\n",
      "step №608: loss = 31.303844451904297, weights = tensor([6.3789, 1.9932], dtype=torch.float16)\n",
      "step №609: loss = 31.290353775024414, weights = tensor([6.3750, 1.9961], dtype=torch.float16)\n",
      "step №610: loss = 31.275890350341797, weights = tensor([6.3750, 1.9990], dtype=torch.float16)\n",
      "step №611: loss = 31.2623291015625, weights = tensor([6.3750, 2.0020], dtype=torch.float16)\n",
      "step №612: loss = 31.248779296875, weights = tensor([6.3750, 2.0039], dtype=torch.float16)\n",
      "step №613: loss = 31.239757537841797, weights = tensor([6.3750, 2.0059], dtype=torch.float16)\n",
      "step №614: loss = 31.230743408203125, weights = tensor([6.3750, 2.0078], dtype=torch.float16)\n",
      "step №615: loss = 31.22174072265625, weights = tensor([6.3750, 2.0098], dtype=torch.float16)\n",
      "step №616: loss = 31.212738037109375, weights = tensor([6.3750, 2.0117], dtype=torch.float16)\n",
      "step №617: loss = 31.203746795654297, weights = tensor([6.3750, 2.0137], dtype=torch.float16)\n",
      "step №618: loss = 31.19476318359375, weights = tensor([6.3750, 2.0156], dtype=torch.float16)\n",
      "step №619: loss = 31.185791015625, weights = tensor([6.3750, 2.0176], dtype=torch.float16)\n",
      "step №620: loss = 31.17681884765625, weights = tensor([6.3711, 2.0195], dtype=torch.float16)\n",
      "step №621: loss = 31.16692543029785, weights = tensor([6.3711, 2.0215], dtype=torch.float16)\n",
      "step №622: loss = 31.157901763916016, weights = tensor([6.3711, 2.0234], dtype=torch.float16)\n",
      "step №623: loss = 31.148889541625977, weights = tensor([6.3711, 2.0254], dtype=torch.float16)\n",
      "step №624: loss = 31.139881134033203, weights = tensor([6.3711, 2.0273], dtype=torch.float16)\n",
      "step №625: loss = 31.130884170532227, weights = tensor([6.3711, 2.0293], dtype=torch.float16)\n",
      "step №626: loss = 31.121891021728516, weights = tensor([6.3711, 2.0312], dtype=torch.float16)\n",
      "step №627: loss = 31.1129093170166, weights = tensor([6.3711, 2.0332], dtype=torch.float16)\n",
      "step №628: loss = 31.103931427001953, weights = tensor([6.3711, 2.0352], dtype=torch.float16)\n",
      "step №629: loss = 31.0949649810791, weights = tensor([6.3711, 2.0371], dtype=torch.float16)\n",
      "step №630: loss = 31.086002349853516, weights = tensor([6.3711, 2.0391], dtype=torch.float16)\n",
      "step №631: loss = 31.077051162719727, weights = tensor([6.3711, 2.0410], dtype=torch.float16)\n",
      "step №632: loss = 31.068103790283203, weights = tensor([6.3711, 2.0430], dtype=torch.float16)\n",
      "step №633: loss = 31.059167861938477, weights = tensor([6.3672, 2.0449], dtype=torch.float16)\n",
      "step №634: loss = 31.04927635192871, weights = tensor([6.3672, 2.0469], dtype=torch.float16)\n",
      "step №635: loss = 31.0402889251709, weights = tensor([6.3672, 2.0488], dtype=torch.float16)\n",
      "step №636: loss = 31.031301498413086, weights = tensor([6.3672, 2.0508], dtype=torch.float16)\n",
      "step №637: loss = 31.022327423095703, weights = tensor([6.3672, 2.0527], dtype=torch.float16)\n",
      "step №638: loss = 31.013357162475586, weights = tensor([6.3672, 2.0547], dtype=torch.float16)\n",
      "step №639: loss = 31.0044002532959, weights = tensor([6.3672, 2.0566], dtype=torch.float16)\n",
      "step №640: loss = 30.99544334411621, weights = tensor([6.3672, 2.0586], dtype=torch.float16)\n",
      "step №641: loss = 30.986499786376953, weights = tensor([6.3672, 2.0605], dtype=torch.float16)\n",
      "step №642: loss = 30.97756004333496, weights = tensor([6.3672, 2.0625], dtype=torch.float16)\n",
      "step №643: loss = 30.9686336517334, weights = tensor([6.3672, 2.0645], dtype=torch.float16)\n",
      "step №644: loss = 30.959707260131836, weights = tensor([6.3672, 2.0664], dtype=torch.float16)\n",
      "step №645: loss = 30.950794219970703, weights = tensor([6.3633, 2.0684], dtype=torch.float16)\n",
      "step №646: loss = 30.94097328186035, weights = tensor([6.3633, 2.0703], dtype=torch.float16)\n",
      "step №647: loss = 30.9320068359375, weights = tensor([6.3633, 2.0723], dtype=torch.float16)\n",
      "step №648: loss = 30.923044204711914, weights = tensor([6.3633, 2.0742], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №649: loss = 30.914093017578125, weights = tensor([6.3633, 2.0762], dtype=torch.float16)\n",
      "step №650: loss = 30.9051456451416, weights = tensor([6.3633, 2.0781], dtype=torch.float16)\n",
      "step №651: loss = 30.896209716796875, weights = tensor([6.3633, 2.0801], dtype=torch.float16)\n",
      "step №652: loss = 30.887277603149414, weights = tensor([6.3633, 2.0820], dtype=torch.float16)\n",
      "step №653: loss = 30.87835693359375, weights = tensor([6.3633, 2.0840], dtype=torch.float16)\n",
      "step №654: loss = 30.86944007873535, weights = tensor([6.3633, 2.0859], dtype=torch.float16)\n",
      "step №655: loss = 30.86053466796875, weights = tensor([6.3633, 2.0879], dtype=torch.float16)\n",
      "step №656: loss = 30.851633071899414, weights = tensor([6.3633, 2.0898], dtype=torch.float16)\n",
      "step №657: loss = 30.842742919921875, weights = tensor([6.3633, 2.0918], dtype=torch.float16)\n",
      "step №658: loss = 30.8338565826416, weights = tensor([6.3594, 2.0938], dtype=torch.float16)\n",
      "step №659: loss = 30.824047088623047, weights = tensor([6.3594, 2.0957], dtype=torch.float16)\n",
      "step №660: loss = 30.815105438232422, weights = tensor([6.3594, 2.0977], dtype=torch.float16)\n",
      "step №661: loss = 30.806177139282227, weights = tensor([6.3594, 2.0996], dtype=torch.float16)\n",
      "step №662: loss = 30.797252655029297, weights = tensor([6.3594, 2.1016], dtype=torch.float16)\n",
      "step №663: loss = 30.788341522216797, weights = tensor([6.3594, 2.1035], dtype=torch.float16)\n",
      "step №664: loss = 30.779430389404297, weights = tensor([6.3594, 2.1055], dtype=torch.float16)\n",
      "step №665: loss = 30.770532608032227, weights = tensor([6.3594, 2.1074], dtype=torch.float16)\n",
      "step №666: loss = 30.761638641357422, weights = tensor([6.3594, 2.1094], dtype=torch.float16)\n",
      "step №667: loss = 30.752758026123047, weights = tensor([6.3594, 2.1113], dtype=torch.float16)\n",
      "step №668: loss = 30.743877410888672, weights = tensor([6.3594, 2.1133], dtype=torch.float16)\n",
      "step №669: loss = 30.735010147094727, weights = tensor([6.3594, 2.1152], dtype=torch.float16)\n",
      "step №670: loss = 30.726146697998047, weights = tensor([6.3594, 2.1172], dtype=torch.float16)\n",
      "step №671: loss = 30.717296600341797, weights = tensor([6.3555, 2.1191], dtype=torch.float16)\n",
      "step №672: loss = 30.707489013671875, weights = tensor([6.3555, 2.1211], dtype=torch.float16)\n",
      "step №673: loss = 30.698583602905273, weights = tensor([6.3555, 2.1230], dtype=torch.float16)\n",
      "step №674: loss = 30.689682006835938, weights = tensor([6.3555, 2.1250], dtype=torch.float16)\n",
      "step №675: loss = 30.6807918548584, weights = tensor([6.3555, 2.1270], dtype=torch.float16)\n",
      "step №676: loss = 30.671905517578125, weights = tensor([6.3555, 2.1289], dtype=torch.float16)\n",
      "step №677: loss = 30.66303062438965, weights = tensor([6.3555, 2.1309], dtype=torch.float16)\n",
      "step №678: loss = 30.654159545898438, weights = tensor([6.3555, 2.1328], dtype=torch.float16)\n",
      "step №679: loss = 30.645299911499023, weights = tensor([6.3555, 2.1348], dtype=torch.float16)\n",
      "step №680: loss = 30.636444091796875, weights = tensor([6.3555, 2.1367], dtype=torch.float16)\n",
      "step №681: loss = 30.627599716186523, weights = tensor([6.3555, 2.1387], dtype=torch.float16)\n",
      "step №682: loss = 30.618759155273438, weights = tensor([6.3555, 2.1406], dtype=torch.float16)\n",
      "step №683: loss = 30.60993003845215, weights = tensor([6.3516, 2.1426], dtype=torch.float16)\n",
      "step №684: loss = 30.60019302368164, weights = tensor([6.3516, 2.1445], dtype=torch.float16)\n",
      "step №685: loss = 30.59130859375, weights = tensor([6.3516, 2.1465], dtype=torch.float16)\n",
      "step №686: loss = 30.58243179321289, weights = tensor([6.3516, 2.1484], dtype=torch.float16)\n",
      "step №687: loss = 30.573566436767578, weights = tensor([6.3516, 2.1504], dtype=torch.float16)\n",
      "step №688: loss = 30.564701080322266, weights = tensor([6.3516, 2.1523], dtype=torch.float16)\n",
      "step №689: loss = 30.55584716796875, weights = tensor([6.3516, 2.1543], dtype=torch.float16)\n",
      "step №690: loss = 30.547000885009766, weights = tensor([6.3516, 2.1562], dtype=torch.float16)\n",
      "step №691: loss = 30.538166046142578, weights = tensor([6.3516, 2.1582], dtype=torch.float16)\n",
      "step №692: loss = 30.52933120727539, weights = tensor([6.3516, 2.1602], dtype=torch.float16)\n",
      "step №693: loss = 30.5205078125, weights = tensor([6.3516, 2.1621], dtype=torch.float16)\n",
      "step №694: loss = 30.51169204711914, weights = tensor([6.3516, 2.1641], dtype=torch.float16)\n",
      "step №695: loss = 30.502887725830078, weights = tensor([6.3516, 2.1660], dtype=torch.float16)\n",
      "step №696: loss = 30.494083404541016, weights = tensor([6.3477, 2.1680], dtype=torch.float16)\n",
      "step №697: loss = 30.484355926513672, weights = tensor([6.3477, 2.1699], dtype=torch.float16)\n",
      "step №698: loss = 30.475500106811523, weights = tensor([6.3477, 2.1719], dtype=torch.float16)\n",
      "step №699: loss = 30.466655731201172, weights = tensor([6.3477, 2.1738], dtype=torch.float16)\n",
      "step №700: loss = 30.457815170288086, weights = tensor([6.3477, 2.1758], dtype=torch.float16)\n",
      "step №701: loss = 30.448986053466797, weights = tensor([6.3477, 2.1777], dtype=torch.float16)\n",
      "step №702: loss = 30.440160751342773, weights = tensor([6.3477, 2.1797], dtype=torch.float16)\n",
      "step №703: loss = 30.431346893310547, weights = tensor([6.3477, 2.1816], dtype=torch.float16)\n",
      "step №704: loss = 30.422536849975586, weights = tensor([6.3477, 2.1836], dtype=torch.float16)\n",
      "step №705: loss = 30.413738250732422, weights = tensor([6.3477, 2.1855], dtype=torch.float16)\n",
      "step №706: loss = 30.404943466186523, weights = tensor([6.3477, 2.1875], dtype=torch.float16)\n",
      "step №707: loss = 30.396160125732422, weights = tensor([6.3477, 2.1895], dtype=torch.float16)\n",
      "step №708: loss = 30.387380599975586, weights = tensor([6.3477, 2.1914], dtype=torch.float16)\n",
      "step №709: loss = 30.378612518310547, weights = tensor([6.3438, 2.1934], dtype=torch.float16)\n",
      "step №710: loss = 30.3688907623291, weights = tensor([6.3438, 2.1953], dtype=torch.float16)\n",
      "step №711: loss = 30.360071182250977, weights = tensor([6.3438, 2.1973], dtype=torch.float16)\n",
      "step №712: loss = 30.35125160217285, weights = tensor([6.3438, 2.1992], dtype=torch.float16)\n",
      "step №713: loss = 30.342443466186523, weights = tensor([6.3438, 2.2012], dtype=torch.float16)\n",
      "step №714: loss = 30.333642959594727, weights = tensor([6.3438, 2.2031], dtype=torch.float16)\n",
      "step №715: loss = 30.324853897094727, weights = tensor([6.3438, 2.2051], dtype=torch.float16)\n",
      "step №716: loss = 30.316064834594727, weights = tensor([6.3438, 2.2070], dtype=torch.float16)\n",
      "step №717: loss = 30.307287216186523, weights = tensor([6.3438, 2.2090], dtype=torch.float16)\n",
      "step №718: loss = 30.29851722717285, weights = tensor([6.3438, 2.2109], dtype=torch.float16)\n",
      "step №719: loss = 30.289758682250977, weights = tensor([6.3438, 2.2129], dtype=torch.float16)\n",
      "step №720: loss = 30.2810001373291, weights = tensor([6.3438, 2.2148], dtype=torch.float16)\n",
      "step №721: loss = 30.272253036499023, weights = tensor([6.3398, 2.2168], dtype=torch.float16)\n",
      "step №722: loss = 30.26259994506836, weights = tensor([6.3398, 2.2188], dtype=torch.float16)\n",
      "step №723: loss = 30.253803253173828, weights = tensor([6.3398, 2.2207], dtype=torch.float16)\n",
      "step №724: loss = 30.245006561279297, weights = tensor([6.3398, 2.2227], dtype=torch.float16)\n",
      "step №725: loss = 30.236225128173828, weights = tensor([6.3398, 2.2246], dtype=torch.float16)\n",
      "step №726: loss = 30.22744369506836, weights = tensor([6.3398, 2.2266], dtype=torch.float16)\n",
      "step №727: loss = 30.218677520751953, weights = tensor([6.3398, 2.2285], dtype=torch.float16)\n",
      "step №728: loss = 30.209911346435547, weights = tensor([6.3398, 2.2305], dtype=torch.float16)\n",
      "step №729: loss = 30.201160430908203, weights = tensor([6.3398, 2.2324], dtype=torch.float16)\n",
      "step №730: loss = 30.19240951538086, weights = tensor([6.3398, 2.2344], dtype=torch.float16)\n",
      "step №731: loss = 30.183673858642578, weights = tensor([6.3398, 2.2363], dtype=torch.float16)\n",
      "step №732: loss = 30.174938201904297, weights = tensor([6.3398, 2.2383], dtype=torch.float16)\n",
      "step №733: loss = 30.166217803955078, weights = tensor([6.3398, 2.2402], dtype=torch.float16)\n",
      "step №734: loss = 30.15749740600586, weights = tensor([6.3359, 2.2422], dtype=torch.float16)\n",
      "step №735: loss = 30.147857666015625, weights = tensor([6.3359, 2.2441], dtype=torch.float16)\n",
      "step №736: loss = 30.139083862304688, weights = tensor([6.3359, 2.2461], dtype=torch.float16)\n",
      "step №737: loss = 30.130321502685547, weights = tensor([6.3359, 2.2480], dtype=torch.float16)\n",
      "step №738: loss = 30.121566772460938, weights = tensor([6.3359, 2.2500], dtype=torch.float16)\n",
      "step №739: loss = 30.112823486328125, weights = tensor([6.3359, 2.2520], dtype=torch.float16)\n",
      "step №740: loss = 30.104080200195312, weights = tensor([6.3359, 2.2539], dtype=torch.float16)\n",
      "step №741: loss = 30.095348358154297, weights = tensor([6.3359, 2.2559], dtype=torch.float16)\n",
      "step №742: loss = 30.086624145507812, weights = tensor([6.3359, 2.2578], dtype=torch.float16)\n",
      "step №743: loss = 30.077911376953125, weights = tensor([6.3359, 2.2598], dtype=torch.float16)\n",
      "step №744: loss = 30.069198608398438, weights = tensor([6.3359, 2.2617], dtype=torch.float16)\n",
      "step №745: loss = 30.060497283935547, weights = tensor([6.3359, 2.2637], dtype=torch.float16)\n",
      "step №746: loss = 30.051803588867188, weights = tensor([6.3359, 2.2656], dtype=torch.float16)\n",
      "step №747: loss = 30.043121337890625, weights = tensor([6.3320, 2.2676], dtype=torch.float16)\n",
      "step №748: loss = 30.03348159790039, weights = tensor([6.3320, 2.2695], dtype=torch.float16)\n",
      "step №749: loss = 30.024744033813477, weights = tensor([6.3320, 2.2715], dtype=torch.float16)\n",
      "step №750: loss = 30.016010284423828, weights = tensor([6.3320, 2.2734], dtype=torch.float16)\n",
      "step №751: loss = 30.007287979125977, weights = tensor([6.3320, 2.2754], dtype=torch.float16)\n",
      "step №752: loss = 29.99856948852539, weights = tensor([6.3320, 2.2773], dtype=torch.float16)\n",
      "step №753: loss = 29.9898624420166, weights = tensor([6.3320, 2.2793], dtype=torch.float16)\n",
      "step №754: loss = 29.981159210205078, weights = tensor([6.3320, 2.2812], dtype=torch.float16)\n",
      "step №755: loss = 29.97246742248535, weights = tensor([6.3320, 2.2832], dtype=torch.float16)\n",
      "step №756: loss = 29.96377944946289, weights = tensor([6.3320, 2.2852], dtype=torch.float16)\n",
      "step №757: loss = 29.955102920532227, weights = tensor([6.3320, 2.2871], dtype=torch.float16)\n",
      "step №758: loss = 29.946430206298828, weights = tensor([6.3320, 2.2891], dtype=torch.float16)\n",
      "step №759: loss = 29.937768936157227, weights = tensor([6.3281, 2.2910], dtype=torch.float16)\n",
      "step №760: loss = 29.928197860717773, weights = tensor([6.3281, 2.2930], dtype=torch.float16)\n",
      "step №761: loss = 29.919483184814453, weights = tensor([6.3281, 2.2949], dtype=torch.float16)\n",
      "step №762: loss = 29.9107723236084, weights = tensor([6.3281, 2.2969], dtype=torch.float16)\n",
      "step №763: loss = 29.902074813842773, weights = tensor([6.3281, 2.2988], dtype=torch.float16)\n",
      "step №764: loss = 29.89337730407715, weights = tensor([6.3281, 2.3008], dtype=torch.float16)\n",
      "step №765: loss = 29.884693145751953, weights = tensor([6.3281, 2.3027], dtype=torch.float16)\n",
      "step №766: loss = 29.876012802124023, weights = tensor([6.3281, 2.3047], dtype=torch.float16)\n",
      "step №767: loss = 29.867345809936523, weights = tensor([6.3281, 2.3066], dtype=torch.float16)\n",
      "step №768: loss = 29.858678817749023, weights = tensor([6.3281, 2.3086], dtype=torch.float16)\n",
      "step №769: loss = 29.850025177001953, weights = tensor([6.3281, 2.3105], dtype=torch.float16)\n",
      "step №770: loss = 29.84137535095215, weights = tensor([6.3281, 2.3125], dtype=torch.float16)\n",
      "step №771: loss = 29.832738876342773, weights = tensor([6.3281, 2.3145], dtype=torch.float16)\n",
      "step №772: loss = 29.8241024017334, weights = tensor([6.3242, 2.3164], dtype=torch.float16)\n",
      "step №773: loss = 29.814544677734375, weights = tensor([6.3242, 2.3184], dtype=torch.float16)\n",
      "step №774: loss = 29.805856704711914, weights = tensor([6.3242, 2.3203], dtype=torch.float16)\n",
      "step №775: loss = 29.79718017578125, weights = tensor([6.3242, 2.3223], dtype=torch.float16)\n",
      "step №776: loss = 29.78850746154785, weights = tensor([6.3242, 2.3242], dtype=torch.float16)\n",
      "step №777: loss = 29.77984619140625, weights = tensor([6.3242, 2.3262], dtype=torch.float16)\n",
      "step №778: loss = 29.771188735961914, weights = tensor([6.3242, 2.3281], dtype=torch.float16)\n",
      "step №779: loss = 29.762542724609375, weights = tensor([6.3242, 2.3301], dtype=torch.float16)\n",
      "step №780: loss = 29.7539005279541, weights = tensor([6.3242, 2.3320], dtype=torch.float16)\n",
      "step №781: loss = 29.745269775390625, weights = tensor([6.3242, 2.3340], dtype=torch.float16)\n",
      "step №782: loss = 29.736642837524414, weights = tensor([6.3242, 2.3359], dtype=torch.float16)\n",
      "step №783: loss = 29.72802734375, weights = tensor([6.3242, 2.3379], dtype=torch.float16)\n",
      "step №784: loss = 29.71941566467285, weights = tensor([6.3242, 2.3398], dtype=torch.float16)\n",
      "step №785: loss = 29.7108154296875, weights = tensor([6.3203, 2.3418], dtype=torch.float16)\n",
      "step №786: loss = 29.70125961303711, weights = tensor([6.3203, 2.3438], dtype=torch.float16)\n",
      "step №787: loss = 29.692607879638672, weights = tensor([6.3203, 2.3457], dtype=torch.float16)\n",
      "step №788: loss = 29.683956146240234, weights = tensor([6.3203, 2.3477], dtype=torch.float16)\n",
      "step №789: loss = 29.675317764282227, weights = tensor([6.3203, 2.3496], dtype=torch.float16)\n",
      "step №790: loss = 29.666683197021484, weights = tensor([6.3203, 2.3516], dtype=torch.float16)\n",
      "step №791: loss = 29.658061981201172, weights = tensor([6.3203, 2.3535], dtype=torch.float16)\n",
      "step №792: loss = 29.64944076538086, weights = tensor([6.3203, 2.3555], dtype=torch.float16)\n",
      "step №793: loss = 29.640832901000977, weights = tensor([6.3203, 2.3574], dtype=torch.float16)\n",
      "step №794: loss = 29.63222885131836, weights = tensor([6.3203, 2.3594], dtype=torch.float16)\n",
      "step №795: loss = 29.623638153076172, weights = tensor([6.3203, 2.3613], dtype=torch.float16)\n",
      "step №796: loss = 29.615047454833984, weights = tensor([6.3203, 2.3633], dtype=torch.float16)\n",
      "step №797: loss = 29.606470108032227, weights = tensor([6.3164, 2.3652], dtype=torch.float16)\n",
      "step №798: loss = 29.59698486328125, weights = tensor([6.3164, 2.3672], dtype=torch.float16)\n",
      "step №799: loss = 29.588354110717773, weights = tensor([6.3164, 2.3691], dtype=torch.float16)\n",
      "step №800: loss = 29.579727172851562, weights = tensor([6.3164, 2.3711], dtype=torch.float16)\n",
      "step №801: loss = 29.57111167907715, weights = tensor([6.3164, 2.3730], dtype=torch.float16)\n",
      "step №802: loss = 29.5625, weights = tensor([6.3164, 2.3750], dtype=torch.float16)\n",
      "step №803: loss = 29.55389976501465, weights = tensor([6.3164, 2.3770], dtype=torch.float16)\n",
      "step №804: loss = 29.545303344726562, weights = tensor([6.3164, 2.3789], dtype=torch.float16)\n",
      "step №805: loss = 29.536718368530273, weights = tensor([6.3164, 2.3809], dtype=torch.float16)\n",
      "step №806: loss = 29.52813720703125, weights = tensor([6.3164, 2.3828], dtype=torch.float16)\n",
      "step №807: loss = 29.519567489624023, weights = tensor([6.3164, 2.3848], dtype=torch.float16)\n",
      "step №808: loss = 29.511001586914062, weights = tensor([6.3164, 2.3867], dtype=torch.float16)\n",
      "step №809: loss = 29.5024471282959, weights = tensor([6.3164, 2.3887], dtype=torch.float16)\n",
      "step №810: loss = 29.493896484375, weights = tensor([6.3125, 2.3906], dtype=torch.float16)\n",
      "step №811: loss = 29.484424591064453, weights = tensor([6.3125, 2.3926], dtype=torch.float16)\n",
      "step №812: loss = 29.475818634033203, weights = tensor([6.3125, 2.3945], dtype=torch.float16)\n",
      "step №813: loss = 29.46722412109375, weights = tensor([6.3125, 2.3965], dtype=torch.float16)\n",
      "step №814: loss = 29.458637237548828, weights = tensor([6.3125, 2.3984], dtype=torch.float16)\n",
      "step №815: loss = 29.450061798095703, weights = tensor([6.3125, 2.4004], dtype=torch.float16)\n",
      "step №816: loss = 29.441486358642578, weights = tensor([6.3125, 2.4023], dtype=torch.float16)\n",
      "step №817: loss = 29.43292236328125, weights = tensor([6.3125, 2.4043], dtype=torch.float16)\n",
      "step №818: loss = 29.424365997314453, weights = tensor([6.3125, 2.4062], dtype=torch.float16)\n",
      "step №819: loss = 29.415821075439453, weights = tensor([6.3125, 2.4082], dtype=torch.float16)\n",
      "step №820: loss = 29.407276153564453, weights = tensor([6.3125, 2.4102], dtype=torch.float16)\n",
      "step №821: loss = 29.39874267578125, weights = tensor([6.3125, 2.4121], dtype=torch.float16)\n",
      "step №822: loss = 29.390216827392578, weights = tensor([6.3125, 2.4141], dtype=torch.float16)\n",
      "step №823: loss = 29.381702423095703, weights = tensor([6.3086, 2.4160], dtype=torch.float16)\n",
      "step №824: loss = 29.372228622436523, weights = tensor([6.3086, 2.4180], dtype=torch.float16)\n",
      "step №825: loss = 29.363658905029297, weights = tensor([6.3086, 2.4199], dtype=torch.float16)\n",
      "step №826: loss = 29.355093002319336, weights = tensor([6.3086, 2.4219], dtype=torch.float16)\n",
      "step №827: loss = 29.346538543701172, weights = tensor([6.3086, 2.4238], dtype=torch.float16)\n",
      "step №828: loss = 29.337987899780273, weights = tensor([6.3086, 2.4258], dtype=torch.float16)\n",
      "step №829: loss = 29.329448699951172, weights = tensor([6.3086, 2.4277], dtype=torch.float16)\n",
      "step №830: loss = 29.320913314819336, weights = tensor([6.3086, 2.4297], dtype=torch.float16)\n",
      "step №831: loss = 29.312389373779297, weights = tensor([6.3086, 2.4316], dtype=torch.float16)\n",
      "step №832: loss = 29.303869247436523, weights = tensor([6.3086, 2.4336], dtype=torch.float16)\n",
      "step №833: loss = 29.295360565185547, weights = tensor([6.3086, 2.4355], dtype=torch.float16)\n",
      "step №834: loss = 29.286855697631836, weights = tensor([6.3086, 2.4375], dtype=torch.float16)\n",
      "step №835: loss = 29.278362274169922, weights = tensor([6.3047, 2.4395], dtype=torch.float16)\n",
      "step №836: loss = 29.26896095275879, weights = tensor([6.3047, 2.4414], dtype=torch.float16)\n",
      "step №837: loss = 29.260412216186523, weights = tensor([6.3047, 2.4434], dtype=torch.float16)\n",
      "step №838: loss = 29.25187110900879, weights = tensor([6.3047, 2.4453], dtype=torch.float16)\n",
      "step №839: loss = 29.24334144592285, weights = tensor([6.3047, 2.4473], dtype=torch.float16)\n",
      "step №840: loss = 29.234811782836914, weights = tensor([6.3047, 2.4492], dtype=torch.float16)\n",
      "step №841: loss = 29.226293563842773, weights = tensor([6.3047, 2.4512], dtype=torch.float16)\n",
      "step №842: loss = 29.217782974243164, weights = tensor([6.3047, 2.4531], dtype=torch.float16)\n",
      "step №843: loss = 29.20928382873535, weights = tensor([6.3047, 2.4551], dtype=torch.float16)\n",
      "step №844: loss = 29.20078468322754, weights = tensor([6.3047, 2.4570], dtype=torch.float16)\n",
      "step №845: loss = 29.192296981811523, weights = tensor([6.3047, 2.4590], dtype=torch.float16)\n",
      "step №846: loss = 29.18381690979004, weights = tensor([6.3047, 2.4609], dtype=torch.float16)\n",
      "step №847: loss = 29.17534828186035, weights = tensor([6.3047, 2.4629], dtype=torch.float16)\n",
      "step №848: loss = 29.166879653930664, weights = tensor([6.3008, 2.4648], dtype=torch.float16)\n",
      "step №849: loss = 29.157489776611328, weights = tensor([6.3008, 2.4668], dtype=torch.float16)\n",
      "step №850: loss = 29.148967742919922, weights = tensor([6.3008, 2.4688], dtype=torch.float16)\n",
      "step №851: loss = 29.140460968017578, weights = tensor([6.3008, 2.4707], dtype=torch.float16)\n",
      "step №852: loss = 29.131954193115234, weights = tensor([6.3008, 2.4727], dtype=torch.float16)\n",
      "step №853: loss = 29.123462677001953, weights = tensor([6.3008, 2.4746], dtype=torch.float16)\n",
      "step №854: loss = 29.114971160888672, weights = tensor([6.3008, 2.4766], dtype=torch.float16)\n",
      "step №855: loss = 29.106494903564453, weights = tensor([6.3008, 2.4785], dtype=torch.float16)\n",
      "step №856: loss = 29.098018646240234, weights = tensor([6.3008, 2.4805], dtype=torch.float16)\n",
      "step №857: loss = 29.089557647705078, weights = tensor([6.3008, 2.4824], dtype=torch.float16)\n",
      "step №858: loss = 29.081096649169922, weights = tensor([6.3008, 2.4844], dtype=torch.float16)\n",
      "step №859: loss = 29.072650909423828, weights = tensor([6.3008, 2.4863], dtype=torch.float16)\n",
      "step №860: loss = 29.064205169677734, weights = tensor([6.3008, 2.4883], dtype=torch.float16)\n",
      "step №861: loss = 29.055774688720703, weights = tensor([6.2969, 2.4902], dtype=torch.float16)\n",
      "step №862: loss = 29.04638671875, weights = tensor([6.2969, 2.4922], dtype=torch.float16)\n",
      "step №863: loss = 29.03790283203125, weights = tensor([6.2969, 2.4941], dtype=torch.float16)\n",
      "step №864: loss = 29.0294189453125, weights = tensor([6.2969, 2.4961], dtype=torch.float16)\n",
      "step №865: loss = 29.020946502685547, weights = tensor([6.2969, 2.4980], dtype=torch.float16)\n",
      "step №866: loss = 29.012481689453125, weights = tensor([6.2969, 2.5000], dtype=torch.float16)\n",
      "step №867: loss = 29.0040283203125, weights = tensor([6.2969, 2.5020], dtype=torch.float16)\n",
      "step №868: loss = 28.995574951171875, weights = tensor([6.2969, 2.5039], dtype=torch.float16)\n",
      "step №869: loss = 28.987133026123047, weights = tensor([6.2969, 2.5059], dtype=torch.float16)\n",
      "step №870: loss = 28.97869873046875, weights = tensor([6.2969, 2.5078], dtype=torch.float16)\n",
      "step №871: loss = 28.97027587890625, weights = tensor([6.2969, 2.5098], dtype=torch.float16)\n",
      "step №872: loss = 28.96185302734375, weights = tensor([6.2969, 2.5117], dtype=torch.float16)\n",
      "step №873: loss = 28.953441619873047, weights = tensor([6.2930, 2.5137], dtype=torch.float16)\n",
      "step №874: loss = 28.94412612915039, weights = tensor([6.2930, 2.5156], dtype=torch.float16)\n",
      "step №875: loss = 28.9356632232666, weights = tensor([6.2930, 2.5176], dtype=torch.float16)\n",
      "step №876: loss = 28.927204132080078, weights = tensor([6.2930, 2.5195], dtype=torch.float16)\n",
      "step №877: loss = 28.91875648498535, weights = tensor([6.2930, 2.5215], dtype=torch.float16)\n",
      "step №878: loss = 28.91031265258789, weights = tensor([6.2930, 2.5234], dtype=torch.float16)\n",
      "step №879: loss = 28.901880264282227, weights = tensor([6.2930, 2.5254], dtype=torch.float16)\n",
      "step №880: loss = 28.893451690673828, weights = tensor([6.2930, 2.5273], dtype=torch.float16)\n",
      "step №881: loss = 28.885034561157227, weights = tensor([6.2930, 2.5293], dtype=torch.float16)\n",
      "step №882: loss = 28.87662124633789, weights = tensor([6.2930, 2.5312], dtype=torch.float16)\n",
      "step №883: loss = 28.86821937561035, weights = tensor([6.2930, 2.5332], dtype=torch.float16)\n",
      "step №884: loss = 28.859821319580078, weights = tensor([6.2930, 2.5352], dtype=torch.float16)\n",
      "step №885: loss = 28.8514347076416, weights = tensor([6.2930, 2.5371], dtype=torch.float16)\n",
      "step №886: loss = 28.84305191040039, weights = tensor([6.2891, 2.5391], dtype=torch.float16)\n",
      "step №887: loss = 28.8337459564209, weights = tensor([6.2891, 2.5410], dtype=torch.float16)\n",
      "step №888: loss = 28.825307846069336, weights = tensor([6.2891, 2.5430], dtype=torch.float16)\n",
      "step №889: loss = 28.816883087158203, weights = tensor([6.2891, 2.5449], dtype=torch.float16)\n",
      "step №890: loss = 28.808462142944336, weights = tensor([6.2891, 2.5469], dtype=torch.float16)\n",
      "step №891: loss = 28.8000545501709, weights = tensor([6.2891, 2.5488], dtype=torch.float16)\n",
      "step №892: loss = 28.79164695739746, weights = tensor([6.2891, 2.5508], dtype=torch.float16)\n",
      "step №893: loss = 28.783252716064453, weights = tensor([6.2891, 2.5527], dtype=torch.float16)\n",
      "step №894: loss = 28.77486228942871, weights = tensor([6.2891, 2.5547], dtype=torch.float16)\n",
      "step №895: loss = 28.7664852142334, weights = tensor([6.2891, 2.5566], dtype=torch.float16)\n",
      "step №896: loss = 28.758108139038086, weights = tensor([6.2891, 2.5586], dtype=torch.float16)\n",
      "step №897: loss = 28.749744415283203, weights = tensor([6.2891, 2.5605], dtype=torch.float16)\n",
      "step №898: loss = 28.741384506225586, weights = tensor([6.2891, 2.5625], dtype=torch.float16)\n",
      "step №899: loss = 28.7330379486084, weights = tensor([6.2852, 2.5645], dtype=torch.float16)\n",
      "step №900: loss = 28.72373390197754, weights = tensor([6.2852, 2.5664], dtype=torch.float16)\n",
      "step №901: loss = 28.71533203125, weights = tensor([6.2852, 2.5684], dtype=torch.float16)\n",
      "step №902: loss = 28.706933975219727, weights = tensor([6.2852, 2.5703], dtype=torch.float16)\n",
      "step №903: loss = 28.69854736328125, weights = tensor([6.2852, 2.5723], dtype=torch.float16)\n",
      "step №904: loss = 28.69016456604004, weights = tensor([6.2852, 2.5742], dtype=torch.float16)\n",
      "step №905: loss = 28.681793212890625, weights = tensor([6.2852, 2.5762], dtype=torch.float16)\n",
      "step №906: loss = 28.673425674438477, weights = tensor([6.2852, 2.5781], dtype=torch.float16)\n",
      "step №907: loss = 28.665069580078125, weights = tensor([6.2852, 2.5801], dtype=torch.float16)\n",
      "step №908: loss = 28.65671730041504, weights = tensor([6.2852, 2.5820], dtype=torch.float16)\n",
      "step №909: loss = 28.64837646484375, weights = tensor([6.2852, 2.5840], dtype=torch.float16)\n",
      "step №910: loss = 28.640039443969727, weights = tensor([6.2852, 2.5859], dtype=torch.float16)\n",
      "step №911: loss = 28.6317138671875, weights = tensor([6.2812, 2.5879], dtype=torch.float16)\n",
      "step №912: loss = 28.622478485107422, weights = tensor([6.2812, 2.5898], dtype=torch.float16)\n",
      "step №913: loss = 28.614099502563477, weights = tensor([6.2812, 2.5918], dtype=torch.float16)\n",
      "step №914: loss = 28.605724334716797, weights = tensor([6.2812, 2.5938], dtype=torch.float16)\n",
      "step №915: loss = 28.597362518310547, weights = tensor([6.2812, 2.5957], dtype=torch.float16)\n",
      "step №916: loss = 28.589000701904297, weights = tensor([6.2812, 2.5977], dtype=torch.float16)\n",
      "step №917: loss = 28.580652236938477, weights = tensor([6.2812, 2.5996], dtype=torch.float16)\n",
      "step №918: loss = 28.572307586669922, weights = tensor([6.2812, 2.6016], dtype=torch.float16)\n",
      "step №919: loss = 28.563976287841797, weights = tensor([6.2812, 2.6035], dtype=torch.float16)\n",
      "step №920: loss = 28.555644989013672, weights = tensor([6.2812, 2.6055], dtype=torch.float16)\n",
      "step №921: loss = 28.547327041625977, weights = tensor([6.2812, 2.6074], dtype=torch.float16)\n",
      "step №922: loss = 28.539012908935547, weights = tensor([6.2812, 2.6094], dtype=torch.float16)\n",
      "step №923: loss = 28.530712127685547, weights = tensor([6.2812, 2.6113], dtype=torch.float16)\n",
      "step №924: loss = 28.522411346435547, weights = tensor([6.2773, 2.6133], dtype=torch.float16)\n",
      "step №925: loss = 28.5131893157959, weights = tensor([6.2773, 2.6152], dtype=torch.float16)\n",
      "step №926: loss = 28.504837036132812, weights = tensor([6.2773, 2.6172], dtype=torch.float16)\n",
      "step №927: loss = 28.496496200561523, weights = tensor([6.2773, 2.6191], dtype=torch.float16)\n",
      "step №928: loss = 28.4881591796875, weights = tensor([6.2773, 2.6211], dtype=torch.float16)\n",
      "step №929: loss = 28.479833602905273, weights = tensor([6.2773, 2.6230], dtype=torch.float16)\n",
      "step №930: loss = 28.471511840820312, weights = tensor([6.2773, 2.6250], dtype=torch.float16)\n",
      "step №931: loss = 28.46320152282715, weights = tensor([6.2773, 2.6270], dtype=torch.float16)\n",
      "step №932: loss = 28.45489501953125, weights = tensor([6.2773, 2.6289], dtype=torch.float16)\n",
      "step №933: loss = 28.44659996032715, weights = tensor([6.2773, 2.6309], dtype=torch.float16)\n",
      "step №934: loss = 28.438308715820312, weights = tensor([6.2773, 2.6328], dtype=torch.float16)\n",
      "step №935: loss = 28.430028915405273, weights = tensor([6.2773, 2.6348], dtype=torch.float16)\n",
      "step №936: loss = 28.4217529296875, weights = tensor([6.2773, 2.6367], dtype=torch.float16)\n",
      "step №937: loss = 28.413488388061523, weights = tensor([6.2734, 2.6387], dtype=torch.float16)\n",
      "step №938: loss = 28.40427017211914, weights = tensor([6.2734, 2.6406], dtype=torch.float16)\n",
      "step №939: loss = 28.395954132080078, weights = tensor([6.2734, 2.6426], dtype=torch.float16)\n",
      "step №940: loss = 28.387638092041016, weights = tensor([6.2734, 2.6445], dtype=torch.float16)\n",
      "step №941: loss = 28.37933349609375, weights = tensor([6.2734, 2.6465], dtype=torch.float16)\n",
      "step №942: loss = 28.371036529541016, weights = tensor([6.2734, 2.6484], dtype=torch.float16)\n",
      "step №943: loss = 28.362751007080078, weights = tensor([6.2734, 2.6504], dtype=torch.float16)\n",
      "step №944: loss = 28.35446548461914, weights = tensor([6.2734, 2.6523], dtype=torch.float16)\n",
      "step №945: loss = 28.34619140625, weights = tensor([6.2734, 2.6543], dtype=torch.float16)\n",
      "step №946: loss = 28.33792495727539, weights = tensor([6.2734, 2.6562], dtype=torch.float16)\n",
      "step №947: loss = 28.329669952392578, weights = tensor([6.2734, 2.6582], dtype=torch.float16)\n",
      "step №948: loss = 28.321414947509766, weights = tensor([6.2734, 2.6602], dtype=torch.float16)\n",
      "step №949: loss = 28.31317138671875, weights = tensor([6.2695, 2.6621], dtype=torch.float16)\n",
      "step №950: loss = 28.30402183532715, weights = tensor([6.2695, 2.6641], dtype=torch.float16)\n",
      "step №951: loss = 28.295726776123047, weights = tensor([6.2695, 2.6660], dtype=torch.float16)\n",
      "step №952: loss = 28.28743553161621, weights = tensor([6.2695, 2.6680], dtype=torch.float16)\n",
      "step №953: loss = 28.279155731201172, weights = tensor([6.2695, 2.6699], dtype=torch.float16)\n",
      "step №954: loss = 28.2708797454834, weights = tensor([6.2695, 2.6719], dtype=torch.float16)\n",
      "step №955: loss = 28.262615203857422, weights = tensor([6.2695, 2.6738], dtype=torch.float16)\n",
      "step №956: loss = 28.25435447692871, weights = tensor([6.2695, 2.6758], dtype=torch.float16)\n",
      "step №957: loss = 28.246105194091797, weights = tensor([6.2695, 2.6777], dtype=torch.float16)\n",
      "step №958: loss = 28.23785972595215, weights = tensor([6.2695, 2.6797], dtype=torch.float16)\n",
      "step №959: loss = 28.229625701904297, weights = tensor([6.2695, 2.6816], dtype=torch.float16)\n",
      "step №960: loss = 28.22139549255371, weights = tensor([6.2695, 2.6836], dtype=torch.float16)\n",
      "step №961: loss = 28.213176727294922, weights = tensor([6.2695, 2.6855], dtype=torch.float16)\n",
      "step №962: loss = 28.2049617767334, weights = tensor([6.2656, 2.6875], dtype=torch.float16)\n",
      "step №963: loss = 28.195825576782227, weights = tensor([6.2656, 2.6895], dtype=torch.float16)\n",
      "step №964: loss = 28.18755531311035, weights = tensor([6.2656, 2.6914], dtype=torch.float16)\n",
      "step №965: loss = 28.179296493530273, weights = tensor([6.2656, 2.6934], dtype=torch.float16)\n",
      "step №966: loss = 28.171045303344727, weights = tensor([6.2656, 2.6953], dtype=torch.float16)\n",
      "step №967: loss = 28.162805557250977, weights = tensor([6.2656, 2.6973], dtype=torch.float16)\n",
      "step №968: loss = 28.154565811157227, weights = tensor([6.2656, 2.6992], dtype=torch.float16)\n",
      "step №969: loss = 28.146337509155273, weights = tensor([6.2656, 2.7012], dtype=torch.float16)\n",
      "step №970: loss = 28.13811683654785, weights = tensor([6.2656, 2.7031], dtype=torch.float16)\n",
      "step №971: loss = 28.129907608032227, weights = tensor([6.2656, 2.7051], dtype=torch.float16)\n",
      "step №972: loss = 28.1216983795166, weights = tensor([6.2656, 2.7070], dtype=torch.float16)\n",
      "step №973: loss = 28.113500595092773, weights = tensor([6.2656, 2.7090], dtype=torch.float16)\n",
      "step №974: loss = 28.105310440063477, weights = tensor([6.2656, 2.7109], dtype=torch.float16)\n",
      "step №975: loss = 28.097131729125977, weights = tensor([6.2617, 2.7129], dtype=torch.float16)\n",
      "step №976: loss = 28.087993621826172, weights = tensor([6.2617, 2.7148], dtype=torch.float16)\n",
      "step №977: loss = 28.079761505126953, weights = tensor([6.2617, 2.7168], dtype=torch.float16)\n",
      "step №978: loss = 28.071529388427734, weights = tensor([6.2617, 2.7188], dtype=torch.float16)\n",
      "step №979: loss = 28.063312530517578, weights = tensor([6.2617, 2.7207], dtype=torch.float16)\n",
      "step №980: loss = 28.055095672607422, weights = tensor([6.2617, 2.7227], dtype=torch.float16)\n",
      "step №981: loss = 28.046894073486328, weights = tensor([6.2617, 2.7246], dtype=torch.float16)\n",
      "step №982: loss = 28.038692474365234, weights = tensor([6.2617, 2.7266], dtype=torch.float16)\n",
      "step №983: loss = 28.030506134033203, weights = tensor([6.2617, 2.7285], dtype=torch.float16)\n",
      "step №984: loss = 28.022319793701172, weights = tensor([6.2617, 2.7305], dtype=torch.float16)\n",
      "step №985: loss = 28.014148712158203, weights = tensor([6.2617, 2.7324], dtype=torch.float16)\n",
      "step №986: loss = 28.005977630615234, weights = tensor([6.2617, 2.7344], dtype=torch.float16)\n",
      "step №987: loss = 27.997821807861328, weights = tensor([6.2578, 2.7363], dtype=torch.float16)\n",
      "step №988: loss = 27.988754272460938, weights = tensor([6.2578, 2.7383], dtype=torch.float16)\n",
      "step №989: loss = 27.980541229248047, weights = tensor([6.2578, 2.7402], dtype=torch.float16)\n",
      "step №990: loss = 27.972335815429688, weights = tensor([6.2578, 2.7422], dtype=torch.float16)\n",
      "step №991: loss = 27.964141845703125, weights = tensor([6.2578, 2.7441], dtype=torch.float16)\n",
      "step №992: loss = 27.955947875976562, weights = tensor([6.2578, 2.7461], dtype=torch.float16)\n",
      "step №993: loss = 27.947765350341797, weights = tensor([6.2578, 2.7480], dtype=torch.float16)\n",
      "step №994: loss = 27.939590454101562, weights = tensor([6.2578, 2.7500], dtype=torch.float16)\n",
      "step №995: loss = 27.931427001953125, weights = tensor([6.2578, 2.7520], dtype=torch.float16)\n",
      "step №996: loss = 27.923263549804688, weights = tensor([6.2578, 2.7539], dtype=torch.float16)\n",
      "step №997: loss = 27.915111541748047, weights = tensor([6.2578, 2.7559], dtype=torch.float16)\n",
      "step №998: loss = 27.906967163085938, weights = tensor([6.2578, 2.7578], dtype=torch.float16)\n",
      "step №999: loss = 27.898834228515625, weights = tensor([6.2578, 2.7598], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#first let's train our model with a stopping criteria as numbers of steps \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "793f260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative loss = 27.898834228515625')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d1693c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 312.77996826171875\n",
      "did one more step, loss reduced by 135.29200744628906\n",
      "did one more step, loss reduced by 58.35686492919922\n",
      "did one more step, loss reduced by 25.21221160888672\n",
      "did one more step, loss reduced by 10.955741882324219\n",
      "did one more step, loss reduced by 4.708599090576172\n",
      "did one more step, loss reduced by 2.051380157470703\n",
      "did one more step, loss reduced by 0.8823318481445312\n",
      "did one more step, loss reduced by 0.3917655944824219\n",
      "did one more step, loss reduced by 0.18954849243164062\n",
      "did one more step, loss reduced by 0.09249114990234375\n",
      "did one more step, loss reduced by 0.0444488525390625\n",
      "did one more step, loss reduced by 0.0313262939453125\n",
      "did one more step, loss reduced by 0.027313232421875\n",
      "did one more step, loss reduced by 0.0223846435546875\n",
      "did one more step, loss reduced by 0.020538330078125\n",
      "did one more step, loss reduced by 0.0195770263671875\n",
      "did one more step, loss reduced by 0.01955413818359375\n",
      "did one more step, loss reduced by 0.019535064697265625\n",
      "did one more step, loss reduced by 0.019504547119140625\n",
      "did one more step, loss reduced by 0.0194854736328125\n",
      "did one more step, loss reduced by 0.019458770751953125\n",
      "did one more step, loss reduced by 0.019435882568359375\n",
      "did one more step, loss reduced by 0.019412994384765625\n",
      "did one more step, loss reduced by 0.019390106201171875\n",
      "did one more step, loss reduced by 0.019367218017578125\n",
      "did one more step, loss reduced by 0.019344329833984375\n",
      "did one more step, loss reduced by 0.019321441650390625\n",
      "did one more step, loss reduced by 0.01929473876953125\n",
      "did one more step, loss reduced by 0.019275665283203125\n",
      "did one more step, loss reduced by 0.020233154296875\n",
      "did one more step, loss reduced by 0.019351959228515625\n",
      "did one more step, loss reduced by 0.019321441650390625\n",
      "did one more step, loss reduced by 0.019298553466796875\n",
      "did one more step, loss reduced by 0.019283294677734375\n",
      "did one more step, loss reduced by 0.01924896240234375\n",
      "did one more step, loss reduced by 0.01923370361328125\n",
      "did one more step, loss reduced by 0.01920318603515625\n",
      "did one more step, loss reduced by 0.020259857177734375\n",
      "did one more step, loss reduced by 0.01927947998046875\n",
      "did one more step, loss reduced by 0.019256591796875\n",
      "did one more step, loss reduced by 0.018550872802734375\n",
      "did one more step, loss reduced by 0.019214630126953125\n",
      "did one more step, loss reduced by 0.019184112548828125\n",
      "did one more step, loss reduced by 0.0191650390625\n",
      "did one more step, loss reduced by 0.020183563232421875\n",
      "did one more step, loss reduced by 0.019245147705078125\n",
      "did one more step, loss reduced by 0.019207000732421875\n",
      "did one more step, loss reduced by 0.019191741943359375\n",
      "did one more step, loss reduced by 0.0191650390625\n",
      "did one more step, loss reduced by 0.0191497802734375\n",
      "did one more step, loss reduced by 0.0191192626953125\n",
      "did one more step, loss reduced by 0.020111083984375\n",
      "did one more step, loss reduced by 0.019195556640625\n",
      "did one more step, loss reduced by 0.01917266845703125\n",
      "did one more step, loss reduced by 0.019145965576171875\n",
      "did one more step, loss reduced by 0.019123077392578125\n",
      "did one more step, loss reduced by 0.01910400390625\n",
      "did one more step, loss reduced by 0.019077301025390625\n",
      "did one more step, loss reduced by 0.020030975341796875\n",
      "did one more step, loss reduced by 0.019153594970703125\n",
      "did one more step, loss reduced by 0.019130706787109375\n",
      "did one more step, loss reduced by 0.019115447998046875\n",
      "did one more step, loss reduced by 0.01906585693359375\n",
      "did one more step, loss reduced by 0.019069671630859375\n",
      "did one more step, loss reduced by 0.0190277099609375\n",
      "did one more step, loss reduced by 0.0199737548828125\n",
      "did one more step, loss reduced by 0.01910400390625\n",
      "did one more step, loss reduced by 0.01908111572265625\n",
      "did one more step, loss reduced by 0.01906585693359375\n",
      "did one more step, loss reduced by 0.01903533935546875\n",
      "did one more step, loss reduced by 0.019012451171875\n",
      "did one more step, loss reduced by 0.01898956298828125\n",
      "did one more step, loss reduced by 0.0189666748046875\n",
      "did one more step, loss reduced by 0.020000457763671875\n",
      "did one more step, loss reduced by 0.019031524658203125\n",
      "did one more step, loss reduced by 0.019023895263671875\n",
      "did one more step, loss reduced by 0.01898956298828125\n",
      "did one more step, loss reduced by 0.01898193359375\n",
      "did one more step, loss reduced by 0.018939971923828125\n",
      "did one more step, loss reduced by 0.018924713134765625\n",
      "did one more step, loss reduced by 0.01992034912109375\n",
      "did one more step, loss reduced by 0.019001007080078125\n",
      "did one more step, loss reduced by 0.0189666748046875\n",
      "did one more step, loss reduced by 0.018955230712890625\n",
      "did one more step, loss reduced by 0.018924713134765625\n",
      "did one more step, loss reduced by 0.0189056396484375\n",
      "did one more step, loss reduced by 0.018878936767578125\n",
      "did one more step, loss reduced by 0.01984405517578125\n",
      "did one more step, loss reduced by 0.018970489501953125\n",
      "did one more step, loss reduced by 0.018924713134765625\n",
      "did one more step, loss reduced by 0.01889801025390625\n",
      "did one more step, loss reduced by 0.018886566162109375\n",
      "did one more step, loss reduced by 0.01885986328125\n",
      "did one more step, loss reduced by 0.01885223388671875\n",
      "did one more step, loss reduced by 0.01976776123046875\n",
      "did one more step, loss reduced by 0.018909454345703125\n",
      "did one more step, loss reduced by 0.018886566162109375\n",
      "did one more step, loss reduced by 0.01886749267578125\n",
      "did one more step, loss reduced by 0.018840789794921875\n",
      "did one more step, loss reduced by 0.0188140869140625\n",
      "did one more step, loss reduced by 0.018795013427734375\n",
      "did one more step, loss reduced by 0.018772125244140625\n",
      "did one more step, loss reduced by 0.018451690673828125\n",
      "did one more step, loss reduced by 0.018848419189453125\n",
      "did one more step, loss reduced by 0.018825531005859375\n",
      "did one more step, loss reduced by 0.018798828125\n",
      "did one more step, loss reduced by 0.018772125244140625\n",
      "did one more step, loss reduced by 0.017414093017578125\n",
      "did one more step, loss reduced by 0.01738739013671875\n",
      "did one more step, loss reduced by 0.01837158203125\n",
      "did one more step, loss reduced by 0.018802642822265625\n",
      "did one more step, loss reduced by 0.01879119873046875\n",
      "did one more step, loss reduced by 0.01741790771484375\n",
      "did one more step, loss reduced by 0.017398834228515625\n",
      "did one more step, loss reduced by 0.018718719482421875\n",
      "did one more step, loss reduced by 0.0186920166015625\n",
      "did one more step, loss reduced by 0.018672943115234375\n",
      "did one more step, loss reduced by 0.019710540771484375\n",
      "did one more step, loss reduced by 0.018749237060546875\n",
      "did one more step, loss reduced by 0.018718719482421875\n",
      "did one more step, loss reduced by 0.0186920166015625\n",
      "did one more step, loss reduced by 0.0186767578125\n",
      "did one more step, loss reduced by 0.018650054931640625\n",
      "did one more step, loss reduced by 0.018627166748046875\n",
      "did one more step, loss reduced by 0.019641876220703125\n",
      "did one more step, loss reduced by 0.01869964599609375\n",
      "did one more step, loss reduced by 0.018680572509765625\n",
      "did one more step, loss reduced by 0.018650054931640625\n",
      "did one more step, loss reduced by 0.0186309814453125\n",
      "did one more step, loss reduced by 0.01860809326171875\n",
      "did one more step, loss reduced by 0.018585205078125\n",
      "did one more step, loss reduced by 0.019561767578125\n",
      "did one more step, loss reduced by 0.0186614990234375\n",
      "did one more step, loss reduced by 0.0186309814453125\n",
      "did one more step, loss reduced by 0.01861572265625\n",
      "did one more step, loss reduced by 0.018585205078125\n",
      "did one more step, loss reduced by 0.01856231689453125\n",
      "did one more step, loss reduced by 0.0185394287109375\n",
      "did one more step, loss reduced by 0.019500732421875\n",
      "did one more step, loss reduced by 0.01861572265625\n",
      "did one more step, loss reduced by 0.018585205078125\n",
      "did one more step, loss reduced by 0.018566131591796875\n",
      "did one more step, loss reduced by 0.018550872802734375\n",
      "did one more step, loss reduced by 0.01851654052734375\n",
      "did one more step, loss reduced by 0.01849365234375\n",
      "did one more step, loss reduced by 0.018474578857421875\n",
      "did one more step, loss reduced by 0.01953125\n",
      "did one more step, loss reduced by 0.018543243408203125\n",
      "did one more step, loss reduced by 0.01851654052734375\n",
      "did one more step, loss reduced by 0.018505096435546875\n",
      "did one more step, loss reduced by 0.0184783935546875\n",
      "did one more step, loss reduced by 0.0184478759765625\n",
      "did one more step, loss reduced by 0.018421173095703125\n",
      "did one more step, loss reduced by 0.019458770751953125\n",
      "did one more step, loss reduced by 0.018505096435546875\n",
      "did one more step, loss reduced by 0.0184783935546875\n",
      "did one more step, loss reduced by 0.018459320068359375\n",
      "did one more step, loss reduced by 0.0184326171875\n",
      "did one more step, loss reduced by 0.018413543701171875\n",
      "did one more step, loss reduced by 0.01837921142578125\n",
      "did one more step, loss reduced by 0.019382476806640625\n",
      "did one more step, loss reduced by 0.01845550537109375\n",
      "did one more step, loss reduced by 0.018451690673828125\n",
      "did one more step, loss reduced by 0.018402099609375\n",
      "did one more step, loss reduced by 0.018390655517578125\n",
      "did one more step, loss reduced by 0.018367767333984375\n",
      "did one more step, loss reduced by 0.018344879150390625\n",
      "did one more step, loss reduced by 0.0193023681640625\n",
      "did one more step, loss reduced by 0.018413543701171875\n",
      "did one more step, loss reduced by 0.018398284912109375\n",
      "did one more step, loss reduced by 0.01837158203125\n",
      "did one more step, loss reduced by 0.018344879150390625\n",
      "did one more step, loss reduced by 0.018321990966796875\n",
      "did one more step, loss reduced by 0.018299102783203125\n",
      "did one more step, loss reduced by 0.0192413330078125\n",
      "did one more step, loss reduced by 0.018367767333984375\n",
      "did one more step, loss reduced by 0.0183563232421875\n",
      "did one more step, loss reduced by 0.018321990966796875\n",
      "did one more step, loss reduced by 0.018310546875\n",
      "did one more step, loss reduced by 0.01827239990234375\n",
      "did one more step, loss reduced by 0.01824951171875\n",
      "did one more step, loss reduced by 0.0182342529296875\n",
      "did one more step, loss reduced by 0.019268035888671875\n",
      "did one more step, loss reduced by 0.018299102783203125\n",
      "did one more step, loss reduced by 0.018283843994140625\n",
      "did one more step, loss reduced by 0.018260955810546875\n",
      "did one more step, loss reduced by 0.018238067626953125\n",
      "did one more step, loss reduced by 0.01821136474609375\n",
      "did one more step, loss reduced by 0.018192291259765625\n",
      "did one more step, loss reduced by 0.019184112548828125\n",
      "did one more step, loss reduced by 0.018268585205078125\n",
      "did one more step, loss reduced by 0.0182342529296875\n",
      "did one more step, loss reduced by 0.018218994140625\n",
      "did one more step, loss reduced by 0.01819610595703125\n",
      "did one more step, loss reduced by 0.0181732177734375\n",
      "did one more step, loss reduced by 0.01813507080078125\n",
      "did one more step, loss reduced by 0.0191192626953125\n",
      "did one more step, loss reduced by 0.018215179443359375\n",
      "did one more step, loss reduced by 0.0182037353515625\n",
      "did one more step, loss reduced by 0.0181732177734375\n",
      "did one more step, loss reduced by 0.0181427001953125\n",
      "did one more step, loss reduced by 0.018131256103515625\n",
      "did one more step, loss reduced by 0.01810455322265625\n",
      "did one more step, loss reduced by 0.019031524658203125\n",
      "did one more step, loss reduced by 0.018184661865234375\n",
      "did one more step, loss reduced by 0.018157958984375\n",
      "did one more step, loss reduced by 0.01812744140625\n",
      "did one more step, loss reduced by 0.018096923828125\n",
      "did one more step, loss reduced by 0.018096923828125\n",
      "did one more step, loss reduced by 0.015472412109375\n",
      "did one more step, loss reduced by 0.015472412109375\n",
      "did one more step, loss reduced by 0.016448974609375\n",
      "did one more step, loss reduced by 0.0181121826171875\n",
      "did one more step, loss reduced by 0.018096923828125\n",
      "did one more step, loss reduced by 0.018070220947265625\n",
      "did one more step, loss reduced by 0.015472412109375\n",
      "did one more step, loss reduced by 0.0154571533203125\n",
      "did one more step, loss reduced by 0.015438079833984375\n",
      "did one more step, loss reduced by 0.0154266357421875\n",
      "did one more step, loss reduced by 0.01641082763671875\n",
      "did one more step, loss reduced by 0.018070220947265625\n",
      "did one more step, loss reduced by 0.015472412109375\n",
      "did one more step, loss reduced by 0.015445709228515625\n",
      "did one more step, loss reduced by 0.015445709228515625\n",
      "did one more step, loss reduced by 0.015407562255859375\n",
      "did one more step, loss reduced by 0.015407562255859375\n",
      "did one more step, loss reduced by 0.015380859375\n",
      "did one more step, loss reduced by 0.016345977783203125\n",
      "did one more step, loss reduced by 0.015453338623046875\n",
      "did one more step, loss reduced by 0.015438079833984375\n",
      "did one more step, loss reduced by 0.015411376953125\n",
      "did one more step, loss reduced by 0.01540374755859375\n",
      "did one more step, loss reduced by 0.015380859375\n",
      "did one more step, loss reduced by 0.01537322998046875\n",
      "did one more step, loss reduced by 0.01534271240234375\n",
      "did one more step, loss reduced by 0.0153350830078125\n",
      "did one more step, loss reduced by 0.016357421875\n",
      "did one more step, loss reduced by 0.015399932861328125\n",
      "did one more step, loss reduced by 0.015377044677734375\n",
      "did one more step, loss reduced by 0.015380859375\n",
      "did one more step, loss reduced by 0.015338897705078125\n",
      "did one more step, loss reduced by 0.0153350830078125\n",
      "did one more step, loss reduced by 0.015308380126953125\n",
      "did one more step, loss reduced by 0.015300750732421875\n",
      "did one more step, loss reduced by 0.016277313232421875\n",
      "did one more step, loss reduced by 0.0153656005859375\n",
      "did one more step, loss reduced by 0.015346527099609375\n",
      "did one more step, loss reduced by 0.0153350830078125\n",
      "did one more step, loss reduced by 0.01531219482421875\n",
      "did one more step, loss reduced by 0.015300750732421875\n",
      "did one more step, loss reduced by 0.015277862548828125\n",
      "did one more step, loss reduced by 0.01526641845703125\n",
      "did one more step, loss reduced by 0.016193389892578125\n",
      "did one more step, loss reduced by 0.0153350830078125\n",
      "did one more step, loss reduced by 0.0153045654296875\n",
      "did one more step, loss reduced by 0.01531219482421875\n",
      "did one more step, loss reduced by 0.015270233154296875\n",
      "did one more step, loss reduced by 0.015270233154296875\n",
      "did one more step, loss reduced by 0.0152435302734375\n",
      "did one more step, loss reduced by 0.015228271484375\n",
      "did one more step, loss reduced by 0.0152130126953125\n",
      "did one more step, loss reduced by 0.016201019287109375\n",
      "did one more step, loss reduced by 0.0152740478515625\n",
      "did one more step, loss reduced by 0.01526641845703125\n",
      "did one more step, loss reduced by 0.0152435302734375\n",
      "did one more step, loss reduced by 0.01523590087890625\n",
      "did one more step, loss reduced by 0.01520538330078125\n",
      "did one more step, loss reduced by 0.01519775390625\n",
      "did one more step, loss reduced by 0.015178680419921875\n",
      "did one more step, loss reduced by 0.01612091064453125\n",
      "did one more step, loss reduced by 0.01523590087890625\n",
      "did one more step, loss reduced by 0.0152435302734375\n",
      "did one more step, loss reduced by 0.01520538330078125\n",
      "did one more step, loss reduced by 0.01519775390625\n",
      "did one more step, loss reduced by 0.015171051025390625\n",
      "did one more step, loss reduced by 0.015163421630859375\n",
      "did one more step, loss reduced by 0.015140533447265625\n",
      "did one more step, loss reduced by 0.015132904052734375\n",
      "did one more step, loss reduced by 0.01612091064453125\n",
      "did one more step, loss reduced by 0.01519775390625\n",
      "did one more step, loss reduced by 0.01517486572265625\n",
      "did one more step, loss reduced by 0.015163421630859375\n",
      "did one more step, loss reduced by 0.015140533447265625\n",
      "did one more step, loss reduced by 0.01512908935546875\n",
      "did one more step, loss reduced by 0.015106201171875\n",
      "did one more step, loss reduced by 0.015094757080078125\n",
      "did one more step, loss reduced by 0.0160369873046875\n",
      "did one more step, loss reduced by 0.01517486572265625\n",
      "did one more step, loss reduced by 0.015132904052734375\n",
      "did one more step, loss reduced by 0.01512908935546875\n",
      "did one more step, loss reduced by 0.015106201171875\n",
      "did one more step, loss reduced by 0.0150909423828125\n",
      "did one more step, loss reduced by 0.01507568359375\n",
      "did one more step, loss reduced by 0.0150604248046875\n",
      "did one more step, loss reduced by 0.01503753662109375\n",
      "did one more step, loss reduced by 0.01605224609375\n",
      "did one more step, loss reduced by 0.015106201171875\n",
      "did one more step, loss reduced by 0.01509857177734375\n",
      "did one more step, loss reduced by 0.015064239501953125\n",
      "did one more step, loss reduced by 0.015064239501953125\n",
      "did one more step, loss reduced by 0.01503753662109375\n",
      "did one more step, loss reduced by 0.0150299072265625\n",
      "did one more step, loss reduced by 0.0149993896484375\n",
      "did one more step, loss reduced by 0.015979766845703125\n",
      "did one more step, loss reduced by 0.01506805419921875\n",
      "did one more step, loss reduced by 0.0150604248046875\n",
      "did one more step, loss reduced by 0.015033721923828125\n",
      "did one more step, loss reduced by 0.015026092529296875\n",
      "did one more step, loss reduced by 0.015003204345703125\n",
      "did one more step, loss reduced by 0.014995574951171875\n",
      "did one more step, loss reduced by 0.0149688720703125\n",
      "did one more step, loss reduced by 0.01496124267578125\n",
      "did one more step, loss reduced by 0.01596832275390625\n",
      "did one more step, loss reduced by 0.015026092529296875\n",
      "did one more step, loss reduced by 0.015003204345703125\n",
      "did one more step, loss reduced by 0.014987945556640625\n",
      "did one more step, loss reduced by 0.0149688720703125\n",
      "did one more step, loss reduced by 0.01496124267578125\n",
      "did one more step, loss reduced by 0.01493072509765625\n",
      "did one more step, loss reduced by 0.014923095703125\n",
      "did one more step, loss reduced by 0.015895843505859375\n",
      "did one more step, loss reduced by 0.01499176025390625\n",
      "did one more step, loss reduced by 0.0149688720703125\n",
      "did one more step, loss reduced by 0.01495361328125\n",
      "did one more step, loss reduced by 0.0149383544921875\n",
      "did one more step, loss reduced by 0.014923095703125\n",
      "did one more step, loss reduced by 0.01490020751953125\n",
      "did one more step, loss reduced by 0.014896392822265625\n",
      "did one more step, loss reduced by 0.015804290771484375\n",
      "did one more step, loss reduced by 0.014957427978515625\n",
      "did one more step, loss reduced by 0.014934539794921875\n",
      "did one more step, loss reduced by 0.014923095703125\n",
      "did one more step, loss reduced by 0.01490020751953125\n",
      "did one more step, loss reduced by 0.014888763427734375\n",
      "did one more step, loss reduced by 0.014865875244140625\n",
      "did one more step, loss reduced by 0.01485443115234375\n",
      "did one more step, loss reduced by 0.01483154296875\n",
      "did one more step, loss reduced by 0.015827178955078125\n",
      "did one more step, loss reduced by 0.014892578125\n",
      "did one more step, loss reduced by 0.014892578125\n",
      "did one more step, loss reduced by 0.014862060546875\n",
      "did one more step, loss reduced by 0.014862060546875\n",
      "did one more step, loss reduced by 0.01483154296875\n",
      "did one more step, loss reduced by 0.01482391357421875\n",
      "did one more step, loss reduced by 0.014781951904296875\n",
      "did one more step, loss reduced by 0.01575469970703125\n",
      "did one more step, loss reduced by 0.014865875244140625\n",
      "did one more step, loss reduced by 0.014850616455078125\n",
      "did one more step, loss reduced by 0.01483154296875\n",
      "did one more step, loss reduced by 0.01482391357421875\n",
      "did one more step, loss reduced by 0.01479339599609375\n",
      "did one more step, loss reduced by 0.0147857666015625\n",
      "did one more step, loss reduced by 0.01476287841796875\n",
      "did one more step, loss reduced by 0.0147552490234375\n",
      "did one more step, loss reduced by 0.015743255615234375\n",
      "did one more step, loss reduced by 0.0148162841796875\n",
      "did one more step, loss reduced by 0.014801025390625\n",
      "did one more step, loss reduced by 0.0147857666015625\n",
      "did one more step, loss reduced by 0.01476287841796875\n",
      "did one more step, loss reduced by 0.0147552490234375\n",
      "did one more step, loss reduced by 0.014720916748046875\n",
      "did one more step, loss reduced by 0.014720916748046875\n",
      "did one more step, loss reduced by 0.015659332275390625\n",
      "did one more step, loss reduced by 0.014789581298828125\n",
      "did one more step, loss reduced by 0.01476287841796875\n",
      "did one more step, loss reduced by 0.014751434326171875\n",
      "did one more step, loss reduced by 0.014728546142578125\n",
      "did one more step, loss reduced by 0.01471710205078125\n",
      "did one more step, loss reduced by 0.0146942138671875\n",
      "did one more step, loss reduced by 0.014682769775390625\n",
      "did one more step, loss reduced by 0.014659881591796875\n",
      "did one more step, loss reduced by 0.0156707763671875\n",
      "did one more step, loss reduced by 0.014728546142578125\n",
      "did one more step, loss reduced by 0.014720916748046875\n",
      "did one more step, loss reduced by 0.0146942138671875\n",
      "did one more step, loss reduced by 0.014690399169921875\n",
      "did one more step, loss reduced by 0.014652252197265625\n",
      "did one more step, loss reduced by 0.0146484375\n",
      "did one more step, loss reduced by 0.0146331787109375\n",
      "did one more step, loss reduced by 0.01558685302734375\n",
      "did one more step, loss reduced by 0.014690399169921875\n",
      "did one more step, loss reduced by 0.014690399169921875\n",
      "did one more step, loss reduced by 0.01465606689453125\n",
      "did one more step, loss reduced by 0.0146484375\n",
      "did one more step, loss reduced by 0.01462554931640625\n",
      "did one more step, loss reduced by 0.014617919921875\n",
      "did one more step, loss reduced by 0.01458740234375\n",
      "did one more step, loss reduced by 0.01457977294921875\n",
      "did one more step, loss reduced by 0.015598297119140625\n",
      "did one more step, loss reduced by 0.0146484375\n",
      "did one more step, loss reduced by 0.014621734619140625\n",
      "did one more step, loss reduced by 0.014621734619140625\n",
      "did one more step, loss reduced by 0.014583587646484375\n",
      "did one more step, loss reduced by 0.014583587646484375\n",
      "did one more step, loss reduced by 0.014556884765625\n",
      "did one more step, loss reduced by 0.0145416259765625\n",
      "did one more step, loss reduced by 0.0155181884765625\n",
      "did one more step, loss reduced by 0.014614105224609375\n",
      "did one more step, loss reduced by 0.014591217041015625\n",
      "did one more step, loss reduced by 0.014575958251953125\n",
      "did one more step, loss reduced by 0.014556884765625\n",
      "did one more step, loss reduced by 0.01454925537109375\n",
      "did one more step, loss reduced by 0.014522552490234375\n",
      "did one more step, loss reduced by 0.014507293701171875\n",
      "did one more step, loss reduced by 0.01543426513671875\n",
      "did one more step, loss reduced by 0.014583587646484375\n",
      "did one more step, loss reduced by 0.014556884765625\n",
      "did one more step, loss reduced by 0.014553070068359375\n",
      "did one more step, loss reduced by 0.014507293701171875\n",
      "did one more step, loss reduced by 0.01451873779296875\n",
      "did one more step, loss reduced by 0.014492034912109375\n",
      "did one more step, loss reduced by 0.014469146728515625\n",
      "did one more step, loss reduced by 0.014461517333984375\n",
      "did one more step, loss reduced by 0.01544189453125\n",
      "did one more step, loss reduced by 0.014522552490234375\n",
      "did one more step, loss reduced by 0.0145111083984375\n",
      "did one more step, loss reduced by 0.014484405517578125\n",
      "did one more step, loss reduced by 0.0144805908203125\n",
      "did one more step, loss reduced by 0.0144500732421875\n",
      "did one more step, loss reduced by 0.014446258544921875\n",
      "did one more step, loss reduced by 0.014423370361328125\n",
      "did one more step, loss reduced by 0.0153656005859375\n",
      "did one more step, loss reduced by 0.014484405517578125\n",
      "did one more step, loss reduced by 0.014484405517578125\n",
      "did one more step, loss reduced by 0.01444244384765625\n",
      "did one more step, loss reduced by 0.0144500732421875\n",
      "did one more step, loss reduced by 0.0144195556640625\n",
      "did one more step, loss reduced by 0.014404296875\n",
      "did one more step, loss reduced by 0.0143890380859375\n",
      "did one more step, loss reduced by 0.014373779296875\n",
      "did one more step, loss reduced by 0.0153656005859375\n",
      "did one more step, loss reduced by 0.014438629150390625\n",
      "did one more step, loss reduced by 0.0144195556640625\n",
      "did one more step, loss reduced by 0.01441192626953125\n",
      "did one more step, loss reduced by 0.014385223388671875\n",
      "did one more step, loss reduced by 0.014369964599609375\n",
      "did one more step, loss reduced by 0.014354705810546875\n",
      "did one more step, loss reduced by 0.014339447021484375\n",
      "did one more step, loss reduced by 0.015285491943359375\n",
      "did one more step, loss reduced by 0.01441192626953125\n",
      "did one more step, loss reduced by 0.014377593994140625\n",
      "did one more step, loss reduced by 0.014377593994140625\n",
      "did one more step, loss reduced by 0.014354705810546875\n",
      "did one more step, loss reduced by 0.014331817626953125\n",
      "did one more step, loss reduced by 0.014324188232421875\n",
      "did one more step, loss reduced by 0.014301300048828125\n",
      "did one more step, loss reduced by 0.014278411865234375\n",
      "did one more step, loss reduced by 0.015300750732421875\n",
      "did one more step, loss reduced by 0.014347076416015625\n",
      "did one more step, loss reduced by 0.01434326171875\n",
      "did one more step, loss reduced by 0.014316558837890625\n",
      "did one more step, loss reduced by 0.01430511474609375\n",
      "did one more step, loss reduced by 0.0142822265625\n",
      "did one more step, loss reduced by 0.014270782470703125\n",
      "did one more step, loss reduced by 0.014247894287109375\n",
      "did one more step, loss reduced by 0.015224456787109375\n",
      "did one more step, loss reduced by 0.01430511474609375\n",
      "did one more step, loss reduced by 0.014308929443359375\n",
      "did one more step, loss reduced by 0.0142822265625\n",
      "did one more step, loss reduced by 0.0142669677734375\n",
      "did one more step, loss reduced by 0.014251708984375\n",
      "did one more step, loss reduced by 0.0142364501953125\n",
      "did one more step, loss reduced by 0.014209747314453125\n",
      "did one more step, loss reduced by 0.01421356201171875\n",
      "did one more step, loss reduced by 0.0152130126953125\n",
      "did one more step, loss reduced by 0.014270782470703125\n",
      "did one more step, loss reduced by 0.014247894287109375\n",
      "did one more step, loss reduced by 0.014232635498046875\n",
      "did one more step, loss reduced by 0.01421356201171875\n",
      "did one more step, loss reduced by 0.0142059326171875\n",
      "did one more step, loss reduced by 0.0141754150390625\n",
      "did one more step, loss reduced by 0.01416778564453125\n",
      "did one more step, loss reduced by 0.01513671875\n",
      "did one more step, loss reduced by 0.014240264892578125\n",
      "did one more step, loss reduced by 0.01421356201171875\n",
      "did one more step, loss reduced by 0.01419830322265625\n",
      "did one more step, loss reduced by 0.01418304443359375\n",
      "did one more step, loss reduced by 0.01416778564453125\n",
      "did one more step, loss reduced by 0.014141082763671875\n",
      "did one more step, loss reduced by 0.01413726806640625\n",
      "did one more step, loss reduced by 0.01505279541015625\n",
      "did one more step, loss reduced by 0.0142059326171875\n",
      "did one more step, loss reduced by 0.0141754150390625\n",
      "did one more step, loss reduced by 0.01416778564453125\n",
      "did one more step, loss reduced by 0.014148712158203125\n",
      "did one more step, loss reduced by 0.014133453369140625\n",
      "did one more step, loss reduced by 0.014110565185546875\n",
      "did one more step, loss reduced by 0.014095306396484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.014080047607421875\n",
      "did one more step, loss reduced by 0.01506805419921875\n",
      "did one more step, loss reduced by 0.0141448974609375\n",
      "did one more step, loss reduced by 0.014129638671875\n",
      "did one more step, loss reduced by 0.0141143798828125\n",
      "did one more step, loss reduced by 0.01409912109375\n",
      "did one more step, loss reduced by 0.014072418212890625\n",
      "did one more step, loss reduced by 0.014068603515625\n",
      "did one more step, loss reduced by 0.014041900634765625\n",
      "did one more step, loss reduced by 0.014987945556640625\n",
      "did one more step, loss reduced by 0.0141143798828125\n",
      "did one more step, loss reduced by 0.014095306396484375\n",
      "did one more step, loss reduced by 0.014072418212890625\n",
      "did one more step, loss reduced by 0.014068603515625\n",
      "did one more step, loss reduced by 0.0140380859375\n",
      "did one more step, loss reduced by 0.014034271240234375\n",
      "did one more step, loss reduced by 0.014007568359375\n",
      "did one more step, loss reduced by 0.013996124267578125\n",
      "did one more step, loss reduced by 0.01499176025390625\n",
      "did one more step, loss reduced by 0.01406097412109375\n",
      "did one more step, loss reduced by 0.01404571533203125\n",
      "did one more step, loss reduced by 0.01403045654296875\n",
      "did one more step, loss reduced by 0.014003753662109375\n",
      "did one more step, loss reduced by 0.013996124267578125\n",
      "did one more step, loss reduced by 0.01397705078125\n",
      "did one more step, loss reduced by 0.0139617919921875\n",
      "did one more step, loss reduced by 0.014904022216796875\n",
      "did one more step, loss reduced by 0.01403045654296875\n",
      "did one more step, loss reduced by 0.014011383056640625\n",
      "did one more step, loss reduced by 0.013996124267578125\n",
      "did one more step, loss reduced by 0.013973236083984375\n",
      "did one more step, loss reduced by 0.013957977294921875\n",
      "did one more step, loss reduced by 0.013942718505859375\n",
      "did one more step, loss reduced by 0.013927459716796875\n",
      "did one more step, loss reduced by 0.013904571533203125\n",
      "did one more step, loss reduced by 0.01491546630859375\n",
      "did one more step, loss reduced by 0.01397705078125\n",
      "did one more step, loss reduced by 0.0139617919921875\n",
      "did one more step, loss reduced by 0.01393890380859375\n",
      "did one more step, loss reduced by 0.013927459716796875\n",
      "did one more step, loss reduced by 0.013904571533203125\n",
      "did one more step, loss reduced by 0.01389312744140625\n",
      "did one more step, loss reduced by 0.0138702392578125\n",
      "did one more step, loss reduced by 0.01483917236328125\n",
      "did one more step, loss reduced by 0.013935089111328125\n",
      "did one more step, loss reduced by 0.0139312744140625\n",
      "did one more step, loss reduced by 0.0139007568359375\n",
      "did one more step, loss reduced by 0.013896942138671875\n",
      "did one more step, loss reduced by 0.0138702392578125\n",
      "did one more step, loss reduced by 0.013858795166015625\n",
      "did one more step, loss reduced by 0.013835906982421875\n",
      "did one more step, loss reduced by 0.013824462890625\n",
      "did one more step, loss reduced by 0.014842987060546875\n",
      "did one more step, loss reduced by 0.013889312744140625\n",
      "did one more step, loss reduced by 0.0138702392578125\n",
      "did one more step, loss reduced by 0.013858795166015625\n",
      "did one more step, loss reduced by 0.013835906982421875\n",
      "did one more step, loss reduced by 0.013824462890625\n",
      "did one more step, loss reduced by 0.013797760009765625\n",
      "did one more step, loss reduced by 0.013790130615234375\n",
      "did one more step, loss reduced by 0.014766693115234375\n",
      "did one more step, loss reduced by 0.013858795166015625\n",
      "did one more step, loss reduced by 0.013835906982421875\n",
      "did one more step, loss reduced by 0.013820648193359375\n",
      "did one more step, loss reduced by 0.01380157470703125\n",
      "did one more step, loss reduced by 0.0137939453125\n",
      "did one more step, loss reduced by 0.013763427734375\n",
      "did one more step, loss reduced by 0.01375579833984375\n",
      "did one more step, loss reduced by 0.014682769775390625\n",
      "did one more step, loss reduced by 0.013824462890625\n",
      "did one more step, loss reduced by 0.013795852661132812\n",
      "did one more step, loss reduced by 0.0137939453125\n",
      "did one more step, loss reduced by 0.013767242431640625\n",
      "did one more step, loss reduced by 0.013757705688476562\n",
      "did one more step, loss reduced by 0.01373291015625\n",
      "did one more step, loss reduced by 0.0137176513671875\n",
      "did one more step, loss reduced by 0.013702392578125\n",
      "did one more step, loss reduced by 0.014690399169921875\n",
      "did one more step, loss reduced by 0.013763427734375\n",
      "did one more step, loss reduced by 0.013757705688476562\n",
      "did one more step, loss reduced by 0.01373291015625\n",
      "did one more step, loss reduced by 0.013723373413085938\n",
      "did one more step, loss reduced by 0.013692855834960938\n",
      "did one more step, loss reduced by 0.013690948486328125\n",
      "did one more step, loss reduced by 0.013666152954101562\n",
      "did one more step, loss reduced by 0.01461029052734375\n",
      "did one more step, loss reduced by 0.013731002807617188\n",
      "did one more step, loss reduced by 0.013723373413085938\n",
      "did one more step, loss reduced by 0.013698577880859375\n",
      "did one more step, loss reduced by 0.0136871337890625\n",
      "did one more step, loss reduced by 0.013660430908203125\n",
      "did one more step, loss reduced by 0.013652801513671875\n",
      "did one more step, loss reduced by 0.013629913330078125\n",
      "did one more step, loss reduced by 0.013622283935546875\n",
      "did one more step, loss reduced by 0.014612197875976562\n",
      "did one more step, loss reduced by 0.013685226440429688\n",
      "did one more step, loss reduced by 0.01366424560546875\n",
      "did one more step, loss reduced by 0.013654708862304688\n",
      "did one more step, loss reduced by 0.013628005981445312\n",
      "did one more step, loss reduced by 0.01361846923828125\n",
      "did one more step, loss reduced by 0.013591766357421875\n",
      "did one more step, loss reduced by 0.013589859008789062\n",
      "did one more step, loss reduced by 0.014528274536132812\n",
      "did one more step, loss reduced by 0.013654708862304688\n",
      "did one more step, loss reduced by 0.013628005981445312\n",
      "did one more step, loss reduced by 0.013620376586914062\n",
      "did one more step, loss reduced by 0.0135955810546875\n",
      "did one more step, loss reduced by 0.013580322265625\n",
      "did one more step, loss reduced by 0.0135650634765625\n",
      "did one more step, loss reduced by 0.0135498046875\n",
      "did one more step, loss reduced by 0.013525009155273438\n",
      "did one more step, loss reduced by 0.0145416259765625\n",
      "did one more step, loss reduced by 0.0135955810546875\n",
      "did one more step, loss reduced by 0.01358795166015625\n",
      "did one more step, loss reduced by 0.01355743408203125\n",
      "did one more step, loss reduced by 0.0135498046875\n",
      "did one more step, loss reduced by 0.013528823852539062\n",
      "did one more step, loss reduced by 0.013517379760742188\n",
      "did one more step, loss reduced by 0.013490676879882812\n",
      "did one more step, loss reduced by 0.014463424682617188\n",
      "did one more step, loss reduced by 0.013561248779296875\n",
      "did one more step, loss reduced by 0.0135498046875\n",
      "did one more step, loss reduced by 0.009021759033203125\n",
      "did one more step, loss reduced by 0.009014129638671875\n",
      "did one more step, loss reduced by 0.009002685546875\n",
      "did one more step, loss reduced by 0.009002685546875\n",
      "did one more step, loss reduced by 0.008991241455078125\n",
      "did one more step, loss reduced by 0.008983612060546875\n",
      "did one more step, loss reduced by 0.00897216796875\n",
      "did one more step, loss reduced by 0.00897216796875\n",
      "did one more step, loss reduced by 0.009893417358398438\n",
      "did one more step, loss reduced by 0.009023666381835938\n",
      "did one more step, loss reduced by 0.009012222290039062\n",
      "did one more step, loss reduced by 0.009008407592773438\n",
      "did one more step, loss reduced by 0.008996963500976562\n",
      "did one more step, loss reduced by 0.008993148803710938\n",
      "did one more step, loss reduced by 0.008981704711914062\n",
      "did one more step, loss reduced by 0.008977890014648438\n",
      "did one more step, loss reduced by 0.008966445922851562\n",
      "did one more step, loss reduced by 0.008962631225585938\n",
      "did one more step, loss reduced by 0.008951187133789062\n",
      "did one more step, loss reduced by 0.008947372436523438\n",
      "did one more step, loss reduced by 0.008935928344726562\n",
      "did one more step, loss reduced by 0.009891510009765625\n",
      "did one more step, loss reduced by 0.0089874267578125\n",
      "did one more step, loss reduced by 0.0089874267578125\n",
      "did one more step, loss reduced by 0.008974075317382812\n",
      "did one more step, loss reduced by 0.008970260620117188\n",
      "did one more step, loss reduced by 0.0089569091796875\n",
      "did one more step, loss reduced by 0.0089569091796875\n",
      "did one more step, loss reduced by 0.008943557739257812\n",
      "did one more step, loss reduced by 0.008939743041992188\n",
      "did one more step, loss reduced by 0.0089263916015625\n",
      "did one more step, loss reduced by 0.0089263916015625\n",
      "did one more step, loss reduced by 0.008913040161132812\n",
      "did one more step, loss reduced by 0.009820938110351562\n",
      "did one more step, loss reduced by 0.008966445922851562\n",
      "did one more step, loss reduced by 0.008962631225585938\n",
      "did one more step, loss reduced by 0.008951187133789062\n",
      "did one more step, loss reduced by 0.008947372436523438\n",
      "did one more step, loss reduced by 0.008935928344726562\n",
      "did one more step, loss reduced by 0.008932113647460938\n",
      "did one more step, loss reduced by 0.008920669555664062\n",
      "did one more step, loss reduced by 0.008916854858398438\n",
      "did one more step, loss reduced by 0.008905410766601562\n",
      "did one more step, loss reduced by 0.008901596069335938\n",
      "did one more step, loss reduced by 0.008890151977539062\n",
      "did one more step, loss reduced by 0.008886337280273438\n",
      "did one more step, loss reduced by 0.009809494018554688\n",
      "did one more step, loss reduced by 0.008941650390625\n",
      "did one more step, loss reduced by 0.008928298950195312\n",
      "did one more step, loss reduced by 0.008924484252929688\n",
      "did one more step, loss reduced by 0.0089111328125\n",
      "did one more step, loss reduced by 0.0089111328125\n",
      "did one more step, loss reduced by 0.008897781372070312\n",
      "did one more step, loss reduced by 0.008893966674804688\n",
      "did one more step, loss reduced by 0.008880615234375\n",
      "did one more step, loss reduced by 0.008880615234375\n",
      "did one more step, loss reduced by 0.008867263793945312\n",
      "did one more step, loss reduced by 0.008863449096679688\n",
      "did one more step, loss reduced by 0.00885009765625\n",
      "did one more step, loss reduced by 0.009807586669921875\n",
      "did one more step, loss reduced by 0.008905410766601562\n",
      "did one more step, loss reduced by 0.008901596069335938\n",
      "did one more step, loss reduced by 0.008890151977539062\n",
      "did one more step, loss reduced by 0.008886337280273438\n",
      "did one more step, loss reduced by 0.008874893188476562\n",
      "did one more step, loss reduced by 0.008871078491210938\n",
      "did one more step, loss reduced by 0.008859634399414062\n",
      "did one more step, loss reduced by 0.008855819702148438\n",
      "did one more step, loss reduced by 0.008844375610351562\n",
      "did one more step, loss reduced by 0.008840560913085938\n",
      "did one more step, loss reduced by 0.008829116821289062\n",
      "did one more step, loss reduced by 0.009737014770507812\n",
      "did one more step, loss reduced by 0.008884429931640625\n",
      "did one more step, loss reduced by 0.008876800537109375\n",
      "did one more step, loss reduced by 0.0088653564453125\n",
      "did one more step, loss reduced by 0.0088653564453125\n",
      "did one more step, loss reduced by 0.008853912353515625\n",
      "did one more step, loss reduced by 0.008846282958984375\n",
      "did one more step, loss reduced by 0.0088348388671875\n",
      "did one more step, loss reduced by 0.0088348388671875\n",
      "did one more step, loss reduced by 0.008823394775390625\n",
      "did one more step, loss reduced by 0.008815765380859375\n",
      "did one more step, loss reduced by 0.0088043212890625\n",
      "did one more step, loss reduced by 0.0088043212890625\n",
      "did one more step, loss reduced by 0.00972747802734375\n",
      "did one more step, loss reduced by 0.008855819702148438\n",
      "did one more step, loss reduced by 0.008844375610351562\n",
      "did one more step, loss reduced by 0.008840560913085938\n",
      "did one more step, loss reduced by 0.008829116821289062\n",
      "did one more step, loss reduced by 0.008825302124023438\n",
      "did one more step, loss reduced by 0.008813858032226562\n",
      "did one more step, loss reduced by 0.008810043334960938\n",
      "did one more step, loss reduced by 0.008798599243164062\n",
      "did one more step, loss reduced by 0.008794784545898438\n",
      "did one more step, loss reduced by 0.008783340454101562\n",
      "did one more step, loss reduced by 0.008779525756835938\n",
      "did one more step, loss reduced by 0.008768081665039062\n",
      "did one more step, loss reduced by 0.009721755981445312\n",
      "did one more step, loss reduced by 0.008819580078125\n",
      "did one more step, loss reduced by 0.008819580078125\n",
      "did one more step, loss reduced by 0.008808135986328125\n",
      "did one more step, loss reduced by 0.008800506591796875\n",
      "did one more step, loss reduced by 0.0087890625\n",
      "did one more step, loss reduced by 0.0087890625\n",
      "did one more step, loss reduced by 0.008777618408203125\n",
      "did one more step, loss reduced by 0.008769989013671875\n",
      "did one more step, loss reduced by 0.008758544921875\n",
      "did one more step, loss reduced by 0.008758544921875\n",
      "did one more step, loss reduced by 0.008747100830078125\n",
      "did one more step, loss reduced by 0.009653091430664062\n",
      "did one more step, loss reduced by 0.00879669189453125\n",
      "did one more step, loss reduced by 0.00879669189453125\n",
      "did one more step, loss reduced by 0.00878143310546875\n",
      "did one more step, loss reduced by 0.00878143310546875\n",
      "did one more step, loss reduced by 0.00876617431640625\n",
      "did one more step, loss reduced by 0.00876617431640625\n",
      "did one more step, loss reduced by 0.00875091552734375\n",
      "did one more step, loss reduced by 0.00875091552734375\n",
      "did one more step, loss reduced by 0.00873565673828125\n",
      "did one more step, loss reduced by 0.00873565673828125\n",
      "did one more step, loss reduced by 0.00872039794921875\n",
      "did one more step, loss reduced by 0.00872039794921875\n",
      "did one more step, loss reduced by 0.009639739990234375\n",
      "did one more step, loss reduced by 0.0087738037109375\n",
      "did one more step, loss reduced by 0.008762359619140625\n",
      "did one more step, loss reduced by 0.008754730224609375\n",
      "did one more step, loss reduced by 0.0087432861328125\n",
      "did one more step, loss reduced by 0.0087432861328125\n",
      "did one more step, loss reduced by 0.008731842041015625\n",
      "did one more step, loss reduced by 0.008724212646484375\n",
      "did one more step, loss reduced by 0.0087127685546875\n",
      "did one more step, loss reduced by 0.0087127685546875\n",
      "did one more step, loss reduced by 0.008701324462890625\n",
      "did one more step, loss reduced by 0.008693695068359375\n",
      "did one more step, loss reduced by 0.0086822509765625\n",
      "did one more step, loss reduced by 0.009639739990234375\n",
      "did one more step, loss reduced by 0.008737564086914062\n",
      "did one more step, loss reduced by 0.008733749389648438\n",
      "did one more step, loss reduced by 0.008722305297851562\n",
      "did one more step, loss reduced by 0.008718490600585938\n",
      "did one more step, loss reduced by 0.008707046508789062\n",
      "did one more step, loss reduced by 0.008703231811523438\n",
      "did one more step, loss reduced by 0.008691787719726562\n",
      "did one more step, loss reduced by 0.008687973022460938\n",
      "did one more step, loss reduced by 0.008676528930664062\n",
      "did one more step, loss reduced by 0.008672714233398438\n",
      "did one more step, loss reduced by 0.008661270141601562\n",
      "did one more step, loss reduced by 0.009571075439453125\n",
      "did one more step, loss reduced by 0.008714675903320312\n",
      "did one more step, loss reduced by 0.008710861206054688\n",
      "did one more step, loss reduced by 0.008697509765625\n",
      "did one more step, loss reduced by 0.008697509765625\n",
      "did one more step, loss reduced by 0.008684158325195312\n",
      "did one more step, loss reduced by 0.008680343627929688\n",
      "did one more step, loss reduced by 0.0086669921875\n",
      "did one more step, loss reduced by 0.0086669921875\n",
      "did one more step, loss reduced by 0.008653640747070312\n",
      "did one more step, loss reduced by 0.008649826049804688\n",
      "did one more step, loss reduced by 0.008636474609375\n",
      "did one more step, loss reduced by 0.008636474609375\n",
      "did one more step, loss reduced by 0.009557723999023438\n",
      "did one more step, loss reduced by 0.008687973022460938\n",
      "did one more step, loss reduced by 0.008676528930664062\n",
      "did one more step, loss reduced by 0.008672714233398438\n",
      "did one more step, loss reduced by 0.008661270141601562\n",
      "did one more step, loss reduced by 0.008657455444335938\n",
      "did one more step, loss reduced by 0.008646011352539062\n",
      "did one more step, loss reduced by 0.008642196655273438\n",
      "did one more step, loss reduced by 0.008630752563476562\n",
      "did one more step, loss reduced by 0.008626937866210938\n",
      "did one more step, loss reduced by 0.008615493774414062\n",
      "did one more step, loss reduced by 0.008611679077148438\n",
      "did one more step, loss reduced by 0.008600234985351562\n",
      "did one more step, loss reduced by 0.009555816650390625\n",
      "did one more step, loss reduced by 0.0086517333984375\n",
      "did one more step, loss reduced by 0.0086517333984375\n",
      "did one more step, loss reduced by 0.008638381958007812\n",
      "did one more step, loss reduced by 0.008634567260742188\n",
      "did one more step, loss reduced by 0.0086212158203125\n",
      "did one more step, loss reduced by 0.0086212158203125\n",
      "did one more step, loss reduced by 0.008607864379882812\n",
      "did one more step, loss reduced by 0.008604049682617188\n",
      "did one more step, loss reduced by 0.0085906982421875\n",
      "did one more step, loss reduced by 0.0085906982421875\n",
      "did one more step, loss reduced by 0.008577346801757812\n",
      "did one more step, loss reduced by 0.009485244750976562\n",
      "did one more step, loss reduced by 0.008630752563476562\n",
      "did one more step, loss reduced by 0.008626937866210938\n",
      "did one more step, loss reduced by 0.008615493774414062\n",
      "did one more step, loss reduced by 0.008611679077148438\n",
      "did one more step, loss reduced by 0.008600234985351562\n",
      "did one more step, loss reduced by 0.008596420288085938\n",
      "did one more step, loss reduced by 0.008584976196289062\n",
      "did one more step, loss reduced by 0.008581161499023438\n",
      "did one more step, loss reduced by 0.008569717407226562\n",
      "did one more step, loss reduced by 0.008565902709960938\n",
      "did one more step, loss reduced by 0.008554458618164062\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.009471893310546875\n",
      "did one more step, loss reduced by 0.00860595703125\n",
      "did one more step, loss reduced by 0.008594512939453125\n",
      "did one more step, loss reduced by 0.008586883544921875\n",
      "did one more step, loss reduced by 0.008575439453125\n",
      "did one more step, loss reduced by 0.008575439453125\n",
      "did one more step, loss reduced by 0.008563995361328125\n",
      "did one more step, loss reduced by 0.008556365966796875\n",
      "did one more step, loss reduced by 0.008544921875\n",
      "did one more step, loss reduced by 0.008544921875\n",
      "did one more step, loss reduced by 0.008533477783203125\n",
      "did one more step, loss reduced by 0.008525848388671875\n",
      "did one more step, loss reduced by 0.008514404296875\n",
      "did one more step, loss reduced by 0.009473800659179688\n",
      "did one more step, loss reduced by 0.008569717407226562\n",
      "did one more step, loss reduced by 0.008565902709960938\n",
      "did one more step, loss reduced by 0.008554458618164062\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.008539199829101562\n",
      "did one more step, loss reduced by 0.008535385131835938\n",
      "did one more step, loss reduced by 0.008523941040039062\n",
      "did one more step, loss reduced by 0.008520126342773438\n",
      "did one more step, loss reduced by 0.008508682250976562\n",
      "did one more step, loss reduced by 0.008504867553710938\n",
      "did one more step, loss reduced by 0.008493423461914062\n",
      "did one more step, loss reduced by 0.009401321411132812\n",
      "did one more step, loss reduced by 0.008548736572265625\n",
      "did one more step, loss reduced by 0.008541107177734375\n",
      "did one more step, loss reduced by 0.0085296630859375\n",
      "did one more step, loss reduced by 0.0085296630859375\n",
      "did one more step, loss reduced by 0.008518218994140625\n",
      "did one more step, loss reduced by 0.008510589599609375\n",
      "did one more step, loss reduced by 0.0084991455078125\n",
      "did one more step, loss reduced by 0.0084991455078125\n",
      "did one more step, loss reduced by 0.008487701416015625\n",
      "did one more step, loss reduced by 0.008480072021484375\n",
      "did one more step, loss reduced by 0.0084686279296875\n",
      "did one more step, loss reduced by 0.0084686279296875\n",
      "did one more step, loss reduced by 0.009389877319335938\n",
      "did one more step, loss reduced by 0.00852203369140625\n",
      "did one more step, loss reduced by 0.00850677490234375\n",
      "did one more step, loss reduced by 0.00850677490234375\n",
      "did one more step, loss reduced by 0.00849151611328125\n",
      "did one more step, loss reduced by 0.00849151611328125\n",
      "did one more step, loss reduced by 0.00847625732421875\n",
      "did one more step, loss reduced by 0.00847625732421875\n",
      "did one more step, loss reduced by 0.00846099853515625\n",
      "did one more step, loss reduced by 0.00846099853515625\n",
      "did one more step, loss reduced by 0.00844573974609375\n",
      "did one more step, loss reduced by 0.00844573974609375\n",
      "did one more step, loss reduced by 0.00843048095703125\n",
      "did one more step, loss reduced by 0.009387969970703125\n",
      "did one more step, loss reduced by 0.00848388671875\n",
      "did one more step, loss reduced by 0.00848388671875\n",
      "did one more step, loss reduced by 0.008472442626953125\n",
      "did one more step, loss reduced by 0.008464813232421875\n",
      "did one more step, loss reduced by 0.008453369140625\n",
      "did one more step, loss reduced by 0.008453369140625\n",
      "did one more step, loss reduced by 0.008441925048828125\n",
      "did one more step, loss reduced by 0.008434295654296875\n",
      "did one more step, loss reduced by 0.0084228515625\n",
      "did one more step, loss reduced by 0.0084228515625\n",
      "did one more step, loss reduced by 0.008411407470703125\n",
      "did one more step, loss reduced by 0.00931549072265625\n",
      "did one more step, loss reduced by 0.008462905883789062\n",
      "did one more step, loss reduced by 0.008459091186523438\n",
      "did one more step, loss reduced by 0.008447647094726562\n",
      "did one more step, loss reduced by 0.008443832397460938\n",
      "did one more step, loss reduced by 0.008432388305664062\n",
      "did one more step, loss reduced by 0.008428573608398438\n",
      "did one more step, loss reduced by 0.008417129516601562\n",
      "did one more step, loss reduced by 0.008413314819335938\n",
      "did one more step, loss reduced by 0.008401870727539062\n",
      "did one more step, loss reduced by 0.008398056030273438\n",
      "did one more step, loss reduced by 0.008386611938476562\n",
      "did one more step, loss reduced by 0.008382797241210938\n",
      "did one more step, loss reduced by 0.009305953979492188\n",
      "did one more step, loss reduced by 0.0084381103515625\n",
      "did one more step, loss reduced by 0.008424758911132812\n",
      "did one more step, loss reduced by 0.008420944213867188\n",
      "did one more step, loss reduced by 0.0084075927734375\n",
      "did one more step, loss reduced by 0.0084075927734375\n",
      "did one more step, loss reduced by 0.008394241333007812\n",
      "did one more step, loss reduced by 0.008390426635742188\n",
      "did one more step, loss reduced by 0.0083770751953125\n",
      "did one more step, loss reduced by 0.0083770751953125\n",
      "did one more step, loss reduced by 0.008363723754882812\n",
      "did one more step, loss reduced by 0.008359909057617188\n",
      "did one more step, loss reduced by 0.0083465576171875\n",
      "did one more step, loss reduced by 0.009304046630859375\n",
      "did one more step, loss reduced by 0.008401870727539062\n",
      "did one more step, loss reduced by 0.008398056030273438\n",
      "did one more step, loss reduced by 0.008386611938476562\n",
      "did one more step, loss reduced by 0.008382797241210938\n",
      "did one more step, loss reduced by 0.008371353149414062\n",
      "did one more step, loss reduced by 0.008367538452148438\n",
      "did one more step, loss reduced by 0.008356094360351562\n",
      "did one more step, loss reduced by 0.008352279663085938\n",
      "did one more step, loss reduced by 0.008340835571289062\n",
      "did one more step, loss reduced by 0.008337020874023438\n",
      "did one more step, loss reduced by 0.008325576782226562\n",
      "did one more step, loss reduced by 0.009235382080078125\n",
      "did one more step, loss reduced by 0.008378982543945312\n",
      "did one more step, loss reduced by 0.008375167846679688\n",
      "did one more step, loss reduced by 0.00836181640625\n",
      "did one more step, loss reduced by 0.00836181640625\n",
      "did one more step, loss reduced by 0.008348464965820312\n",
      "did one more step, loss reduced by 0.008344650268554688\n",
      "did one more step, loss reduced by 0.008331298828125\n",
      "did one more step, loss reduced by 0.008331298828125\n",
      "did one more step, loss reduced by 0.008317947387695312\n",
      "did one more step, loss reduced by 0.008314132690429688\n",
      "did one more step, loss reduced by 0.00830078125\n",
      "did one more step, loss reduced by 0.00830078125\n",
      "did one more step, loss reduced by 0.009222030639648438\n",
      "did one more step, loss reduced by 0.008352279663085938\n",
      "did one more step, loss reduced by 0.008340835571289062\n",
      "did one more step, loss reduced by 0.008337020874023438\n",
      "did one more step, loss reduced by 0.008325576782226562\n",
      "did one more step, loss reduced by 0.008321762084960938\n",
      "did one more step, loss reduced by 0.008310317993164062\n",
      "did one more step, loss reduced by 0.008306503295898438\n",
      "did one more step, loss reduced by 0.008295059204101562\n",
      "did one more step, loss reduced by 0.008291244506835938\n",
      "did one more step, loss reduced by 0.008279800415039062\n",
      "did one more step, loss reduced by 0.008275985717773438\n",
      "did one more step, loss reduced by 0.008264541625976562\n",
      "did one more step, loss reduced by 0.009218215942382812\n",
      "did one more step, loss reduced by 0.0083160400390625\n",
      "did one more step, loss reduced by 0.0083160400390625\n",
      "did one more step, loss reduced by 0.008304595947265625\n",
      "did one more step, loss reduced by 0.008296966552734375\n",
      "did one more step, loss reduced by 0.0082855224609375\n",
      "did one more step, loss reduced by 0.0082855224609375\n",
      "did one more step, loss reduced by 0.008274078369140625\n",
      "did one more step, loss reduced by 0.008266448974609375\n",
      "did one more step, loss reduced by 0.0082550048828125\n",
      "did one more step, loss reduced by 0.0082550048828125\n",
      "did one more step, loss reduced by 0.008243560791015625\n",
      "did one more step, loss reduced by 0.009149551391601562\n",
      "did one more step, loss reduced by 0.008295059204101562\n",
      "did one more step, loss reduced by 0.008291244506835938\n",
      "did one more step, loss reduced by 0.008279800415039062\n",
      "did one more step, loss reduced by 0.008275985717773438\n",
      "did one more step, loss reduced by 0.008264541625976562\n",
      "did one more step, loss reduced by 0.008260726928710938\n",
      "did one more step, loss reduced by 0.008249282836914062\n",
      "did one more step, loss reduced by 0.008245468139648438\n",
      "did one more step, loss reduced by 0.008234024047851562\n",
      "did one more step, loss reduced by 0.008230209350585938\n",
      "did one more step, loss reduced by 0.008218765258789062\n",
      "did one more step, loss reduced by 0.008214950561523438\n",
      "did one more step, loss reduced by 0.009136199951171875\n",
      "did one more step, loss reduced by 0.008270263671875\n",
      "did one more step, loss reduced by 0.008258819580078125\n",
      "did one more step, loss reduced by 0.008251190185546875\n",
      "did one more step, loss reduced by 0.00823974609375\n",
      "did one more step, loss reduced by 0.00823974609375\n",
      "did one more step, loss reduced by 0.008228302001953125\n",
      "did one more step, loss reduced by 0.008220672607421875\n",
      "did one more step, loss reduced by 0.008209228515625\n",
      "did one more step, loss reduced by 0.008209228515625\n",
      "did one more step, loss reduced by 0.008197784423828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.008190155029296875\n",
      "did one more step, loss reduced by 0.0081787109375\n",
      "did one more step, loss reduced by 0.009138107299804688\n",
      "did one more step, loss reduced by 0.00823211669921875\n",
      "did one more step, loss reduced by 0.00823211669921875\n",
      "did one more step, loss reduced by 0.00821685791015625\n",
      "did one more step, loss reduced by 0.00821685791015625\n",
      "did one more step, loss reduced by 0.00820159912109375\n",
      "did one more step, loss reduced by 0.00820159912109375\n",
      "did one more step, loss reduced by 0.00818634033203125\n",
      "did one more step, loss reduced by 0.00818634033203125\n",
      "did one more step, loss reduced by 0.00817108154296875\n",
      "did one more step, loss reduced by 0.00817108154296875\n",
      "did one more step, loss reduced by 0.00815582275390625\n",
      "did one more step, loss reduced by 0.009067535400390625\n",
      "did one more step, loss reduced by 0.008213043212890625\n",
      "did one more step, loss reduced by 0.008205413818359375\n",
      "did one more step, loss reduced by 0.0081939697265625\n",
      "did one more step, loss reduced by 0.0081939697265625\n",
      "did one more step, loss reduced by 0.008182525634765625\n",
      "did one more step, loss reduced by 0.008174896240234375\n",
      "did one more step, loss reduced by 0.0081634521484375\n",
      "did one more step, loss reduced by 0.0081634521484375\n",
      "did one more step, loss reduced by 0.008152008056640625\n",
      "did one more step, loss reduced by 0.008144378662109375\n",
      "did one more step, loss reduced by 0.0081329345703125\n",
      "did one more step, loss reduced by 0.0081329345703125\n",
      "did one more step, loss reduced by 0.009054183959960938\n",
      "did one more step, loss reduced by 0.008184432983398438\n",
      "did one more step, loss reduced by 0.008172988891601562\n",
      "did one more step, loss reduced by 0.008169174194335938\n",
      "did one more step, loss reduced by 0.008157730102539062\n",
      "did one more step, loss reduced by 0.008153915405273438\n",
      "did one more step, loss reduced by 0.008142471313476562\n",
      "did one more step, loss reduced by 0.008138656616210938\n",
      "did one more step, loss reduced by 0.008127212524414062\n",
      "did one more step, loss reduced by 0.008123397827148438\n",
      "did one more step, loss reduced by 0.008111953735351562\n",
      "did one more step, loss reduced by 0.008108139038085938\n",
      "did one more step, loss reduced by 0.008096694946289062\n",
      "did one more step, loss reduced by 0.009052276611328125\n",
      "did one more step, loss reduced by 0.008148193359375\n",
      "did one more step, loss reduced by 0.008148193359375\n",
      "did one more step, loss reduced by 0.008134841918945312\n",
      "did one more step, loss reduced by 0.008131027221679688\n",
      "did one more step, loss reduced by 0.00811767578125\n",
      "did one more step, loss reduced by 0.00811767578125\n",
      "did one more step, loss reduced by 0.008104324340820312\n",
      "did one more step, loss reduced by 0.008100509643554688\n",
      "did one more step, loss reduced by 0.008087158203125\n",
      "did one more step, loss reduced by 0.008087158203125\n",
      "did one more step, loss reduced by 0.008073806762695312\n",
      "did one more step, loss reduced by 0.008981704711914062\n",
      "did one more step, loss reduced by 0.008127212524414062\n",
      "did one more step, loss reduced by 0.008123397827148438\n",
      "did one more step, loss reduced by 0.008111953735351562\n",
      "did one more step, loss reduced by 0.008108139038085938\n",
      "did one more step, loss reduced by 0.008096694946289062\n",
      "did one more step, loss reduced by 0.008092880249023438\n",
      "did one more step, loss reduced by 0.008081436157226562\n",
      "did one more step, loss reduced by 0.008077621459960938\n",
      "did one more step, loss reduced by 0.008066177368164062\n",
      "did one more step, loss reduced by 0.008062362670898438\n",
      "did one more step, loss reduced by 0.008050918579101562\n",
      "did one more step, loss reduced by 0.008047103881835938\n",
      "did one more step, loss reduced by 0.008970260620117188\n",
      "did one more step, loss reduced by 0.0081024169921875\n",
      "did one more step, loss reduced by 0.008089065551757812\n",
      "did one more step, loss reduced by 0.008085250854492188\n",
      "did one more step, loss reduced by 0.0080718994140625\n",
      "did one more step, loss reduced by 0.0080718994140625\n",
      "did one more step, loss reduced by 0.008058547973632812\n",
      "did one more step, loss reduced by 0.008054733276367188\n",
      "did one more step, loss reduced by 0.0080413818359375\n",
      "did one more step, loss reduced by 0.0080413818359375\n",
      "did one more step, loss reduced by 0.008028030395507812\n",
      "did one more step, loss reduced by 0.008024215698242188\n",
      "did one more step, loss reduced by 0.0080108642578125\n",
      "did one more step, loss reduced by 0.008968353271484375\n",
      "did one more step, loss reduced by 0.008066177368164062\n",
      "did one more step, loss reduced by 0.008062362670898438\n",
      "did one more step, loss reduced by 0.008050918579101562\n",
      "did one more step, loss reduced by 0.008047103881835938\n",
      "did one more step, loss reduced by 0.008035659790039062\n",
      "did one more step, loss reduced by 0.008031845092773438\n",
      "did one more step, loss reduced by 0.008020401000976562\n",
      "did one more step, loss reduced by 0.008016586303710938\n",
      "did one more step, loss reduced by 0.008005142211914062\n",
      "did one more step, loss reduced by 0.008001327514648438\n",
      "did one more step, loss reduced by 0.007989883422851562\n",
      "did one more step, loss reduced by 0.008897781372070312\n",
      "did one more step, loss reduced by 0.008045196533203125\n",
      "did one more step, loss reduced by 0.008037567138671875\n",
      "did one more step, loss reduced by 0.008026123046875\n",
      "did one more step, loss reduced by 0.008026123046875\n",
      "did one more step, loss reduced by 0.008014678955078125\n",
      "did one more step, loss reduced by 0.008007049560546875\n",
      "did one more step, loss reduced by 0.00799560546875\n",
      "did one more step, loss reduced by 0.00799560546875\n",
      "did one more step, loss reduced by 0.007984161376953125\n",
      "did one more step, loss reduced by 0.007976531982421875\n",
      "did one more step, loss reduced by 0.007965087890625\n",
      "did one more step, loss reduced by 0.007965087890625\n",
      "did one more step, loss reduced by 0.00888824462890625\n",
      "did one more step, loss reduced by 0.008016586303710938\n",
      "did one more step, loss reduced by 0.008005142211914062\n",
      "did one more step, loss reduced by 0.008001327514648438\n",
      "did one more step, loss reduced by 0.007989883422851562\n",
      "did one more step, loss reduced by 0.007986068725585938\n",
      "did one more step, loss reduced by 0.007974624633789062\n",
      "did one more step, loss reduced by 0.007970809936523438\n",
      "did one more step, loss reduced by 0.007959365844726562\n",
      "did one more step, loss reduced by 0.007955551147460938\n",
      "did one more step, loss reduced by 0.007944107055664062\n",
      "did one more step, loss reduced by 0.007940292358398438\n",
      "did one more step, loss reduced by 0.007928848266601562\n",
      "did one more step, loss reduced by 0.008882522583007812\n",
      "did one more step, loss reduced by 0.0079803466796875\n",
      "did one more step, loss reduced by 0.0079803466796875\n",
      "did one more step, loss reduced by 0.007968902587890625\n",
      "did one more step, loss reduced by 0.007961273193359375\n",
      "did one more step, loss reduced by 0.0079498291015625\n",
      "did one more step, loss reduced by 0.0079498291015625\n",
      "did one more step, loss reduced by 0.007938385009765625\n",
      "did one more step, loss reduced by 0.007930755615234375\n",
      "did one more step, loss reduced by 0.0079193115234375\n",
      "did one more step, loss reduced by 0.0079193115234375\n",
      "did one more step, loss reduced by 0.007907867431640625\n",
      "did one more step, loss reduced by 0.008813858032226562\n",
      "did one more step, loss reduced by 0.00795745849609375\n",
      "did one more step, loss reduced by 0.00795745849609375\n",
      "did one more step, loss reduced by 0.00794219970703125\n",
      "did one more step, loss reduced by 0.00794219970703125\n",
      "did one more step, loss reduced by 0.00792694091796875\n",
      "did one more step, loss reduced by 0.00792694091796875\n",
      "did one more step, loss reduced by 0.00791168212890625\n",
      "did one more step, loss reduced by 0.00791168212890625\n",
      "did one more step, loss reduced by 0.00789642333984375\n",
      "did one more step, loss reduced by 0.00789642333984375\n",
      "did one more step, loss reduced by 0.00788116455078125\n",
      "did one more step, loss reduced by 0.00788116455078125\n",
      "did one more step, loss reduced by 0.008800506591796875\n",
      "did one more step, loss reduced by 0.0079345703125\n",
      "did one more step, loss reduced by 0.007923126220703125\n",
      "did one more step, loss reduced by 0.007915496826171875\n",
      "did one more step, loss reduced by 0.007904052734375\n",
      "did one more step, loss reduced by 0.007904052734375\n",
      "did one more step, loss reduced by 0.007892608642578125\n",
      "did one more step, loss reduced by 0.007884979248046875\n",
      "did one more step, loss reduced by 0.00787353515625\n",
      "did one more step, loss reduced by 0.00787353515625\n",
      "did one more step, loss reduced by 0.007862091064453125\n",
      "did one more step, loss reduced by 0.007854461669921875\n",
      "did one more step, loss reduced by 0.007843017578125\n",
      "did one more step, loss reduced by 0.008800506591796875\n",
      "did one more step, loss reduced by 0.007898330688476562\n",
      "did one more step, loss reduced by 0.007894515991210938\n",
      "did one more step, loss reduced by 0.007883071899414062\n",
      "did one more step, loss reduced by 0.007879257202148438\n",
      "did one more step, loss reduced by 0.007867813110351562\n",
      "did one more step, loss reduced by 0.007863998413085938\n",
      "did one more step, loss reduced by 0.007852554321289062\n",
      "did one more step, loss reduced by 0.007848739624023438\n",
      "did one more step, loss reduced by 0.007837295532226562\n",
      "did one more step, loss reduced by 0.007833480834960938\n",
      "did one more step, loss reduced by 0.007822036743164062\n",
      "did one more step, loss reduced by 0.008731842041015625\n",
      "did one more step, loss reduced by 0.007875442504882812\n",
      "did one more step, loss reduced by 0.007871627807617188\n",
      "did one more step, loss reduced by 0.0078582763671875\n",
      "did one more step, loss reduced by 0.0078582763671875\n",
      "did one more step, loss reduced by 0.007844924926757812\n",
      "did one more step, loss reduced by 0.007841110229492188\n",
      "did one more step, loss reduced by 0.0078277587890625\n",
      "did one more step, loss reduced by 0.0078277587890625\n",
      "did one more step, loss reduced by 0.007814407348632812\n",
      "did one more step, loss reduced by 0.0078105926513671875\n",
      "did one more step, loss reduced by 0.0077972412109375\n",
      "did one more step, loss reduced by 0.0077972412109375\n",
      "did one more step, loss reduced by 0.008718490600585938\n",
      "did one more step, loss reduced by 0.007848739624023438\n",
      "did one more step, loss reduced by 0.007837295532226562\n",
      "did one more step, loss reduced by 0.007833480834960938\n",
      "did one more step, loss reduced by 0.007822036743164062\n",
      "did one more step, loss reduced by 0.007818222045898438\n",
      "did one more step, loss reduced by 0.0078067779541015625\n",
      "did one more step, loss reduced by 0.0078029632568359375\n",
      "did one more step, loss reduced by 0.0077915191650390625\n",
      "did one more step, loss reduced by 0.0077877044677734375\n",
      "did one more step, loss reduced by 0.0077762603759765625\n",
      "did one more step, loss reduced by 0.0077724456787109375\n",
      "did one more step, loss reduced by 0.0077610015869140625\n",
      "did one more step, loss reduced by 0.008716583251953125\n",
      "did one more step, loss reduced by 0.0078125\n",
      "did one more step, loss reduced by 0.0078125\n",
      "did one more step, loss reduced by 0.0077991485595703125\n",
      "did one more step, loss reduced by 0.0077953338623046875\n",
      "did one more step, loss reduced by 0.007781982421875\n",
      "did one more step, loss reduced by 0.007781982421875\n",
      "did one more step, loss reduced by 0.0077686309814453125\n",
      "did one more step, loss reduced by 0.0077648162841796875\n",
      "did one more step, loss reduced by 0.00775146484375\n",
      "did one more step, loss reduced by 0.00775146484375\n",
      "did one more step, loss reduced by 0.0077381134033203125\n",
      "did one more step, loss reduced by 0.008646011352539062\n",
      "did one more step, loss reduced by 0.0077915191650390625\n",
      "did one more step, loss reduced by 0.0077877044677734375\n",
      "did one more step, loss reduced by 0.0077762603759765625\n",
      "did one more step, loss reduced by 0.0077724456787109375\n",
      "did one more step, loss reduced by 0.0077610015869140625\n",
      "did one more step, loss reduced by 0.0077571868896484375\n",
      "did one more step, loss reduced by 0.0077457427978515625\n",
      "did one more step, loss reduced by 0.0077419281005859375\n",
      "did one more step, loss reduced by 0.0077304840087890625\n",
      "did one more step, loss reduced by 0.0077266693115234375\n",
      "did one more step, loss reduced by 0.0077152252197265625\n",
      "did one more step, loss reduced by 0.0077114105224609375\n",
      "did one more step, loss reduced by 0.008632659912109375\n",
      "did one more step, loss reduced by 0.0077667236328125\n",
      "did one more step, loss reduced by 0.007755279541015625\n",
      "did one more step, loss reduced by 0.007747650146484375\n",
      "did one more step, loss reduced by 0.0077362060546875\n",
      "did one more step, loss reduced by 0.0077362060546875\n",
      "did one more step, loss reduced by 0.007724761962890625\n",
      "did one more step, loss reduced by 0.007717132568359375\n",
      "did one more step, loss reduced by 0.0077056884765625\n",
      "did one more step, loss reduced by 0.0077056884765625\n",
      "did one more step, loss reduced by 0.007694244384765625\n",
      "did one more step, loss reduced by 0.007686614990234375\n",
      "did one more step, loss reduced by 0.0076751708984375\n",
      "did one more step, loss reduced by 0.008634567260742188\n",
      "did one more step, loss reduced by 0.0077304840087890625\n",
      "did one more step, loss reduced by 0.0077266693115234375\n",
      "did one more step, loss reduced by 0.0077152252197265625\n",
      "did one more step, loss reduced by 0.0077114105224609375\n",
      "did one more step, loss reduced by 0.0076999664306640625\n",
      "did one more step, loss reduced by 0.0076961517333984375\n",
      "did one more step, loss reduced by 0.0076847076416015625\n",
      "did one more step, loss reduced by 0.0076808929443359375\n",
      "did one more step, loss reduced by 0.0076694488525390625\n",
      "did one more step, loss reduced by 0.0076656341552734375\n",
      "did one more step, loss reduced by 0.0076541900634765625\n",
      "did one more step, loss reduced by 0.008562088012695312\n",
      "did one more step, loss reduced by 0.0077056884765625\n",
      "did one more step, loss reduced by 0.0077056884765625\n",
      "did one more step, loss reduced by 0.0076904296875\n",
      "did one more step, loss reduced by 0.0076904296875\n",
      "did one more step, loss reduced by 0.0076751708984375\n",
      "did one more step, loss reduced by 0.0076751708984375\n",
      "did one more step, loss reduced by 0.007659912109375\n",
      "did one more step, loss reduced by 0.007659912109375\n",
      "did one more step, loss reduced by 0.0076446533203125\n",
      "did one more step, loss reduced by 0.0076446533203125\n",
      "did one more step, loss reduced by 0.00762939453125\n",
      "did one more step, loss reduced by 0.00762939453125\n",
      "did one more step, loss reduced by 0.008550643920898438\n",
      "did one more step, loss reduced by 0.00768280029296875\n",
      "did one more step, loss reduced by 0.00766754150390625\n",
      "did one more step, loss reduced by 0.00766754150390625\n",
      "did one more step, loss reduced by 0.00765228271484375\n",
      "did one more step, loss reduced by 0.00765228271484375\n",
      "did one more step, loss reduced by 0.00763702392578125\n",
      "did one more step, loss reduced by 0.00763702392578125\n",
      "did one more step, loss reduced by 0.00762176513671875\n",
      "did one more step, loss reduced by 0.00762176513671875\n",
      "did one more step, loss reduced by 0.00760650634765625\n",
      "did one more step, loss reduced by 0.00760650634765625\n",
      "did one more step, loss reduced by 0.00759124755859375\n",
      "did one more step, loss reduced by 0.008548736572265625\n",
      "did one more step, loss reduced by 0.0076446533203125\n",
      "did one more step, loss reduced by 0.0076446533203125\n",
      "did one more step, loss reduced by 0.00762939453125\n",
      "did one more step, loss reduced by 0.00762939453125\n",
      "did one more step, loss reduced by 0.0076141357421875\n",
      "did one more step, loss reduced by 0.0076141357421875\n",
      "did one more step, loss reduced by 0.007598876953125\n",
      "did one more step, loss reduced by 0.007598876953125\n",
      "did one more step, loss reduced by 0.0075836181640625\n",
      "did one more step, loss reduced by 0.0075836181640625\n",
      "did one more step, loss reduced by 0.007568359375\n",
      "did one more step, loss reduced by 0.008480072021484375\n",
      "did one more step, loss reduced by 0.0076236724853515625\n",
      "did one more step, loss reduced by 0.0076198577880859375\n",
      "did one more step, loss reduced by 0.0076084136962890625\n",
      "did one more step, loss reduced by 0.0076045989990234375\n",
      "did one more step, loss reduced by 0.0075931549072265625\n",
      "did one more step, loss reduced by 0.0075893402099609375\n",
      "did one more step, loss reduced by 0.0075778961181640625\n",
      "did one more step, loss reduced by 0.0075740814208984375\n",
      "did one more step, loss reduced by 0.0075626373291015625\n",
      "did one more step, loss reduced by 0.0075588226318359375\n",
      "did one more step, loss reduced by 0.0075473785400390625\n",
      "did one more step, loss reduced by 0.0075435638427734375\n",
      "did one more step, loss reduced by 0.008466720581054688\n",
      "did one more step, loss reduced by 0.007598876953125\n",
      "did one more step, loss reduced by 0.0075836181640625\n",
      "did one more step, loss reduced by 0.0075836181640625\n",
      "did one more step, loss reduced by 0.007568359375\n",
      "did one more step, loss reduced by 0.007568359375\n",
      "did one more step, loss reduced by 0.0075531005859375\n",
      "did one more step, loss reduced by 0.0075531005859375\n",
      "did one more step, loss reduced by 0.007537841796875\n",
      "did one more step, loss reduced by 0.007537841796875\n",
      "did one more step, loss reduced by 0.0075225830078125\n",
      "did one more step, loss reduced by 0.0075225830078125\n",
      "did one more step, loss reduced by 0.00750732421875\n",
      "did one more step, loss reduced by 0.008464813232421875\n",
      "did one more step, loss reduced by 0.0075626373291015625\n",
      "did one more step, loss reduced by 0.0075588226318359375\n",
      "did one more step, loss reduced by 0.0075473785400390625\n",
      "did one more step, loss reduced by 0.0075435638427734375\n",
      "did one more step, loss reduced by 0.0075321197509765625\n",
      "did one more step, loss reduced by 0.0075283050537109375\n",
      "did one more step, loss reduced by 0.0075168609619140625\n",
      "did one more step, loss reduced by 0.0075130462646484375\n",
      "did one more step, loss reduced by 0.0075016021728515625\n",
      "did one more step, loss reduced by 0.0074977874755859375\n",
      "did one more step, loss reduced by 0.00748443603515625\n",
      "did one more step, loss reduced by 0.008398056030273438\n",
      "did one more step, loss reduced by 0.007537841796875\n",
      "did one more step, loss reduced by 0.007537841796875\n",
      "did one more step, loss reduced by 0.0075225830078125\n",
      "did one more step, loss reduced by 0.0075225830078125\n",
      "did one more step, loss reduced by 0.00750732421875\n",
      "did one more step, loss reduced by 0.00750732421875\n",
      "did one more step, loss reduced by 0.0074920654296875\n",
      "did one more step, loss reduced by 0.0074920654296875\n",
      "did one more step, loss reduced by 0.007476806640625\n",
      "did one more step, loss reduced by 0.007476806640625\n",
      "did one more step, loss reduced by 0.0074615478515625\n",
      "did one more step, loss reduced by 0.0074615478515625\n",
      "did one more step, loss reduced by 0.008380889892578125\n",
      "did one more step, loss reduced by 0.00751495361328125\n",
      "did one more step, loss reduced by 0.00749969482421875\n",
      "did one more step, loss reduced by 0.00749969482421875\n",
      "did one more step, loss reduced by 0.00748443603515625\n",
      "did one more step, loss reduced by 0.00748443603515625\n",
      "did one more step, loss reduced by 0.00746917724609375\n",
      "did one more step, loss reduced by 0.00746917724609375\n",
      "did one more step, loss reduced by 0.00745391845703125\n",
      "did one more step, loss reduced by 0.00745391845703125\n",
      "did one more step, loss reduced by 0.00743865966796875\n",
      "did one more step, loss reduced by 0.00743865966796875\n",
      "did one more step, loss reduced by 0.00742340087890625\n",
      "did one more step, loss reduced by 0.008380889892578125\n",
      "did one more step, loss reduced by 0.007476806640625\n",
      "did one more step, loss reduced by 0.007476806640625\n",
      "did one more step, loss reduced by 0.0074615478515625\n",
      "did one more step, loss reduced by 0.0074615478515625\n",
      "did one more step, loss reduced by 0.0074462890625\n",
      "did one more step, loss reduced by 0.0074462890625\n",
      "did one more step, loss reduced by 0.0074310302734375\n",
      "did one more step, loss reduced by 0.0074310302734375\n",
      "did one more step, loss reduced by 0.007415771484375\n",
      "did one more step, loss reduced by 0.007415771484375\n",
      "did one more step, loss reduced by 0.0074005126953125\n",
      "did one more step, loss reduced by 0.008314132690429688\n",
      "did one more step, loss reduced by 0.00745391845703125\n",
      "did one more step, loss reduced by 0.00745391845703125\n",
      "did one more step, loss reduced by 0.00743865966796875\n",
      "did one more step, loss reduced by 0.00743865966796875\n",
      "did one more step, loss reduced by 0.00742340087890625\n",
      "did one more step, loss reduced by 0.00742340087890625\n",
      "did one more step, loss reduced by 0.00740814208984375\n",
      "did one more step, loss reduced by 0.00740814208984375\n",
      "did one more step, loss reduced by 0.00739288330078125\n",
      "did one more step, loss reduced by 0.00739288330078125\n",
      "did one more step, loss reduced by 0.00737762451171875\n",
      "did one more step, loss reduced by 0.00737762451171875\n",
      "did one more step, loss reduced by 0.008296966552734375\n",
      "did one more step, loss reduced by 0.0074310302734375\n",
      "did one more step, loss reduced by 0.007415771484375\n",
      "did one more step, loss reduced by 0.007415771484375\n",
      "did one more step, loss reduced by 0.0074005126953125\n",
      "did one more step, loss reduced by 0.0074005126953125\n",
      "did one more step, loss reduced by 0.00738525390625\n",
      "did one more step, loss reduced by 0.00738525390625\n",
      "did one more step, loss reduced by 0.0073699951171875\n",
      "did one more step, loss reduced by 0.0073699951171875\n",
      "did one more step, loss reduced by 0.007354736328125\n",
      "did one more step, loss reduced by 0.007354736328125\n",
      "did one more step, loss reduced by 0.0073394775390625\n",
      "did one more step, loss reduced by 0.008298873901367188\n",
      "did one more step, loss reduced by 0.00739288330078125\n",
      "did one more step, loss reduced by 0.00739288330078125\n",
      "did one more step, loss reduced by 0.00737762451171875\n",
      "did one more step, loss reduced by 0.00737762451171875\n",
      "did one more step, loss reduced by 0.00736236572265625\n",
      "did one more step, loss reduced by 0.00736236572265625\n",
      "did one more step, loss reduced by 0.00734710693359375\n",
      "did one more step, loss reduced by 0.00734710693359375\n",
      "did one more step, loss reduced by 0.00733184814453125\n",
      "did one more step, loss reduced by 0.00733184814453125\n",
      "did one more step, loss reduced by 0.00731658935546875\n",
      "did one more step, loss reduced by 0.008228302001953125\n",
      "did one more step, loss reduced by 0.0073699951171875\n",
      "did one more step, loss reduced by 0.0073699951171875\n",
      "did one more step, loss reduced by 0.007354736328125\n",
      "did one more step, loss reduced by 0.007354736328125\n",
      "did one more step, loss reduced by 0.0073394775390625\n",
      "did one more step, loss reduced by 0.0073394775390625\n",
      "did one more step, loss reduced by 0.00732421875\n",
      "did one more step, loss reduced by 0.00732421875\n",
      "did one more step, loss reduced by 0.0073089599609375\n",
      "did one more step, loss reduced by 0.0073089599609375\n",
      "did one more step, loss reduced by 0.007293701171875\n",
      "did one more step, loss reduced by 0.007293701171875\n",
      "did one more step, loss reduced by 0.008213043212890625\n",
      "did one more step, loss reduced by 0.00734710693359375\n",
      "did one more step, loss reduced by 0.00733184814453125\n",
      "did one more step, loss reduced by 0.00733184814453125\n",
      "did one more step, loss reduced by 0.00731658935546875\n",
      "did one more step, loss reduced by 0.00731658935546875\n",
      "did one more step, loss reduced by 0.00730133056640625\n",
      "did one more step, loss reduced by 0.00730133056640625\n",
      "did one more step, loss reduced by 0.00728607177734375\n",
      "did one more step, loss reduced by 0.00728607177734375\n",
      "did one more step, loss reduced by 0.00727081298828125\n",
      "did one more step, loss reduced by 0.00727081298828125\n",
      "did one more step, loss reduced by 0.00725555419921875\n",
      "did one more step, loss reduced by 0.008214950561523438\n",
      "did one more step, loss reduced by 0.0073089599609375\n",
      "did one more step, loss reduced by 0.0073089599609375\n",
      "did one more step, loss reduced by 0.007293701171875\n",
      "did one more step, loss reduced by 0.007293701171875\n",
      "did one more step, loss reduced by 0.0072784423828125\n",
      "did one more step, loss reduced by 0.0072784423828125\n",
      "did one more step, loss reduced by 0.00726318359375\n",
      "did one more step, loss reduced by 0.00726318359375\n",
      "did one more step, loss reduced by 0.0072479248046875\n",
      "did one more step, loss reduced by 0.0072479248046875\n",
      "did one more step, loss reduced by 0.007232666015625\n",
      "did one more step, loss reduced by 0.008144378662109375\n",
      "did one more step, loss reduced by 0.00728607177734375\n",
      "did one more step, loss reduced by 0.00728607177734375\n",
      "did one more step, loss reduced by 0.00727081298828125\n",
      "did one more step, loss reduced by 0.00727081298828125\n",
      "did one more step, loss reduced by 0.00725555419921875\n",
      "did one more step, loss reduced by 0.00725555419921875\n",
      "did one more step, loss reduced by 0.00724029541015625\n",
      "did one more step, loss reduced by 0.00724029541015625\n",
      "did one more step, loss reduced by 0.00722503662109375\n",
      "did one more step, loss reduced by 0.00722503662109375\n",
      "did one more step, loss reduced by 0.00720977783203125\n",
      "did one more step, loss reduced by 0.00720977783203125\n",
      "did one more step, loss reduced by 0.008131027221679688\n",
      "did one more step, loss reduced by 0.00726318359375\n",
      "did one more step, loss reduced by 0.0072479248046875\n",
      "did one more step, loss reduced by 0.0072479248046875\n",
      "did one more step, loss reduced by 0.007232666015625\n",
      "did one more step, loss reduced by 0.007232666015625\n",
      "did one more step, loss reduced by 0.0072174072265625\n",
      "did one more step, loss reduced by 0.0072174072265625\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.0071868896484375\n",
      "did one more step, loss reduced by 0.0071868896484375\n",
      "did one more step, loss reduced by 0.007171630859375\n",
      "did one more step, loss reduced by 0.008129119873046875\n",
      "did one more step, loss reduced by 0.00722503662109375\n",
      "did one more step, loss reduced by 0.00722503662109375\n",
      "did one more step, loss reduced by 0.00720977783203125\n",
      "did one more step, loss reduced by 0.00720977783203125\n",
      "did one more step, loss reduced by 0.00719451904296875\n",
      "did one more step, loss reduced by 0.00719451904296875\n",
      "did one more step, loss reduced by 0.00717926025390625\n",
      "did one more step, loss reduced by 0.00717926025390625\n",
      "did one more step, loss reduced by 0.00716400146484375\n",
      "did one more step, loss reduced by 0.00716400146484375\n",
      "did one more step, loss reduced by 0.00714874267578125\n",
      "did one more step, loss reduced by 0.008060455322265625\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.0072021484375\n",
      "did one more step, loss reduced by 0.0071868896484375\n",
      "did one more step, loss reduced by 0.0071868896484375\n",
      "did one more step, loss reduced by 0.007171630859375\n",
      "did one more step, loss reduced by 0.007171630859375\n",
      "did one more step, loss reduced by 0.0071563720703125\n",
      "did one more step, loss reduced by 0.0071563720703125\n",
      "did one more step, loss reduced by 0.00714111328125\n",
      "did one more step, loss reduced by 0.00714111328125\n",
      "did one more step, loss reduced by 0.0071258544921875\n",
      "did one more step, loss reduced by 0.0071258544921875\n",
      "did one more step, loss reduced by 0.008047103881835938\n",
      "did one more step, loss reduced by 0.00717926025390625\n",
      "did one more step, loss reduced by 0.00716400146484375\n",
      "did one more step, loss reduced by 0.00716400146484375\n",
      "did one more step, loss reduced by 0.00714874267578125\n",
      "did one more step, loss reduced by 0.00714874267578125\n",
      "did one more step, loss reduced by 0.00713348388671875\n",
      "did one more step, loss reduced by 0.00713348388671875\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.00710296630859375\n",
      "did one more step, loss reduced by 0.00710296630859375\n",
      "did one more step, loss reduced by 0.00708770751953125\n",
      "did one more step, loss reduced by 0.008045196533203125\n",
      "did one more step, loss reduced by 0.00714111328125\n",
      "did one more step, loss reduced by 0.00714111328125\n",
      "did one more step, loss reduced by 0.0071258544921875\n",
      "did one more step, loss reduced by 0.0071258544921875\n",
      "did one more step, loss reduced by 0.007110595703125\n",
      "did one more step, loss reduced by 0.007110595703125\n",
      "did one more step, loss reduced by 0.0070953369140625\n",
      "did one more step, loss reduced by 0.0070953369140625\n",
      "did one more step, loss reduced by 0.007080078125\n",
      "did one more step, loss reduced by 0.007080078125\n",
      "did one more step, loss reduced by 0.0070648193359375\n",
      "did one more step, loss reduced by 0.007978439331054688\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.00711822509765625\n",
      "did one more step, loss reduced by 0.00710296630859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did one more step, loss reduced by 0.00710296630859375\n",
      "did one more step, loss reduced by 0.00708770751953125\n",
      "did one more step, loss reduced by 0.00708770751953125\n",
      "did one more step, loss reduced by 0.00707244873046875\n",
      "did one more step, loss reduced by 0.00707244873046875\n",
      "did one more step, loss reduced by 0.00705718994140625\n",
      "did one more step, loss reduced by 0.00705718994140625\n",
      "did one more step, loss reduced by 0.00704193115234375\n",
      "did one more step, loss reduced by 0.00704193115234375\n",
      "did one more step, loss reduced by 0.007961273193359375\n",
      "did one more step, loss reduced by 0.0070953369140625\n",
      "did one more step, loss reduced by 0.007080078125\n",
      "did one more step, loss reduced by 0.007080078125\n",
      "did one more step, loss reduced by 0.0070648193359375\n",
      "did one more step, loss reduced by 0.0070648193359375\n",
      "did one more step, loss reduced by 0.007049560546875\n",
      "did one more step, loss reduced by 0.007049560546875\n",
      "did one more step, loss reduced by 0.0070343017578125\n",
      "did one more step, loss reduced by 0.0070343017578125\n",
      "did one more step, loss reduced by 0.00701904296875\n",
      "did one more step, loss reduced by 0.00701904296875\n",
      "did one more step, loss reduced by 0.0070037841796875\n",
      "did one more step, loss reduced by 0.007961273193359375\n",
      "did one more step, loss reduced by 0.00705718994140625\n",
      "did one more step, loss reduced by 0.00705718994140625\n",
      "did one more step, loss reduced by 0.00704193115234375\n",
      "did one more step, loss reduced by 0.00704193115234375\n",
      "did one more step, loss reduced by 0.00702667236328125\n",
      "did one more step, loss reduced by 0.00702667236328125\n",
      "did one more step, loss reduced by 0.00701141357421875\n",
      "did one more step, loss reduced by 0.00701141357421875\n",
      "did one more step, loss reduced by 0.00699615478515625\n",
      "did one more step, loss reduced by 0.00699615478515625\n",
      "did one more step, loss reduced by 0.00698089599609375\n",
      "did one more step, loss reduced by 0.007894515991210938\n",
      "did one more step, loss reduced by 0.0070343017578125\n",
      "did one more step, loss reduced by 0.0070343017578125\n",
      "did one more step, loss reduced by 0.00701904296875\n",
      "did one more step, loss reduced by 0.00701904296875\n",
      "did one more step, loss reduced by 0.0070037841796875\n",
      "did one more step, loss reduced by 0.0070037841796875\n",
      "did one more step, loss reduced by 0.006988525390625\n",
      "did one more step, loss reduced by 0.006988525390625\n",
      "did one more step, loss reduced by 0.0069732666015625\n",
      "did one more step, loss reduced by 0.0069732666015625\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.007877349853515625\n",
      "did one more step, loss reduced by 0.00701141357421875\n",
      "did one more step, loss reduced by 0.00699615478515625\n",
      "did one more step, loss reduced by 0.00699615478515625\n",
      "did one more step, loss reduced by 0.00698089599609375\n",
      "did one more step, loss reduced by 0.00698089599609375\n",
      "did one more step, loss reduced by 0.00696563720703125\n",
      "did one more step, loss reduced by 0.00696563720703125\n",
      "did one more step, loss reduced by 0.00695037841796875\n",
      "did one more step, loss reduced by 0.00695037841796875\n",
      "did one more step, loss reduced by 0.00693511962890625\n",
      "did one more step, loss reduced by 0.00693511962890625\n",
      "did one more step, loss reduced by 0.00691986083984375\n",
      "did one more step, loss reduced by 0.007879257202148438\n",
      "did one more step, loss reduced by 0.0069732666015625\n",
      "did one more step, loss reduced by 0.0069732666015625\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.0069580078125\n",
      "did one more step, loss reduced by 0.0069427490234375\n",
      "did one more step, loss reduced by 0.0069427490234375\n",
      "did one more step, loss reduced by 0.006927490234375\n",
      "did one more step, loss reduced by 0.006927490234375\n",
      "did one more step, loss reduced by 0.0069122314453125\n",
      "did one more step, loss reduced by 0.0069122314453125\n",
      "did one more step, loss reduced by 0.00689697265625\n",
      "did one more step, loss reduced by 0.007808685302734375\n",
      "did one more step, loss reduced by 0.00695037841796875\n",
      "did one more step, loss reduced by 0.00695037841796875\n",
      "did one more step, loss reduced by 0.00693511962890625\n",
      "did one more step, loss reduced by 0.00693511962890625\n",
      "did one more step, loss reduced by 0.00691986083984375\n",
      "did one more step, loss reduced by 0.00691986083984375\n",
      "did one more step, loss reduced by 0.00690460205078125\n",
      "did one more step, loss reduced by 0.00690460205078125\n",
      "did one more step, loss reduced by 0.00688934326171875\n",
      "did one more step, loss reduced by 0.00688934326171875\n",
      "did one more step, loss reduced by 0.00687408447265625\n",
      "did one more step, loss reduced by 0.00687408447265625\n",
      "did one more step, loss reduced by 0.007793426513671875\n",
      "did one more step, loss reduced by 0.006927490234375\n",
      "did one more step, loss reduced by 0.0069122314453125\n",
      "did one more step, loss reduced by 0.0069122314453125\n",
      "did one more step, loss reduced by 0.00689697265625\n",
      "did one more step, loss reduced by 0.00689697265625\n",
      "did one more step, loss reduced by 0.0068817138671875\n",
      "did one more step, loss reduced by 0.0068817138671875\n",
      "did one more step, loss reduced by 0.006866455078125\n",
      "did one more step, loss reduced by 0.006866455078125\n",
      "did one more step, loss reduced by 0.0068511962890625\n",
      "did one more step, loss reduced by 0.0068511962890625\n",
      "did one more step, loss reduced by 0.0068359375\n",
      "did one more step, loss reduced by 0.0077953338623046875\n",
      "did one more step, loss reduced by 0.00688934326171875\n",
      "did one more step, loss reduced by 0.00688934326171875\n",
      "did one more step, loss reduced by 0.00687408447265625\n",
      "did one more step, loss reduced by 0.00687408447265625\n",
      "did one more step, loss reduced by 0.00685882568359375\n",
      "did one more step, loss reduced by 0.00685882568359375\n",
      "did one more step, loss reduced by 0.00684356689453125\n",
      "did one more step, loss reduced by 0.00684356689453125\n",
      "did one more step, loss reduced by 0.00682830810546875\n",
      "did one more step, loss reduced by 0.00682830810546875\n",
      "did one more step, loss reduced by 0.00681304931640625\n",
      "did one more step, loss reduced by 0.007724761962890625\n",
      "did one more step, loss reduced by 0.006866455078125\n",
      "did one more step, loss reduced by 0.006866455078125\n",
      "did one more step, loss reduced by 0.0068511962890625\n",
      "did one more step, loss reduced by 0.0068511962890625\n",
      "did one more step, loss reduced by 0.0068359375\n",
      "did one more step, loss reduced by 0.0068359375\n",
      "did one more step, loss reduced by 0.0068206787109375\n",
      "did one more step, loss reduced by 0.0068206787109375\n",
      "did one more step, loss reduced by 0.006805419921875\n",
      "did one more step, loss reduced by 0.006805419921875\n",
      "did one more step, loss reduced by 0.0067901611328125\n",
      "did one more step, loss reduced by 0.0067901611328125\n",
      "did one more step, loss reduced by 0.0077114105224609375\n",
      "did one more step, loss reduced by 0.00684356689453125\n",
      "did one more step, loss reduced by 0.00682830810546875\n",
      "did one more step, loss reduced by 0.00682830810546875\n",
      "did one more step, loss reduced by 0.00681304931640625\n",
      "did one more step, loss reduced by 0.00681304931640625\n",
      "did one more step, loss reduced by 0.00679779052734375\n",
      "did one more step, loss reduced by 0.00679779052734375\n",
      "did one more step, loss reduced by 0.00678253173828125\n",
      "did one more step, loss reduced by 0.00678253173828125\n",
      "did one more step, loss reduced by 0.00676727294921875\n",
      "did one more step, loss reduced by 0.00676727294921875\n",
      "did one more step, loss reduced by 0.00675201416015625\n",
      "did one more step, loss reduced by 0.007709503173828125\n",
      "did one more step, loss reduced by 0.006805419921875\n",
      "did one more step, loss reduced by 0.006805419921875\n",
      "did one more step, loss reduced by 0.0067901611328125\n",
      "did one more step, loss reduced by 0.0067901611328125\n",
      "did one more step, loss reduced by 0.00677490234375\n",
      "did one more step, loss reduced by 0.00677490234375\n",
      "did one more step, loss reduced by 0.0067596435546875\n",
      "did one more step, loss reduced by 0.0067596435546875\n",
      "did one more step, loss reduced by 0.006744384765625\n",
      "did one more step, loss reduced by 0.006744384765625\n",
      "did one more step, loss reduced by 0.0067291259765625\n",
      "did one more step, loss reduced by 0.007640838623046875\n",
      "did one more step, loss reduced by 0.00678253173828125\n",
      "did one more step, loss reduced by 0.0135498046875\n",
      "did one more step, loss reduced by 0.013519287109375\n",
      "did one more step, loss reduced by 0.01348876953125\n",
      "did one more step, loss reduced by 0.013458251953125\n",
      "did one more step, loss reduced by 0.013427734375\n",
      "did one more step, loss reduced by 0.013397216796875\n",
      "did one more step, loss reduced by 0.014440536499023438\n",
      "did one more step, loss reduced by 0.0134735107421875\n",
      "did one more step, loss reduced by 0.0134429931640625\n",
      "did one more step, loss reduced by 0.0134124755859375\n",
      "did one more step, loss reduced by 0.0133819580078125\n",
      "did one more step, loss reduced by 0.0133514404296875\n",
      "did one more step, loss reduced by 0.014347076416015625\n",
      "did one more step, loss reduced by 0.013427734375\n",
      "did one more step, loss reduced by 0.013397216796875\n",
      "did one more step, loss reduced by 0.01336669921875\n",
      "did one more step, loss reduced by 0.013336181640625\n",
      "did one more step, loss reduced by 0.0133056640625\n",
      "did one more step, loss reduced by 0.014257431030273438\n",
      "did one more step, loss reduced by 0.0133819580078125\n",
      "did one more step, loss reduced by 0.0133514404296875\n",
      "did one more step, loss reduced by 0.0133209228515625\n",
      "did one more step, loss reduced by 0.0132904052734375\n",
      "did one more step, loss reduced by 0.0132598876953125\n",
      "did one more step, loss reduced by 0.0132293701171875\n",
      "did one more step, loss reduced by 0.014270782470703125\n",
      "did one more step, loss reduced by 0.0133056640625\n",
      "did one more step, loss reduced by 0.013275146484375\n",
      "did one more step, loss reduced by 0.01324462890625\n",
      "did one more step, loss reduced by 0.013214111328125\n",
      "did one more step, loss reduced by 0.01318359375\n",
      "did one more step, loss reduced by 0.014179229736328125\n",
      "did one more step, loss reduced by 0.0132598876953125\n",
      "did one more step, loss reduced by 0.0132293701171875\n",
      "did one more step, loss reduced by 0.0131988525390625\n",
      "did one more step, loss reduced by 0.0131683349609375\n",
      "did one more step, loss reduced by 0.0131378173828125\n",
      "did one more step, loss reduced by 0.014089584350585938\n",
      "did one more step, loss reduced by 0.013214111328125\n",
      "did one more step, loss reduced by 0.01318359375\n",
      "did one more step, loss reduced by 0.013153076171875\n",
      "did one more step, loss reduced by 0.01312255859375\n",
      "did one more step, loss reduced by 0.013092041015625\n",
      "did one more step, loss reduced by 0.0130615234375\n",
      "did one more step, loss reduced by 0.014102935791015625\n",
      "did one more step, loss reduced by 0.0131378173828125\n",
      "did one more step, loss reduced by 0.0131072998046875\n",
      "did one more step, loss reduced by 0.0130767822265625\n",
      "did one more step, loss reduced by 0.0130462646484375\n",
      "did one more step, loss reduced by 0.0130157470703125\n",
      "did one more step, loss reduced by 0.014013290405273438\n",
      "did one more step, loss reduced by 0.013092041015625\n",
      "did one more step, loss reduced by 0.0130615234375\n",
      "did one more step, loss reduced by 0.013031005859375\n",
      "did one more step, loss reduced by 0.01300048828125\n",
      "did one more step, loss reduced by 0.012969970703125\n",
      "did one more step, loss reduced by 0.013919830322265625\n",
      "did one more step, loss reduced by 0.0130462646484375\n",
      "did one more step, loss reduced by 0.0130157470703125\n",
      "did one more step, loss reduced by 0.0129852294921875\n",
      "did one more step, loss reduced by 0.0129547119140625\n",
      "did one more step, loss reduced by 0.0129241943359375\n",
      "did one more step, loss reduced by 0.0128936767578125\n",
      "did one more step, loss reduced by 0.013935089111328125\n",
      "did one more step, loss reduced by 0.012969970703125\n",
      "did one more step, loss reduced by 0.012939453125\n",
      "did one more step, loss reduced by 0.012908935546875\n",
      "did one more step, loss reduced by 0.01287841796875\n",
      "did one more step, loss reduced by 0.012847900390625\n",
      "did one more step, loss reduced by 0.013845443725585938\n",
      "did one more step, loss reduced by 0.0129241943359375\n",
      "did one more step, loss reduced by 0.0128936767578125\n",
      "did one more step, loss reduced by 0.0128631591796875\n",
      "did one more step, loss reduced by 0.0128326416015625\n",
      "did one more step, loss reduced by 0.0128021240234375\n",
      "did one more step, loss reduced by 0.013751983642578125\n",
      "did one more step, loss reduced by 0.01287841796875\n",
      "did one more step, loss reduced by 0.012847900390625\n",
      "did one more step, loss reduced by 0.0128173828125\n",
      "did one more step, loss reduced by 0.012786865234375\n",
      "did one more step, loss reduced by 0.01275634765625\n",
      "did one more step, loss reduced by 0.012725830078125\n",
      "did one more step, loss reduced by 0.0009365081787109375\n",
      "did one more step, loss reduced by 0.0128326416015625\n",
      "did one more step, loss reduced by 0.0128021240234375\n",
      "did one more step, loss reduced by 0.0127716064453125\n",
      "did one more step, loss reduced by 0.0127410888671875\n",
      "did one more step, loss reduced by 0.0127105712890625\n",
      "did one more step, loss reduced by 0.0\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "#now stopping criteria is difference between errors  \n",
    "learning_rait = 0.0006 \n",
    "w = torch.zeros(2, dtype = torch.float16)\n",
    "y_pred = predict(w, x)\n",
    "previous_error = mseerror(y, y_pred)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "error = mseerror(y, y_pred)\n",
    "y_pred = predict(w, x)\n",
    "beta1 = beta1gradient(x, y, y_pred)\n",
    "beta0 = beta0gradient(x, y, y_pred)\n",
    "w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "previous_error = error\n",
    "error = mseerror(y, y_pred)\n",
    "while previous_error - error > 0.0000000005:\n",
    "    y_pred = predict(w, x)\n",
    "    beta1 = beta1gradient(x, y, y_pred)\n",
    "    beta0 = beta0gradient(x, y, y_pred)\n",
    "    w -= learning_rait*torch.tensor([beta1,beta0]) \n",
    "    previous_error = error \n",
    "    error = mseerror(y, y_pred)\n",
    "    print(f'did one more step, loss reduced by {previous_error - error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ad64464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Manually calculated derivative. Slope = 6.0078125, intercept = 4.33984375, loss = 21.95675277709961')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAEJCAYAAACOmc2qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABSzklEQVR4nO3deVyU5frH8c/MMCwKqCgg7kvuC5Km4oZLKqC472YupVamHVvMPJllaurPjmlp51RmaeaSueWa5i6uue8bICoCouww6/37Axkl3FB0QK/36+WrmHnmmWtuZh6+c9/PXKNRSimEEEIIIfIhrb0LEEIIIYR4VBJkhBBCCJFvSZARQgghRL4lQUYIIYQQ+ZYEGSGEEELkWxJkhBBCCJFv3TfIXL58mSpVqvDKK69ku2706NFUqVKFGzduPLHi7mb06NHMmTMH4Inf/7Jlyxg6dOgj3/5h6tu6dSszZszI8b6HDh3KsmXLcm27Ox07dowRI0bkuKZM33zzDZs2bQJgxowZrFix4pH3dT/Lli2jS5cudOjQgXbt2vHvf/+bpKQkAL7++mvGjx//RO73UV29epWhQ4fSqVMnQkJC2LFjx123O3LkCF27diUoKIj+/fsTExNju+5///sfgYGBtG7dmq+//hqlFImJiXTs2DHLv2rVqjF37lwANm7cSEhICB07duTVV1/l0qVLWe4vKiqKpk2bZnmuHj16lF69etGxY0dCQkJYuXKl7brhw4fTunVr231NmjTpgY998ODBnD9//r7bHD16lE8++eSB+3rSPv74Y44fP54r+4qMjKR+/focO3bsrtfv2bPH9hzu0aMHR48eBUApxVdffUVwcDDBwcF8+OGHpKWlARnH5ddee42goCC6du3K2rVrbfs7cOAAXbp0oWPHjvTo0eOu9ztx4sQsx7XTp0/Tq1cvOnToQKdOndi2bRsAK1asyPKcatmyJTVq1OD69esANGjQIMv1q1atuu9YXL58GT8/vxyM3pO1Y8cO21h17tyZnTt3Zrk+MTGRkJCQe/7ujEYjn3zyCW3atKFTp07MmDEDq9UKwJkzZ/Dz88syPhcvXrRd169fPzp16kSXLl1sz7URI0Zk2b5u3bq88cYbAGzevJn69etnuT45ORmAH3/8kXbt2tGhQwcGDBhge31nvn4z/9WuXZsJEyYQGhqa5fK2bdtSpUoVjh8/zoQJE7Jc17BhQ0JCQgC4ceMGr7/+OsHBwbRv356DBw/axmL+/Pm0bduWjh078u677xIfHw9AfHw8//rXv2jbti2dO3dm/vz5ttvEx8fz3nvv0alTJwIDA7P9nTAajQwcOJD169c/+Jep7iMyMlLVqlVLNWrUSF2+fNl2eUpKimrdurWqXLmyiouLu98uct2HH36ofvjhB6WUeuL3//vvv6shQ4Y88u0fpr6ZM2eqzz77LMf7HjJkiPr9999zbbvc9Morr6h169Y90fs4cuSIatmypbp586ZSSimz2azGjh2r3n33XaXUo4/rkxQSEqIWLFiglFLqxIkTql69espgMGTZxmAwqGbNmqkDBw4opZRasGCBev3115VSSm3dulV17NhRpaSkqPT0dNW3b1+1Zs2abPczb9481bdvX2U0GlVaWpry9fVV4eHhSiml5s6dqwYPHmzbdvny5apFixZZnqtWq1UFBASoXbt2KaWUioqKUg0bNlRhYWFKKaUaN26srl27losjk+FxX2+5pUWLFuro0aOPvZ/09HTVs2dPVadOnbvuz2AwqIYNG6oTJ04opZTavHmzatOmjVJKqQ0bNqiuXbsqg8GgrFarGj58uPrvf/+rlFKqb9++aubMmUoppZKSklTnzp3VqVOnbLWHhoYqpZT6888/VXBwcJb7XLNmjWrQoEGWcW7fvr3auHGjUkqpM2fOqDp16mR7XhqNRtWjRw+1cOFCpZRSFy5csNX6sCIjI1WdOnVydJsnJTExUdWvX1+dPXtWKaXUqVOnVN26dVVSUpJSKuO11qZNG1WjRo17PhdmzJihBg0apNLT05XValX//ve/1fz585VSSi1cuFB9/PHH2W6TmpqqGjdurLZu3aqUUmrjxo2qbdu22bY7cuSIat68ubp69apSSqlp06apb7/9Ntt2u3btUkFBQba6f/nlF9WnT59s223atEkFBQWpxMTEbNcNHz5cTZs2LdvlkZGRqlmzZurkyZNKKaVGjBhhq+HkyZOqSZMmKjU1Ve3evVs1bdpURUVFKaUyjinDhw9XSik1atQo9dFHHymz2awMBoN6/fXX1ebNm5VSSg0dOlRNnTpVKZVxjKlXr55tHwcPHlSdOnVStWvXfqi/JQ4PCjo6nY6goCD++OMPWzr8888/adWqFT/++CMAVquVSZMmceTIEVJSUlBKMWHCBOrWrcvo0aNxdXXlzJkzXLt2jSpVqjBlyhQKFixIlSpV2L17Nx4eHgC2nwsXLnzP/d3NwIEDCQoKokePHgDMnj2b+Ph4xowZk2W7I0eOMGHCBNLS0tDr9YwaNQp/f3+WLl3K4sWLMZlMJCQkMHjwYPr06ZPltrGxsYwbN46LFy+i1Wrp1asXr776Kv369aNv374EBgYCZPsZIDU1lU8//ZSIiAji4+MpWLAg06ZNIykpiUWLFmGxWHBzc2PkyJH89ttvLFy4EKvVSuHChRk7diwVK1YkOjqa0aNHExMTQ4kSJYiLi7vrWNxvuwsXLjBx4kTi4+OxWCz069ePbt26sXfvXiZOnEiBAgVISUlh1KhRTJkyhYULFxIQEMCGDRvw9PQEoHv37rz99tuUKVOG8ePHk5KSQmxsLFWrVuWrr75i6dKlHD9+nKlTp6LT6fjrr7+oVKkSrq6ubNmyhf/+97+2WgYMGMDWrVsJDw+/a133Exsbi1KK9PR02/P0nXfe4dy5c9m2PXfuHOPHjyc+Ph6NRsOgQYPo1KkTe/fuZdq0aZQoUYKLFy/i7OzM5MmTqVixIkajkWnTprF//34sFgvVq1fn448/xtXVNcu+v/vuO9asWZPtPn/66SeKFCli+/nUqVMkJCTYnlfVq1fn119/RaPRZLndsWPHcHV1tT3Xu3XrxqRJk7h58yYbN26kffv2FChQAIAuXbqwatUqgoODbbePiIjg22+/ZenSpej1eoxGI0op20xVSkoKTk5OtufKpk2bmDNnTpbnq9FoZNiwYTRq1AiA4sWL4+HhwbVr19DpdKSkpDB27FiioqKoWbMmH374IYULF77v76tly5bMmDGD1NRUpk+fTunSpTl37hxms5nPPvuMEiVKMHPmTJKSkvjoo4/44osv2Lx5M99++y0mkwlnZ2c+/PBD/Pz8+Prrrzl8+DAxMTFUqVKFyZMn83//939s3boVnU6Hn58f48aNw9HRkW+//ZY///wTq9VKyZIlGTduHN7e3vTr14/q1avz999/c/PmTTp27MiIESOYPn06MTExvP/++0ydOhVfX1/bY1ixYoVtlutOU6dOpUqVKtku/+yzz+jSpYvtOf9Pjo6ObN++Hb1ej1KKyMhI23OmTZs2tGjRAr1eT3JyMjdu3LCN8YkTJ5g8eTIArq6uNGjQgI0bN1K1alUsFguJiYnZfteQ8Zr74YcfGDZsWJbZh+XLl6PT6QC4dOkS7u7utp8zff/993h4eNCrVy8ADh06hFarpU+fPiQlJdG2bVvefPPNbLe7F5PJxOTJk9m9ezc6nY7atWvz0Ucf4erqyq+//sqiRYvQ6/U4OTkxfvx4XnjhhXtefqeHfT2aTCbGjRtHpUqVAHjhhRdQSnHz5k1cXV2ZN28e//d//8e//vWvez6GEydO0K5dO9sYv/zyy8yZM4dXXnmFQ4cOERkZSefOndHpdAwZMoQ2bdqwa9cuSpcuTUBAAACtWrWiVKlSWfZrNBoZPXo0Y8aMwcfHxzbeDg4OrF27FldXV0aOHMlLL71EsWLF+PTTT23HpVq1avHDDz9k2V98fDzjxo3j22+/xc3NLct1K1eu5PLly/znP//J9vjGjh3LwIEDqVatGmazma1btzJu3DgAqlWrRrly5dixYweRkZE0atSI4sWLAxnP3Y8//hij0ciJEycYO3YsOp0OnU5H8+bN2bBhA35+foSGhjJ9+nQg4xizZMkSChUqBGTM8Lz33nv873//u+f4Z3G/lJOZoI8dO6YCAwNtl/fv31+dOXPG9i7u4MGDavjw4cpisSillPrf//6nhg4dqpTKmEHp2bOnMhgMymg0qk6dOqmlS5cqpbLPWDzs/v45I7Nx40bVtWtXpZRSFotFtWjRQl24cCHLYzEajapx48Zqy5YtSimljh07ptq3b6+SkpJUjx491I0bN5RSSh06dMj2ruHOd4jDhg1TU6ZMUUplpPl27dqp8PDwbLMPd/6cWd+6devU559/bttm7Nixavz48UqprDMHe/fuVX369FGpqalKKaV27NhhG/e33npLTZ8+XSmlVHh4uKpTp85dZ1rutZ3JZFLBwcHq+PHjtscQFBSkDh06pPbs2aOqVq1qm3Xbs2ePateunVIqI1Fnjvf58+dV8+bNlcViUZMnT1YrVqywjW379u3V+vXrs41B5u8rKSlJ1atXT8XExCillJo6dar6z3/+c9+67sdoNKp3331XVatWTXXq1El99tlnasuWLcpqtWYZV5PJpFq1aqU2bNiglFLq2rVrqmnTpurgwYO2x71//36llFK//vqr6ty5s1JKqa+//lpNnjzZtr8vv/xSjRs37r413c+aNWtU79691aRJk1S3bt1Uz549bTMed1q9erUaNGhQlsuaNm2qTp06pQYNGqRWr15tu3zXrl2qU6dOWbZ955131KxZs7Jctnz5clWjRg3VuHFj5e/vb5ududP9Zg8XLVqkAgICVFpamjp8+LB666231NWrV5XZbFbjx49Xb7755gMff+Ysx549e1S1atVs7/LmzJmj+vbtq5TK+noLCwtT7du3t70uz549qxo3bqxSUlLUzJkzVdu2bZXJZFJKKfXzzz+rvn37qrS0NGWxWNQ777yjli9frpYvX67+9a9/2bZbtGiRbXbrlVdeUYMHD1ZGo1ElJCSotm3b2t4p5saMzJIlS9QHH3zwUPuLjY1VTZo0UTVq1LDNjGSaP3++qlu3rgoODrb9fl599VU1Y8YMZbVaVVxcnAoODlZjx45VSim1c+dO5evrq5o2barq1KmjDh48qJRSKjk5WXXu3FmdOXPmrjNfVqtVtWrVSlWtWtU2q5ApLi5O1atXT126dMl22eLFi9X48eNVSkqKSkhIUD179lRz586975jcOSMzY8YM9fbbbyuj0agsFosaPXq0Gjt2rDKbzapGjRoqOjpaKZXx3F20aNE9L88tX375perSpUu2y+/3u/vmm2/Ua6+9ppKTk5XBYFDvvvuubZZq3Lhx6pdfflFms1mdP39eNWzYUB09elR99913avjw4eqjjz5SnTt3Vv3797cd+zItWLBA9e/fP8tlw4YNU+vWrVNWq1Xt379f1a9f3zZ7kclgMKh+/fqpyZMnZ7l86tSpasyYMdnqNxgMKiAgwHb8u1PmjJTZbFZKKRUTE6Nq1qyZZZv33ntP/fzzz2r//v0qICDA9vdj/vz5qnLlyio6Olp99NFH6qOPPlJGo1ElJyerfv36qUGDBtlm1GfNmqV69uypOnfunOXYlulhZ/cfOCMDULNmTXQ6HcePH6do0aKkpKRQuXJl2/V+fn4UKlSIRYsWERkZyd69eylYsKDt+qZNm+Lo6AhA5cqVSUhIuO/9PWh//9SiRQsmTpzI6dOniY6OplSpUlSoUCHLNmfPnkWr1dK8eXPbY/rjjz8A+O9//8u2bdsIDw/n9OnTpKamZruP0NBQPvjgAwDc3NxYvXr1fR/DnQIDAyldujTz588nIiKCffv23XWteOvWrURERNje9UDGOm18fDyhoaF8+OGHAJQtW5YGDRrc9b7utV14eDiXLl3KMkuVnp7OyZMnqVixIj4+PpQsWTLb/rp3785nn33Ga6+9xu+//07Xrl3RarV88MEH7Nq1i++//57w8HBiYmLuOm6ZXF1dad26NatWrWLAgAH88ccfLFiw4L511alT55770+v1fPnll4waNYq9e/eyf/9+PvzwQ/z9/fnqq69s24WHh2MwGGjTpg0A3t7etGnThh07dtCgQQOqVq1KvXr1AOjatSvjx4/n5s2bbN26laSkJEJDQ4GMd3BFixbNVsfDvgM0m80cPHiQQYMG8dFHH3H06FEGDx7MqlWr8Pb2tm1ntVqzzdIopdDpdCilslynlEKrvX2aW1RUFDt37mTChAm2y86cOcOsWbNYu3YtZcqUYd68eQwfPpyVK1dmu5+7+e6775g3bx4//PADzs7O+Pr6MmvWLNv1b7/9Nk2aNMFoNNpe4w9SokQJqlWrBmTMTC1fvjzbNrt27SImJoYBAwbYLtNoNLb1/zp16uDgkHH4ylzzd3Z2BrD9/t955x2OHTtG165dgYyxzTzPBKBnz57o9Xr0ej2BgYHs3LmTFi1a3LPuh52ROXHiBAsXLmTBggUPMxwUK1aMHTt2cOLECQYMGEDFihUpX748AK+88gp9+/blq6++YsSIEfzyyy9MmTKFL774gg4dOlCyZEmaN29Oeno6169fZ+zYscyfP59atWqxadMmRowYwYYNG/j3v/9Nv379qFy58l3P/9FoNGzatInIyEj69u1LxYoV8ff3B2DJkiW0atWK0qVL27bPnP3ONHDgQObPn5/l93U/27dvZ+TIkej1eiBjJnvYsGHodDoCAwPp1asXzZs3p0mTJgQEBNzz8n962NdjJrPZzOTJk9m+fTs//fTTQ9WeafDgwUyfPp1evXrh7u5OcHAwZ8+eBeDTTz+1bVexYkWCg4PZsmULer2ebdu2MW/ePHx9fdm0aRNDhgxhy5YtttfPzz//nO38vm+++cb2//Xq1cPPz49du3bZnts3btxgxIgRttmaTAaDgSVLltz1PMkNGzZQunRp2/HvTj///DNDhw61zbDd77hUr149hg0bxttvv41Go6Fr164ULlwYvV7P6NGjmTJlCp07d6ZYsWI0btyYQ4cOYTKZuHz5Mq6urixatIiIiAj69u1L2bJlqVmzZk5+DQA8VJAB6NChA6tWrcLDw4OOHTtmuW7r1q1MnDiRgQMH0qpVKypUqJDlxK/MAwxkvGDUXb7eyWg0PvT+/kmn09GzZ0+WLl1KTExMliBw5zb//EWcPXsWd3d3evbsSY8ePahbty6BgYFs2bIl2+0dHByy3P7OaeA7H4/JZMp2219//ZUlS5bQt29fQkJCKFy4MJcvX862ndVqpWPHjrbAZLVaiYmJoVChQtnGLfMg/k/32i5z+erOkzavX7+Om5sbhw8fti1X/FO9evUwm80cPXqU1atXs3jxYgDeffddLBYLQUFBNG/enKioqLv+Xu/Uo0cP21JZxYoVKV26NGfOnLlnXfezdOlSihQpQqtWrejQoQMdOnTgzTffpGXLlllOWrVYLHd9AZrNZoC7ToXrdDqsVitjxoyxHSxTUlIwGAzZth0yZAhDhgy5b60AXl5euLu78/LLLwNQu3ZtSpUqxenTp7MEGR8fnywn95pMJuLj4/H29s52XUxMjG06FzIOTK1bt86y/LVz505efPFFypQpA0Dfvn354osvuHnzpm1J924yp7fPnz/PokWLbNPfBw4cICEhgVatWgHYwtXDLinAwx0PrFZrtlAaFRWFl5cXGzduzPJ8/edr4fr161itVqxWK6+//rptOc9oNGZ5E3Xn7f4ZCu+mU6dOdOrU6YGPb8WKFaSkpNiOQ5lLVaNGjbKNG0BSUhJ79uyhdevWANSoUYOqVaty9uxZDAYDVquV6tWro9Fo6N69O/PmzQMygv4XX3xhG4OxY8fywgsvcODAAUqUKEGtWrWAjKWOSZMmcfLkSQ4cOEBYWBg//fQTCQkJJCUlMXjwYGbNmsXGjRsJCgpCq9VSunRpGjVqxKlTp2xBZu3atXz88cfZHmPVqlWpWrWqbfzudUy6m3/+YbRarbZj57Rp0zh79iyhoaF89913rFy5khkzZtzz8js97OsRICEhgREjRqCUYvHixXcNOg+6/cCBA21vHP/44w/KlCmDxWLhu+++o1+/frbXYub4eHl5UbFiRduS5csvv8zHH39MZGQkFStW5OTJk5jNZurXr2+7n8TERH799VeGDh1qG7M7x/v06dO89dZbvPzyy3z44YdZXovbt2+natWqWUJoprVr19KlS5dsl9+4cYMjR45kCU9FixZFKUV8fLxtiTMmJgZvb2+Sk5OpX78+3bt3BzKWrGfOnEnhwoWJiorigw8+sN3mv//9L2XKlMHLywvAdv9ly5blxRdf5OjRo48UZB7649cdO3Zk/fr1rF27lvbt22e5bteuXbRo0YI+ffpQs2ZNNm3ahMVieeA+PTw8bGeE3znD8Sj76969O5s2beLEiRO2A8OdKlSogEajYdeuXUDGu6b+/ftz8OBBPDw8eOutt2jSpIktxPzz/vz9/fn999+BjANQ//79CQ8Px8PDw/YO5/z585w5cybbfe/cuZPOnTvTvXt3ypcvz+bNm2371+l0tj+qTZo0Yc2aNbY/VgsXLqR///5AxqxWZoi4evUqe/fuves43Gu78uXL4+zsbAsMUVFRtG/f/qE+ndG9e3c+//xzqlSpYluz3blzJ8OGDbOdn3HkyJG7PqY7Zc6wzJo1y/akf9S6tFot06ZN49q1a7bLzp07R4kSJWzrrJDxe3dwcODPP/8EMl5kGzZssJ3/cfr0aU6fPg3A4sWL8fPzw93dnSZNmrBgwQKMRiNWq5WxY8fedR35Yb344os4Ojranl8XLlwgMjLS9ocgk6+vL/Hx8bZPBPz+++/UqVMHd3d3WrVqxapVq0hNTcVoNLJs2TJbMALYt28fDRs2zLK/6tWrs3//ftsnTTZt2kSpUqXuG2IA3n//fZKTk7OEGMgIdBMmTLB9KmHOnDm0bds2R0HmXu583vj7+7Nr1y4uXLgAwLZt2+jQoYPtnKg7+fv7s3r1atvv6tNPP2XNmjU0adKEpUuX2j7dMWPGDEaNGmW73apVq7BarSQkJLBu3TpatmyZrY5H8e9//5sNGzawcuVKVq5ciZeXF9OmTcsSYiDjOTxmzBj+/vtvIOP5e/HiRXx9fTl9+jQfffSRbQZpxYoVtt/t119/zcKFCwEICwtj8+bNtGnThipVqnDu3DnCwsKAjNdkWloaVatWZefOnbZ6RowYQb169fj+++9xdHTkq6++ss1iREdHs3fvXl566SUg44/1pUuXss0gnzt3jpkzZ2KxWEhPT2fBggVZztV6kKZNm7Jw4UJMJhNWq5UFCxbQuHFjbty4QUBAAIULF2bAgAH861//4tixY/e8/FFZLBaGDBlCqVKl+PHHH3McYiDjk0SffPIJSilSUlL46aefCAkJQafTsXnzZpYsWQLAlStX+PPPP2nbti3NmjXj8uXLtuPb/v370Wg0ttdY5mv4zpBXsGBBFixYYDuGnTx5kqNHj9K0aVOuXbtG//79eeuttxgzZky21+G+fftsgfROSikOHDhw1+sOHjxIrVq1sr1ZaN68ue0xnT59mgsXLtCgQQNiYmLo16+f7XX27bff0q5dOzQaDYsWLWLmzJlAxhuM3377jfbt21O6dGlq1Khh+6TS9evXOXTo0COFGMjBjIy3tzcVK1bEzc0t24l9vXr14r333iMkJASz2Uzjxo1tJ9jdz8cff8z48eNxd3enUaNGthNKH2V/RYsWpWbNmlSsWNE2XXknR0dHvv76ayZNmsTUqVPR6/V8/fXX1KhRg1WrVhEYGIhGo6F+/fp4eHgQERGR5faffPIJn376KSEhISilGDp0KDVr1uTNN99k9OjRbNu2jQoVKtx1mm7QoEF88sknLF26FMj4g545BdmwYUPef/99Pv/8c8aOHcvgwYMZNGgQGo0GV1dXvvnmGzQaDePGjeOjjz4iKCiI4sWLZ/sDmOle2zk6OjJ79mwmTpzIDz/8gNls5p133qFu3br3DEWZOnXqxH/+858sf8hHjhzJsGHDKFCgAK6urrz00ku2af+WLVvyn//8566zU927d2f27Nm2P8D3qwsypm979eqV7Y9Aly5dSEtLY/DgwRiNRjQaDeXKlWPOnDlZXsx6vZ7Zs2czYcIEvv76aywWC8OGDaNhw4bs3buXYsWK8dVXX3HlyhU8PDyYOnUqAG+99ZZtStRisVCtWjVGjx5933G6H0dHR+bMmcOECRP48ssvAZg0aRLe3t5ER0czZMgQvvvuO7y9vfnmm28YP348aWlpFC5cmClTptjG9ezZs3Tv3h2TyUSrVq2yzBBERERkWx709/fntddeo1+/fuj1egoVKsTs2bPvW+uhQ4fYsGED5cqVo3fv3rbL33//fQICAujXrx+9e/fGarVSpUoVPv/8cwD++usvFi1axPfff/9IY1SnTh1mzZrF22+/bRuDd9991/bu89tvv73rEnOvXr24cuUKXbp0QSlF/fr16devH1qtlujoaHr06IFGo8HHx8d2kixkzGx069aNlJQU+vTpYzuot27dmg8++IBPP/2UJk2aPNJjuZ+OHTsyYcIEatWqxaxZs5g0aRJmsxlHR0emTZtG8eLF6dSpE5cuXaJr167odDoqVarExIkTARg1ahQffPABK1asQKfTMXnyZNsbjE8//dTWOsHFxYWvv/462wnq/5Q51j/88INt2ThzViciIgJPT89sx9S3336b8ePH247RgYGBtjcnmbMk77zzzj3v880332TKlCl06tQJs9lM7dq1GTt2LO7u7rz55psMGDAAZ2dndDodEyZMwMPD466XP6p169Zx+PBhUlNTbcszcO8TtzMtXLiQ48ePM3HiRLp27cqRI0do3749FouFHj162E6anzZtGuPGjWP58uVYLBbGjBlDxYoVgYw3cp999hlpaWm2v0uZJwzf7TWs0+myHMN0Oh3Tp0/Hw8ODTz75hLS0NObPn2/7aLOjoyO//fabbX93Cwc3b94kNTU1y4xupvDw8LueZjBu3Dg+/vhj2rdvj0ajYerUqbi5ueHm5saQIUPo3r07VquVunXr2tooDBkyhFGjRtG+fXuUUowYMYLatWsDt593mR9uGTZsmO26nNKoB60H5BM3btygW7duLFiwwPaiFuJ+9u7dy+eff56j853EvZnNZt5///0sy0F51d0+XShyR3h4OEuXLuX999+3dyniOfFMdPZdsmQJwcHBvPbaaxJihLCTzI/Ui+dbWFgY/fr1s3cZ4jnyzMzICCGEEOL580zMyAghhBDi+SRBRgghhBD5lgQZIYQQQuRbEmSEEEIIkW89fCtGkefdvJmC1Zrzc7eLFnUlLi75CVSUP8l4ZCXjcZuMRVb5fTy0Wg1Fitz7629E/iBB5hlitapHCjKZtxW3yXhkJeNxm4xFVjIewt5kaclONm/eTJcuXQgKCrJ1qAwNDSUkJIQ2bdrYvt5cCCGEEPcmQcYOIiMjGTduHLNnz2bVqlWcPHmSbdu2MWbMGGbPns3atWs5fvw427Zts3epQgghRJ4mQcYONm7cSHBwMMWLF0ev1zN9+nRcXFwoW7YspUuXxsHBgZCQENavX2/vUoUQQog8Tc6RsYOIiAj0ej1vvPEGUVFRNG/enEqVKtm+NBPAy8uL6OjoHO23aNH7fznc/Xh6uj3ybZ9FMh5ZyXjcJmORlYyHsDcJMnZgsVg4cOAA8+fPp0CBArz55ps4Oztn+ep2pVSWnx9GXFzyI5145+npRmxsUo5v96yS8chKxuM2GYus8vt4aLWax3oDKPIGCTJ2UKxYMfz9/fHw8ADg5ZdfZv369eh0Ots2sbGxeHl52atEIYQQIl+Qc2TsoEWLFuzcuZPExEQsFgs7duwgMDCQsLAwIiIisFgsrF69mmbNmtm7VCGEyLMiY5IZ9W0o4dcS7V2KsCOZkbEDX19fXn/9dfr06YPJZKJx48b07t2bChUqMHz4cAwGAwEBAQQGBtq7VCGEyJNS003MWn4Mk8WKh7uzvcsRdqRRSkk3o2eEnCOTO2Q8spLxuE3GIit7jYdVKWYtO4Yp4givlg6nSKuB6DxK5Xg/co7Ms0FmZIQQQuQrm3aepNbVZdR1DUdLCTQu7vYuSdiRBBkhhBD5glKK8N0bqXXid1ycTDi+2BFHvxA0OvlT9jyT374QQog8z5oUS9KWuRS7dpIrWm8KdxqGk1cZe5cl8gAJMkIIIfIsZbViOrERw/7fMZsVaw0NeLn3K7gUk0Z8IoMEGSGEEHmS5UYk6dvmYo29yDWXivz3ui99OjbER0KMuIMEGSGEEHmKspgwHvoD46E1aJwKcKlST77c60hQg7LUrSKNQkVWEmSEEELkGeZr5zBs/xFrfBQOlRpxvWIIM5ecoWoZd7oEVLB3eSIPkiAjhBDC7pQxDcO+3zCd3IzGtSguQe9i8KzGNz/tx9VFz9CONdFppRm9yE6CjBBCCLsyRxwmfec8VMpN9DXb4PRSF5SDE98vPcqNRAMf9n2RQgUd7V2myKMkyAghhLALa1oihtAFmC/sRVukFC6th6HzqgjA6l1hHL0QR9/WlXmhZCE7VyryMgkyQgghniqlFOZzoaTv/hVMBhzrdcbRt52tsd3xi3Gs2BFGwxretHyxpJ2rFXmdBBkhhBBPjTUxlvSdP2O5fByddyWcmg1EV6SE7frr8Wn8b9UJSnoWpH/bqmg0GjtWK/IDCTJCCCGeOGW1Yjq+EcOB30GjxalxP/TVW6DR3D6B12S2MGvFcaxKMaxzLZwcdXasWOQXEmSEEEI8UZa4SNK3/4g1NgxdGV+cm7yK1rVotu0WbDxHxLUkhnephbdHATtUKvIjCTJCCCGeCGU2ZjS2O7wWjVMBnFu+gUPFBnddLtpx5Crbj1ylnX9Z/Cp72qFakV9JkBFCCJHrzFFnMGyfizXhGg6VGuPs3xuNs+tdt424lsT8P89SvVwROjeVpnciZyTICCGEyDVZGtu5FcMl+H0cStW85/bJaSZmLT+Ge0E9QzrUQKuVk3tFzkiQEUIIkSvMEYcyGtulxqOv1Ranel3Q6J3uub1VKb7/4yTxyQZG962LewFpeidyToKMEEKIx2JNTchobHdxH1qPUri0Ho7O68FLRH/sCufYxTj6ta1ChRLuT6FS8SySICOEEOKRKKUwndlB+p5FtxrbdcHRN9jW2O5+jl6IY9XOMBrVLE7zOiUeuL0Q9yJBRgghRI5ZE2O4tvE/pIcdRVe8Mk7NBqAr/HCBJDY+je//OEEpL1f6ta0iTe/EY5EgI4QQ4qEpq+VWY7tlaLQ6nJq8ir5a8yyN7e7HaLIwa/kxrAqGda6Jk16a3onHI0FGCCHEQ8na2K4OJTu+xU3Dw5+gq5Tilz/Pcik6mRHdauNVRJreiccnQUYIIcR9KbMR48FVGI+sQ+NcEOdWb+FQ4SUc3N0hNumh97P9yFV2HouifaNy1Hmh2BOsWDxPJMgIIYS4J3PUGdK3z0UlXMOhchOcG/a6Z2O7+wmLSmTBxrPUKO9Bpybln0Cl4nklQUYIIUQ2ypiKYe9vmE5tQePm+cDGdveTlGpk9vJjFCroyFBpeidymQQZIYQQWZjCD2LYOQ+VlvBQje3ux2pVfPfHSRJSjHz0Sl1cXfS5XK143kmQsZN+/fpx48YNHBwyfgXjx48nJSWFL774AoPBQFBQECNHjrRzlUKI54k1Nf5WY7v9aD1K49JmxEM1trufFTvDOBF2g/6BVSjvI03vRO6TIGMHSinCw8PZsmWLLcikp6cTGBjI/Pnz8fHxYejQoWzbto2AgAA7VyuEeNYppTCf3ZnR2M5swPGlrjj6BqHRPt6fiMPnr7M6NJwmtX1o5itN78STIUHGDi5evAjAoEGDiI+Pp0ePHlSuXJmyZctSunRpAEJCQli/fr0EGSHEE2VNjCF9x09YrpxEV7wyzs0Goi3s89j7jbmZyvd/nKSMtyuvtK4sTe/EEyNBxg4SExPx9/dn7NixmEwmXn31VV5//XU8PT1t23h5eREdHZ2j/RYtmvNPEmTy9HR75Ns+i2Q8spLxuO1ZGQtltZCwbzU3ty0CnQPFgobi5vfyQze2y3S38Ug3mvl83gF0Wg1jX2tI8aIFc6tsIbKRIGMHfn5++Pn52X7u1q0bM2fOpG7durbLlFI5fgcTF5eM1apyXI+npxuxOegF8ayT8chKxuO2Z2UsLNcjSN8+F+v1cBzK+uHU5FUMBYtguJ6So/3cbTyUUsxZc4rwq4m8090XndWaZ8dMq9U81htAkTdIkLGDAwcOYDKZ8Pf3BzJe+CVLliQ2Nta2TWxsLF5eXvYqUQjxDMpobLfyVmM7V5xffguH8i/l6rLP1sNXCT1+jY5NylO7YtFc268Q95KzOUSRK5KSkpg6dSoGg4Hk5GSWL1/Ou+++S1hYGBEREVgsFlavXk2zZs3sXaoQIg+xKsXizeeY+utBlmw5z4HTMdxITEepB8/Emq+eJuX3sRgPr0FfuTEFu09CX6F+roaYC1cT+HXjWWpVKEpI43K5tl8h7kdmZOygRYsWHDlyhE6dOmG1WunTpw9+fn5MnjyZ4cOHYzAYCAgIIDAw0N6lCiHyCKUUC/48y5ZDVyjpWZBNByIxWzICTCFXRyr4uFOhhDsVfNwp5+OOi1PG4V0ZUjIa253emtHYrt0oHEpWz/X6ElONzF5+nCJuTgwOqY5WTu4VT4lGPUyUF/mCnCOTO2Q8spLxuM2eY/H7tgus2R1BUIMydG/xAiazlciYZC5eTSAsKpGLVxOJvpkGgAYoUawgjYtc46XETehNyehrtcX5pc5oHB6tsd3dZI6H1ar4cvFhzl1O4N/96lK2eP44IVrOkXk2yIyMEELkcev2RLBmdwQBdUrQrXlFAPQO2owZmBK3m8wlp5kIi0rkyqWrlAhbSfm481w2F2FRShDRoV6Uu3Cc8iXcqVCiEBV83PFwd8qVpaXlOy5yKuImA4Or5psQI54dEmSEECIP23r4Cr9tvUD9al70a1PlvsGjoLMDVYzHKRe+GKxGHF/qStEyzQmKTuHi1UTCriby199X2LAvEoBCBR0pn7kkVcKd8ncsST2sQ2djWbM7gma+JWhaW5reiadPgowQQuRR+05FM3/9GWpXLMrr7avf98sWrQnRGY3trp5C51MF56YD0RYujhfgVdSNhtWLA2C2ZC5JZSxHXYxK5PD560DGklTxogVuBZuMWZuSngVx0N39cyFXY5P5Yc1JyhV3o2/rSrn98IV4KBJkhBAiDzp64Trf/3GSSqUK8WanmvcME8pqwXRsA4YDy0HrgFPTAeirNrtnYzsHnZbyPhmzL61uta5KSTfZzrO5eDWRI+fj2HXsGgCODlrKFHfLcjJx0ULOGE1Wpiw8hFaj4a3ONdE76J7IOAjxIBJkhBAijzkbGc+s5ccp5enKiG6+OOnvHhIs1yNI3/Yj1rgIW2M7bcEiOb6/gs56apYvSs3yGX1flFJcT0i/Y9Ymgc0Hr/Dn/owlKfcCegq66Ll2I5WRPXwpVsjl0R+sEI9JgowQQuQhEdeSmLH0CMUKOTOypy8FnLMfppXZiPHvFRiPrr/V2G4YDuXr5VpPGI1Gg2dhFzwLu9CgujeQsSR1Ofb2klREdBKDQmrYwo8Q9iJBRggh8oiouBS+XHyYAk4OvNezDu4FHLNtY756ivTtP6ESo9FXaYZTw55onJ78dxk56LSUK+5OueLutHwx4zL5aL7ICyTICCFEHnA9IY1piw6j1Wp4v5cfHu7OWa7PaGy3GNPp7U+0sZ0Q+Y0EGSGEsLOEFCPTFh3GYLQwqo8f3h4FslxvCjuAYed8VHoi+tpBONXrlKuN7YTIzyTICCGEHaWmm/jP4sPEJxt4v6cfZbxvN5SzptzEsOsXzOF/oy1aBpegkeiKlbNfsULkQRJkhBDCTgxGC1/9dpSr11N4p3ttXihVCMj41JDp9DYMexeDxYxj/e441m6LRiuHbCH+SV4VQghhByazlW+WH+PC1QTe7FjT9ukfa8I10rf/hCXqNDqfqjg3G4C2UHE7VytE3iVBRgghnjKL1cp3f5zgRNgNBgZVpV5VL5TVjPHoBox/rwCdA07NBqKv0vSeje2EEBkkyAghxFOklOLn9Wf4+0wsvVq+QFPfEliuh5O+bW5GY7tydXFq/MojNbYT4nkkQUYIIZ4SpRSLN59n59EoQhqVo/WL3qTvWYzp2AY0zm44t34bffl69i5TiHxFgowQQjwlq0PD+XN/JK3qlqJ9hTRSlo5FJcagr9oMpwZPp7GdEM8aCTJCCPEUbDoQyfIdYQRUL0wXxx2kr92Bxt0bl/Yf4lCimr3LEyLfkiAjhBBP2K5jUfy66RydytygRcJKzNFJOPoG41i3ExqH7F9DIIR4eBJkhBDiCTp0Npbf1x3kHa+DVEi+gLZoWZylsZ0QuUaCjBBCPCGnwq5zaO0yxhT+GyelcKzf41ZjO529SxPimSFBRgghnoDws+cwbJpD9wLXwLsKBZsPQlvI295lCfHMkSAjhBC5SFnNXA9dhfuJNRTU6bA26Id77ZZoNBp7lybEM0mCjBBC5BJLbDjJW37AOf4yJ1Q5KnZ8g0I+8vUCQjxJEmSEEOIxKbMBw4HlGI9tIMXqwkpjSzr06oKnp6u9SxPimSdBRgghHoP5yknSt89FJcVyhOosS6nDiN4NKSkhRoinQoKMEEI8AmVIwbBnEaYzO8Ddm9/0ndhzvTDv9vClvI+7vcsT4rkhQUYIIXJAKYU57ACGXfNR6cnoagfz7YVynIpJ4e0uNalaVr7sUYinSb4f3o6mTJnC6NGjAQgNDSUkJIQ2bdowffp0O1cmhLgbc2Ic6X/OJH3TLDQFi+DU8RN+uFyVk5HJvNa+GnUqFbN3iUI8dyTI2Mnu3btZvnw5AOnp6YwZM4bZs2ezdu1ajh8/zrZt2+xcoRAik1JWjCe3EPndvzBfPoFTg544dxzLz3tTOHz+On3bVMa/hnw6SQh7kCBjB/Hx8UyfPp033ngDgKNHj1K2bFlKly6Ng4MDISEhrF+/3s5VCiEArPFRpK2egmHnzzj5VKRg9wnoawey8K8L7D4RTZdmFWj5Yil7lynEc0vOkbGDTz75hJEjRxIVFQVATEwMnp6etuu9vLyIjo7O8X6LFn30T0l4ero98m2fRTIeWT2P46EsZuL3rCR+x29o9I4Ua/cWbr4Zje3mrzvF5oNX6Nz8BQa0r/5cN7t7Hp8bIm+RIPOU/fbbb/j4+ODv78+yZcsAsFqtWQ6ESqlHOjDGxSVjtaoc387T043Y2KQc3+5ZJeOR1fM4HpbYMNK3/Yj1RiQOFV7CqVFfDAUK467RMH/1CZZsOU8zXx/aNyjN9evJ9i7XbvL7c0Or1TzWG0CRN0iQecrWrl1LbGwsHTt2JCEhgdTUVK5cuYJOd/tL5GJjY/Hy8rJjlUI8n5TJgOHv5ZiObUDjUgjnNiPQl3vRdv2GPREs2XKeelW9eLVt1ed6JkaIvEKCzFM2d+5c2/8vW7aMffv28dlnn9GmTRsiIiIoVaoUq1evpmvXrnasUoi8IyouhVW7wjGZrU/0fnyMEfgnb8TNmsAZ59r87dwM00E9HDwGgNWqOHrhOjUreDAkpDparYQYIfICCTJ5gJOTE5MnT2b48OEYDAYCAgIIDAy0d1lC2J3ZYuW/K08QE5+GZyHnJ3Ifziqd5iqUWuoMNyjEQm1HLptKQIIFSM2ybRPfkvRu9QIOOvmchBB5hUYplfOTKkSeJOfI5A4Zj6zsOR4rdlxk1a5whnephV9lzwffIAeUUpgv7scQ+gsqPQVH3yAcX+yAxsHxnreR50ZW+X085ByZZ4PMyAgh8qRL0Ums2R1BwxreuR5irMk3SN85D8ulw2iLlcMl+H10Rcvk6n0IIZ4OCTJCiDzHbLHyw+pTuLro6fNy5Vzbr1JWTKe2Yti7BKxWnBr2Ql+zNRqt7sE3FkLkSRJkhBB5zurQcC7HJjO8ay1cXfS5sk9L/FUM23/Ccu0supI1cG7aH627fDpQiPxOgowQIk+JuJaxpORfwxu/So+/pKQsZoxH1mI8uAr0TjgHvIZD5Sby0WkhnhESZIQQeYbZYmXOmowlpd65sKRkiblI+vYfsd64jEOF+jg16ou2QKFcqFQIkVdIkBFC5Bl/7MpYUhrRtfZjLSkpkwHDgWWYjv+JxqUQLm3ewaGcXy5WKoTIKyTICCHyhNtLSsWpU6nYI+/HfPk46Tt+QiVdR1+9JU71u6FxLJCLlQoh8hIJMkIIu8tYUjqJW0E9fVpXeqR9qPRk0vcsxHx2F5pCxXEJ+QgHnyq5XKkQIq+RICOEsLtVu8K5HJvCiG61KeicsyUlpRTmC3sxhC5AGVJx9AvB0S/kvo3thBDPDgkyQgi7Cr+WyNrdETSqWZw6L+RsScmaHHersd0RtJ7lcWk3CF3R0k+oUiFEXiRBRghhNyZzxqeU3Avq6f3ywy8pKWXFdHILhn2/gbLi1LD3rcZ28h1IQjxvJMgIIezmj9AwrsSm8E4OlpQsN69i2D4XS/S5W43tBqB1z92vMBBC5B8SZIQQdhEWlcja3ZdoXLM4vg+xpJTR2G4NxoN/ZDS2a/46DpUaS2M7IZ5zEmSEEE+dyWzlxxwsKVliLpC+bS7Wm9LYTgiRlQQZIcRTt2pXGFeup/Cv7rUpcJ8lJWVKx7B/GabjG9EULIJL23dwKCuN7YQQt0mQEUI8VWFRiazbc4nGtYpTu+K9l5TMkccyGtslx91qbNcdjaPLU6xUCJEfSJARQjw1mUtKhVwd6d3q7ktK1vQkDLsXYj4XirZQcZw7jMGh+ON/75IQ4tkkQUYI8dTcXlLyzbakJI3thBCPQoKMEOKpCItKZO2eCJrU8qF2xaJZrrMmx5G+42cskUfRelbApf1AdB7S2E4I8WASZIQQT5zJbGHOmlMUdnWiV6sXbJcrZcV0YjOG/UszGtv590ZfQxrbCSEengQZIcQTt3JnOFf/saRkuXmF9O1zsUafR1eqJs5N+6N1k8Z2QoickSAjhHiiLl5NZN3eCJrUzlhSUhYzxsNrMB7KbGw3GIdKjaSxnRDikUiQEUI8MRlLSiczlpRaVsISfT5jFubmFRwqNsSpUR+0Lu72LlMIkY9JkBFCPDErdoYRFZfKe12qoj24mNTjm241tvsXDmXr2Ls8IcQzQIKMEOKJuHA1gfV7L9GtchplD3yJKfnGrcZ23aSxnRAi10iQEULkOpPZwsLVBxlUKJTa18+jKVziVmO7B3+vkhBC5IQEGSFErlJKsXfNH7xmXUdBnRlHv444+rVHo7v3dyoJIcSjkmYNdjJjxgyCg4Np164dc+fOBSA0NJSQkBDatGnD9OnT7VyhEDlnTbpO3Iqp+EavwODsQcGun+FUr7OEGCHEEyMzMnawb98+9uzZw6pVqzCbzQQHB+Pv78+YMWOYP38+Pj4+DB06lG3bthEQEGDvcoV4IGW1Yjr5F4Z9S9GYLayz+NO+50B0LvL1AkKIJ0tmZOygfv36zJs3DwcHB+Li4rBYLCQmJlK2bFlKly6Ng4MDISEhrF+/3t6lCvFAlhtXSF01EUPoAmIdS/JFfAeqB3ajgIQYIcRTIDMydqLX65k5cyY//vgjgYGBxMTE4Ol5u6upl5cX0dHRdqxQiPtTFhPGQ6sxHl6NRu9Cgu8rTNyqoWntktQsX/TBOxBCiFwgQcaORowYweDBg3njjTcIDw/P0tlUKZXjTqdFi7o+ci2enm6PfNtnkYxHVv8cj/TLZ4hdMxvT9cu41miKa4v+TPn2IMUKWxjWo062b7Z+lshzIysZD2FvEmTs4MKFCxiNRqpVq4aLiwtt2rRh/fr16HQ62zaxsbF4eXnlaL9xcclYrSrH9Xh6uhEbm5Tj2z2rZDyyunM8lDENw/7fMZ34K6OxXeBINGV8mbv+PFdik3mvZx1SktJJSUq3c9VPhjw3ssrv46HVah7rDaDIG+QcGTu4fPkyH3/8MUajEaPRyF9//UWvXr0ICwsjIiICi8XC6tWradasmb1LFcLGfOkoKUs/xnTiL/Q1WlGw+0Qcyvhy/koCG/ZdIqBOCWqU97B3mUKI54zMyNhBQEAAR48epVOnTuh0Otq0aUO7du3w8PBg+PDhGAwGAgICCAwMtHepQmBJSSBt83eYz+9BW7gELh3/jc77BQCMJgtz1pzCw92JHi1esHOlQojnkUYplfO1CJEnydJS7pDxyKCUwnx+N8Y9C7EaUnH0C8GxTrssPWEWbz7Hhn2RvNerDjXKPfuzMfLcyCq/j4csLT0bZEZGCJGNNek66Tt/xhJ5DKeSldH590fnUTLLNucvJ/Dnvkia1ynxXIQYIUTeJEFGCGFzZ2M7AKdGfSkR0JHrcalZtjOaLMxZewoPd2e6y5KSEMKOJMgIIYCMxnbp2+dgjbmIrnRtnJv2R+taFI1Wl23bZdsvEn0jlfd71cHFSQ4jQgj7kSOQEM+5LI3tHAvg3HIoDhUb3rOP0bnL8WzcH0lzv5JUlyUlIYSdSZARIg+6HJOMo16LZ2GXHDdGzAnLtXOkb5+LNf4qDi/449SoD1rnezc4M9g+peRM9+YVn1hdQgjxsCTICJHHnAi/wX8WH0YpcHXRU6GEO+V93G3/dXV5/K65ypiGYd9STCc3o3H1wCXoXRxK137g7ZZtu0jMzTQ+kCUlIUQeIUciIfKQuIR0/rfyBCWKFqRV3VJcjEok7Goixy7EkfnBeu8iLpQv4U4FH3cqlChEaS9X9A4P39vSfOkw6TvmoVJuoq/5Mk4vdUWjd37g7c5GxrPpQCQt/EpSTZaUhBB5hAQZIfIIk9nK7BXHMFusDOtSi+IeBWjul/GR5zSDmfCoRC5GJXLxaiKnIm6y50TGl4o66DSU9nKjQgl32z+vuyxJWdMSMYT+ivnCHrRFSuLy8lu2xnYPYjBZ+HHtKYoWcqZ7C1lSEkLkHRJkhMgjFv51jrCoJIZ1zggxd3JxcqBaOQ/bTIhSiptJBi5evR1udhy9yl9/XwagoLPD7VkbH3fKG07CgcVgSsexbudbje0e/uX/+7YLGUtKvf1wdpTDhhAi75AjkhB5wK5jUWw9dIWghmWoW8XzgdtrNBo83J3xcHemXtWMLxe1WK1cvZ7KxasJtoCzK/w4JQruBn0Ukcqbo549KKoqUOFaCmW83R5qSerExTj+OnCZFi+WpFrZIo/9WIUQIjdJkBHCzi5FJzFvwxmqlilMl2YVHnk/Oq2W0l6ulPZypVltH0wnNmLYvxqr0nDeO5hdhqpcvJpI/Nlzt7bXUMbblQo+hShfwo0KJQrhXSTrkpTBaGHGokMZS0ryKSUhRB4kQUYIO0pJN/HNsmO4uuh5o2NNdNrH/0J6y41I0rfNxRqb0diuYNP++LkWxe/W9RlLUrdmba4msvNYFH8dvL0kVc4n80Rid45ciCMqLoVRsqQkhMij5MgkhJ1YleL7P05yM8nA6L4v4l7Q8bH2pywmjAdXYTy8Fo1TAZxbvoFDxQbZTvot4uZE3Spe1K2SsSRltSquXk+5da5NAhevJrF6dziZXyfbrnF5qsqSkhAij5IgI4SdrA4N5+iFOF5pU5mKJQs91r7M185i2D4Xa3wUDpUa4eTf+76N7e6k1Woo5eVKKS9XmvmWACDdaCbiWhLRN9NoH/ACifGpD9iLEELYhwQZIezg+MU4Vu4Iw7+GNy38Sj74BveQ0djut1uN7YriEvQeDqVrPXZ9zo4OVClThCpliuCkz/5dS0IIkVdIkBHiKbsen8b/Vp2gpKcrrwZWfeSvIDBHHCZ9Z2ZjuzY4vdTloRrbCSHEs0SCjBBPkclsYdaK41gVDOtS85FmO6xpiRh2/YL54j60RUrh0noYOi/5RJEQ4vkkQUaIp2jBxrNEXEtieNdaeBcp8OAb3EEphfncLtJ3LwSTAcd6nXH0zVljOyGEeNbIEVCIp2T7katsPxJFO/+y+FV6cNO7O1kTY0nf8ROWKyfQeVfCqdlAdEVKPKFKhRAi/5AgI8RTEH4tkV/+PEv1ckXo3PThm94pqxXT8T8xHFgGGi1Ojfuhr94Cjebx+80IIcSzQIKMEE9YcpqJWcuO415Qz9AONdBqH+7kXktcJOnbf8QaG4aujC/OTV5F61r0CVcrhBD5iwQZIZ4gq1Xx3aoTJKQYGN23Lm4FHtz0TpmNGA/9cbuxXas3cahQ/5E/3SSEEM8yCTJCPEGrdoVxPOwGr7atQoUS7g/c3hx1JqOxXcI1HCo1xtm/Nxpn16dQqRBC5E8SZIR4Qo6cv86qXeE0rlWcgDr3PzFXGVMx7P0N06ktaNyK4RL8Pg6laj6lSoUQIv+SICPEExATn8b3f5ykjJcr/dpUue+ykDn8EOm75qFS49HXaotTvS5o9E5PsVohhMi/JMgIkcuMJguzlx0D4K0utXC8R9M7a2oChtAFGY3tPErh0no4Oq+H/0STEEIICTJC5CqlFPP/PMOlmGTe6VYbr8Iud93GfHYn6XsW3Wps1wVH32BpbCeEEI9Ajpx28s0337Bu3ToAAgICGDVqFKGhoXzxxRcYDAaCgoIYOXKknasUObXtyFV2HbtGh8bl8H2hWLbrrYkxpO/4OaOxXfHKODUbgK6wNLYTQohHJUHGDkJDQ9m5cyfLly9Ho9Hw+uuvs3r1aqZNm8b8+fPx8fFh6NChbNu2jYCAAHuXKx5SWFQiv248S83yHnRoXD7LdcpqyWhst385aLU4NXkVfbXm0thOCCEekwQZO/D09GT06NE4Omb0FKlYsSLh4eGULVuW0qVLAxASEsL69eslyOQTSalGZi0/RqGCTgz5R9M7S9wl0rfPvdXYrs6txnYedqxWCCGeHRJk7KBSpUq2/w8PD2fdunW88soreHre/v4dLy8voqOj7VGeyCGrVfG/VSdITDExpt+LuLrogVuN7Q6uwnhkHRrngji3eguHCi9JYzshhMhFEmTs6Ny5cwwdOpRRo0ah0+kIDw+3XaeUyvEfvKJFH71xmqen2yPf9lmUk/GYv+4UJ8NvMrxHHV6qVRKAtEsnuL7mv5huXMW1dguKvtwfnUv+HWN5ftwmY5GVjIewNwkydvL3338zYsQIxowZQ7t27di3bx+xsbG262NjY/Hy8srRPuPikrFaVY5r8fR0IzY2Kce3e1blZDwOnYtlyaazNPP1wa+CBzFXojHsXYLp1FY0bp64BH+AplQNbiQDyflzjOX5cZuMRVb5fTy0Ws1jvQEUeYMEGTuIiopi2LBhTJ8+HX9/fwB8fX0JCwsjIiKCUqVKsXr1arp27WrnSsX9RN9M5YfVpyhb3I2+rStjCj+IYec8VFoC+tqBONXtLI3thBDiCZMgYwdz5szBYDAwefJk22W9evVi8uTJDB8+HIPBQEBAAIGBgXasUtyPwWRh1rLjaDUwrG1pzFu+xRx2AK1HaVzajJDGdkII8ZRolFI5X4sQeZIsLeWOB42HUoofVp9kz4lrfNzYQLELq8FixPHFjjj6BqHRPlvvD+T5cZuMRVb5fTxkaenZ8GwdcYV4CrYcusLZU+cZV/owRU6FoSteGedmA9EW9rF3aUII8dyRICNEDlyIvEHUjuWMKXwEB6Mepyb90VcLkMZ2QghhJxJkhHhIiZEXMK/5lg4u16F0HQo264+2YBF7lyWEEM81CTJCPIAyG0n/ewXqyDpccSL5pUEUr9NUGtsJIUQeIEFGiPswXz1N+o65qIRo9hlewK1xbxr6VbR3WUIIIW6RICPEXShDCoa9v2E6vRWjswffJ7amRM26tH5RQowQQuQlEmSE+IeU03tJWfcdKi0BY+WXGX/Qh2Ke7vR+ubK9SxNCCPEPEmSEuMWaGo9h1y8khR1AW7Q0upbDmbr2Okpn5K1OtdA7yCeThBAir5EgI+zKYrVyJTaFsKhEUtPNFHZzooirE0XcnCjs5oSTXvfEa1BKYTqzHcOexWAxUqR5X4wVm/P9mrNcjUvh3Z51KFrI+YnXIYQQIuckyIinRinFzSQDF68mZvyLSiT8WiJGk/Wetyno7JARbu4IOLf/OVPEzYmCzg6P/Akia0I06Tt+wnL1FDqfKjg3HUiRSpVYuO4ke09G06VZBWqU83jUhyyEEOIJkyAjnpg0g5nwqIzAkhlcEpKNADjoNJTxdqNp7RJUKOFOhRLuuBdwJD7ZwM2kO/4lG4hPMnAjyUBkdDKJKUb++SUMegctRVydbgeeu4SeQq6O6LS3l4aU1YLx6AaMfy8HrQNOTQegr9oMjUbLybA4Fm8+T50XihHsX/YpjpgQQoickiAjcoXFauVyTMYSUWZoibqeYgsd3kVcqF62COV93KlQohClvVzves6Ji5MDPkUL3vN+zBYrCcnGLAEn47/pxCcZuHg1gZtJBsyWrHFHowH3go4UcXWioksCzdI2UtgYTYJHdVJqdqeQlzdFTIp0o4Ep8/6mqLszr7evhlZ6xQghRJ4mQUbkmFKKuMR0Ll5NtAWXiGtJGM0ZS0SuLnoqlHCnflUvypdwp7yPO64u+ly5bwedlqKFnO97zopSiuQ0U7ZZnfjEZF6I3UathIMkW535MSWAIzfKwvkwIAwAnVaDTqfl3/3qUsA5d2oWQgjx5EiQEQ+Umm4m7FpGYAm7NduSmJK5RKSlrLcrzercWiLyccezsItdu95qNBrcCjjiVsCRMt5uAJivniJ9+wKUMRp9lWa4N+zJGxpnbt5aysqc1UlINtKifhl85OReIYTIFyTIiCzMloxPEV28mnB7iSgu1Xa9t0cBapTzsJ3XUtrLFQdd3v1YckZju8WYTm9H4+6FS7tROJSsDoATUNyjAMU9CmS5jaenG7GxSXaoVgghRE5JkHnOWaxWdhy+wuHT0RlLRNFJmP6xRNSgujcVbi0RFcxHyy2msAMYds5HpSfh6BuMY92OaByc7F2WEEKIXCRB5jm3/1QM3/1xEr2DlrLebjSvU9I221KskHO+/GJEa8pNDLt+wRz+N9qiZXAJGomuWDl7lyWEEOIJkCDznKtfzRu/6sXRWa15eonoYSilMJ3ehmHvYrCYcazfHcfabdFo5WkuhBDPKjnCP+e0Wg2lnoFzQqwJ10jf/hOWqNPofKri3GwA2kLF7V2WEEKIJ0yCjMjXlNV8q7HdCtA54NRsIPoqzfLlkpgQQoickyAj8i3L9XDSt/2INe4SDuXq4tT4FbQFi9i7LCGEEE+RBBmR7yizAcOBFZiObUDj7IZz67fRl69n77KEEELYgQQZka+Yr5wkfcdPqMQY9FUDcGrQA43Tvb/SQAghxLNNgozIF5QhBcOeRZjO7EDj7o1L+w9xKFHN3mUJIYSwMwkyIk9TSmEOO4Bh13xUevKtxnad0Dg42rs0IYQQeYAEGZFnZTS2m485/CDaYmVxCXoPXbGy9i5LCCFEHiJBRuQ5Slkxnd6OYc9isJpxatADfa22aLQ6e5cmhBAij8nfrVzzseTkZNq3b8/ly5cBCA0NJSQkhDZt2jB9+nQ7V2c/1vhrpK2egmHHT+g8y1Gw2wQcfYMlxAghhLgrCTJ2cOTIEXr37k14eDgA6enpjBkzhtmzZ7N27VqOHz/Otm3b7FvkU6asZgyHVpPy+8dY4i7h1GwgLu1GoS3kbe/ShBBC5GESZOxgyZIljBs3Di8vLwCOHj1K2bJlKV26NA4ODoSEhLB+/Xo7V/n0WGLDSF3+Gcb9S3EoU4eCPb7AsWqAdOcVQgjxQHKOjB1MnDgxy88xMTF4enrafvby8iI6Ovppl/XUZTS2W57R2M6lEM6th6MvX9feZQkhhMhHJMjkAVarNcvsg1LqkWYjihZ1feQaPD3dHvm2jyI17AjX1/4Pc3w0bn6t8WjZD51z3mls97THI6+T8bhNxiIrGQ9hbxJk8oDixYsTGxtr+zk2Nta27JQTcXHJWK0qx7fzfIrffq3Sk0nfsxjz2R1oCnnj0n40lKjKjSQrJOWNb+B+muORH8h43CZjkVV+Hw+tVvNYbwBF3iBBJg/w9fUlLCyMiIgISpUqxerVq+natau9y8pVGY3t9mPY9UtGY7s67XF8sYM0thNCCPFYJMjkAU5OTkyePJnhw4djMBgICAggMDDQ3mXlGmvKTQw752GOOCSN7YQQQuQqCTJ2tHnzZtv/+/v7s2rVKjtWk/uUsmI6tRXD3t/AasGpQU/0tdpITxghhBC5RoKMeCKs8VGkb5+L5dpZdCWq4dxsIFr3nJ/3I4QQQtyPBBmRq5TVjPHIOowHV4LOEeeA13Co3ER6wgghhHgiJMiIXGOJuUj69rlYb0TiUOElnBr1RVugsL3LEkII8QyTICMemzIZMBxYhun4nxmN7dqMQF/uRXuXJYQQ4jkgQUY8FvPl46Tv+BmVFIu+WnOcGvRA41jA3mUJIYR4TkiQEY8ko7HdQsxnd6EpVByXkI9w8Kli77KEEEI8ZyTIiBxRSmG+uB9D6C+o9BRpbCeEEMKuJMiIh2ZNvkH6znlYLh1G61kel+D30RUtY++yhBBCPMckyIgHut3YbglYrTg17IW+ZmtpbCeEEMLuJMiI+7LEX8Ww/aeMxnYla+DctL80thNCCJFnSJARd6UsZoxH1mI8uAr0TtLYTgghRJ4kQUZkk9HY7kesNy7jUKH+rcZ2hexdlhBCCJGNBBlhk6WxXYHCuLR5B4dyfvYuSwghhLgnCTICyGxs9xMq6Tr66i1xqt9NGtsJIYTI8yTIPOeUMY2YVT+Rdmwr2kLFce4wBofile1dlhBCCPFQJMg858yRx0g/sQNHvxAc/UKksZ0QQoh8RYLMc86hfD3KvdiIuASzvUsRQgghckxr7wKEfWm0WrSOLvYuQwghhHgkEmSEEEIIkW9JkBFCCCFEviVBRgghhBD5lgQZIYQQQuRbEmSEEEIIkW9JkBFCCCFEviV9ZJ4hWu2jfzP149z2WSTjkZWMx20yFlnl5/HIz7WL2zRKKWXvIoQQQgghHoUsLQkhhBAi35IgI4QQQoh8S4KMEEIIIfItCTJCCCGEyLckyAghhBAi35IgI4QQQoh8S4KMEEIIIfItCTJCCCGEyLckyAghhBAi35Ig85z7448/CA4Opk2bNixYsMDe5djVN998Q7t27WjXrh1Tp061dzl5xpQpUxg9erS9y7CrzZs306VLF4KCgpgwYYK9y7G7lStX2l4rU6ZMsXc54jknQeY5Fh0dzfTp0/n1119ZsWIFixcv5vz58/Yuyy5CQ0PZuXMny5cvZ8WKFZw4cYKNGzfauyy72717N8uXL7d3GXYVGRnJuHHjmD17NqtWreLkyZNs27bN3mXZTVpaGhMnTmT+/PmsXLmSAwcOEBoaau+yxHNMgsxzLDQ0lIYNG1K4cGEKFChA27ZtWb9+vb3LsgtPT09Gjx6No6Mjer2eihUrcvXqVXuXZVfx8fFMnz6dN954w96l2NXGjRsJDg6mePHi6PV6pk+fjq+vr73LshuLxYLVaiUtLQ2z2YzZbMbJycneZYnnmASZ51hMTAyenp62n728vIiOjrZjRfZTqVIl6tSpA0B4eDjr1q0jICDAvkXZ2SeffMLIkSNxd3e3dyl2FRERgcVi4Y033qBjx478+uuvFCpUyN5l2Y2rqyvvvPMOQUFBBAQEULJkSV588UV7lyWeYxJknmNWqxWN5vbX2Culsvz8PDp37hyDBg1i1KhRlCtXzt7l2M1vv/2Gj48P/v7+9i7F7iwWC7t372bSpEksXryYo0ePPtfLbadPn+b3339ny5Yt7NixA61Wy5w5c+xdlniOSZB5jhUvXpzY2Fjbz7GxsXh5edmxIvv6+++/GTBgAO+99x6dO3e2dzl2tXbtWnbt2kXHjh2ZOXMmmzdvZtKkSfYuyy6KFSuGv78/Hh4eODs78/LLL3P06FF7l2U3O3fuxN/fn6JFi+Lo6EiXLl3Yt2+fvcsSzzEJMs+xRo0asXv3bm7cuEFaWhp//vknzZo1s3dZdhEVFcWwYcOYNm0a7dq1s3c5djd37lxWr17NypUrGTFiBC1btmTMmDH2LssuWrRowc6dO0lMTMRisbBjxw5q1Khh77LspmrVqoSGhpKamopSis2bN1OrVi17lyWeYw72LkDYj7e3NyNHjuTVV1/FZDLRrVs3ateube+y7GLOnDkYDAYmT55su6xXr1707t3bjlWJvMDX15fXX3+dPn36YDKZaNy4MV27drV3WXbTpEkTTp48SZcuXdDr9dSqVYshQ4bYuyzxHNMopZS9ixBCCCGEeBSytCSEEEKIfEuCjBBCCCHyLQkyQgghhMi3JMgIIYQQIt+SICOEEEKIfEuCjBBCCCHyLQkyQgghhMi3JMgIIYQQIt/6fxgvGD0CkVlhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred)\n",
    "plt.title(f'Manually calculated derivative. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "688ca691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1316.4000244140625, weights = tensor([0.2286, 0.0396], dtype=torch.float16, requires_grad=True)\n",
      "step №1: loss = 1228.2037353515625, weights = tensor([0.4492, 0.0779], dtype=torch.float16, requires_grad=True)\n",
      "step №2: loss = 1146.08984375, weights = tensor([0.6621, 0.1150], dtype=torch.float16, requires_grad=True)\n",
      "step №3: loss = 1069.607177734375, weights = tensor([0.8677, 0.1509], dtype=torch.float16, requires_grad=True)\n",
      "step №4: loss = 998.3416137695312, weights = tensor([1.0664, 0.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №5: loss = 931.8660278320312, weights = tensor([1.2578, 0.2191], dtype=torch.float16, requires_grad=True)\n",
      "step №6: loss = 870.0784301757812, weights = tensor([1.4424, 0.2517], dtype=torch.float16, requires_grad=True)\n",
      "step №7: loss = 812.5750732421875, weights = tensor([1.6201, 0.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №8: loss = 759.1357421875, weights = tensor([1.7920, 0.3137], dtype=torch.float16, requires_grad=True)\n",
      "step №9: loss = 709.2653198242188, weights = tensor([1.9580, 0.3433], dtype=torch.float16, requires_grad=True)\n",
      "step №10: loss = 662.7821044921875, weights = tensor([2.1172, 0.3718], dtype=torch.float16, requires_grad=True)\n",
      "step №11: loss = 619.7615966796875, weights = tensor([2.2715, 0.3994], dtype=torch.float16, requires_grad=True)\n",
      "step №12: loss = 579.5235595703125, weights = tensor([2.4199, 0.4263], dtype=torch.float16, requires_grad=True)\n",
      "step №13: loss = 542.1541748046875, weights = tensor([2.5625, 0.4524], dtype=torch.float16, requires_grad=True)\n",
      "step №14: loss = 507.49761962890625, weights = tensor([2.7012, 0.4775], dtype=torch.float16, requires_grad=True)\n",
      "step №15: loss = 474.9783630371094, weights = tensor([2.8340, 0.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №16: loss = 444.90777587890625, weights = tensor([2.9629, 0.5259], dtype=torch.float16, requires_grad=True)\n",
      "step №17: loss = 416.73126220703125, weights = tensor([3.0879, 0.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №18: loss = 390.3766174316406, weights = tensor([3.2070, 0.5713], dtype=torch.float16, requires_grad=True)\n",
      "step №19: loss = 366.1141662597656, weights = tensor([3.3223, 0.5928], dtype=torch.float16, requires_grad=True)\n",
      "step №20: loss = 343.47210693359375, weights = tensor([3.4336, 0.6138], dtype=torch.float16, requires_grad=True)\n",
      "step №21: loss = 322.3515319824219, weights = tensor([3.5410, 0.6343], dtype=torch.float16, requires_grad=True)\n",
      "step №22: loss = 302.6742858886719, weights = tensor([3.6445, 0.6538], dtype=torch.float16, requires_grad=True)\n",
      "step №23: loss = 284.38055419921875, weights = tensor([3.7441, 0.6729], dtype=torch.float16, requires_grad=True)\n",
      "step №24: loss = 267.38116455078125, weights = tensor([3.8418, 0.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №25: loss = 251.30142211914062, weights = tensor([3.9355, 0.7095], dtype=torch.float16, requires_grad=True)\n",
      "step №26: loss = 236.4002227783203, weights = tensor([4.0273, 0.7271], dtype=torch.float16, requires_grad=True)\n",
      "step №27: loss = 222.3282470703125, weights = tensor([4.1133, 0.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №28: loss = 209.5992431640625, weights = tensor([4.1953, 0.7607], dtype=torch.float16, requires_grad=True)\n",
      "step №29: loss = 197.8590087890625, weights = tensor([4.2773, 0.7769], dtype=torch.float16, requires_grad=True)\n",
      "step №30: loss = 186.5400848388672, weights = tensor([4.3555, 0.7925], dtype=torch.float16, requires_grad=True)\n",
      "step №31: loss = 176.13232421875, weights = tensor([4.4297, 0.8076], dtype=torch.float16, requires_grad=True)\n",
      "step №32: loss = 166.580810546875, weights = tensor([4.5039, 0.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №33: loss = 157.37557983398438, weights = tensor([4.5742, 0.8364], dtype=torch.float16, requires_grad=True)\n",
      "step №34: loss = 148.9563751220703, weights = tensor([4.6406, 0.8501], dtype=torch.float16, requires_grad=True)\n",
      "step №35: loss = 141.2737579345703, weights = tensor([4.7070, 0.8638], dtype=torch.float16, requires_grad=True)\n",
      "step №36: loss = 133.8592071533203, weights = tensor([4.7695, 0.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №37: loss = 127.1189193725586, weights = tensor([4.8320, 0.8896], dtype=torch.float16, requires_grad=True)\n",
      "step №38: loss = 120.62664794921875, weights = tensor([4.8906, 0.9019], dtype=torch.float16, requires_grad=True)\n",
      "step №39: loss = 114.74942779541016, weights = tensor([4.9453, 0.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №40: loss = 109.43647766113281, weights = tensor([5.0000, 0.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №41: loss = 104.31565856933594, weights = tensor([5.0508, 0.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №42: loss = 99.70820617675781, weights = tensor([5.1016, 0.9487], dtype=torch.float16, requires_grad=True)\n",
      "step №43: loss = 95.26761627197266, weights = tensor([5.1523, 0.9595], dtype=torch.float16, requires_grad=True)\n",
      "step №44: loss = 90.99317169189453, weights = tensor([5.1992, 0.9702], dtype=torch.float16, requires_grad=True)\n",
      "step №45: loss = 87.17263793945312, weights = tensor([5.2461, 0.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №46: loss = 83.49485778808594, weights = tensor([5.2891, 0.9907], dtype=torch.float16, requires_grad=True)\n",
      "step №47: loss = 80.22734069824219, weights = tensor([5.3320, 1.0010], dtype=torch.float16, requires_grad=True)\n",
      "step №48: loss = 77.0732192993164, weights = tensor([5.3711, 1.0107], dtype=torch.float16, requires_grad=True)\n",
      "step №49: loss = 74.2972640991211, weights = tensor([5.4102, 1.0205], dtype=torch.float16, requires_grad=True)\n",
      "step №50: loss = 71.6153564453125, weights = tensor([5.4492, 1.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №51: loss = 69.0420150756836, weights = tensor([5.4844, 1.0381], dtype=torch.float16, requires_grad=True)\n",
      "step №52: loss = 66.7929916381836, weights = tensor([5.5195, 1.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №53: loss = 64.62013244628906, weights = tensor([5.5547, 1.0557], dtype=torch.float16, requires_grad=True)\n",
      "step №54: loss = 62.52344512939453, weights = tensor([5.5859, 1.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №55: loss = 60.71037673950195, weights = tensor([5.6172, 1.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №56: loss = 58.9710693359375, weights = tensor([5.6484, 1.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №57: loss = 57.29193115234375, weights = tensor([5.6797, 1.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №58: loss = 55.6729850769043, weights = tensor([5.7070, 1.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №59: loss = 54.293617248535156, weights = tensor([5.7344, 1.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №60: loss = 52.9608268737793, weights = tensor([5.7617, 1.1104], dtype=torch.float16, requires_grad=True)\n",
      "step №61: loss = 51.68627166748047, weights = tensor([5.7891, 1.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №62: loss = 50.457794189453125, weights = tensor([5.8125, 1.1240], dtype=torch.float16, requires_grad=True)\n",
      "step №63: loss = 49.4303092956543, weights = tensor([5.8359, 1.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №64: loss = 48.437110900878906, weights = tensor([5.8594, 1.1377], dtype=torch.float16, requires_grad=True)\n",
      "step №65: loss = 47.47820281982422, weights = tensor([5.8828, 1.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №66: loss = 46.55358123779297, weights = tensor([5.9023, 1.1514], dtype=torch.float16, requires_grad=True)\n",
      "step №67: loss = 45.797218322753906, weights = tensor([5.9219, 1.1572], dtype=torch.float16, requires_grad=True)\n",
      "step №68: loss = 45.07522964477539, weights = tensor([5.9414, 1.1631], dtype=torch.float16, requires_grad=True)\n",
      "step №69: loss = 44.377113342285156, weights = tensor([5.9609, 1.1689], dtype=torch.float16, requires_grad=True)\n",
      "step №70: loss = 43.7028694152832, weights = tensor([5.9805, 1.1748], dtype=torch.float16, requires_grad=True)\n",
      "step №71: loss = 43.0525016784668, weights = tensor([6.0000, 1.1807], dtype=torch.float16, requires_grad=True)\n",
      "step №72: loss = 42.42600631713867, weights = tensor([6.0156, 1.1865], dtype=torch.float16, requires_grad=True)\n",
      "step №73: loss = 41.930870056152344, weights = tensor([6.0312, 1.1924], dtype=torch.float16, requires_grad=True)\n",
      "step №74: loss = 41.451377868652344, weights = tensor([6.0469, 1.1982], dtype=torch.float16, requires_grad=True)\n",
      "step №75: loss = 40.987510681152344, weights = tensor([6.0625, 1.2041], dtype=torch.float16, requires_grad=True)\n",
      "step №76: loss = 40.539276123046875, weights = tensor([6.0781, 1.2100], dtype=torch.float16, requires_grad=True)\n",
      "step №77: loss = 40.1066780090332, weights = tensor([6.0938, 1.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №78: loss = 39.69822311401367, weights = tensor([6.1094, 1.2197], dtype=torch.float16, requires_grad=True)\n",
      "step №79: loss = 39.30512237548828, weights = tensor([6.1211, 1.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №80: loss = 39.01002502441406, weights = tensor([6.1328, 1.2295], dtype=torch.float16, requires_grad=True)\n",
      "step №81: loss = 38.723838806152344, weights = tensor([6.1445, 1.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №82: loss = 38.44655227661133, weights = tensor([6.1562, 1.2393], dtype=torch.float16, requires_grad=True)\n",
      "step №83: loss = 38.17817687988281, weights = tensor([6.1680, 1.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №84: loss = 37.91869354248047, weights = tensor([6.1797, 1.2490], dtype=torch.float16, requires_grad=True)\n",
      "step №85: loss = 37.668128967285156, weights = tensor([6.1914, 1.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №86: loss = 37.42646408081055, weights = tensor([6.2031, 1.2588], dtype=torch.float16, requires_grad=True)\n",
      "step №87: loss = 37.19370651245117, weights = tensor([6.2148, 1.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №88: loss = 36.969852447509766, weights = tensor([6.2227, 1.2686], dtype=torch.float16, requires_grad=True)\n",
      "step №89: loss = 36.81342315673828, weights = tensor([6.2305, 1.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №90: loss = 36.66120529174805, weights = tensor([6.2383, 1.2783], dtype=torch.float16, requires_grad=True)\n",
      "step №91: loss = 36.513206481933594, weights = tensor([6.2461, 1.2822], dtype=torch.float16, requires_grad=True)\n",
      "step №92: loss = 36.376461029052734, weights = tensor([6.2539, 1.2861], dtype=torch.float16, requires_grad=True)\n",
      "step №93: loss = 36.24378204345703, weights = tensor([6.2617, 1.2900], dtype=torch.float16, requires_grad=True)\n",
      "step №94: loss = 36.11516189575195, weights = tensor([6.2695, 1.2939], dtype=torch.float16, requires_grad=True)\n",
      "step №95: loss = 35.9906005859375, weights = tensor([6.2773, 1.2979], dtype=torch.float16, requires_grad=True)\n",
      "step №96: loss = 35.87009048461914, weights = tensor([6.2852, 1.3018], dtype=torch.float16, requires_grad=True)\n",
      "step №97: loss = 35.75364303588867, weights = tensor([6.2930, 1.3057], dtype=torch.float16, requires_grad=True)\n",
      "step №98: loss = 35.641258239746094, weights = tensor([6.3008, 1.3096], dtype=torch.float16, requires_grad=True)\n",
      "step №99: loss = 35.532936096191406, weights = tensor([6.3086, 1.3135], dtype=torch.float16, requires_grad=True)\n",
      "step №100: loss = 35.42865753173828, weights = tensor([6.3164, 1.3174], dtype=torch.float16, requires_grad=True)\n",
      "step №101: loss = 35.32844924926758, weights = tensor([6.3203, 1.3213], dtype=torch.float16, requires_grad=True)\n",
      "step №102: loss = 35.26721954345703, weights = tensor([6.3242, 1.3252], dtype=torch.float16, requires_grad=True)\n",
      "step №103: loss = 35.20716094970703, weights = tensor([6.3281, 1.3291], dtype=torch.float16, requires_grad=True)\n",
      "step №104: loss = 35.148277282714844, weights = tensor([6.3320, 1.3330], dtype=torch.float16, requires_grad=True)\n",
      "step №105: loss = 35.09056854248047, weights = tensor([6.3359, 1.3369], dtype=torch.float16, requires_grad=True)\n",
      "step №106: loss = 35.03403854370117, weights = tensor([6.3398, 1.3408], dtype=torch.float16, requires_grad=True)\n",
      "step №107: loss = 34.978675842285156, weights = tensor([6.3438, 1.3447], dtype=torch.float16, requires_grad=True)\n",
      "step №108: loss = 34.92449188232422, weights = tensor([6.3477, 1.3486], dtype=torch.float16, requires_grad=True)\n",
      "step №109: loss = 34.871482849121094, weights = tensor([6.3516, 1.3525], dtype=torch.float16, requires_grad=True)\n",
      "step №110: loss = 34.81965637207031, weights = tensor([6.3555, 1.3564], dtype=torch.float16, requires_grad=True)\n",
      "step №111: loss = 34.76899337768555, weights = tensor([6.3594, 1.3604], dtype=torch.float16, requires_grad=True)\n",
      "step №112: loss = 34.71950912475586, weights = tensor([6.3633, 1.3643], dtype=torch.float16, requires_grad=True)\n",
      "step №113: loss = 34.671199798583984, weights = tensor([6.3672, 1.3682], dtype=torch.float16, requires_grad=True)\n",
      "step №114: loss = 34.62407302856445, weights = tensor([6.3711, 1.3721], dtype=torch.float16, requires_grad=True)\n",
      "step №115: loss = 34.57810974121094, weights = tensor([6.3750, 1.3760], dtype=torch.float16, requires_grad=True)\n",
      "step №116: loss = 34.5333251953125, weights = tensor([6.3789, 1.3799], dtype=torch.float16, requires_grad=True)\n",
      "step №117: loss = 34.489715576171875, weights = tensor([6.3828, 1.3838], dtype=torch.float16, requires_grad=True)\n",
      "step №118: loss = 34.447288513183594, weights = tensor([6.3867, 1.3877], dtype=torch.float16, requires_grad=True)\n",
      "step №119: loss = 34.40602493286133, weights = tensor([6.3906, 1.3916], dtype=torch.float16, requires_grad=True)\n",
      "step №120: loss = 34.36594009399414, weights = tensor([6.3945, 1.3955], dtype=torch.float16, requires_grad=True)\n",
      "step №121: loss = 34.327030181884766, weights = tensor([6.3984, 1.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №122: loss = 34.29478073120117, weights = tensor([6.4023, 1.4014], dtype=torch.float16, requires_grad=True)\n",
      "step №123: loss = 34.26362228393555, weights = tensor([6.4062, 1.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №124: loss = 34.233558654785156, weights = tensor([6.4102, 1.4072], dtype=torch.float16, requires_grad=True)\n",
      "step №125: loss = 34.204593658447266, weights = tensor([6.4102, 1.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №126: loss = 34.188499450683594, weights = tensor([6.4102, 1.4131], dtype=torch.float16, requires_grad=True)\n",
      "step №127: loss = 34.17243194580078, weights = tensor([6.4102, 1.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №128: loss = 34.156375885009766, weights = tensor([6.4102, 1.4189], dtype=torch.float16, requires_grad=True)\n",
      "step №129: loss = 34.140342712402344, weights = tensor([6.4102, 1.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №130: loss = 34.12432098388672, weights = tensor([6.4102, 1.4248], dtype=torch.float16, requires_grad=True)\n",
      "step №131: loss = 34.10832214355469, weights = tensor([6.4102, 1.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №132: loss = 34.09233474731445, weights = tensor([6.4102, 1.4307], dtype=torch.float16, requires_grad=True)\n",
      "step №133: loss = 34.07637023925781, weights = tensor([6.4102, 1.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №134: loss = 34.06041717529297, weights = tensor([6.4102, 1.4365], dtype=torch.float16, requires_grad=True)\n",
      "step №135: loss = 34.04448699951172, weights = tensor([6.4102, 1.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №136: loss = 34.028568267822266, weights = tensor([6.4102, 1.4424], dtype=torch.float16, requires_grad=True)\n",
      "step №137: loss = 34.012672424316406, weights = tensor([6.4102, 1.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №138: loss = 33.996788024902344, weights = tensor([6.4102, 1.4482], dtype=torch.float16, requires_grad=True)\n",
      "step №139: loss = 33.98093032836914, weights = tensor([6.4102, 1.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №140: loss = 33.9650764465332, weights = tensor([6.4102, 1.4541], dtype=torch.float16, requires_grad=True)\n",
      "step №141: loss = 33.94925308227539, weights = tensor([6.4102, 1.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №142: loss = 33.933433532714844, weights = tensor([6.4102, 1.4600], dtype=torch.float16, requires_grad=True)\n",
      "step №143: loss = 33.917640686035156, weights = tensor([6.4102, 1.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №144: loss = 33.901859283447266, weights = tensor([6.4102, 1.4658], dtype=torch.float16, requires_grad=True)\n",
      "step №145: loss = 33.88610076904297, weights = tensor([6.4102, 1.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №146: loss = 33.87035369873047, weights = tensor([6.4102, 1.4717], dtype=torch.float16, requires_grad=True)\n",
      "step №147: loss = 33.85462951660156, weights = tensor([6.4102, 1.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №148: loss = 33.83891677856445, weights = tensor([6.4102, 1.4775], dtype=torch.float16, requires_grad=True)\n",
      "step №149: loss = 33.82322692871094, weights = tensor([6.4102, 1.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №150: loss = 33.80754852294922, weights = tensor([6.4102, 1.4834], dtype=torch.float16, requires_grad=True)\n",
      "step №151: loss = 33.791893005371094, weights = tensor([6.4102, 1.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №152: loss = 33.776248931884766, weights = tensor([6.4102, 1.4893], dtype=torch.float16, requires_grad=True)\n",
      "step №153: loss = 33.76062774658203, weights = tensor([6.4102, 1.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №154: loss = 33.745018005371094, weights = tensor([6.4102, 1.4951], dtype=torch.float16, requires_grad=True)\n",
      "step №155: loss = 33.729434967041016, weights = tensor([6.4102, 1.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №156: loss = 33.7138557434082, weights = tensor([6.4102, 1.5010], dtype=torch.float16, requires_grad=True)\n",
      "step №157: loss = 33.698307037353516, weights = tensor([6.4102, 1.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №158: loss = 33.682762145996094, weights = tensor([6.4102, 1.5068], dtype=torch.float16, requires_grad=True)\n",
      "step №159: loss = 33.66724395751953, weights = tensor([6.4102, 1.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №160: loss = 33.651737213134766, weights = tensor([6.4102, 1.5127], dtype=torch.float16, requires_grad=True)\n",
      "step №161: loss = 33.636253356933594, weights = tensor([6.4102, 1.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №162: loss = 33.62078094482422, weights = tensor([6.4102, 1.5186], dtype=torch.float16, requires_grad=True)\n",
      "step №163: loss = 33.60533142089844, weights = tensor([6.4102, 1.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №164: loss = 33.58989334106445, weights = tensor([6.4102, 1.5244], dtype=torch.float16, requires_grad=True)\n",
      "step №165: loss = 33.57447814941406, weights = tensor([6.4102, 1.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №166: loss = 33.55907440185547, weights = tensor([6.4102, 1.5303], dtype=torch.float16, requires_grad=True)\n",
      "step №167: loss = 33.54369354248047, weights = tensor([6.4102, 1.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №168: loss = 33.528324127197266, weights = tensor([6.4102, 1.5361], dtype=torch.float16, requires_grad=True)\n",
      "step №169: loss = 33.512977600097656, weights = tensor([6.4102, 1.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №170: loss = 33.497642517089844, weights = tensor([6.4102, 1.5420], dtype=torch.float16, requires_grad=True)\n",
      "step №171: loss = 33.48233413696289, weights = tensor([6.4102, 1.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №172: loss = 33.4670295715332, weights = tensor([6.4102, 1.5479], dtype=torch.float16, requires_grad=True)\n",
      "step №173: loss = 33.45175552368164, weights = tensor([6.4102, 1.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №174: loss = 33.436485290527344, weights = tensor([6.4102, 1.5537], dtype=torch.float16, requires_grad=True)\n",
      "step №175: loss = 33.421241760253906, weights = tensor([6.4102, 1.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №176: loss = 33.406009674072266, weights = tensor([6.4102, 1.5596], dtype=torch.float16, requires_grad=True)\n",
      "step №177: loss = 33.39080047607422, weights = tensor([6.4102, 1.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №178: loss = 33.37560272216797, weights = tensor([6.4102, 1.5654], dtype=torch.float16, requires_grad=True)\n",
      "step №179: loss = 33.36042785644531, weights = tensor([6.4102, 1.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №180: loss = 33.34526443481445, weights = tensor([6.4102, 1.5713], dtype=torch.float16, requires_grad=True)\n",
      "step №181: loss = 33.33012390136719, weights = tensor([6.4102, 1.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №182: loss = 33.31499481201172, weights = tensor([6.4102, 1.5771], dtype=torch.float16, requires_grad=True)\n",
      "step №183: loss = 33.299888610839844, weights = tensor([6.4102, 1.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №184: loss = 33.284793853759766, weights = tensor([6.4102, 1.5830], dtype=torch.float16, requires_grad=True)\n",
      "step №185: loss = 33.26972198486328, weights = tensor([6.4102, 1.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №186: loss = 33.254661560058594, weights = tensor([6.4102, 1.5889], dtype=torch.float16, requires_grad=True)\n",
      "step №187: loss = 33.239627838134766, weights = tensor([6.4102, 1.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №188: loss = 33.2245979309082, weights = tensor([6.4102, 1.5947], dtype=torch.float16, requires_grad=True)\n",
      "step №189: loss = 33.209598541259766, weights = tensor([6.4102, 1.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №190: loss = 33.194602966308594, weights = tensor([6.4102, 1.6006], dtype=torch.float16, requires_grad=True)\n",
      "step №191: loss = 33.17963409423828, weights = tensor([6.4102, 1.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №192: loss = 33.164676666259766, weights = tensor([6.4102, 1.6064], dtype=torch.float16, requires_grad=True)\n",
      "step №193: loss = 33.149742126464844, weights = tensor([6.4102, 1.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №194: loss = 33.13481903076172, weights = tensor([6.4102, 1.6123], dtype=torch.float16, requires_grad=True)\n",
      "step №195: loss = 33.11991882324219, weights = tensor([6.4102, 1.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №196: loss = 33.10503005981445, weights = tensor([6.4102, 1.6182], dtype=torch.float16, requires_grad=True)\n",
      "step №197: loss = 33.09016418457031, weights = tensor([6.4102, 1.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №198: loss = 33.07530975341797, weights = tensor([6.4102, 1.6240], dtype=torch.float16, requires_grad=True)\n",
      "step №199: loss = 33.06047821044922, weights = tensor([6.4102, 1.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №200: loss = 33.045658111572266, weights = tensor([6.4102, 1.6299], dtype=torch.float16, requires_grad=True)\n",
      "step №201: loss = 33.030860900878906, weights = tensor([6.4102, 1.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №202: loss = 33.016075134277344, weights = tensor([6.4102, 1.6357], dtype=torch.float16, requires_grad=True)\n",
      "step №203: loss = 33.00131607055664, weights = tensor([6.4102, 1.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №204: loss = 32.9865608215332, weights = tensor([6.4102, 1.6416], dtype=torch.float16, requires_grad=True)\n",
      "step №205: loss = 32.97183609008789, weights = tensor([6.4102, 1.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №206: loss = 32.957115173339844, weights = tensor([6.4102, 1.6475], dtype=torch.float16, requires_grad=True)\n",
      "step №207: loss = 32.942420959472656, weights = tensor([6.4102, 1.6504], dtype=torch.float16, requires_grad=True)\n",
      "step №208: loss = 32.927738189697266, weights = tensor([6.4102, 1.6533], dtype=torch.float16, requires_grad=True)\n",
      "step №209: loss = 32.91307830810547, weights = tensor([6.4102, 1.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №210: loss = 32.89842987060547, weights = tensor([6.4102, 1.6592], dtype=torch.float16, requires_grad=True)\n",
      "step №211: loss = 32.88380432128906, weights = tensor([6.4102, 1.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №212: loss = 32.86919021606445, weights = tensor([6.4102, 1.6650], dtype=torch.float16, requires_grad=True)\n",
      "step №213: loss = 32.85459899902344, weights = tensor([6.4102, 1.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №214: loss = 32.84001922607422, weights = tensor([6.4102, 1.6709], dtype=torch.float16, requires_grad=True)\n",
      "step №215: loss = 32.825462341308594, weights = tensor([6.4102, 1.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №216: loss = 32.810916900634766, weights = tensor([6.4102, 1.6768], dtype=torch.float16, requires_grad=True)\n",
      "step №217: loss = 32.79639434814453, weights = tensor([6.4102, 1.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №218: loss = 32.781883239746094, weights = tensor([6.4102, 1.6826], dtype=torch.float16, requires_grad=True)\n",
      "step №219: loss = 32.767398834228516, weights = tensor([6.4102, 1.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №220: loss = 32.7529182434082, weights = tensor([6.4102, 1.6885], dtype=torch.float16, requires_grad=True)\n",
      "step №221: loss = 32.738468170166016, weights = tensor([6.4102, 1.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №222: loss = 32.724021911621094, weights = tensor([6.4102, 1.6943], dtype=torch.float16, requires_grad=True)\n",
      "step №223: loss = 32.70960235595703, weights = tensor([6.4102, 1.6973], dtype=torch.float16, requires_grad=True)\n",
      "step №224: loss = 32.695194244384766, weights = tensor([6.4102, 1.7002], dtype=torch.float16, requires_grad=True)\n",
      "step №225: loss = 32.680809020996094, weights = tensor([6.4102, 1.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №226: loss = 32.66643524169922, weights = tensor([6.4102, 1.7061], dtype=torch.float16, requires_grad=True)\n",
      "step №227: loss = 32.65208435058594, weights = tensor([6.4102, 1.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №228: loss = 32.63774490356445, weights = tensor([6.4102, 1.7119], dtype=torch.float16, requires_grad=True)\n",
      "step №229: loss = 32.62342834472656, weights = tensor([6.4102, 1.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №230: loss = 32.60912322998047, weights = tensor([6.4102, 1.7178], dtype=torch.float16, requires_grad=True)\n",
      "step №231: loss = 32.59484100341797, weights = tensor([6.4102, 1.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №232: loss = 32.580570220947266, weights = tensor([6.4102, 1.7236], dtype=torch.float16, requires_grad=True)\n",
      "step №233: loss = 32.566322326660156, weights = tensor([6.4102, 1.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №234: loss = 32.552085876464844, weights = tensor([6.4102, 1.7295], dtype=torch.float16, requires_grad=True)\n",
      "step №235: loss = 32.53787612915039, weights = tensor([6.4102, 1.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №236: loss = 32.5236701965332, weights = tensor([6.4102, 1.7354], dtype=torch.float16, requires_grad=True)\n",
      "step №237: loss = 32.50949478149414, weights = tensor([6.4102, 1.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №238: loss = 32.495323181152344, weights = tensor([6.4102, 1.7412], dtype=torch.float16, requires_grad=True)\n",
      "step №239: loss = 32.481178283691406, weights = tensor([6.4102, 1.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №240: loss = 32.467044830322266, weights = tensor([6.4102, 1.7471], dtype=torch.float16, requires_grad=True)\n",
      "step №241: loss = 32.45293426513672, weights = tensor([6.4102, 1.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №242: loss = 32.43883514404297, weights = tensor([6.4102, 1.7529], dtype=torch.float16, requires_grad=True)\n",
      "step №243: loss = 32.42475891113281, weights = tensor([6.4102, 1.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №244: loss = 32.41069412231445, weights = tensor([6.4102, 1.7588], dtype=torch.float16, requires_grad=True)\n",
      "step №245: loss = 32.39665222167969, weights = tensor([6.4102, 1.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №246: loss = 32.38262176513672, weights = tensor([6.4102, 1.7646], dtype=torch.float16, requires_grad=True)\n",
      "step №247: loss = 32.368614196777344, weights = tensor([6.4102, 1.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №248: loss = 32.354618072509766, weights = tensor([6.4102, 1.7705], dtype=torch.float16, requires_grad=True)\n",
      "step №249: loss = 32.34064483642578, weights = tensor([6.4102, 1.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №250: loss = 32.326683044433594, weights = tensor([6.4102, 1.7764], dtype=torch.float16, requires_grad=True)\n",
      "step №251: loss = 32.312747955322266, weights = tensor([6.4102, 1.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №252: loss = 32.2988166809082, weights = tensor([6.4102, 1.7822], dtype=torch.float16, requires_grad=True)\n",
      "step №253: loss = 32.284915924072266, weights = tensor([6.4102, 1.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №254: loss = 32.271018981933594, weights = tensor([6.4102, 1.7881], dtype=torch.float16, requires_grad=True)\n",
      "step №255: loss = 32.25714874267578, weights = tensor([6.4102, 1.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №256: loss = 32.243289947509766, weights = tensor([6.4102, 1.7939], dtype=torch.float16, requires_grad=True)\n",
      "step №257: loss = 32.229454040527344, weights = tensor([6.4102, 1.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №258: loss = 32.21562957763672, weights = tensor([6.4102, 1.7998], dtype=torch.float16, requires_grad=True)\n",
      "step №259: loss = 32.20182800292969, weights = tensor([6.4102, 1.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №260: loss = 32.18803787231445, weights = tensor([6.4102, 1.8057], dtype=torch.float16, requires_grad=True)\n",
      "step №261: loss = 32.17427062988281, weights = tensor([6.4102, 1.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №262: loss = 32.16051483154297, weights = tensor([6.4102, 1.8115], dtype=torch.float16, requires_grad=True)\n",
      "step №263: loss = 32.14678192138672, weights = tensor([6.4102, 1.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №264: loss = 32.133060455322266, weights = tensor([6.4102, 1.8174], dtype=torch.float16, requires_grad=True)\n",
      "step №265: loss = 32.119361877441406, weights = tensor([6.4102, 1.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №266: loss = 32.105674743652344, weights = tensor([6.4102, 1.8232], dtype=torch.float16, requires_grad=True)\n",
      "step №267: loss = 32.09201431274414, weights = tensor([6.4102, 1.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №268: loss = 32.0783576965332, weights = tensor([6.4102, 1.8291], dtype=torch.float16, requires_grad=True)\n",
      "step №269: loss = 32.06473159790039, weights = tensor([6.4102, 1.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №270: loss = 32.051109313964844, weights = tensor([6.4102, 1.8350], dtype=torch.float16, requires_grad=True)\n",
      "step №271: loss = 32.037513732910156, weights = tensor([6.4102, 1.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №272: loss = 32.023929595947266, weights = tensor([6.4102, 1.8408], dtype=torch.float16, requires_grad=True)\n",
      "step №273: loss = 32.01036834716797, weights = tensor([6.4102, 1.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №274: loss = 31.9968204498291, weights = tensor([6.4102, 1.8467], dtype=torch.float16, requires_grad=True)\n",
      "step №275: loss = 31.983291625976562, weights = tensor([6.4102, 1.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №276: loss = 31.969776153564453, weights = tensor([6.4102, 1.8525], dtype=torch.float16, requires_grad=True)\n",
      "step №277: loss = 31.956283569335938, weights = tensor([6.4102, 1.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №278: loss = 31.94280433654785, weights = tensor([6.4102, 1.8584], dtype=torch.float16, requires_grad=True)\n",
      "step №279: loss = 31.92934226989746, weights = tensor([6.4102, 1.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №280: loss = 31.915897369384766, weights = tensor([6.4102, 1.8643], dtype=torch.float16, requires_grad=True)\n",
      "step №281: loss = 31.9024715423584, weights = tensor([6.4102, 1.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №282: loss = 31.889062881469727, weights = tensor([6.4102, 1.8701], dtype=torch.float16, requires_grad=True)\n",
      "step №283: loss = 31.875675201416016, weights = tensor([6.4102, 1.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №284: loss = 31.862293243408203, weights = tensor([6.4102, 1.8760], dtype=torch.float16, requires_grad=True)\n",
      "step №285: loss = 31.848941802978516, weights = tensor([6.4102, 1.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №286: loss = 31.835596084594727, weights = tensor([6.4102, 1.8818], dtype=torch.float16, requires_grad=True)\n",
      "step №287: loss = 31.822275161743164, weights = tensor([6.4102, 1.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №288: loss = 31.808963775634766, weights = tensor([6.4102, 1.8877], dtype=torch.float16, requires_grad=True)\n",
      "step №289: loss = 31.795679092407227, weights = tensor([6.4102, 1.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №290: loss = 31.78240394592285, weights = tensor([6.4102, 1.8936], dtype=torch.float16, requires_grad=True)\n",
      "step №291: loss = 31.769149780273438, weights = tensor([6.4102, 1.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №292: loss = 31.755908966064453, weights = tensor([6.4102, 1.8994], dtype=torch.float16, requires_grad=True)\n",
      "step №293: loss = 31.742691040039062, weights = tensor([6.4102, 1.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №294: loss = 31.7294864654541, weights = tensor([6.4102, 1.9053], dtype=torch.float16, requires_grad=True)\n",
      "step №295: loss = 31.716299057006836, weights = tensor([6.4102, 1.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №296: loss = 31.703128814697266, weights = tensor([6.4102, 1.9111], dtype=torch.float16, requires_grad=True)\n",
      "step №297: loss = 31.689977645874023, weights = tensor([6.4102, 1.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №298: loss = 31.676843643188477, weights = tensor([6.4102, 1.9170], dtype=torch.float16, requires_grad=True)\n",
      "step №299: loss = 31.66373062133789, weights = tensor([6.4102, 1.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №300: loss = 31.650623321533203, weights = tensor([6.4102, 1.9229], dtype=torch.float16, requires_grad=True)\n",
      "step №301: loss = 31.63754653930664, weights = tensor([6.4102, 1.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №302: loss = 31.624475479125977, weights = tensor([6.4102, 1.9287], dtype=torch.float16, requires_grad=True)\n",
      "step №303: loss = 31.61142921447754, weights = tensor([6.4102, 1.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №304: loss = 31.598392486572266, weights = tensor([6.4102, 1.9346], dtype=torch.float16, requires_grad=True)\n",
      "step №305: loss = 31.58538246154785, weights = tensor([6.4102, 1.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №306: loss = 31.5723819732666, weights = tensor([6.4102, 1.9404], dtype=torch.float16, requires_grad=True)\n",
      "step №307: loss = 31.559402465820312, weights = tensor([6.4102, 1.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №308: loss = 31.546436309814453, weights = tensor([6.4102, 1.9463], dtype=torch.float16, requires_grad=True)\n",
      "step №309: loss = 31.533493041992188, weights = tensor([6.4102, 1.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №310: loss = 31.52056312561035, weights = tensor([6.4102, 1.9521], dtype=torch.float16, requires_grad=True)\n",
      "step №311: loss = 31.50765037536621, weights = tensor([6.4102, 1.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №312: loss = 31.494754791259766, weights = tensor([6.4102, 1.9580], dtype=torch.float16, requires_grad=True)\n",
      "step №313: loss = 31.48187828063965, weights = tensor([6.4102, 1.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №314: loss = 31.469018936157227, weights = tensor([6.4102, 1.9639], dtype=torch.float16, requires_grad=True)\n",
      "step №315: loss = 31.456180572509766, weights = tensor([6.4102, 1.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №316: loss = 31.443347930908203, weights = tensor([6.4102, 1.9697], dtype=torch.float16, requires_grad=True)\n",
      "step №317: loss = 31.430545806884766, weights = tensor([6.4102, 1.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №318: loss = 31.417749404907227, weights = tensor([6.4102, 1.9756], dtype=torch.float16, requires_grad=True)\n",
      "step №319: loss = 31.404977798461914, weights = tensor([6.4102, 1.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №320: loss = 31.392215728759766, weights = tensor([6.4102, 1.9814], dtype=torch.float16, requires_grad=True)\n",
      "step №321: loss = 31.379480361938477, weights = tensor([6.4102, 1.9844], dtype=torch.float16, requires_grad=True)\n",
      "step №322: loss = 31.36675453186035, weights = tensor([6.4102, 1.9873], dtype=torch.float16, requires_grad=True)\n",
      "step №323: loss = 31.354049682617188, weights = tensor([6.4102, 1.9902], dtype=torch.float16, requires_grad=True)\n",
      "step №324: loss = 31.341358184814453, weights = tensor([6.4102, 1.9932], dtype=torch.float16, requires_grad=True)\n",
      "step №325: loss = 31.328689575195312, weights = tensor([6.4102, 1.9961], dtype=torch.float16, requires_grad=True)\n",
      "step №326: loss = 31.3160343170166, weights = tensor([6.4102, 1.9990], dtype=torch.float16, requires_grad=True)\n",
      "step №327: loss = 31.303396224975586, weights = tensor([6.4102, 2.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №328: loss = 31.290775299072266, weights = tensor([6.4102, 2.0039], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №329: loss = 31.282373428344727, weights = tensor([6.4102, 2.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №330: loss = 31.273975372314453, weights = tensor([6.4102, 2.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №331: loss = 31.265588760375977, weights = tensor([6.4102, 2.0098], dtype=torch.float16, requires_grad=True)\n",
      "step №332: loss = 31.257205963134766, weights = tensor([6.4102, 2.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №333: loss = 31.24883460998535, weights = tensor([6.4102, 2.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №334: loss = 31.240467071533203, weights = tensor([6.4102, 2.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №335: loss = 31.23211097717285, weights = tensor([6.4102, 2.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №336: loss = 31.223758697509766, weights = tensor([6.4102, 2.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №337: loss = 31.215417861938477, weights = tensor([6.4102, 2.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №338: loss = 31.207080841064453, weights = tensor([6.4102, 2.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №339: loss = 31.198755264282227, weights = tensor([6.4102, 2.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №340: loss = 31.190433502197266, weights = tensor([6.4102, 2.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №341: loss = 31.1821231842041, weights = tensor([6.4102, 2.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №342: loss = 31.173816680908203, weights = tensor([6.4102, 2.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №343: loss = 31.1655216217041, weights = tensor([6.4102, 2.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №344: loss = 31.157230377197266, weights = tensor([6.4102, 2.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №345: loss = 31.148950576782227, weights = tensor([6.4102, 2.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №346: loss = 31.140674591064453, weights = tensor([6.4102, 2.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №347: loss = 31.132410049438477, weights = tensor([6.4102, 2.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №348: loss = 31.124149322509766, weights = tensor([6.4102, 2.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №349: loss = 31.11590003967285, weights = tensor([6.4102, 2.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №350: loss = 31.107654571533203, weights = tensor([6.4102, 2.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №351: loss = 31.09942054748535, weights = tensor([6.4102, 2.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №352: loss = 31.091190338134766, weights = tensor([6.4102, 2.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №353: loss = 31.082971572875977, weights = tensor([6.4102, 2.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №354: loss = 31.074756622314453, weights = tensor([6.4102, 2.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №355: loss = 31.066553115844727, weights = tensor([6.4102, 2.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №356: loss = 31.058353424072266, weights = tensor([6.4102, 2.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №357: loss = 31.0501651763916, weights = tensor([6.4102, 2.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №358: loss = 31.041980743408203, weights = tensor([6.4102, 2.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №359: loss = 31.0338077545166, weights = tensor([6.4102, 2.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №360: loss = 31.025638580322266, weights = tensor([6.4102, 2.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №361: loss = 31.017480850219727, weights = tensor([6.4102, 2.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №362: loss = 31.009326934814453, weights = tensor([6.4102, 2.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №363: loss = 31.001184463500977, weights = tensor([6.4102, 2.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №364: loss = 30.993045806884766, weights = tensor([6.4102, 2.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №365: loss = 30.98491859436035, weights = tensor([6.4102, 2.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №366: loss = 30.976795196533203, weights = tensor([6.4102, 2.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №367: loss = 30.96868324279785, weights = tensor([6.4102, 2.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №368: loss = 30.960575103759766, weights = tensor([6.4102, 2.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №369: loss = 30.952478408813477, weights = tensor([6.4102, 2.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №370: loss = 30.944385528564453, weights = tensor([6.4102, 2.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №371: loss = 30.936304092407227, weights = tensor([6.4102, 2.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №372: loss = 30.928226470947266, weights = tensor([6.4102, 2.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №373: loss = 30.9201602935791, weights = tensor([6.4102, 2.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №374: loss = 30.912097930908203, weights = tensor([6.4102, 2.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №375: loss = 30.9040470123291, weights = tensor([6.4102, 2.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №376: loss = 30.895999908447266, weights = tensor([6.4102, 2.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №377: loss = 30.887964248657227, weights = tensor([6.4102, 2.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №378: loss = 30.879932403564453, weights = tensor([6.4102, 2.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №379: loss = 30.871912002563477, weights = tensor([6.4102, 2.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №380: loss = 30.863895416259766, weights = tensor([6.4102, 2.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №381: loss = 30.85589027404785, weights = tensor([6.4102, 2.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №382: loss = 30.847888946533203, weights = tensor([6.4102, 2.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №383: loss = 30.83989906311035, weights = tensor([6.4102, 2.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №384: loss = 30.831912994384766, weights = tensor([6.4102, 2.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №385: loss = 30.823938369750977, weights = tensor([6.4102, 2.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №386: loss = 30.815967559814453, weights = tensor([6.4102, 2.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №387: loss = 30.808008193969727, weights = tensor([6.4102, 2.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №388: loss = 30.800052642822266, weights = tensor([6.4102, 2.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №389: loss = 30.7921085357666, weights = tensor([6.4062, 2.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №390: loss = 30.771764755249023, weights = tensor([6.4062, 2.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №391: loss = 30.763769149780273, weights = tensor([6.4062, 2.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №392: loss = 30.755773544311523, weights = tensor([6.4062, 2.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №393: loss = 30.747791290283203, weights = tensor([6.4062, 2.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №394: loss = 30.73981285095215, weights = tensor([6.4062, 2.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №395: loss = 30.731847763061523, weights = tensor([6.4062, 2.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №396: loss = 30.7238826751709, weights = tensor([6.4062, 2.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №397: loss = 30.715930938720703, weights = tensor([6.4062, 2.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №398: loss = 30.707983016967773, weights = tensor([6.4062, 2.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №399: loss = 30.700048446655273, weights = tensor([6.4062, 2.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №400: loss = 30.692113876342773, weights = tensor([6.4062, 2.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №401: loss = 30.684192657470703, weights = tensor([6.4023, 2.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №402: loss = 30.66391944885254, weights = tensor([6.4023, 2.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №403: loss = 30.65594482421875, weights = tensor([6.4023, 2.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №404: loss = 30.647974014282227, weights = tensor([6.4023, 2.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №405: loss = 30.6400146484375, weights = tensor([6.4023, 2.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №406: loss = 30.63205909729004, weights = tensor([6.4023, 2.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №407: loss = 30.624114990234375, weights = tensor([6.4023, 2.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №408: loss = 30.616174697875977, weights = tensor([6.4023, 2.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №409: loss = 30.608245849609375, weights = tensor([6.4023, 2.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №410: loss = 30.60032081604004, weights = tensor([6.4023, 2.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №411: loss = 30.5924072265625, weights = tensor([6.4023, 2.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №412: loss = 30.584497451782227, weights = tensor([6.4023, 2.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №413: loss = 30.57659912109375, weights = tensor([6.4023, 2.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №414: loss = 30.56870460510254, weights = tensor([6.3984, 2.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №415: loss = 30.548442840576172, weights = tensor([6.3984, 2.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №416: loss = 30.54049301147461, weights = tensor([6.3984, 2.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №417: loss = 30.532556533813477, weights = tensor([6.3984, 2.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №418: loss = 30.52462387084961, weights = tensor([6.3984, 2.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №419: loss = 30.516704559326172, weights = tensor([6.3984, 2.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №420: loss = 30.508785247802734, weights = tensor([6.3984, 2.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №421: loss = 30.500879287719727, weights = tensor([6.3984, 2.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №422: loss = 30.492977142333984, weights = tensor([6.3984, 2.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №423: loss = 30.485088348388672, weights = tensor([6.3984, 2.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №424: loss = 30.47719955444336, weights = tensor([6.3984, 2.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №425: loss = 30.469324111938477, weights = tensor([6.3984, 2.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №426: loss = 30.46145248413086, weights = tensor([6.3984, 2.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №427: loss = 30.453594207763672, weights = tensor([6.3945, 2.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №428: loss = 30.433334350585938, weights = tensor([6.3945, 2.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №429: loss = 30.4254207611084, weights = tensor([6.3945, 2.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №430: loss = 30.417510986328125, weights = tensor([6.3945, 2.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №431: loss = 30.40961265563965, weights = tensor([6.3945, 2.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №432: loss = 30.401718139648438, weights = tensor([6.3945, 2.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №433: loss = 30.393835067749023, weights = tensor([6.3945, 2.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №434: loss = 30.385955810546875, weights = tensor([6.3945, 2.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №435: loss = 30.378087997436523, weights = tensor([6.3945, 2.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №436: loss = 30.370223999023438, weights = tensor([6.3945, 2.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №437: loss = 30.36237144470215, weights = tensor([6.3945, 2.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №438: loss = 30.354522705078125, weights = tensor([6.3945, 2.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №439: loss = 30.3466854095459, weights = tensor([6.3906, 2.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №440: loss = 30.326496124267578, weights = tensor([6.3906, 2.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №441: loss = 30.318603515625, weights = tensor([6.3906, 2.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №442: loss = 30.310718536376953, weights = tensor([6.3906, 2.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №443: loss = 30.302845001220703, weights = tensor([6.3906, 2.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №444: loss = 30.294971466064453, weights = tensor([6.3906, 2.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №445: loss = 30.287109375, weights = tensor([6.3906, 2.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №446: loss = 30.279254913330078, weights = tensor([6.3906, 2.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №447: loss = 30.271411895751953, weights = tensor([6.3906, 2.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №448: loss = 30.263568878173828, weights = tensor([6.3906, 2.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №449: loss = 30.2557373046875, weights = tensor([6.3906, 2.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №450: loss = 30.247913360595703, weights = tensor([6.3906, 2.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №451: loss = 30.240100860595703, weights = tensor([6.3906, 2.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №452: loss = 30.232288360595703, weights = tensor([6.3867, 2.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №453: loss = 30.212108612060547, weights = tensor([6.3867, 2.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №454: loss = 30.20424461364746, weights = tensor([6.3867, 2.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №455: loss = 30.196392059326172, weights = tensor([6.3867, 2.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №456: loss = 30.18854331970215, weights = tensor([6.3867, 2.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №457: loss = 30.180706024169922, weights = tensor([6.3867, 2.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №458: loss = 30.17287254333496, weights = tensor([6.3867, 2.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №459: loss = 30.165050506591797, weights = tensor([6.3867, 2.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №460: loss = 30.1572322845459, weights = tensor([6.3867, 2.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №461: loss = 30.149425506591797, weights = tensor([6.3867, 2.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №462: loss = 30.14162254333496, weights = tensor([6.3867, 2.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №463: loss = 30.133831024169922, weights = tensor([6.3867, 2.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №464: loss = 30.12604331970215, weights = tensor([6.3867, 2.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №465: loss = 30.118267059326172, weights = tensor([6.3828, 2.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №466: loss = 30.098093032836914, weights = tensor([6.3828, 2.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №467: loss = 30.09026527404785, weights = tensor([6.3828, 2.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №468: loss = 30.08243751525879, weights = tensor([6.3828, 2.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №469: loss = 30.074621200561523, weights = tensor([6.3828, 2.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №470: loss = 30.06681251525879, weights = tensor([6.3828, 2.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №471: loss = 30.05901527404785, weights = tensor([6.3828, 2.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №472: loss = 30.051218032836914, weights = tensor([6.3828, 2.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №473: loss = 30.043432235717773, weights = tensor([6.3828, 2.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №474: loss = 30.035654067993164, weights = tensor([6.3828, 2.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №475: loss = 30.02788734436035, weights = tensor([6.3828, 2.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №476: loss = 30.02012062072754, weights = tensor([6.3828, 2.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №477: loss = 30.012365341186523, weights = tensor([6.3789, 2.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №478: loss = 29.992259979248047, weights = tensor([6.3789, 2.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №479: loss = 29.984455108642578, weights = tensor([6.3789, 2.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №480: loss = 29.97665023803711, weights = tensor([6.3789, 2.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №481: loss = 29.968860626220703, weights = tensor([6.3789, 2.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №482: loss = 29.961071014404297, weights = tensor([6.3789, 2.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №483: loss = 29.953296661376953, weights = tensor([6.3789, 2.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №484: loss = 29.94552230834961, weights = tensor([6.3789, 2.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №485: loss = 29.937763214111328, weights = tensor([6.3789, 2.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №486: loss = 29.930004119873047, weights = tensor([6.3789, 2.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №487: loss = 29.922260284423828, weights = tensor([6.3789, 2.3145], dtype=torch.float16, requires_grad=True)\n",
      "step №488: loss = 29.91451644897461, weights = tensor([6.3789, 2.3164], dtype=torch.float16, requires_grad=True)\n",
      "step №489: loss = 29.906787872314453, weights = tensor([6.3789, 2.3184], dtype=torch.float16, requires_grad=True)\n",
      "step №490: loss = 29.899059295654297, weights = tensor([6.3750, 2.3203], dtype=torch.float16, requires_grad=True)\n",
      "step №491: loss = 29.87896728515625, weights = tensor([6.3750, 2.3223], dtype=torch.float16, requires_grad=True)\n",
      "step №492: loss = 29.871185302734375, weights = tensor([6.3750, 2.3242], dtype=torch.float16, requires_grad=True)\n",
      "step №493: loss = 29.863414764404297, weights = tensor([6.3750, 2.3262], dtype=torch.float16, requires_grad=True)\n",
      "step №494: loss = 29.85565185546875, weights = tensor([6.3750, 2.3281], dtype=torch.float16, requires_grad=True)\n",
      "step №495: loss = 29.847900390625, weights = tensor([6.3750, 2.3301], dtype=torch.float16, requires_grad=True)\n",
      "step №496: loss = 29.84014892578125, weights = tensor([6.3750, 2.3320], dtype=torch.float16, requires_grad=True)\n",
      "step №497: loss = 29.832408905029297, weights = tensor([6.3750, 2.3340], dtype=torch.float16, requires_grad=True)\n",
      "step №498: loss = 29.824676513671875, weights = tensor([6.3750, 2.3359], dtype=torch.float16, requires_grad=True)\n",
      "step №499: loss = 29.81695556640625, weights = tensor([6.3750, 2.3379], dtype=torch.float16, requires_grad=True)\n",
      "step №500: loss = 29.809234619140625, weights = tensor([6.3750, 2.3398], dtype=torch.float16, requires_grad=True)\n",
      "step №501: loss = 29.801525115966797, weights = tensor([6.3750, 2.3418], dtype=torch.float16, requires_grad=True)\n",
      "step №502: loss = 29.7938232421875, weights = tensor([6.3750, 2.3438], dtype=torch.float16, requires_grad=True)\n",
      "step №503: loss = 29.7861328125, weights = tensor([6.3711, 2.3457], dtype=torch.float16, requires_grad=True)\n",
      "step №504: loss = 29.766040802001953, weights = tensor([6.3711, 2.3477], dtype=torch.float16, requires_grad=True)\n",
      "step №505: loss = 29.7582950592041, weights = tensor([6.3711, 2.3496], dtype=torch.float16, requires_grad=True)\n",
      "step №506: loss = 29.750553131103516, weights = tensor([6.3711, 2.3516], dtype=torch.float16, requires_grad=True)\n",
      "step №507: loss = 29.742822647094727, weights = tensor([6.3711, 2.3535], dtype=torch.float16, requires_grad=True)\n",
      "step №508: loss = 29.735095977783203, weights = tensor([6.3711, 2.3555], dtype=torch.float16, requires_grad=True)\n",
      "step №509: loss = 29.727380752563477, weights = tensor([6.3711, 2.3574], dtype=torch.float16, requires_grad=True)\n",
      "step №510: loss = 29.719669342041016, weights = tensor([6.3711, 2.3594], dtype=torch.float16, requires_grad=True)\n",
      "step №511: loss = 29.71196937561035, weights = tensor([6.3711, 2.3613], dtype=torch.float16, requires_grad=True)\n",
      "step №512: loss = 29.704273223876953, weights = tensor([6.3711, 2.3633], dtype=torch.float16, requires_grad=True)\n",
      "step №513: loss = 29.69658851623535, weights = tensor([6.3711, 2.3652], dtype=torch.float16, requires_grad=True)\n",
      "step №514: loss = 29.688907623291016, weights = tensor([6.3711, 2.3672], dtype=torch.float16, requires_grad=True)\n",
      "step №515: loss = 29.681238174438477, weights = tensor([6.3672, 2.3691], dtype=torch.float16, requires_grad=True)\n",
      "step №516: loss = 29.66121482849121, weights = tensor([6.3672, 2.3711], dtype=torch.float16, requires_grad=True)\n",
      "step №517: loss = 29.653491973876953, weights = tensor([6.3672, 2.3730], dtype=torch.float16, requires_grad=True)\n",
      "step №518: loss = 29.64577293395996, weights = tensor([6.3672, 2.3750], dtype=torch.float16, requires_grad=True)\n",
      "step №519: loss = 29.6380672454834, weights = tensor([6.3672, 2.3770], dtype=torch.float16, requires_grad=True)\n",
      "step №520: loss = 29.630361557006836, weights = tensor([6.3672, 2.3789], dtype=torch.float16, requires_grad=True)\n",
      "step №521: loss = 29.622669219970703, weights = tensor([6.3672, 2.3809], dtype=torch.float16, requires_grad=True)\n",
      "step №522: loss = 29.614980697631836, weights = tensor([6.3672, 2.3828], dtype=torch.float16, requires_grad=True)\n",
      "step №523: loss = 29.6073055267334, weights = tensor([6.3672, 2.3848], dtype=torch.float16, requires_grad=True)\n",
      "step №524: loss = 29.59963035583496, weights = tensor([6.3672, 2.3867], dtype=torch.float16, requires_grad=True)\n",
      "step №525: loss = 29.591968536376953, weights = tensor([6.3672, 2.3887], dtype=torch.float16, requires_grad=True)\n",
      "step №526: loss = 29.58431053161621, weights = tensor([6.3672, 2.3906], dtype=torch.float16, requires_grad=True)\n",
      "step №527: loss = 29.5766658782959, weights = tensor([6.3672, 2.3926], dtype=torch.float16, requires_grad=True)\n",
      "step №528: loss = 29.569021224975586, weights = tensor([6.3633, 2.3945], dtype=torch.float16, requires_grad=True)\n",
      "step №529: loss = 29.54901123046875, weights = tensor([6.3633, 2.3965], dtype=torch.float16, requires_grad=True)\n",
      "step №530: loss = 29.54131507873535, weights = tensor([6.3633, 2.3984], dtype=torch.float16, requires_grad=True)\n",
      "step №531: loss = 29.53363037109375, weights = tensor([6.3633, 2.4004], dtype=torch.float16, requires_grad=True)\n",
      "step №532: loss = 29.525949478149414, weights = tensor([6.3633, 2.4023], dtype=torch.float16, requires_grad=True)\n",
      "step №533: loss = 29.518280029296875, weights = tensor([6.3633, 2.4043], dtype=torch.float16, requires_grad=True)\n",
      "step №534: loss = 29.5106143951416, weights = tensor([6.3633, 2.4062], dtype=torch.float16, requires_grad=True)\n",
      "step №535: loss = 29.502960205078125, weights = tensor([6.3633, 2.4082], dtype=torch.float16, requires_grad=True)\n",
      "step №536: loss = 29.495309829711914, weights = tensor([6.3633, 2.4102], dtype=torch.float16, requires_grad=True)\n",
      "step №537: loss = 29.4876708984375, weights = tensor([6.3633, 2.4121], dtype=torch.float16, requires_grad=True)\n",
      "step №538: loss = 29.48003578186035, weights = tensor([6.3633, 2.4141], dtype=torch.float16, requires_grad=True)\n",
      "step №539: loss = 29.472412109375, weights = tensor([6.3633, 2.4160], dtype=torch.float16, requires_grad=True)\n",
      "step №540: loss = 29.464792251586914, weights = tensor([6.3633, 2.4180], dtype=torch.float16, requires_grad=True)\n",
      "step №541: loss = 29.457183837890625, weights = tensor([6.3594, 2.4199], dtype=torch.float16, requires_grad=True)\n",
      "step №542: loss = 29.437175750732422, weights = tensor([6.3594, 2.4219], dtype=torch.float16, requires_grad=True)\n",
      "step №543: loss = 29.429515838623047, weights = tensor([6.3594, 2.4238], dtype=torch.float16, requires_grad=True)\n",
      "step №544: loss = 29.421855926513672, weights = tensor([6.3594, 2.4258], dtype=torch.float16, requires_grad=True)\n",
      "step №545: loss = 29.414209365844727, weights = tensor([6.3594, 2.4277], dtype=torch.float16, requires_grad=True)\n",
      "step №546: loss = 29.406566619873047, weights = tensor([6.3594, 2.4297], dtype=torch.float16, requires_grad=True)\n",
      "step №547: loss = 29.398937225341797, weights = tensor([6.3594, 2.4316], dtype=torch.float16, requires_grad=True)\n",
      "step №548: loss = 29.391307830810547, weights = tensor([6.3594, 2.4336], dtype=torch.float16, requires_grad=True)\n",
      "step №549: loss = 29.383691787719727, weights = tensor([6.3594, 2.4355], dtype=torch.float16, requires_grad=True)\n",
      "step №550: loss = 29.376079559326172, weights = tensor([6.3594, 2.4375], dtype=torch.float16, requires_grad=True)\n",
      "step №551: loss = 29.368480682373047, weights = tensor([6.3594, 2.4395], dtype=torch.float16, requires_grad=True)\n",
      "step №552: loss = 29.360881805419922, weights = tensor([6.3594, 2.4414], dtype=torch.float16, requires_grad=True)\n",
      "step №553: loss = 29.353296279907227, weights = tensor([6.3555, 2.4434], dtype=torch.float16, requires_grad=True)\n",
      "step №554: loss = 29.333358764648438, weights = tensor([6.3555, 2.4453], dtype=torch.float16, requires_grad=True)\n",
      "step №555: loss = 29.325719833374023, weights = tensor([6.3555, 2.4473], dtype=torch.float16, requires_grad=True)\n",
      "step №556: loss = 29.318084716796875, weights = tensor([6.3555, 2.4492], dtype=torch.float16, requires_grad=True)\n",
      "step №557: loss = 29.310461044311523, weights = tensor([6.3555, 2.4512], dtype=torch.float16, requires_grad=True)\n",
      "step №558: loss = 29.302841186523438, weights = tensor([6.3555, 2.4531], dtype=torch.float16, requires_grad=True)\n",
      "step №559: loss = 29.29523277282715, weights = tensor([6.3555, 2.4551], dtype=torch.float16, requires_grad=True)\n",
      "step №560: loss = 29.287628173828125, weights = tensor([6.3555, 2.4570], dtype=torch.float16, requires_grad=True)\n",
      "step №561: loss = 29.2800350189209, weights = tensor([6.3555, 2.4590], dtype=torch.float16, requires_grad=True)\n",
      "step №562: loss = 29.272445678710938, weights = tensor([6.3555, 2.4609], dtype=torch.float16, requires_grad=True)\n",
      "step №563: loss = 29.264867782592773, weights = tensor([6.3555, 2.4629], dtype=torch.float16, requires_grad=True)\n",
      "step №564: loss = 29.257293701171875, weights = tensor([6.3555, 2.4648], dtype=torch.float16, requires_grad=True)\n",
      "step №565: loss = 29.249731063842773, weights = tensor([6.3555, 2.4668], dtype=torch.float16, requires_grad=True)\n",
      "step №566: loss = 29.242172241210938, weights = tensor([6.3516, 2.4688], dtype=torch.float16, requires_grad=True)\n",
      "step №567: loss = 29.222248077392578, weights = tensor([6.3516, 2.4707], dtype=torch.float16, requires_grad=True)\n",
      "step №568: loss = 29.21463394165039, weights = tensor([6.3516, 2.4727], dtype=torch.float16, requires_grad=True)\n",
      "step №569: loss = 29.20703125, weights = tensor([6.3516, 2.4746], dtype=torch.float16, requires_grad=True)\n",
      "step №570: loss = 29.19943618774414, weights = tensor([6.3516, 2.4766], dtype=torch.float16, requires_grad=True)\n",
      "step №571: loss = 29.191852569580078, weights = tensor([6.3516, 2.4785], dtype=torch.float16, requires_grad=True)\n",
      "step №572: loss = 29.184268951416016, weights = tensor([6.3516, 2.4805], dtype=torch.float16, requires_grad=True)\n",
      "step №573: loss = 29.17669677734375, weights = tensor([6.3516, 2.4824], dtype=torch.float16, requires_grad=True)\n",
      "step №574: loss = 29.169132232666016, weights = tensor([6.3516, 2.4844], dtype=torch.float16, requires_grad=True)\n",
      "step №575: loss = 29.161579132080078, weights = tensor([6.3516, 2.4863], dtype=torch.float16, requires_grad=True)\n",
      "step №576: loss = 29.15402603149414, weights = tensor([6.3516, 2.4883], dtype=torch.float16, requires_grad=True)\n",
      "step №577: loss = 29.146484375, weights = tensor([6.3516, 2.4902], dtype=torch.float16, requires_grad=True)\n",
      "step №578: loss = 29.13895034790039, weights = tensor([6.3516, 2.4922], dtype=torch.float16, requires_grad=True)\n",
      "step №579: loss = 29.131427764892578, weights = tensor([6.3477, 2.4941], dtype=torch.float16, requires_grad=True)\n",
      "step №580: loss = 29.111501693725586, weights = tensor([6.3477, 2.4961], dtype=torch.float16, requires_grad=True)\n",
      "step №581: loss = 29.103923797607422, weights = tensor([6.3477, 2.4980], dtype=torch.float16, requires_grad=True)\n",
      "step №582: loss = 29.096349716186523, weights = tensor([6.3477, 2.5000], dtype=torch.float16, requires_grad=True)\n",
      "step №583: loss = 29.088787078857422, weights = tensor([6.3477, 2.5020], dtype=torch.float16, requires_grad=True)\n",
      "step №584: loss = 29.081228256225586, weights = tensor([6.3477, 2.5039], dtype=torch.float16, requires_grad=True)\n",
      "step №585: loss = 29.073680877685547, weights = tensor([6.3477, 2.5059], dtype=torch.float16, requires_grad=True)\n",
      "step №586: loss = 29.066137313842773, weights = tensor([6.3477, 2.5078], dtype=torch.float16, requires_grad=True)\n",
      "step №587: loss = 29.058605194091797, weights = tensor([6.3477, 2.5098], dtype=torch.float16, requires_grad=True)\n",
      "step №588: loss = 29.051076889038086, weights = tensor([6.3477, 2.5117], dtype=torch.float16, requires_grad=True)\n",
      "step №589: loss = 29.043560028076172, weights = tensor([6.3477, 2.5137], dtype=torch.float16, requires_grad=True)\n",
      "step №590: loss = 29.036046981811523, weights = tensor([6.3477, 2.5156], dtype=torch.float16, requires_grad=True)\n",
      "step №591: loss = 29.028545379638672, weights = tensor([6.3438, 2.5176], dtype=torch.float16, requires_grad=True)\n",
      "step №592: loss = 29.008691787719727, weights = tensor([6.3438, 2.5195], dtype=torch.float16, requires_grad=True)\n",
      "step №593: loss = 29.001134872436523, weights = tensor([6.3438, 2.5215], dtype=torch.float16, requires_grad=True)\n",
      "step №594: loss = 28.99358558654785, weights = tensor([6.3438, 2.5234], dtype=torch.float16, requires_grad=True)\n",
      "step №595: loss = 28.986047744750977, weights = tensor([6.3438, 2.5254], dtype=torch.float16, requires_grad=True)\n",
      "step №596: loss = 28.9785099029541, weights = tensor([6.3438, 2.5273], dtype=torch.float16, requires_grad=True)\n",
      "step №597: loss = 28.970983505249023, weights = tensor([6.3438, 2.5293], dtype=torch.float16, requires_grad=True)\n",
      "step №598: loss = 28.963464736938477, weights = tensor([6.3438, 2.5312], dtype=torch.float16, requires_grad=True)\n",
      "step №599: loss = 28.955957412719727, weights = tensor([6.3438, 2.5332], dtype=torch.float16, requires_grad=True)\n",
      "step №600: loss = 28.948450088500977, weights = tensor([6.3438, 2.5352], dtype=torch.float16, requires_grad=True)\n",
      "step №601: loss = 28.940954208374023, weights = tensor([6.3438, 2.5371], dtype=torch.float16, requires_grad=True)\n",
      "step №602: loss = 28.9334659576416, weights = tensor([6.3438, 2.5391], dtype=torch.float16, requires_grad=True)\n",
      "step №603: loss = 28.925989151000977, weights = tensor([6.3438, 2.5410], dtype=torch.float16, requires_grad=True)\n",
      "step №604: loss = 28.91851234436035, weights = tensor([6.3398, 2.5430], dtype=torch.float16, requires_grad=True)\n",
      "step №605: loss = 28.898670196533203, weights = tensor([6.3398, 2.5449], dtype=torch.float16, requires_grad=True)\n",
      "step №606: loss = 28.89113998413086, weights = tensor([6.3398, 2.5469], dtype=torch.float16, requires_grad=True)\n",
      "step №607: loss = 28.883625030517578, weights = tensor([6.3398, 2.5488], dtype=torch.float16, requires_grad=True)\n",
      "step №608: loss = 28.876110076904297, weights = tensor([6.3398, 2.5508], dtype=torch.float16, requires_grad=True)\n",
      "step №609: loss = 28.868610382080078, weights = tensor([6.3398, 2.5527], dtype=torch.float16, requires_grad=True)\n",
      "step №610: loss = 28.86111068725586, weights = tensor([6.3398, 2.5547], dtype=torch.float16, requires_grad=True)\n",
      "step №611: loss = 28.853626251220703, weights = tensor([6.3398, 2.5566], dtype=torch.float16, requires_grad=True)\n",
      "step №612: loss = 28.846141815185547, weights = tensor([6.3398, 2.5586], dtype=torch.float16, requires_grad=True)\n",
      "step №613: loss = 28.838672637939453, weights = tensor([6.3398, 2.5605], dtype=torch.float16, requires_grad=True)\n",
      "step №614: loss = 28.83120346069336, weights = tensor([6.3398, 2.5625], dtype=torch.float16, requires_grad=True)\n",
      "step №615: loss = 28.823749542236328, weights = tensor([6.3398, 2.5645], dtype=torch.float16, requires_grad=True)\n",
      "step №616: loss = 28.816295623779297, weights = tensor([6.3398, 2.5664], dtype=torch.float16, requires_grad=True)\n",
      "step №617: loss = 28.808856964111328, weights = tensor([6.3359, 2.5684], dtype=torch.float16, requires_grad=True)\n",
      "step №618: loss = 28.789016723632812, weights = tensor([6.3359, 2.5703], dtype=torch.float16, requires_grad=True)\n",
      "step №619: loss = 28.781524658203125, weights = tensor([6.3359, 2.5723], dtype=torch.float16, requires_grad=True)\n",
      "step №620: loss = 28.774032592773438, weights = tensor([6.3359, 2.5742], dtype=torch.float16, requires_grad=True)\n",
      "step №621: loss = 28.766551971435547, weights = tensor([6.3359, 2.5762], dtype=torch.float16, requires_grad=True)\n",
      "step №622: loss = 28.759078979492188, weights = tensor([6.3359, 2.5781], dtype=torch.float16, requires_grad=True)\n",
      "step №623: loss = 28.751617431640625, weights = tensor([6.3359, 2.5801], dtype=torch.float16, requires_grad=True)\n",
      "step №624: loss = 28.744155883789062, weights = tensor([6.3359, 2.5820], dtype=torch.float16, requires_grad=True)\n",
      "step №625: loss = 28.736705780029297, weights = tensor([6.3359, 2.5840], dtype=torch.float16, requires_grad=True)\n",
      "step №626: loss = 28.729263305664062, weights = tensor([6.3359, 2.5859], dtype=torch.float16, requires_grad=True)\n",
      "step №627: loss = 28.721832275390625, weights = tensor([6.3359, 2.5879], dtype=torch.float16, requires_grad=True)\n",
      "step №628: loss = 28.714401245117188, weights = tensor([6.3359, 2.5898], dtype=torch.float16, requires_grad=True)\n",
      "step №629: loss = 28.706981658935547, weights = tensor([6.3320, 2.5918], dtype=torch.float16, requires_grad=True)\n",
      "step №630: loss = 28.687213897705078, weights = tensor([6.3320, 2.5938], dtype=torch.float16, requires_grad=True)\n",
      "step №631: loss = 28.67974281311035, weights = tensor([6.3320, 2.5957], dtype=torch.float16, requires_grad=True)\n",
      "step №632: loss = 28.67227554321289, weights = tensor([6.3320, 2.5977], dtype=torch.float16, requires_grad=True)\n",
      "step №633: loss = 28.664819717407227, weights = tensor([6.3320, 2.5996], dtype=torch.float16, requires_grad=True)\n",
      "step №634: loss = 28.657367706298828, weights = tensor([6.3320, 2.6016], dtype=torch.float16, requires_grad=True)\n",
      "step №635: loss = 28.649927139282227, weights = tensor([6.3320, 2.6035], dtype=torch.float16, requires_grad=True)\n",
      "step №636: loss = 28.64249038696289, weights = tensor([6.3320, 2.6055], dtype=torch.float16, requires_grad=True)\n",
      "step №637: loss = 28.63506507873535, weights = tensor([6.3320, 2.6074], dtype=torch.float16, requires_grad=True)\n",
      "step №638: loss = 28.627643585205078, weights = tensor([6.3320, 2.6094], dtype=torch.float16, requires_grad=True)\n",
      "step №639: loss = 28.6202335357666, weights = tensor([6.3320, 2.6113], dtype=torch.float16, requires_grad=True)\n",
      "step №640: loss = 28.61282730102539, weights = tensor([6.3320, 2.6133], dtype=torch.float16, requires_grad=True)\n",
      "step №641: loss = 28.605432510375977, weights = tensor([6.3320, 2.6152], dtype=torch.float16, requires_grad=True)\n",
      "step №642: loss = 28.598041534423828, weights = tensor([6.3281, 2.6172], dtype=torch.float16, requires_grad=True)\n",
      "step №643: loss = 28.578283309936523, weights = tensor([6.3281, 2.6191], dtype=torch.float16, requires_grad=True)\n",
      "step №644: loss = 28.570837020874023, weights = tensor([6.3281, 2.6211], dtype=torch.float16, requires_grad=True)\n",
      "step №645: loss = 28.563404083251953, weights = tensor([6.3281, 2.6230], dtype=torch.float16, requires_grad=True)\n",
      "step №646: loss = 28.55597496032715, weights = tensor([6.3281, 2.6250], dtype=torch.float16, requires_grad=True)\n",
      "step №647: loss = 28.548559188842773, weights = tensor([6.3281, 2.6270], dtype=torch.float16, requires_grad=True)\n",
      "step №648: loss = 28.5411434173584, weights = tensor([6.3281, 2.6289], dtype=torch.float16, requires_grad=True)\n",
      "step №649: loss = 28.533740997314453, weights = tensor([6.3281, 2.6309], dtype=torch.float16, requires_grad=True)\n",
      "step №650: loss = 28.526342391967773, weights = tensor([6.3281, 2.6328], dtype=torch.float16, requires_grad=True)\n",
      "step №651: loss = 28.518957138061523, weights = tensor([6.3281, 2.6348], dtype=torch.float16, requires_grad=True)\n",
      "step №652: loss = 28.511571884155273, weights = tensor([6.3281, 2.6367], dtype=torch.float16, requires_grad=True)\n",
      "step №653: loss = 28.504199981689453, weights = tensor([6.3281, 2.6387], dtype=torch.float16, requires_grad=True)\n",
      "step №654: loss = 28.4968318939209, weights = tensor([6.3281, 2.6406], dtype=torch.float16, requires_grad=True)\n",
      "step №655: loss = 28.489477157592773, weights = tensor([6.3242, 2.6426], dtype=torch.float16, requires_grad=True)\n",
      "step №656: loss = 28.4697208404541, weights = tensor([6.3242, 2.6445], dtype=torch.float16, requires_grad=True)\n",
      "step №657: loss = 28.462310791015625, weights = tensor([6.3242, 2.6465], dtype=torch.float16, requires_grad=True)\n",
      "step №658: loss = 28.454904556274414, weights = tensor([6.3242, 2.6484], dtype=torch.float16, requires_grad=True)\n",
      "step №659: loss = 28.447509765625, weights = tensor([6.3242, 2.6504], dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №660: loss = 28.44011878967285, weights = tensor([6.3242, 2.6523], dtype=torch.float16, requires_grad=True)\n",
      "step №661: loss = 28.4327392578125, weights = tensor([6.3242, 2.6543], dtype=torch.float16, requires_grad=True)\n",
      "step №662: loss = 28.425363540649414, weights = tensor([6.3242, 2.6562], dtype=torch.float16, requires_grad=True)\n",
      "step №663: loss = 28.417999267578125, weights = tensor([6.3242, 2.6582], dtype=torch.float16, requires_grad=True)\n",
      "step №664: loss = 28.4106388092041, weights = tensor([6.3242, 2.6602], dtype=torch.float16, requires_grad=True)\n",
      "step №665: loss = 28.403289794921875, weights = tensor([6.3242, 2.6621], dtype=torch.float16, requires_grad=True)\n",
      "step №666: loss = 28.395944595336914, weights = tensor([6.3242, 2.6641], dtype=torch.float16, requires_grad=True)\n",
      "step №667: loss = 28.38861083984375, weights = tensor([6.3203, 2.6660], dtype=torch.float16, requires_grad=True)\n",
      "step №668: loss = 28.36892318725586, weights = tensor([6.3203, 2.6680], dtype=torch.float16, requires_grad=True)\n",
      "step №669: loss = 28.361536026000977, weights = tensor([6.3203, 2.6699], dtype=torch.float16, requires_grad=True)\n",
      "step №670: loss = 28.35415267944336, weights = tensor([6.3203, 2.6719], dtype=torch.float16, requires_grad=True)\n",
      "step №671: loss = 28.346782684326172, weights = tensor([6.3203, 2.6738], dtype=torch.float16, requires_grad=True)\n",
      "step №672: loss = 28.339412689208984, weights = tensor([6.3203, 2.6758], dtype=torch.float16, requires_grad=True)\n",
      "step №673: loss = 28.332056045532227, weights = tensor([6.3203, 2.6777], dtype=torch.float16, requires_grad=True)\n",
      "step №674: loss = 28.324703216552734, weights = tensor([6.3203, 2.6797], dtype=torch.float16, requires_grad=True)\n",
      "step №675: loss = 28.317363739013672, weights = tensor([6.3203, 2.6816], dtype=torch.float16, requires_grad=True)\n",
      "step №676: loss = 28.31002426147461, weights = tensor([6.3203, 2.6836], dtype=torch.float16, requires_grad=True)\n",
      "step №677: loss = 28.302698135375977, weights = tensor([6.3203, 2.6855], dtype=torch.float16, requires_grad=True)\n",
      "step №678: loss = 28.29537582397461, weights = tensor([6.3203, 2.6875], dtype=torch.float16, requires_grad=True)\n",
      "step №679: loss = 28.288066864013672, weights = tensor([6.3203, 2.6895], dtype=torch.float16, requires_grad=True)\n",
      "step №680: loss = 28.280757904052734, weights = tensor([6.3164, 2.6914], dtype=torch.float16, requires_grad=True)\n",
      "step №681: loss = 28.261083602905273, weights = tensor([6.3164, 2.6934], dtype=torch.float16, requires_grad=True)\n",
      "step №682: loss = 28.25372314453125, weights = tensor([6.3164, 2.6953], dtype=torch.float16, requires_grad=True)\n",
      "step №683: loss = 28.246374130249023, weights = tensor([6.3164, 2.6973], dtype=torch.float16, requires_grad=True)\n",
      "step №684: loss = 28.239028930664062, weights = tensor([6.3164, 2.6992], dtype=torch.float16, requires_grad=True)\n",
      "step №685: loss = 28.2316951751709, weights = tensor([6.3164, 2.7012], dtype=torch.float16, requires_grad=True)\n",
      "step №686: loss = 28.224365234375, weights = tensor([6.3164, 2.7031], dtype=torch.float16, requires_grad=True)\n",
      "step №687: loss = 28.2170467376709, weights = tensor([6.3164, 2.7051], dtype=torch.float16, requires_grad=True)\n",
      "step №688: loss = 28.209732055664062, weights = tensor([6.3164, 2.7070], dtype=torch.float16, requires_grad=True)\n",
      "step №689: loss = 28.202428817749023, weights = tensor([6.3164, 2.7090], dtype=torch.float16, requires_grad=True)\n",
      "step №690: loss = 28.19512939453125, weights = tensor([6.3164, 2.7109], dtype=torch.float16, requires_grad=True)\n",
      "step №691: loss = 28.187841415405273, weights = tensor([6.3164, 2.7129], dtype=torch.float16, requires_grad=True)\n",
      "step №692: loss = 28.180557250976562, weights = tensor([6.3164, 2.7148], dtype=torch.float16, requires_grad=True)\n",
      "step №693: loss = 28.17328453063965, weights = tensor([6.3125, 2.7168], dtype=torch.float16, requires_grad=True)\n",
      "step №694: loss = 28.153614044189453, weights = tensor([6.3125, 2.7188], dtype=torch.float16, requires_grad=True)\n",
      "step №695: loss = 28.146289825439453, weights = tensor([6.3125, 2.7207], dtype=torch.float16, requires_grad=True)\n",
      "step №696: loss = 28.138965606689453, weights = tensor([6.3125, 2.7227], dtype=torch.float16, requires_grad=True)\n",
      "step №697: loss = 28.13165283203125, weights = tensor([6.3125, 2.7246], dtype=torch.float16, requires_grad=True)\n",
      "step №698: loss = 28.124347686767578, weights = tensor([6.3125, 2.7266], dtype=torch.float16, requires_grad=True)\n",
      "step №699: loss = 28.117053985595703, weights = tensor([6.3125, 2.7285], dtype=torch.float16, requires_grad=True)\n",
      "step №700: loss = 28.109760284423828, weights = tensor([6.3125, 2.7305], dtype=torch.float16, requires_grad=True)\n",
      "step №701: loss = 28.10247802734375, weights = tensor([6.3125, 2.7324], dtype=torch.float16, requires_grad=True)\n",
      "step №702: loss = 28.095203399658203, weights = tensor([6.3125, 2.7344], dtype=torch.float16, requires_grad=True)\n",
      "step №703: loss = 28.087940216064453, weights = tensor([6.3125, 2.7363], dtype=torch.float16, requires_grad=True)\n",
      "step №704: loss = 28.080677032470703, weights = tensor([6.3125, 2.7383], dtype=torch.float16, requires_grad=True)\n",
      "step №705: loss = 28.07342529296875, weights = tensor([6.3086, 2.7402], dtype=torch.float16, requires_grad=True)\n",
      "step №706: loss = 28.053823471069336, weights = tensor([6.3086, 2.7422], dtype=torch.float16, requires_grad=True)\n",
      "step №707: loss = 28.046520233154297, weights = tensor([6.3086, 2.7441], dtype=torch.float16, requires_grad=True)\n",
      "step №708: loss = 28.039220809936523, weights = tensor([6.3086, 2.7461], dtype=torch.float16, requires_grad=True)\n",
      "step №709: loss = 28.031932830810547, weights = tensor([6.3086, 2.7480], dtype=torch.float16, requires_grad=True)\n",
      "step №710: loss = 28.024648666381836, weights = tensor([6.3086, 2.7500], dtype=torch.float16, requires_grad=True)\n",
      "step №711: loss = 28.017375946044922, weights = tensor([6.3086, 2.7520], dtype=torch.float16, requires_grad=True)\n",
      "step №712: loss = 28.010107040405273, weights = tensor([6.3086, 2.7539], dtype=torch.float16, requires_grad=True)\n",
      "step №713: loss = 28.002849578857422, weights = tensor([6.3086, 2.7559], dtype=torch.float16, requires_grad=True)\n",
      "step №714: loss = 27.995595932006836, weights = tensor([6.3086, 2.7578], dtype=torch.float16, requires_grad=True)\n",
      "step №715: loss = 27.988353729248047, weights = tensor([6.3086, 2.7598], dtype=torch.float16, requires_grad=True)\n",
      "step №716: loss = 27.981115341186523, weights = tensor([6.3086, 2.7617], dtype=torch.float16, requires_grad=True)\n",
      "step №717: loss = 27.973888397216797, weights = tensor([6.3086, 2.7637], dtype=torch.float16, requires_grad=True)\n",
      "step №718: loss = 27.966665267944336, weights = tensor([6.3047, 2.7656], dtype=torch.float16, requires_grad=True)\n",
      "step №719: loss = 27.94707679748535, weights = tensor([6.3047, 2.7676], dtype=torch.float16, requires_grad=True)\n",
      "step №720: loss = 27.93979835510254, weights = tensor([6.3047, 2.7695], dtype=torch.float16, requires_grad=True)\n",
      "step №721: loss = 27.932531356811523, weights = tensor([6.3047, 2.7715], dtype=torch.float16, requires_grad=True)\n",
      "step №722: loss = 27.92527198791504, weights = tensor([6.3047, 2.7734], dtype=torch.float16, requires_grad=True)\n",
      "step №723: loss = 27.91802406311035, weights = tensor([6.3047, 2.7754], dtype=torch.float16, requires_grad=True)\n",
      "step №724: loss = 27.910776138305664, weights = tensor([6.3047, 2.7773], dtype=torch.float16, requires_grad=True)\n",
      "step №725: loss = 27.903539657592773, weights = tensor([6.3047, 2.7793], dtype=torch.float16, requires_grad=True)\n",
      "step №726: loss = 27.896310806274414, weights = tensor([6.3047, 2.7812], dtype=torch.float16, requires_grad=True)\n",
      "step №727: loss = 27.88909339904785, weights = tensor([6.3047, 2.7832], dtype=torch.float16, requires_grad=True)\n",
      "step №728: loss = 27.88187599182129, weights = tensor([6.3047, 2.7852], dtype=torch.float16, requires_grad=True)\n",
      "step №729: loss = 27.874670028686523, weights = tensor([6.3047, 2.7871], dtype=torch.float16, requires_grad=True)\n",
      "step №730: loss = 27.86747169494629, weights = tensor([6.3047, 2.7891], dtype=torch.float16, requires_grad=True)\n",
      "step №731: loss = 27.86028480529785, weights = tensor([6.3008, 2.7910], dtype=torch.float16, requires_grad=True)\n",
      "step №732: loss = 27.840694427490234, weights = tensor([6.3008, 2.7930], dtype=torch.float16, requires_grad=True)\n",
      "step №733: loss = 27.833454132080078, weights = tensor([6.3008, 2.7949], dtype=torch.float16, requires_grad=True)\n",
      "step №734: loss = 27.826213836669922, weights = tensor([6.3008, 2.7969], dtype=torch.float16, requires_grad=True)\n",
      "step №735: loss = 27.818988800048828, weights = tensor([6.3008, 2.7988], dtype=torch.float16, requires_grad=True)\n",
      "step №736: loss = 27.811763763427734, weights = tensor([6.3008, 2.8008], dtype=torch.float16, requires_grad=True)\n",
      "step №737: loss = 27.804553985595703, weights = tensor([6.3008, 2.8027], dtype=torch.float16, requires_grad=True)\n",
      "step №738: loss = 27.797344207763672, weights = tensor([6.3008, 2.8047], dtype=torch.float16, requires_grad=True)\n",
      "step №739: loss = 27.790149688720703, weights = tensor([6.3008, 2.8066], dtype=torch.float16, requires_grad=True)\n",
      "step №740: loss = 27.782955169677734, weights = tensor([6.3008, 2.8086], dtype=torch.float16, requires_grad=True)\n",
      "step №741: loss = 27.775775909423828, weights = tensor([6.3008, 2.8105], dtype=torch.float16, requires_grad=True)\n",
      "step №742: loss = 27.768596649169922, weights = tensor([6.3008, 2.8125], dtype=torch.float16, requires_grad=True)\n",
      "step №743: loss = 27.761432647705078, weights = tensor([6.2969, 2.8145], dtype=torch.float16, requires_grad=True)\n",
      "step №744: loss = 27.741912841796875, weights = tensor([6.2969, 2.8164], dtype=torch.float16, requires_grad=True)\n",
      "step №745: loss = 27.734691619873047, weights = tensor([6.2969, 2.8184], dtype=torch.float16, requires_grad=True)\n",
      "step №746: loss = 27.72747802734375, weights = tensor([6.2969, 2.8203], dtype=torch.float16, requires_grad=True)\n",
      "step №747: loss = 27.72027587890625, weights = tensor([6.2969, 2.8223], dtype=torch.float16, requires_grad=True)\n",
      "step №748: loss = 27.71307373046875, weights = tensor([6.2969, 2.8242], dtype=torch.float16, requires_grad=True)\n",
      "step №749: loss = 27.705883026123047, weights = tensor([6.2969, 2.8262], dtype=torch.float16, requires_grad=True)\n",
      "step №750: loss = 27.698699951171875, weights = tensor([6.2969, 2.8281], dtype=torch.float16, requires_grad=True)\n",
      "step №751: loss = 27.6915283203125, weights = tensor([6.2969, 2.8301], dtype=torch.float16, requires_grad=True)\n",
      "step №752: loss = 27.684356689453125, weights = tensor([6.2969, 2.8320], dtype=torch.float16, requires_grad=True)\n",
      "step №753: loss = 27.677196502685547, weights = tensor([6.2969, 2.8340], dtype=torch.float16, requires_grad=True)\n",
      "step №754: loss = 27.6700439453125, weights = tensor([6.2969, 2.8359], dtype=torch.float16, requires_grad=True)\n",
      "step №755: loss = 27.66290283203125, weights = tensor([6.2969, 2.8379], dtype=torch.float16, requires_grad=True)\n",
      "step №756: loss = 27.65576171875, weights = tensor([6.2930, 2.8398], dtype=torch.float16, requires_grad=True)\n",
      "step №757: loss = 27.636255264282227, weights = tensor([6.2930, 2.8418], dtype=torch.float16, requires_grad=True)\n",
      "step №758: loss = 27.62906265258789, weights = tensor([6.2930, 2.8438], dtype=torch.float16, requires_grad=True)\n",
      "step №759: loss = 27.62188148498535, weights = tensor([6.2930, 2.8457], dtype=torch.float16, requires_grad=True)\n",
      "step №760: loss = 27.614704132080078, weights = tensor([6.2930, 2.8477], dtype=torch.float16, requires_grad=True)\n",
      "step №761: loss = 27.6075382232666, weights = tensor([6.2930, 2.8496], dtype=torch.float16, requires_grad=True)\n",
      "step №762: loss = 27.60037612915039, weights = tensor([6.2930, 2.8516], dtype=torch.float16, requires_grad=True)\n",
      "step №763: loss = 27.593225479125977, weights = tensor([6.2930, 2.8535], dtype=torch.float16, requires_grad=True)\n",
      "step №764: loss = 27.586078643798828, weights = tensor([6.2930, 2.8555], dtype=torch.float16, requires_grad=True)\n",
      "step №765: loss = 27.578943252563477, weights = tensor([6.2930, 2.8574], dtype=torch.float16, requires_grad=True)\n",
      "step №766: loss = 27.57181167602539, weights = tensor([6.2930, 2.8594], dtype=torch.float16, requires_grad=True)\n",
      "step №767: loss = 27.5646915435791, weights = tensor([6.2930, 2.8613], dtype=torch.float16, requires_grad=True)\n",
      "step №768: loss = 27.557575225830078, weights = tensor([6.2930, 2.8633], dtype=torch.float16, requires_grad=True)\n",
      "step №769: loss = 27.55047035217285, weights = tensor([6.2891, 2.8652], dtype=torch.float16, requires_grad=True)\n",
      "step №770: loss = 27.53096580505371, weights = tensor([6.2891, 2.8672], dtype=torch.float16, requires_grad=True)\n",
      "step №771: loss = 27.5238094329834, weights = tensor([6.2891, 2.8691], dtype=torch.float16, requires_grad=True)\n",
      "step №772: loss = 27.516653060913086, weights = tensor([6.2891, 2.8711], dtype=torch.float16, requires_grad=True)\n",
      "step №773: loss = 27.509510040283203, weights = tensor([6.2891, 2.8730], dtype=torch.float16, requires_grad=True)\n",
      "step №774: loss = 27.502370834350586, weights = tensor([6.2891, 2.8750], dtype=torch.float16, requires_grad=True)\n",
      "step №775: loss = 27.4952449798584, weights = tensor([6.2891, 2.8770], dtype=torch.float16, requires_grad=True)\n",
      "step №776: loss = 27.48811912536621, weights = tensor([6.2891, 2.8789], dtype=torch.float16, requires_grad=True)\n",
      "step №777: loss = 27.481006622314453, weights = tensor([6.2891, 2.8809], dtype=torch.float16, requires_grad=True)\n",
      "step №778: loss = 27.47389793395996, weights = tensor([6.2891, 2.8828], dtype=torch.float16, requires_grad=True)\n",
      "step №779: loss = 27.4668025970459, weights = tensor([6.2891, 2.8848], dtype=torch.float16, requires_grad=True)\n",
      "step №780: loss = 27.459707260131836, weights = tensor([6.2891, 2.8867], dtype=torch.float16, requires_grad=True)\n",
      "step №781: loss = 27.452625274658203, weights = tensor([6.2852, 2.8887], dtype=torch.float16, requires_grad=True)\n",
      "step №782: loss = 27.433191299438477, weights = tensor([6.2852, 2.8906], dtype=torch.float16, requires_grad=True)\n",
      "step №783: loss = 27.426055908203125, weights = tensor([6.2852, 2.8926], dtype=torch.float16, requires_grad=True)\n",
      "step №784: loss = 27.41892433166504, weights = tensor([6.2852, 2.8945], dtype=torch.float16, requires_grad=True)\n",
      "step №785: loss = 27.41180419921875, weights = tensor([6.2852, 2.8965], dtype=torch.float16, requires_grad=True)\n",
      "step №786: loss = 27.404687881469727, weights = tensor([6.2852, 2.8984], dtype=torch.float16, requires_grad=True)\n",
      "step №787: loss = 27.3975830078125, weights = tensor([6.2852, 2.9004], dtype=torch.float16, requires_grad=True)\n",
      "step №788: loss = 27.39048194885254, weights = tensor([6.2852, 2.9023], dtype=torch.float16, requires_grad=True)\n",
      "step №789: loss = 27.383392333984375, weights = tensor([6.2852, 2.9043], dtype=torch.float16, requires_grad=True)\n",
      "step №790: loss = 27.376306533813477, weights = tensor([6.2852, 2.9062], dtype=torch.float16, requires_grad=True)\n",
      "step №791: loss = 27.369232177734375, weights = tensor([6.2852, 2.9082], dtype=torch.float16, requires_grad=True)\n",
      "step №792: loss = 27.36216163635254, weights = tensor([6.2852, 2.9102], dtype=torch.float16, requires_grad=True)\n",
      "step №793: loss = 27.3551025390625, weights = tensor([6.2852, 2.9121], dtype=torch.float16, requires_grad=True)\n",
      "step №794: loss = 27.348047256469727, weights = tensor([6.2812, 2.9141], dtype=torch.float16, requires_grad=True)\n",
      "step №795: loss = 27.328624725341797, weights = tensor([6.2812, 2.9160], dtype=torch.float16, requires_grad=True)\n",
      "step №796: loss = 27.321514129638672, weights = tensor([6.2812, 2.9180], dtype=torch.float16, requires_grad=True)\n",
      "step №797: loss = 27.314416885375977, weights = tensor([6.2812, 2.9199], dtype=torch.float16, requires_grad=True)\n",
      "step №798: loss = 27.307323455810547, weights = tensor([6.2812, 2.9219], dtype=torch.float16, requires_grad=True)\n",
      "step №799: loss = 27.300243377685547, weights = tensor([6.2812, 2.9238], dtype=torch.float16, requires_grad=True)\n",
      "step №800: loss = 27.293163299560547, weights = tensor([6.2812, 2.9258], dtype=torch.float16, requires_grad=True)\n",
      "step №801: loss = 27.286096572875977, weights = tensor([6.2812, 2.9277], dtype=torch.float16, requires_grad=True)\n",
      "step №802: loss = 27.279033660888672, weights = tensor([6.2812, 2.9297], dtype=torch.float16, requires_grad=True)\n",
      "step №803: loss = 27.271984100341797, weights = tensor([6.2812, 2.9316], dtype=torch.float16, requires_grad=True)\n",
      "step №804: loss = 27.264934539794922, weights = tensor([6.2812, 2.9336], dtype=torch.float16, requires_grad=True)\n",
      "step №805: loss = 27.257898330688477, weights = tensor([6.2812, 2.9355], dtype=torch.float16, requires_grad=True)\n",
      "step №806: loss = 27.250865936279297, weights = tensor([6.2812, 2.9375], dtype=torch.float16, requires_grad=True)\n",
      "step №807: loss = 27.243846893310547, weights = tensor([6.2773, 2.9395], dtype=torch.float16, requires_grad=True)\n",
      "step №808: loss = 27.22442626953125, weights = tensor([6.2773, 2.9414], dtype=torch.float16, requires_grad=True)\n",
      "step №809: loss = 27.21735191345215, weights = tensor([6.2773, 2.9434], dtype=torch.float16, requires_grad=True)\n",
      "step №810: loss = 27.210281372070312, weights = tensor([6.2773, 2.9453], dtype=torch.float16, requires_grad=True)\n",
      "step №811: loss = 27.203222274780273, weights = tensor([6.2773, 2.9473], dtype=torch.float16, requires_grad=True)\n",
      "step №812: loss = 27.1961669921875, weights = tensor([6.2773, 2.9492], dtype=torch.float16, requires_grad=True)\n",
      "step №813: loss = 27.189123153686523, weights = tensor([6.2773, 2.9512], dtype=torch.float16, requires_grad=True)\n",
      "step №814: loss = 27.182083129882812, weights = tensor([6.2773, 2.9531], dtype=torch.float16, requires_grad=True)\n",
      "step №815: loss = 27.1750545501709, weights = tensor([6.2773, 2.9551], dtype=torch.float16, requires_grad=True)\n",
      "step №816: loss = 27.16802978515625, weights = tensor([6.2773, 2.9570], dtype=torch.float16, requires_grad=True)\n",
      "step №817: loss = 27.1610164642334, weights = tensor([6.2773, 2.9590], dtype=torch.float16, requires_grad=True)\n",
      "step №818: loss = 27.154006958007812, weights = tensor([6.2773, 2.9609], dtype=torch.float16, requires_grad=True)\n",
      "step №819: loss = 27.147008895874023, weights = tensor([6.2734, 2.9629], dtype=torch.float16, requires_grad=True)\n",
      "step №820: loss = 27.12765884399414, weights = tensor([6.2734, 2.9648], dtype=torch.float16, requires_grad=True)\n",
      "step №821: loss = 27.12060546875, weights = tensor([6.2734, 2.9668], dtype=torch.float16, requires_grad=True)\n",
      "step №822: loss = 27.11355972290039, weights = tensor([6.2734, 2.9688], dtype=torch.float16, requires_grad=True)\n",
      "step №823: loss = 27.106525421142578, weights = tensor([6.2734, 2.9707], dtype=torch.float16, requires_grad=True)\n",
      "step №824: loss = 27.099491119384766, weights = tensor([6.2734, 2.9727], dtype=torch.float16, requires_grad=True)\n",
      "step №825: loss = 27.09246826171875, weights = tensor([6.2734, 2.9746], dtype=torch.float16, requires_grad=True)\n",
      "step №826: loss = 27.085453033447266, weights = tensor([6.2734, 2.9766], dtype=torch.float16, requires_grad=True)\n",
      "step №827: loss = 27.078449249267578, weights = tensor([6.2734, 2.9785], dtype=torch.float16, requires_grad=True)\n",
      "step №828: loss = 27.07144546508789, weights = tensor([6.2734, 2.9805], dtype=torch.float16, requires_grad=True)\n",
      "step №829: loss = 27.064453125, weights = tensor([6.2734, 2.9824], dtype=torch.float16, requires_grad=True)\n",
      "step №830: loss = 27.05746841430664, weights = tensor([6.2734, 2.9844], dtype=torch.float16, requires_grad=True)\n",
      "step №831: loss = 27.050495147705078, weights = tensor([6.2734, 2.9863], dtype=torch.float16, requires_grad=True)\n",
      "step №832: loss = 27.043521881103516, weights = tensor([6.2695, 2.9883], dtype=torch.float16, requires_grad=True)\n",
      "step №833: loss = 27.024181365966797, weights = tensor([6.2695, 2.9902], dtype=torch.float16, requires_grad=True)\n",
      "step №834: loss = 27.01715660095215, weights = tensor([6.2695, 2.9922], dtype=torch.float16, requires_grad=True)\n",
      "step №835: loss = 27.010143280029297, weights = tensor([6.2695, 2.9941], dtype=torch.float16, requires_grad=True)\n",
      "step №836: loss = 27.00313377380371, weights = tensor([6.2695, 2.9961], dtype=torch.float16, requires_grad=True)\n",
      "step №837: loss = 26.996135711669922, weights = tensor([6.2695, 2.9980], dtype=torch.float16, requires_grad=True)\n",
      "step №838: loss = 26.9891414642334, weights = tensor([6.2695, 3.0000], dtype=torch.float16, requires_grad=True)\n",
      "step №839: loss = 26.982158660888672, weights = tensor([6.2695, 3.0020], dtype=torch.float16, requires_grad=True)\n",
      "step №840: loss = 26.97517967224121, weights = tensor([6.2695, 3.0039], dtype=torch.float16, requires_grad=True)\n",
      "step №841: loss = 26.968212127685547, weights = tensor([6.2695, 3.0059], dtype=torch.float16, requires_grad=True)\n",
      "step №842: loss = 26.96124839782715, weights = tensor([6.2695, 3.0078], dtype=torch.float16, requires_grad=True)\n",
      "step №843: loss = 26.954296112060547, weights = tensor([6.2695, 3.0098], dtype=torch.float16, requires_grad=True)\n",
      "step №844: loss = 26.94734764099121, weights = tensor([6.2695, 3.0117], dtype=torch.float16, requires_grad=True)\n",
      "step №845: loss = 26.940410614013672, weights = tensor([6.2656, 3.0137], dtype=torch.float16, requires_grad=True)\n",
      "step №846: loss = 26.92107582092285, weights = tensor([6.2656, 3.0156], dtype=torch.float16, requires_grad=True)\n",
      "step №847: loss = 26.914087295532227, weights = tensor([6.2656, 3.0176], dtype=torch.float16, requires_grad=True)\n",
      "step №848: loss = 26.9070987701416, weights = tensor([6.2656, 3.0195], dtype=torch.float16, requires_grad=True)\n",
      "step №849: loss = 26.900121688842773, weights = tensor([6.2656, 3.0215], dtype=torch.float16, requires_grad=True)\n",
      "step №850: loss = 26.893152236938477, weights = tensor([6.2656, 3.0234], dtype=torch.float16, requires_grad=True)\n",
      "step №851: loss = 26.886194229125977, weights = tensor([6.2656, 3.0254], dtype=torch.float16, requires_grad=True)\n",
      "step №852: loss = 26.879236221313477, weights = tensor([6.2656, 3.0273], dtype=torch.float16, requires_grad=True)\n",
      "step №853: loss = 26.872289657592773, weights = tensor([6.2656, 3.0293], dtype=torch.float16, requires_grad=True)\n",
      "step №854: loss = 26.8653507232666, weights = tensor([6.2656, 3.0312], dtype=torch.float16, requires_grad=True)\n",
      "step №855: loss = 26.858423233032227, weights = tensor([6.2656, 3.0332], dtype=torch.float16, requires_grad=True)\n",
      "step №856: loss = 26.85149574279785, weights = tensor([6.2656, 3.0352], dtype=torch.float16, requires_grad=True)\n",
      "step №857: loss = 26.844579696655273, weights = tensor([6.2617, 3.0371], dtype=torch.float16, requires_grad=True)\n",
      "step №858: loss = 26.825313568115234, weights = tensor([6.2617, 3.0391], dtype=torch.float16, requires_grad=True)\n",
      "step №859: loss = 26.818347930908203, weights = tensor([6.2617, 3.0410], dtype=torch.float16, requires_grad=True)\n",
      "step №860: loss = 26.811382293701172, weights = tensor([6.2617, 3.0430], dtype=torch.float16, requires_grad=True)\n",
      "step №861: loss = 26.804431915283203, weights = tensor([6.2617, 3.0449], dtype=torch.float16, requires_grad=True)\n",
      "step №862: loss = 26.797481536865234, weights = tensor([6.2617, 3.0469], dtype=torch.float16, requires_grad=True)\n",
      "step №863: loss = 26.790546417236328, weights = tensor([6.2617, 3.0488], dtype=torch.float16, requires_grad=True)\n",
      "step №864: loss = 26.783611297607422, weights = tensor([6.2617, 3.0508], dtype=torch.float16, requires_grad=True)\n",
      "step №865: loss = 26.776691436767578, weights = tensor([6.2617, 3.0527], dtype=torch.float16, requires_grad=True)\n",
      "step №866: loss = 26.769771575927734, weights = tensor([6.2617, 3.0547], dtype=torch.float16, requires_grad=True)\n",
      "step №867: loss = 26.762866973876953, weights = tensor([6.2617, 3.0566], dtype=torch.float16, requires_grad=True)\n",
      "step №868: loss = 26.755962371826172, weights = tensor([6.2617, 3.0586], dtype=torch.float16, requires_grad=True)\n",
      "step №869: loss = 26.749073028564453, weights = tensor([6.2617, 3.0605], dtype=torch.float16, requires_grad=True)\n",
      "step №870: loss = 26.742183685302734, weights = tensor([6.2578, 3.0625], dtype=torch.float16, requires_grad=True)\n",
      "step №871: loss = 26.722930908203125, weights = tensor([6.2578, 3.0645], dtype=torch.float16, requires_grad=True)\n",
      "step №872: loss = 26.715988159179688, weights = tensor([6.2578, 3.0664], dtype=torch.float16, requires_grad=True)\n",
      "step №873: loss = 26.709056854248047, weights = tensor([6.2578, 3.0684], dtype=torch.float16, requires_grad=True)\n",
      "step №874: loss = 26.702133178710938, weights = tensor([6.2578, 3.0703], dtype=torch.float16, requires_grad=True)\n",
      "step №875: loss = 26.695220947265625, weights = tensor([6.2578, 3.0723], dtype=torch.float16, requires_grad=True)\n",
      "step №876: loss = 26.688308715820312, weights = tensor([6.2578, 3.0742], dtype=torch.float16, requires_grad=True)\n",
      "step №877: loss = 26.681407928466797, weights = tensor([6.2578, 3.0762], dtype=torch.float16, requires_grad=True)\n",
      "step №878: loss = 26.674514770507812, weights = tensor([6.2578, 3.0781], dtype=torch.float16, requires_grad=True)\n",
      "step №879: loss = 26.667633056640625, weights = tensor([6.2578, 3.0801], dtype=torch.float16, requires_grad=True)\n",
      "step №880: loss = 26.660751342773438, weights = tensor([6.2578, 3.0820], dtype=torch.float16, requires_grad=True)\n",
      "step №881: loss = 26.653881072998047, weights = tensor([6.2578, 3.0840], dtype=torch.float16, requires_grad=True)\n",
      "step №882: loss = 26.647018432617188, weights = tensor([6.2578, 3.0859], dtype=torch.float16, requires_grad=True)\n",
      "step №883: loss = 26.640167236328125, weights = tensor([6.2539, 3.0879], dtype=torch.float16, requires_grad=True)\n",
      "step №884: loss = 26.620914459228516, weights = tensor([6.2539, 3.0898], dtype=torch.float16, requires_grad=True)\n",
      "step №885: loss = 26.6140079498291, weights = tensor([6.2539, 3.0918], dtype=torch.float16, requires_grad=True)\n",
      "step №886: loss = 26.607105255126953, weights = tensor([6.2539, 3.0938], dtype=torch.float16, requires_grad=True)\n",
      "step №887: loss = 26.6002140045166, weights = tensor([6.2539, 3.0957], dtype=torch.float16, requires_grad=True)\n",
      "step №888: loss = 26.593326568603516, weights = tensor([6.2539, 3.0977], dtype=torch.float16, requires_grad=True)\n",
      "step №889: loss = 26.586450576782227, weights = tensor([6.2539, 3.0996], dtype=torch.float16, requires_grad=True)\n",
      "step №890: loss = 26.579578399658203, weights = tensor([6.2539, 3.1016], dtype=torch.float16, requires_grad=True)\n",
      "step №891: loss = 26.572717666625977, weights = tensor([6.2539, 3.1035], dtype=torch.float16, requires_grad=True)\n",
      "step №892: loss = 26.565860748291016, weights = tensor([6.2539, 3.1055], dtype=torch.float16, requires_grad=True)\n",
      "step №893: loss = 26.55901527404785, weights = tensor([6.2539, 3.1074], dtype=torch.float16, requires_grad=True)\n",
      "step №894: loss = 26.552173614501953, weights = tensor([6.2539, 3.1094], dtype=torch.float16, requires_grad=True)\n",
      "step №895: loss = 26.54534339904785, weights = tensor([6.2500, 3.1113], dtype=torch.float16, requires_grad=True)\n",
      "step №896: loss = 26.526159286499023, weights = tensor([6.2500, 3.1133], dtype=torch.float16, requires_grad=True)\n",
      "step №897: loss = 26.519275665283203, weights = tensor([6.2500, 3.1152], dtype=torch.float16, requires_grad=True)\n",
      "step №898: loss = 26.51239585876465, weights = tensor([6.2500, 3.1172], dtype=torch.float16, requires_grad=True)\n",
      "step №899: loss = 26.505529403686523, weights = tensor([6.2500, 3.1191], dtype=torch.float16, requires_grad=True)\n",
      "step №900: loss = 26.4986629486084, weights = tensor([6.2500, 3.1211], dtype=torch.float16, requires_grad=True)\n",
      "step №901: loss = 26.491809844970703, weights = tensor([6.2500, 3.1230], dtype=torch.float16, requires_grad=True)\n",
      "step №902: loss = 26.484960556030273, weights = tensor([6.2500, 3.1250], dtype=torch.float16, requires_grad=True)\n",
      "step №903: loss = 26.478124618530273, weights = tensor([6.2500, 3.1270], dtype=torch.float16, requires_grad=True)\n",
      "step №904: loss = 26.471288681030273, weights = tensor([6.2500, 3.1289], dtype=torch.float16, requires_grad=True)\n",
      "step №905: loss = 26.464466094970703, weights = tensor([6.2500, 3.1309], dtype=torch.float16, requires_grad=True)\n",
      "step №906: loss = 26.4576473236084, weights = tensor([6.2500, 3.1328], dtype=torch.float16, requires_grad=True)\n",
      "step №907: loss = 26.450841903686523, weights = tensor([6.2500, 3.1348], dtype=torch.float16, requires_grad=True)\n",
      "step №908: loss = 26.44403648376465, weights = tensor([6.2461, 3.1367], dtype=torch.float16, requires_grad=True)\n",
      "step №909: loss = 26.42486572265625, weights = tensor([6.2461, 3.1387], dtype=torch.float16, requires_grad=True)\n",
      "step №910: loss = 26.41800880432129, weights = tensor([6.2461, 3.1406], dtype=torch.float16, requires_grad=True)\n",
      "step №911: loss = 26.411163330078125, weights = tensor([6.2461, 3.1426], dtype=torch.float16, requires_grad=True)\n",
      "step №912: loss = 26.404321670532227, weights = tensor([6.2461, 3.1445], dtype=torch.float16, requires_grad=True)\n",
      "step №913: loss = 26.397491455078125, weights = tensor([6.2461, 3.1465], dtype=torch.float16, requires_grad=True)\n",
      "step №914: loss = 26.39066505432129, weights = tensor([6.2461, 3.1484], dtype=torch.float16, requires_grad=True)\n",
      "step №915: loss = 26.38385009765625, weights = tensor([6.2461, 3.1504], dtype=torch.float16, requires_grad=True)\n",
      "step №916: loss = 26.377038955688477, weights = tensor([6.2461, 3.1523], dtype=torch.float16, requires_grad=True)\n",
      "step №917: loss = 26.3702392578125, weights = tensor([6.2461, 3.1543], dtype=torch.float16, requires_grad=True)\n",
      "step №918: loss = 26.36344337463379, weights = tensor([6.2461, 3.1562], dtype=torch.float16, requires_grad=True)\n",
      "step №919: loss = 26.356658935546875, weights = tensor([6.2461, 3.1582], dtype=torch.float16, requires_grad=True)\n",
      "step №920: loss = 26.349878311157227, weights = tensor([6.2461, 3.1602], dtype=torch.float16, requires_grad=True)\n",
      "step №921: loss = 26.343109130859375, weights = tensor([6.2422, 3.1621], dtype=torch.float16, requires_grad=True)\n",
      "step №922: loss = 26.32394027709961, weights = tensor([6.2422, 3.1641], dtype=torch.float16, requires_grad=True)\n",
      "step №923: loss = 26.317119598388672, weights = tensor([6.2422, 3.1660], dtype=torch.float16, requires_grad=True)\n",
      "step №924: loss = 26.310298919677734, weights = tensor([6.2422, 3.1680], dtype=torch.float16, requires_grad=True)\n",
      "step №925: loss = 26.303491592407227, weights = tensor([6.2422, 3.1699], dtype=torch.float16, requires_grad=True)\n",
      "step №926: loss = 26.296688079833984, weights = tensor([6.2422, 3.1719], dtype=torch.float16, requires_grad=True)\n",
      "step №927: loss = 26.289897918701172, weights = tensor([6.2422, 3.1738], dtype=torch.float16, requires_grad=True)\n",
      "step №928: loss = 26.28310775756836, weights = tensor([6.2422, 3.1758], dtype=torch.float16, requires_grad=True)\n",
      "step №929: loss = 26.276330947875977, weights = tensor([6.2422, 3.1777], dtype=torch.float16, requires_grad=True)\n",
      "step №930: loss = 26.26955795288086, weights = tensor([6.2422, 3.1797], dtype=torch.float16, requires_grad=True)\n",
      "step №931: loss = 26.262798309326172, weights = tensor([6.2422, 3.1816], dtype=torch.float16, requires_grad=True)\n",
      "step №932: loss = 26.256038665771484, weights = tensor([6.2422, 3.1836], dtype=torch.float16, requires_grad=True)\n",
      "step №933: loss = 26.249292373657227, weights = tensor([6.2383, 3.1855], dtype=torch.float16, requires_grad=True)\n",
      "step №934: loss = 26.230194091796875, weights = tensor([6.2383, 3.1875], dtype=torch.float16, requires_grad=True)\n",
      "step №935: loss = 26.2233943939209, weights = tensor([6.2383, 3.1895], dtype=torch.float16, requires_grad=True)\n",
      "step №936: loss = 26.216598510742188, weights = tensor([6.2383, 3.1914], dtype=torch.float16, requires_grad=True)\n",
      "step №937: loss = 26.209814071655273, weights = tensor([6.2383, 3.1934], dtype=torch.float16, requires_grad=True)\n",
      "step №938: loss = 26.203033447265625, weights = tensor([6.2383, 3.1953], dtype=torch.float16, requires_grad=True)\n",
      "step №939: loss = 26.196264266967773, weights = tensor([6.2383, 3.1973], dtype=torch.float16, requires_grad=True)\n",
      "step №940: loss = 26.189498901367188, weights = tensor([6.2383, 3.1992], dtype=torch.float16, requires_grad=True)\n",
      "step №941: loss = 26.1827449798584, weights = tensor([6.2383, 3.2012], dtype=torch.float16, requires_grad=True)\n",
      "step №942: loss = 26.175994873046875, weights = tensor([6.2383, 3.2031], dtype=torch.float16, requires_grad=True)\n",
      "step №943: loss = 26.16925621032715, weights = tensor([6.2383, 3.2051], dtype=torch.float16, requires_grad=True)\n",
      "step №944: loss = 26.162521362304688, weights = tensor([6.2383, 3.2070], dtype=torch.float16, requires_grad=True)\n",
      "step №945: loss = 26.155797958374023, weights = tensor([6.2383, 3.2090], dtype=torch.float16, requires_grad=True)\n",
      "step №946: loss = 26.149078369140625, weights = tensor([6.2344, 3.2109], dtype=torch.float16, requires_grad=True)\n",
      "step №947: loss = 26.129993438720703, weights = tensor([6.2344, 3.2129], dtype=torch.float16, requires_grad=True)\n",
      "step №948: loss = 26.123218536376953, weights = tensor([6.2344, 3.2148], dtype=torch.float16, requires_grad=True)\n",
      "step №949: loss = 26.116455078125, weights = tensor([6.2344, 3.2168], dtype=torch.float16, requires_grad=True)\n",
      "step №950: loss = 26.109699249267578, weights = tensor([6.2344, 3.2188], dtype=torch.float16, requires_grad=True)\n",
      "step №951: loss = 26.102954864501953, weights = tensor([6.2344, 3.2207], dtype=torch.float16, requires_grad=True)\n",
      "step №952: loss = 26.096210479736328, weights = tensor([6.2344, 3.2227], dtype=torch.float16, requires_grad=True)\n",
      "step №953: loss = 26.0894775390625, weights = tensor([6.2344, 3.2246], dtype=torch.float16, requires_grad=True)\n",
      "step №954: loss = 26.082752227783203, weights = tensor([6.2344, 3.2266], dtype=torch.float16, requires_grad=True)\n",
      "step №955: loss = 26.076038360595703, weights = tensor([6.2344, 3.2285], dtype=torch.float16, requires_grad=True)\n",
      "step №956: loss = 26.069324493408203, weights = tensor([6.2344, 3.2305], dtype=torch.float16, requires_grad=True)\n",
      "step №957: loss = 26.0626220703125, weights = tensor([6.2344, 3.2324], dtype=torch.float16, requires_grad=True)\n",
      "step №958: loss = 26.055927276611328, weights = tensor([6.2344, 3.2344], dtype=torch.float16, requires_grad=True)\n",
      "step №959: loss = 26.049243927001953, weights = tensor([6.2305, 3.2363], dtype=torch.float16, requires_grad=True)\n",
      "step №960: loss = 26.0301570892334, weights = tensor([6.2305, 3.2383], dtype=torch.float16, requires_grad=True)\n",
      "step №961: loss = 26.023418426513672, weights = tensor([6.2305, 3.2402], dtype=torch.float16, requires_grad=True)\n",
      "step №962: loss = 26.01668357849121, weights = tensor([6.2305, 3.2422], dtype=torch.float16, requires_grad=True)\n",
      "step №963: loss = 26.009960174560547, weights = tensor([6.2305, 3.2441], dtype=torch.float16, requires_grad=True)\n",
      "step №964: loss = 26.00324058532715, weights = tensor([6.2305, 3.2461], dtype=torch.float16, requires_grad=True)\n",
      "step №965: loss = 25.996532440185547, weights = tensor([6.2305, 3.2480], dtype=torch.float16, requires_grad=True)\n",
      "step №966: loss = 25.98982810974121, weights = tensor([6.2305, 3.2500], dtype=torch.float16, requires_grad=True)\n",
      "step №967: loss = 25.983135223388672, weights = tensor([6.2305, 3.2520], dtype=torch.float16, requires_grad=True)\n",
      "step №968: loss = 25.9764461517334, weights = tensor([6.2305, 3.2539], dtype=torch.float16, requires_grad=True)\n",
      "step №969: loss = 25.969768524169922, weights = tensor([6.2305, 3.2559], dtype=torch.float16, requires_grad=True)\n",
      "step №970: loss = 25.96309471130371, weights = tensor([6.2305, 3.2578], dtype=torch.float16, requires_grad=True)\n",
      "step №971: loss = 25.956432342529297, weights = tensor([6.2266, 3.2598], dtype=torch.float16, requires_grad=True)\n",
      "step №972: loss = 25.93741798400879, weights = tensor([6.2266, 3.2617], dtype=torch.float16, requires_grad=True)\n",
      "step №973: loss = 25.93070411682129, weights = tensor([6.2266, 3.2637], dtype=torch.float16, requires_grad=True)\n",
      "step №974: loss = 25.92399024963379, weights = tensor([6.2266, 3.2656], dtype=torch.float16, requires_grad=True)\n",
      "step №975: loss = 25.91729164123535, weights = tensor([6.2266, 3.2676], dtype=torch.float16, requires_grad=True)\n",
      "step №976: loss = 25.910593032836914, weights = tensor([6.2266, 3.2695], dtype=torch.float16, requires_grad=True)\n",
      "step №977: loss = 25.90390968322754, weights = tensor([6.2266, 3.2715], dtype=torch.float16, requires_grad=True)\n",
      "step №978: loss = 25.897226333618164, weights = tensor([6.2266, 3.2734], dtype=torch.float16, requires_grad=True)\n",
      "step №979: loss = 25.89055824279785, weights = tensor([6.2266, 3.2754], dtype=torch.float16, requires_grad=True)\n",
      "step №980: loss = 25.88389015197754, weights = tensor([6.2266, 3.2773], dtype=torch.float16, requires_grad=True)\n",
      "step №981: loss = 25.87723731994629, weights = tensor([6.2266, 3.2793], dtype=torch.float16, requires_grad=True)\n",
      "step №982: loss = 25.87058448791504, weights = tensor([6.2266, 3.2812], dtype=torch.float16, requires_grad=True)\n",
      "step №983: loss = 25.86394691467285, weights = tensor([6.2266, 3.2832], dtype=torch.float16, requires_grad=True)\n",
      "step №984: loss = 25.857309341430664, weights = tensor([6.2227, 3.2852], dtype=torch.float16, requires_grad=True)\n",
      "step №985: loss = 25.838306427001953, weights = tensor([6.2227, 3.2871], dtype=torch.float16, requires_grad=True)\n",
      "step №986: loss = 25.831615447998047, weights = tensor([6.2227, 3.2891], dtype=torch.float16, requires_grad=True)\n",
      "step №987: loss = 25.824939727783203, weights = tensor([6.2227, 3.2910], dtype=torch.float16, requires_grad=True)\n",
      "step №988: loss = 25.81826400756836, weights = tensor([6.2227, 3.2930], dtype=torch.float16, requires_grad=True)\n",
      "step №989: loss = 25.811603546142578, weights = tensor([6.2227, 3.2949], dtype=torch.float16, requires_grad=True)\n",
      "step №990: loss = 25.804943084716797, weights = tensor([6.2227, 3.2969], dtype=torch.float16, requires_grad=True)\n",
      "step №991: loss = 25.798297882080078, weights = tensor([6.2227, 3.2988], dtype=torch.float16, requires_grad=True)\n",
      "step №992: loss = 25.79165267944336, weights = tensor([6.2227, 3.3008], dtype=torch.float16, requires_grad=True)\n",
      "step №993: loss = 25.785022735595703, weights = tensor([6.2227, 3.3027], dtype=torch.float16, requires_grad=True)\n",
      "step №994: loss = 25.778392791748047, weights = tensor([6.2227, 3.3047], dtype=torch.float16, requires_grad=True)\n",
      "step №995: loss = 25.771778106689453, weights = tensor([6.2227, 3.3066], dtype=torch.float16, requires_grad=True)\n",
      "step №996: loss = 25.76516342163086, weights = tensor([6.2227, 3.3086], dtype=torch.float16, requires_grad=True)\n",
      "step №997: loss = 25.758563995361328, weights = tensor([6.2188, 3.3105], dtype=torch.float16, requires_grad=True)\n",
      "step №998: loss = 25.73956298828125, weights = tensor([6.2188, 3.3125], dtype=torch.float16, requires_grad=True)\n",
      "step №999: loss = 25.73291015625, weights = tensor([6.2188, 3.3145], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#now let's use PyTorch gradient calculation. We don't need our gradient function. \n",
    "learning_rait = 0.0006 \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float16, requires_grad = True) #it should be mentioned that in the future \n",
    "                                                            #you will need a derivative with respect to these values\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = mseerror(y, y_pred)\n",
    "    error.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rait*w.grad\n",
    "    w.grad.zero_()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "193b97bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Derivative is calculated by Pytorch. Slope = 6.21875, intercept = 3.314453125, loss = 25.73291015625')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Derivative is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7eecf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №0: loss = 1316.4000244140625, weights = tensor([0.2287, 0.0396], requires_grad=True)\n",
      "step №1: loss = 1228.1724853515625, weights = tensor([0.4494, 0.0779], requires_grad=True)\n",
      "step №2: loss = 1146.02197265625, weights = tensor([0.6623, 0.1150], requires_grad=True)\n",
      "step №3: loss = 1069.5299072265625, weights = tensor([0.8678, 0.1509], requires_grad=True)\n",
      "step №4: loss = 998.3065185546875, weights = tensor([1.0660, 0.1856], requires_grad=True)\n",
      "step №5: loss = 931.9890747070312, weights = tensor([1.2573, 0.2192], requires_grad=True)\n",
      "step №6: loss = 870.2392578125, weights = tensor([1.4418, 0.2518], requires_grad=True)\n",
      "step №7: loss = 812.7423706054688, weights = tensor([1.6199, 0.2833], requires_grad=True)\n",
      "step №8: loss = 759.2057495117188, weights = tensor([1.7916, 0.3138], requires_grad=True)\n",
      "step №9: loss = 709.3562622070312, weights = tensor([1.9574, 0.3434], requires_grad=True)\n",
      "step №10: loss = 662.9398803710938, weights = tensor([2.1173, 0.3720], requires_grad=True)\n",
      "step №11: loss = 619.7203369140625, weights = tensor([2.2716, 0.3997], requires_grad=True)\n",
      "step №12: loss = 579.47705078125, weights = tensor([2.4205, 0.4265], requires_grad=True)\n",
      "step №13: loss = 542.00537109375, weights = tensor([2.5641, 0.4526], requires_grad=True)\n",
      "step №14: loss = 507.1141662597656, weights = tensor([2.7027, 0.4778], requires_grad=True)\n",
      "step №15: loss = 474.62554931640625, weights = tensor([2.8364, 0.5022], requires_grad=True)\n",
      "step №16: loss = 444.3741149902344, weights = tensor([2.9654, 0.5259], requires_grad=True)\n",
      "step №17: loss = 416.2057189941406, weights = tensor([3.0899, 0.5488], requires_grad=True)\n",
      "step №18: loss = 389.9768981933594, weights = tensor([3.2100, 0.5711], requires_grad=True)\n",
      "step №19: loss = 365.5539245605469, weights = tensor([3.3258, 0.5927], requires_grad=True)\n",
      "step №20: loss = 342.81256103515625, weights = tensor([3.4376, 0.6136], requires_grad=True)\n",
      "step №21: loss = 321.6367492675781, weights = tensor([3.5454, 0.6339], requires_grad=True)\n",
      "step №22: loss = 301.9187927246094, weights = tensor([3.6495, 0.6536], requires_grad=True)\n",
      "step №23: loss = 283.5581359863281, weights = tensor([3.7499, 0.6727], requires_grad=True)\n",
      "step №24: loss = 266.4613952636719, weights = tensor([3.8467, 0.6913], requires_grad=True)\n",
      "step №25: loss = 250.5414581298828, weights = tensor([3.9401, 0.7093], requires_grad=True)\n",
      "step №26: loss = 235.71719360351562, weights = tensor([4.0303, 0.7267], requires_grad=True)\n",
      "step №27: loss = 221.9131317138672, weights = tensor([4.1172, 0.7437], requires_grad=True)\n",
      "step №28: loss = 209.05905151367188, weights = tensor([4.2011, 0.7602], requires_grad=True)\n",
      "step №29: loss = 197.08944702148438, weights = tensor([4.2821, 0.7762], requires_grad=True)\n",
      "step №30: loss = 185.94345092773438, weights = tensor([4.3601, 0.7917], requires_grad=True)\n",
      "step №31: loss = 175.564208984375, weights = tensor([4.4355, 0.8068], requires_grad=True)\n",
      "step №32: loss = 165.8989715576172, weights = tensor([4.5081, 0.8215], requires_grad=True)\n",
      "step №33: loss = 156.8985595703125, weights = tensor([4.5782, 0.8358], requires_grad=True)\n",
      "step №34: loss = 148.51712036132812, weights = tensor([4.6459, 0.8496], requires_grad=True)\n",
      "step №35: loss = 140.71206665039062, weights = tensor([4.7111, 0.8631], requires_grad=True)\n",
      "step №36: loss = 133.44369506835938, weights = tensor([4.7741, 0.8763], requires_grad=True)\n",
      "step №37: loss = 126.67497253417969, weights = tensor([4.8348, 0.8890], requires_grad=True)\n",
      "step №38: loss = 120.37154388427734, weights = tensor([4.8933, 0.9015], requires_grad=True)\n",
      "step №39: loss = 114.50130462646484, weights = tensor([4.9498, 0.9136], requires_grad=True)\n",
      "step №40: loss = 109.0344009399414, weights = tensor([5.0043, 0.9253], requires_grad=True)\n",
      "step №41: loss = 103.94309997558594, weights = tensor([5.0569, 0.9368], requires_grad=True)\n",
      "step №42: loss = 99.2015151977539, weights = tensor([5.1076, 0.9480], requires_grad=True)\n",
      "step №43: loss = 94.78550720214844, weights = tensor([5.1566, 0.9588], requires_grad=True)\n",
      "step №44: loss = 90.67268371582031, weights = tensor([5.2037, 0.9694], requires_grad=True)\n",
      "step №45: loss = 86.84214782714844, weights = tensor([5.2493, 0.9798], requires_grad=True)\n",
      "step №46: loss = 83.27444458007812, weights = tensor([5.2932, 0.9899], requires_grad=True)\n",
      "step №47: loss = 79.95153045654297, weights = tensor([5.3355, 0.9997], requires_grad=True)\n",
      "step №48: loss = 76.85648345947266, weights = tensor([5.3764, 1.0093], requires_grad=True)\n",
      "step №49: loss = 73.97358703613281, weights = tensor([5.4158, 1.0186], requires_grad=True)\n",
      "step №50: loss = 71.28828430175781, weights = tensor([5.4538, 1.0278], requires_grad=True)\n",
      "step №51: loss = 68.78691101074219, weights = tensor([5.4904, 1.0367], requires_grad=True)\n",
      "step №52: loss = 66.45682525634766, weights = tensor([5.5258, 1.0454], requires_grad=True)\n",
      "step №53: loss = 64.28620910644531, weights = tensor([5.5599, 1.0539], requires_grad=True)\n",
      "step №54: loss = 62.26410675048828, weights = tensor([5.5927, 1.0622], requires_grad=True)\n",
      "step №55: loss = 60.38026809692383, weights = tensor([5.6244, 1.0703], requires_grad=True)\n",
      "step №56: loss = 58.62517166137695, weights = tensor([5.6550, 1.0783], requires_grad=True)\n",
      "step №57: loss = 56.98993682861328, weights = tensor([5.6845, 1.0860], requires_grad=True)\n",
      "step №58: loss = 55.46631622314453, weights = tensor([5.7130, 1.0936], requires_grad=True)\n",
      "step №59: loss = 54.04662322998047, weights = tensor([5.7404, 1.1011], requires_grad=True)\n",
      "step №60: loss = 52.72371292114258, weights = tensor([5.7669, 1.1084], requires_grad=True)\n",
      "step №61: loss = 51.490867614746094, weights = tensor([5.7924, 1.1155], requires_grad=True)\n",
      "step №62: loss = 50.3419303894043, weights = tensor([5.8170, 1.1225], requires_grad=True)\n",
      "step №63: loss = 49.27111053466797, weights = tensor([5.8407, 1.1293], requires_grad=True)\n",
      "step №64: loss = 48.27302932739258, weights = tensor([5.8635, 1.1360], requires_grad=True)\n",
      "step №65: loss = 47.34267044067383, weights = tensor([5.8856, 1.1426], requires_grad=True)\n",
      "step №66: loss = 46.4753532409668, weights = tensor([5.9069, 1.1490], requires_grad=True)\n",
      "step №67: loss = 45.666770935058594, weights = tensor([5.9274, 1.1554], requires_grad=True)\n",
      "step №68: loss = 44.91286849975586, weights = tensor([5.9471, 1.1616], requires_grad=True)\n",
      "step №69: loss = 44.20984649658203, weights = tensor([5.9662, 1.1677], requires_grad=True)\n",
      "step №70: loss = 43.554229736328125, weights = tensor([5.9846, 1.1736], requires_grad=True)\n",
      "step №71: loss = 42.9427490234375, weights = tensor([6.0023, 1.1795], requires_grad=True)\n",
      "step №72: loss = 42.37236404418945, weights = tensor([6.0193, 1.1853], requires_grad=True)\n",
      "step №73: loss = 41.84023666381836, weights = tensor([6.0358, 1.1910], requires_grad=True)\n",
      "step №74: loss = 41.34374237060547, weights = tensor([6.0517, 1.1965], requires_grad=True)\n",
      "step №75: loss = 40.88042449951172, weights = tensor([6.0670, 1.2020], requires_grad=True)\n",
      "step №76: loss = 40.448001861572266, weights = tensor([6.0817, 1.2074], requires_grad=True)\n",
      "step №77: loss = 40.044349670410156, weights = tensor([6.0959, 1.2127], requires_grad=True)\n",
      "step №78: loss = 39.66746139526367, weights = tensor([6.1096, 1.2180], requires_grad=True)\n",
      "step №79: loss = 39.315513610839844, weights = tensor([6.1228, 1.2231], requires_grad=True)\n",
      "step №80: loss = 38.986785888671875, weights = tensor([6.1355, 1.2282], requires_grad=True)\n",
      "step №81: loss = 38.67967987060547, weights = tensor([6.1478, 1.2332], requires_grad=True)\n",
      "step №82: loss = 38.3927001953125, weights = tensor([6.1596, 1.2381], requires_grad=True)\n",
      "step №83: loss = 38.12447738647461, weights = tensor([6.1709, 1.2429], requires_grad=True)\n",
      "step №84: loss = 37.87371063232422, weights = tensor([6.1819, 1.2477], requires_grad=True)\n",
      "step №85: loss = 37.639183044433594, weights = tensor([6.1925, 1.2525], requires_grad=True)\n",
      "step №86: loss = 37.419803619384766, weights = tensor([6.2026, 1.2571], requires_grad=True)\n",
      "step №87: loss = 37.21451187133789, weights = tensor([6.2124, 1.2617], requires_grad=True)\n",
      "step №88: loss = 37.022342681884766, weights = tensor([6.2219, 1.2662], requires_grad=True)\n",
      "step №89: loss = 36.84238815307617, weights = tensor([6.2310, 1.2707], requires_grad=True)\n",
      "step №90: loss = 36.67381286621094, weights = tensor([6.2397, 1.2752], requires_grad=True)\n",
      "step №91: loss = 36.515838623046875, weights = tensor([6.2482, 1.2795], requires_grad=True)\n",
      "step №92: loss = 36.36772155761719, weights = tensor([6.2563, 1.2839], requires_grad=True)\n",
      "step №93: loss = 36.22878646850586, weights = tensor([6.2641, 1.2881], requires_grad=True)\n",
      "step №94: loss = 36.09841537475586, weights = tensor([6.2716, 1.2924], requires_grad=True)\n",
      "step №95: loss = 35.97600173950195, weights = tensor([6.2789, 1.2965], requires_grad=True)\n",
      "step №96: loss = 35.86101531982422, weights = tensor([6.2859, 1.3007], requires_grad=True)\n",
      "step №97: loss = 35.752925872802734, weights = tensor([6.2926, 1.3048], requires_grad=True)\n",
      "step №98: loss = 35.651268005371094, weights = tensor([6.2991, 1.3088], requires_grad=True)\n",
      "step №99: loss = 35.55561447143555, weights = tensor([6.3053, 1.3128], requires_grad=True)\n",
      "step №100: loss = 35.46552658081055, weights = tensor([6.3113, 1.3168], requires_grad=True)\n",
      "step №101: loss = 35.380638122558594, weights = tensor([6.3170, 1.3208], requires_grad=True)\n",
      "step №102: loss = 35.30057907104492, weights = tensor([6.3226, 1.3247], requires_grad=True)\n",
      "step №103: loss = 35.22502517700195, weights = tensor([6.3279, 1.3285], requires_grad=True)\n",
      "step №104: loss = 35.15367126464844, weights = tensor([6.3331, 1.3324], requires_grad=True)\n",
      "step №105: loss = 35.086219787597656, weights = tensor([6.3380, 1.3362], requires_grad=True)\n",
      "step №106: loss = 35.02241134643555, weights = tensor([6.3427, 1.3399], requires_grad=True)\n",
      "step №107: loss = 34.96198272705078, weights = tensor([6.3473, 1.3437], requires_grad=True)\n",
      "step №108: loss = 34.90471649169922, weights = tensor([6.3517, 1.3474], requires_grad=True)\n",
      "step №109: loss = 34.85039138793945, weights = tensor([6.3559, 1.3511], requires_grad=True)\n",
      "step №110: loss = 34.798797607421875, weights = tensor([6.3600, 1.3547], requires_grad=True)\n",
      "step №111: loss = 34.74974822998047, weights = tensor([6.3638, 1.3584], requires_grad=True)\n",
      "step №112: loss = 34.70308303833008, weights = tensor([6.3676, 1.3620], requires_grad=True)\n",
      "step №113: loss = 34.65862274169922, weights = tensor([6.3712, 1.3656], requires_grad=True)\n",
      "step №114: loss = 34.61622619628906, weights = tensor([6.3746, 1.3691], requires_grad=True)\n",
      "step №115: loss = 34.575740814208984, weights = tensor([6.3779, 1.3726], requires_grad=True)\n",
      "step №116: loss = 34.537044525146484, weights = tensor([6.3811, 1.3762], requires_grad=True)\n",
      "step №117: loss = 34.500022888183594, weights = tensor([6.3842, 1.3796], requires_grad=True)\n",
      "step №118: loss = 34.46454620361328, weights = tensor([6.3871, 1.3831], requires_grad=True)\n",
      "step №119: loss = 34.43051528930664, weights = tensor([6.3899, 1.3866], requires_grad=True)\n",
      "step №120: loss = 34.39780807495117, weights = tensor([6.3926, 1.3900], requires_grad=True)\n",
      "step №121: loss = 34.36639404296875, weights = tensor([6.3952, 1.3934], requires_grad=True)\n",
      "step №122: loss = 34.33611297607422, weights = tensor([6.3977, 1.3968], requires_grad=True)\n",
      "step №123: loss = 34.30693817138672, weights = tensor([6.4001, 1.4002], requires_grad=True)\n",
      "step №124: loss = 34.27875900268555, weights = tensor([6.4023, 1.4035], requires_grad=True)\n",
      "step №125: loss = 34.25154495239258, weights = tensor([6.4045, 1.4069], requires_grad=True)\n",
      "step №126: loss = 34.225196838378906, weights = tensor([6.4066, 1.4102], requires_grad=True)\n",
      "step №127: loss = 34.199684143066406, weights = tensor([6.4086, 1.4135], requires_grad=True)\n",
      "step №128: loss = 34.1749267578125, weights = tensor([6.4105, 1.4168], requires_grad=True)\n",
      "step №129: loss = 34.150882720947266, weights = tensor([6.4124, 1.4201], requires_grad=True)\n",
      "step №130: loss = 34.12749481201172, weights = tensor([6.4141, 1.4234], requires_grad=True)\n",
      "step №131: loss = 34.104736328125, weights = tensor([6.4158, 1.4266], requires_grad=True)\n",
      "step №132: loss = 34.08254623413086, weights = tensor([6.4174, 1.4299], requires_grad=True)\n",
      "step №133: loss = 34.0609016418457, weights = tensor([6.4189, 1.4331], requires_grad=True)\n",
      "step №134: loss = 34.039756774902344, weights = tensor([6.4203, 1.4363], requires_grad=True)\n",
      "step №135: loss = 34.01907730102539, weights = tensor([6.4217, 1.4395], requires_grad=True)\n",
      "step №136: loss = 33.99884033203125, weights = tensor([6.4231, 1.4427], requires_grad=True)\n",
      "step №137: loss = 33.979000091552734, weights = tensor([6.4243, 1.4459], requires_grad=True)\n",
      "step №138: loss = 33.95954895019531, weights = tensor([6.4255, 1.4491], requires_grad=True)\n",
      "step №139: loss = 33.94044494628906, weights = tensor([6.4267, 1.4522], requires_grad=True)\n",
      "step №140: loss = 33.92167282104492, weights = tensor([6.4277, 1.4554], requires_grad=True)\n",
      "step №141: loss = 33.90320587158203, weights = tensor([6.4288, 1.4585], requires_grad=True)\n",
      "step №142: loss = 33.885032653808594, weights = tensor([6.4298, 1.4617], requires_grad=True)\n",
      "step №143: loss = 33.86711883544922, weights = tensor([6.4307, 1.4648], requires_grad=True)\n",
      "step №144: loss = 33.84946823120117, weights = tensor([6.4316, 1.4679], requires_grad=True)\n",
      "step №145: loss = 33.83203887939453, weights = tensor([6.4324, 1.4710], requires_grad=True)\n",
      "step №146: loss = 33.81483840942383, weights = tensor([6.4332, 1.4741], requires_grad=True)\n",
      "step №147: loss = 33.79783630371094, weights = tensor([6.4339, 1.4772], requires_grad=True)\n",
      "step №148: loss = 33.781028747558594, weights = tensor([6.4346, 1.4803], requires_grad=True)\n",
      "step №149: loss = 33.76439666748047, weights = tensor([6.4353, 1.4834], requires_grad=True)\n",
      "step №150: loss = 33.74793243408203, weights = tensor([6.4359, 1.4864], requires_grad=True)\n",
      "step №151: loss = 33.73162078857422, weights = tensor([6.4365, 1.4895], requires_grad=True)\n",
      "step №152: loss = 33.7154541015625, weights = tensor([6.4371, 1.4926], requires_grad=True)\n",
      "step №153: loss = 33.699424743652344, weights = tensor([6.4376, 1.4956], requires_grad=True)\n",
      "step №154: loss = 33.68352508544922, weights = tensor([6.4380, 1.4986], requires_grad=True)\n",
      "step №155: loss = 33.667747497558594, weights = tensor([6.4385, 1.5017], requires_grad=True)\n",
      "step №156: loss = 33.65208435058594, weights = tensor([6.4389, 1.5047], requires_grad=True)\n",
      "step №157: loss = 33.636512756347656, weights = tensor([6.4393, 1.5077], requires_grad=True)\n",
      "step №158: loss = 33.621055603027344, weights = tensor([6.4396, 1.5108], requires_grad=True)\n",
      "step №159: loss = 33.60568618774414, weights = tensor([6.4400, 1.5138], requires_grad=True)\n",
      "step №160: loss = 33.590389251708984, weights = tensor([6.4403, 1.5168], requires_grad=True)\n",
      "step №161: loss = 33.57518768310547, weights = tensor([6.4405, 1.5198], requires_grad=True)\n",
      "step №162: loss = 33.56005859375, weights = tensor([6.4408, 1.5228], requires_grad=True)\n",
      "step №163: loss = 33.54500198364258, weights = tensor([6.4410, 1.5258], requires_grad=True)\n",
      "step №164: loss = 33.53001403808594, weights = tensor([6.4412, 1.5288], requires_grad=True)\n",
      "step №165: loss = 33.51508331298828, weights = tensor([6.4414, 1.5317], requires_grad=True)\n",
      "step №166: loss = 33.500221252441406, weights = tensor([6.4415, 1.5347], requires_grad=True)\n",
      "step №167: loss = 33.48540496826172, weights = tensor([6.4417, 1.5377], requires_grad=True)\n",
      "step №168: loss = 33.47064971923828, weights = tensor([6.4418, 1.5407], requires_grad=True)\n",
      "step №169: loss = 33.45594024658203, weights = tensor([6.4419, 1.5436], requires_grad=True)\n",
      "step №170: loss = 33.441287994384766, weights = tensor([6.4419, 1.5466], requires_grad=True)\n",
      "step №171: loss = 33.42666244506836, weights = tensor([6.4420, 1.5495], requires_grad=True)\n",
      "step №172: loss = 33.4120979309082, weights = tensor([6.4420, 1.5525], requires_grad=True)\n",
      "step №173: loss = 33.39755630493164, weights = tensor([6.4421, 1.5555], requires_grad=True)\n",
      "step №174: loss = 33.38306427001953, weights = tensor([6.4421, 1.5584], requires_grad=True)\n",
      "step №175: loss = 33.36860656738281, weights = tensor([6.4420, 1.5613], requires_grad=True)\n",
      "step №176: loss = 33.35417938232422, weights = tensor([6.4420, 1.5643], requires_grad=True)\n",
      "step №177: loss = 33.339786529541016, weights = tensor([6.4420, 1.5672], requires_grad=True)\n",
      "step №178: loss = 33.32542419433594, weights = tensor([6.4419, 1.5701], requires_grad=True)\n",
      "step №179: loss = 33.311092376708984, weights = tensor([6.4418, 1.5731], requires_grad=True)\n",
      "step №180: loss = 33.29678726196289, weights = tensor([6.4418, 1.5760], requires_grad=True)\n",
      "step №181: loss = 33.282508850097656, weights = tensor([6.4417, 1.5789], requires_grad=True)\n",
      "step №182: loss = 33.26826095581055, weights = tensor([6.4415, 1.5818], requires_grad=True)\n",
      "step №183: loss = 33.254024505615234, weights = tensor([6.4414, 1.5848], requires_grad=True)\n",
      "step №184: loss = 33.23982238769531, weights = tensor([6.4413, 1.5877], requires_grad=True)\n",
      "step №185: loss = 33.22563552856445, weights = tensor([6.4411, 1.5906], requires_grad=True)\n",
      "step №186: loss = 33.21147918701172, weights = tensor([6.4410, 1.5935], requires_grad=True)\n",
      "step №187: loss = 33.19733428955078, weights = tensor([6.4408, 1.5964], requires_grad=True)\n",
      "step №188: loss = 33.1832160949707, weights = tensor([6.4406, 1.5993], requires_grad=True)\n",
      "step №189: loss = 33.16911315917969, weights = tensor([6.4405, 1.6022], requires_grad=True)\n",
      "step №190: loss = 33.155033111572266, weights = tensor([6.4403, 1.6051], requires_grad=True)\n",
      "step №191: loss = 33.140968322753906, weights = tensor([6.4401, 1.6080], requires_grad=True)\n",
      "step №192: loss = 33.126914978027344, weights = tensor([6.4398, 1.6109], requires_grad=True)\n",
      "step №193: loss = 33.11288833618164, weights = tensor([6.4396, 1.6138], requires_grad=True)\n",
      "step №194: loss = 33.09886932373047, weights = tensor([6.4394, 1.6167], requires_grad=True)\n",
      "step №195: loss = 33.084877014160156, weights = tensor([6.4392, 1.6196], requires_grad=True)\n",
      "step №196: loss = 33.070892333984375, weights = tensor([6.4389, 1.6225], requires_grad=True)\n",
      "step №197: loss = 33.056922912597656, weights = tensor([6.4387, 1.6253], requires_grad=True)\n",
      "step №198: loss = 33.04296875, weights = tensor([6.4384, 1.6282], requires_grad=True)\n",
      "step №199: loss = 33.02902603149414, weights = tensor([6.4381, 1.6311], requires_grad=True)\n",
      "step №200: loss = 33.015098571777344, weights = tensor([6.4379, 1.6340], requires_grad=True)\n",
      "step №201: loss = 33.001190185546875, weights = tensor([6.4376, 1.6369], requires_grad=True)\n",
      "step №202: loss = 32.98728561401367, weights = tensor([6.4373, 1.6397], requires_grad=True)\n",
      "step №203: loss = 32.97339630126953, weights = tensor([6.4370, 1.6426], requires_grad=True)\n",
      "step №204: loss = 32.95952224731445, weights = tensor([6.4367, 1.6455], requires_grad=True)\n",
      "step №205: loss = 32.9456672668457, weights = tensor([6.4364, 1.6483], requires_grad=True)\n",
      "step №206: loss = 32.931819915771484, weights = tensor([6.4361, 1.6512], requires_grad=True)\n",
      "step №207: loss = 32.9179801940918, weights = tensor([6.4358, 1.6541], requires_grad=True)\n",
      "step №208: loss = 32.90414810180664, weights = tensor([6.4355, 1.6569], requires_grad=True)\n",
      "step №209: loss = 32.89033508300781, weights = tensor([6.4352, 1.6598], requires_grad=True)\n",
      "step №210: loss = 32.87653732299805, weights = tensor([6.4348, 1.6626], requires_grad=True)\n",
      "step №211: loss = 32.86275100708008, weights = tensor([6.4345, 1.6655], requires_grad=True)\n",
      "step №212: loss = 32.848968505859375, weights = tensor([6.4342, 1.6684], requires_grad=True)\n",
      "step №213: loss = 32.83519744873047, weights = tensor([6.4338, 1.6712], requires_grad=True)\n",
      "step №214: loss = 32.82144546508789, weights = tensor([6.4335, 1.6741], requires_grad=True)\n",
      "step №215: loss = 32.80769348144531, weights = tensor([6.4331, 1.6769], requires_grad=True)\n",
      "step №216: loss = 32.79396057128906, weights = tensor([6.4328, 1.6798], requires_grad=True)\n",
      "step №217: loss = 32.780235290527344, weights = tensor([6.4324, 1.6826], requires_grad=True)\n",
      "step №218: loss = 32.766517639160156, weights = tensor([6.4321, 1.6854], requires_grad=True)\n",
      "step №219: loss = 32.75281524658203, weights = tensor([6.4317, 1.6883], requires_grad=True)\n",
      "step №220: loss = 32.7391242980957, weights = tensor([6.4314, 1.6911], requires_grad=True)\n",
      "step №221: loss = 32.725433349609375, weights = tensor([6.4310, 1.6940], requires_grad=True)\n",
      "step №222: loss = 32.71175765991211, weights = tensor([6.4306, 1.6968], requires_grad=True)\n",
      "step №223: loss = 32.698097229003906, weights = tensor([6.4303, 1.6997], requires_grad=True)\n",
      "step №224: loss = 32.68444061279297, weights = tensor([6.4299, 1.7025], requires_grad=True)\n",
      "step №225: loss = 32.670799255371094, weights = tensor([6.4295, 1.7053], requires_grad=True)\n",
      "step №226: loss = 32.65716552734375, weights = tensor([6.4291, 1.7082], requires_grad=True)\n",
      "step №227: loss = 32.64353942871094, weights = tensor([6.4288, 1.7110], requires_grad=True)\n",
      "step №228: loss = 32.62992477416992, weights = tensor([6.4284, 1.7138], requires_grad=True)\n",
      "step №229: loss = 32.61632537841797, weights = tensor([6.4280, 1.7167], requires_grad=True)\n",
      "step №230: loss = 32.60272979736328, weights = tensor([6.4276, 1.7195], requires_grad=True)\n",
      "step №231: loss = 32.58914566040039, weights = tensor([6.4272, 1.7223], requires_grad=True)\n",
      "step №232: loss = 32.575565338134766, weights = tensor([6.4268, 1.7251], requires_grad=True)\n",
      "step №233: loss = 32.5620002746582, weights = tensor([6.4264, 1.7280], requires_grad=True)\n",
      "step №234: loss = 32.54844665527344, weights = tensor([6.4260, 1.7308], requires_grad=True)\n",
      "step №235: loss = 32.5349006652832, weights = tensor([6.4256, 1.7336], requires_grad=True)\n",
      "step №236: loss = 32.521358489990234, weights = tensor([6.4252, 1.7364], requires_grad=True)\n",
      "step №237: loss = 32.50783157348633, weights = tensor([6.4248, 1.7393], requires_grad=True)\n",
      "step №238: loss = 32.49431610107422, weights = tensor([6.4244, 1.7421], requires_grad=True)\n",
      "step №239: loss = 32.480812072753906, weights = tensor([6.4240, 1.7449], requires_grad=True)\n",
      "step №240: loss = 32.467308044433594, weights = tensor([6.4236, 1.7477], requires_grad=True)\n",
      "step №241: loss = 32.45381546020508, weights = tensor([6.4232, 1.7505], requires_grad=True)\n",
      "step №242: loss = 32.44033432006836, weights = tensor([6.4228, 1.7533], requires_grad=True)\n",
      "step №243: loss = 32.42686462402344, weights = tensor([6.4224, 1.7561], requires_grad=True)\n",
      "step №244: loss = 32.41340255737305, weights = tensor([6.4220, 1.7590], requires_grad=True)\n",
      "step №245: loss = 32.39995193481445, weights = tensor([6.4216, 1.7618], requires_grad=True)\n",
      "step №246: loss = 32.38650894165039, weights = tensor([6.4212, 1.7646], requires_grad=True)\n",
      "step №247: loss = 32.37307357788086, weights = tensor([6.4208, 1.7674], requires_grad=True)\n",
      "step №248: loss = 32.35965347290039, weights = tensor([6.4204, 1.7702], requires_grad=True)\n",
      "step №249: loss = 32.34623718261719, weights = tensor([6.4199, 1.7730], requires_grad=True)\n",
      "step №250: loss = 32.332828521728516, weights = tensor([6.4195, 1.7758], requires_grad=True)\n",
      "step №251: loss = 32.319435119628906, weights = tensor([6.4191, 1.7786], requires_grad=True)\n",
      "step №252: loss = 32.30604553222656, weights = tensor([6.4187, 1.7814], requires_grad=True)\n",
      "step №253: loss = 32.29266357421875, weights = tensor([6.4183, 1.7842], requires_grad=True)\n",
      "step №254: loss = 32.279296875, weights = tensor([6.4178, 1.7870], requires_grad=True)\n",
      "step №255: loss = 32.265933990478516, weights = tensor([6.4174, 1.7898], requires_grad=True)\n",
      "step №256: loss = 32.252586364746094, weights = tensor([6.4170, 1.7926], requires_grad=True)\n",
      "step №257: loss = 32.23924255371094, weights = tensor([6.4166, 1.7954], requires_grad=True)\n",
      "step №258: loss = 32.22590255737305, weights = tensor([6.4162, 1.7982], requires_grad=True)\n",
      "step №259: loss = 32.21257781982422, weights = tensor([6.4157, 1.8010], requires_grad=True)\n",
      "step №260: loss = 32.19926452636719, weights = tensor([6.4153, 1.8038], requires_grad=True)\n",
      "step №261: loss = 32.18596267700195, weights = tensor([6.4149, 1.8066], requires_grad=True)\n",
      "step №262: loss = 32.172664642333984, weights = tensor([6.4145, 1.8094], requires_grad=True)\n",
      "step №263: loss = 32.15938186645508, weights = tensor([6.4140, 1.8122], requires_grad=True)\n",
      "step №264: loss = 32.146095275878906, weights = tensor([6.4136, 1.8150], requires_grad=True)\n",
      "step №265: loss = 32.132835388183594, weights = tensor([6.4132, 1.8177], requires_grad=True)\n",
      "step №266: loss = 32.119571685791016, weights = tensor([6.4128, 1.8205], requires_grad=True)\n",
      "step №267: loss = 32.10631561279297, weights = tensor([6.4123, 1.8233], requires_grad=True)\n",
      "step №268: loss = 32.09307861328125, weights = tensor([6.4119, 1.8261], requires_grad=True)\n",
      "step №269: loss = 32.07984924316406, weights = tensor([6.4115, 1.8289], requires_grad=True)\n",
      "step №270: loss = 32.066612243652344, weights = tensor([6.4110, 1.8317], requires_grad=True)\n",
      "step №271: loss = 32.053401947021484, weights = tensor([6.4106, 1.8345], requires_grad=True)\n",
      "step №272: loss = 32.04019546508789, weights = tensor([6.4102, 1.8372], requires_grad=True)\n",
      "step №273: loss = 32.02699279785156, weights = tensor([6.4098, 1.8400], requires_grad=True)\n",
      "step №274: loss = 32.01380920410156, weights = tensor([6.4093, 1.8428], requires_grad=True)\n",
      "step №275: loss = 32.00062942504883, weights = tensor([6.4089, 1.8456], requires_grad=True)\n",
      "step №276: loss = 31.98745346069336, weights = tensor([6.4085, 1.8483], requires_grad=True)\n",
      "step №277: loss = 31.97429847717285, weights = tensor([6.4080, 1.8511], requires_grad=True)\n",
      "step №278: loss = 31.96114158630371, weights = tensor([6.4076, 1.8539], requires_grad=True)\n",
      "step №279: loss = 31.947998046875, weights = tensor([6.4072, 1.8567], requires_grad=True)\n",
      "step №280: loss = 31.934864044189453, weights = tensor([6.4067, 1.8594], requires_grad=True)\n",
      "step №281: loss = 31.921733856201172, weights = tensor([6.4063, 1.8622], requires_grad=True)\n",
      "step №282: loss = 31.90862464904785, weights = tensor([6.4059, 1.8650], requires_grad=True)\n",
      "step №283: loss = 31.895503997802734, weights = tensor([6.4055, 1.8678], requires_grad=True)\n",
      "step №284: loss = 31.88241195678711, weights = tensor([6.4050, 1.8705], requires_grad=True)\n",
      "step №285: loss = 31.86932373046875, weights = tensor([6.4046, 1.8733], requires_grad=True)\n",
      "step №286: loss = 31.856243133544922, weights = tensor([6.4042, 1.8761], requires_grad=True)\n",
      "step №287: loss = 31.843164443969727, weights = tensor([6.4037, 1.8788], requires_grad=True)\n",
      "step №288: loss = 31.83010482788086, weights = tensor([6.4033, 1.8816], requires_grad=True)\n",
      "step №289: loss = 31.81705093383789, weights = tensor([6.4029, 1.8844], requires_grad=True)\n",
      "step №290: loss = 31.804004669189453, weights = tensor([6.4024, 1.8871], requires_grad=True)\n",
      "step №291: loss = 31.79096031188965, weights = tensor([6.4020, 1.8899], requires_grad=True)\n",
      "step №292: loss = 31.777942657470703, weights = tensor([6.4016, 1.8927], requires_grad=True)\n",
      "step №293: loss = 31.76491928100586, weights = tensor([6.4011, 1.8954], requires_grad=True)\n",
      "step №294: loss = 31.751903533935547, weights = tensor([6.4007, 1.8982], requires_grad=True)\n",
      "step №295: loss = 31.738906860351562, weights = tensor([6.4003, 1.9009], requires_grad=True)\n",
      "step №296: loss = 31.72591209411621, weights = tensor([6.3998, 1.9037], requires_grad=True)\n",
      "step №297: loss = 31.712926864624023, weights = tensor([6.3994, 1.9064], requires_grad=True)\n",
      "step №298: loss = 31.699947357177734, weights = tensor([6.3990, 1.9092], requires_grad=True)\n",
      "step №299: loss = 31.68697738647461, weights = tensor([6.3985, 1.9120], requires_grad=True)\n",
      "step №300: loss = 31.674022674560547, weights = tensor([6.3981, 1.9147], requires_grad=True)\n",
      "step №301: loss = 31.66107749938965, weights = tensor([6.3977, 1.9175], requires_grad=True)\n",
      "step №302: loss = 31.648136138916016, weights = tensor([6.3972, 1.9202], requires_grad=True)\n",
      "step №303: loss = 31.635204315185547, weights = tensor([6.3968, 1.9230], requires_grad=True)\n",
      "step №304: loss = 31.62228012084961, weights = tensor([6.3963, 1.9257], requires_grad=True)\n",
      "step №305: loss = 31.6093692779541, weights = tensor([6.3959, 1.9285], requires_grad=True)\n",
      "step №306: loss = 31.59646224975586, weights = tensor([6.3955, 1.9312], requires_grad=True)\n",
      "step №307: loss = 31.583566665649414, weights = tensor([6.3950, 1.9340], requires_grad=True)\n",
      "step №308: loss = 31.5706787109375, weights = tensor([6.3946, 1.9367], requires_grad=True)\n",
      "step №309: loss = 31.55780029296875, weights = tensor([6.3942, 1.9394], requires_grad=True)\n",
      "step №310: loss = 31.544931411743164, weights = tensor([6.3937, 1.9422], requires_grad=True)\n",
      "step №311: loss = 31.53206443786621, weights = tensor([6.3933, 1.9449], requires_grad=True)\n",
      "step №312: loss = 31.519216537475586, weights = tensor([6.3929, 1.9477], requires_grad=True)\n",
      "step №313: loss = 31.506366729736328, weights = tensor([6.3924, 1.9504], requires_grad=True)\n",
      "step №314: loss = 31.4935359954834, weights = tensor([6.3920, 1.9532], requires_grad=True)\n",
      "step №315: loss = 31.480703353881836, weights = tensor([6.3916, 1.9559], requires_grad=True)\n",
      "step №316: loss = 31.4678897857666, weights = tensor([6.3911, 1.9586], requires_grad=True)\n",
      "step №317: loss = 31.455081939697266, weights = tensor([6.3907, 1.9614], requires_grad=True)\n",
      "step №318: loss = 31.442279815673828, weights = tensor([6.3903, 1.9641], requires_grad=True)\n",
      "step №319: loss = 31.429485321044922, weights = tensor([6.3898, 1.9668], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №320: loss = 31.416706085205078, weights = tensor([6.3894, 1.9696], requires_grad=True)\n",
      "step №321: loss = 31.4039306640625, weights = tensor([6.3890, 1.9723], requires_grad=True)\n",
      "step №322: loss = 31.391155242919922, weights = tensor([6.3885, 1.9750], requires_grad=True)\n",
      "step №323: loss = 31.378408432006836, weights = tensor([6.3881, 1.9778], requires_grad=True)\n",
      "step №324: loss = 31.365652084350586, weights = tensor([6.3877, 1.9805], requires_grad=True)\n",
      "step №325: loss = 31.352914810180664, weights = tensor([6.3872, 1.9832], requires_grad=True)\n",
      "step №326: loss = 31.34018325805664, weights = tensor([6.3868, 1.9860], requires_grad=True)\n",
      "step №327: loss = 31.32745933532715, weights = tensor([6.3864, 1.9887], requires_grad=True)\n",
      "step №328: loss = 31.314743041992188, weights = tensor([6.3859, 1.9914], requires_grad=True)\n",
      "step №329: loss = 31.302032470703125, weights = tensor([6.3855, 1.9941], requires_grad=True)\n",
      "step №330: loss = 31.289342880249023, weights = tensor([6.3851, 1.9969], requires_grad=True)\n",
      "step №331: loss = 31.276653289794922, weights = tensor([6.3846, 1.9996], requires_grad=True)\n",
      "step №332: loss = 31.26397705078125, weights = tensor([6.3842, 2.0023], requires_grad=True)\n",
      "step №333: loss = 31.25130271911621, weights = tensor([6.3838, 2.0050], requires_grad=True)\n",
      "step №334: loss = 31.2386417388916, weights = tensor([6.3833, 2.0078], requires_grad=True)\n",
      "step №335: loss = 31.22598648071289, weights = tensor([6.3829, 2.0105], requires_grad=True)\n",
      "step №336: loss = 31.21333885192871, weights = tensor([6.3825, 2.0132], requires_grad=True)\n",
      "step №337: loss = 31.200708389282227, weights = tensor([6.3820, 2.0159], requires_grad=True)\n",
      "step №338: loss = 31.188079833984375, weights = tensor([6.3816, 2.0186], requires_grad=True)\n",
      "step №339: loss = 31.17545509338379, weights = tensor([6.3812, 2.0214], requires_grad=True)\n",
      "step №340: loss = 31.162845611572266, weights = tensor([6.3808, 2.0241], requires_grad=True)\n",
      "step №341: loss = 31.150238037109375, weights = tensor([6.3803, 2.0268], requires_grad=True)\n",
      "step №342: loss = 31.137645721435547, weights = tensor([6.3799, 2.0295], requires_grad=True)\n",
      "step №343: loss = 31.125057220458984, weights = tensor([6.3795, 2.0322], requires_grad=True)\n",
      "step №344: loss = 31.112483978271484, weights = tensor([6.3790, 2.0349], requires_grad=True)\n",
      "step №345: loss = 31.099918365478516, weights = tensor([6.3786, 2.0376], requires_grad=True)\n",
      "step №346: loss = 31.087352752685547, weights = tensor([6.3782, 2.0404], requires_grad=True)\n",
      "step №347: loss = 31.074804306030273, weights = tensor([6.3777, 2.0431], requires_grad=True)\n",
      "step №348: loss = 31.0622615814209, weights = tensor([6.3773, 2.0458], requires_grad=True)\n",
      "step №349: loss = 31.049724578857422, weights = tensor([6.3769, 2.0485], requires_grad=True)\n",
      "step №350: loss = 31.037200927734375, weights = tensor([6.3764, 2.0512], requires_grad=True)\n",
      "step №351: loss = 31.02468490600586, weights = tensor([6.3760, 2.0539], requires_grad=True)\n",
      "step №352: loss = 31.01217269897461, weights = tensor([6.3756, 2.0566], requires_grad=True)\n",
      "step №353: loss = 30.999670028686523, weights = tensor([6.3751, 2.0593], requires_grad=True)\n",
      "step №354: loss = 30.9871768951416, weights = tensor([6.3747, 2.0620], requires_grad=True)\n",
      "step №355: loss = 30.97469139099121, weights = tensor([6.3743, 2.0647], requires_grad=True)\n",
      "step №356: loss = 30.962223052978516, weights = tensor([6.3739, 2.0674], requires_grad=True)\n",
      "step №357: loss = 30.949756622314453, weights = tensor([6.3734, 2.0701], requires_grad=True)\n",
      "step №358: loss = 30.93729591369629, weights = tensor([6.3730, 2.0728], requires_grad=True)\n",
      "step №359: loss = 30.924840927124023, weights = tensor([6.3726, 2.0755], requires_grad=True)\n",
      "step №360: loss = 30.912403106689453, weights = tensor([6.3721, 2.0782], requires_grad=True)\n",
      "step №361: loss = 30.89996910095215, weights = tensor([6.3717, 2.0809], requires_grad=True)\n",
      "step №362: loss = 30.887548446655273, weights = tensor([6.3713, 2.0836], requires_grad=True)\n",
      "step №363: loss = 30.875125885009766, weights = tensor([6.3709, 2.0863], requires_grad=True)\n",
      "step №364: loss = 30.862720489501953, weights = tensor([6.3704, 2.0890], requires_grad=True)\n",
      "step №365: loss = 30.850317001342773, weights = tensor([6.3700, 2.0917], requires_grad=True)\n",
      "step №366: loss = 30.83793067932129, weights = tensor([6.3696, 2.0944], requires_grad=True)\n",
      "step №367: loss = 30.825551986694336, weights = tensor([6.3691, 2.0971], requires_grad=True)\n",
      "step №368: loss = 30.81317138671875, weights = tensor([6.3687, 2.0997], requires_grad=True)\n",
      "step №369: loss = 30.80080223083496, weights = tensor([6.3683, 2.1024], requires_grad=True)\n",
      "step №370: loss = 30.7884464263916, weights = tensor([6.3679, 2.1051], requires_grad=True)\n",
      "step №371: loss = 30.77609634399414, weights = tensor([6.3674, 2.1078], requires_grad=True)\n",
      "step №372: loss = 30.763757705688477, weights = tensor([6.3670, 2.1105], requires_grad=True)\n",
      "step №373: loss = 30.75142478942871, weights = tensor([6.3666, 2.1132], requires_grad=True)\n",
      "step №374: loss = 30.73910140991211, weights = tensor([6.3661, 2.1159], requires_grad=True)\n",
      "step №375: loss = 30.726791381835938, weights = tensor([6.3657, 2.1186], requires_grad=True)\n",
      "step №376: loss = 30.7144775390625, weights = tensor([6.3653, 2.1212], requires_grad=True)\n",
      "step №377: loss = 30.702173233032227, weights = tensor([6.3649, 2.1239], requires_grad=True)\n",
      "step №378: loss = 30.689891815185547, weights = tensor([6.3644, 2.1266], requires_grad=True)\n",
      "step №379: loss = 30.677600860595703, weights = tensor([6.3640, 2.1293], requires_grad=True)\n",
      "step №380: loss = 30.665334701538086, weights = tensor([6.3636, 2.1320], requires_grad=True)\n",
      "step №381: loss = 30.653066635131836, weights = tensor([6.3631, 2.1346], requires_grad=True)\n",
      "step №382: loss = 30.640804290771484, weights = tensor([6.3627, 2.1373], requires_grad=True)\n",
      "step №383: loss = 30.628559112548828, weights = tensor([6.3623, 2.1400], requires_grad=True)\n",
      "step №384: loss = 30.616317749023438, weights = tensor([6.3619, 2.1427], requires_grad=True)\n",
      "step №385: loss = 30.604089736938477, weights = tensor([6.3614, 2.1453], requires_grad=True)\n",
      "step №386: loss = 30.59186363220215, weights = tensor([6.3610, 2.1480], requires_grad=True)\n",
      "step №387: loss = 30.579641342163086, weights = tensor([6.3606, 2.1507], requires_grad=True)\n",
      "step №388: loss = 30.56743812561035, weights = tensor([6.3602, 2.1534], requires_grad=True)\n",
      "step №389: loss = 30.555240631103516, weights = tensor([6.3597, 2.1560], requires_grad=True)\n",
      "step №390: loss = 30.543045043945312, weights = tensor([6.3593, 2.1587], requires_grad=True)\n",
      "step №391: loss = 30.530858993530273, weights = tensor([6.3589, 2.1614], requires_grad=True)\n",
      "step №392: loss = 30.518688201904297, weights = tensor([6.3585, 2.1640], requires_grad=True)\n",
      "step №393: loss = 30.50652503967285, weights = tensor([6.3580, 2.1667], requires_grad=True)\n",
      "step №394: loss = 30.494359970092773, weights = tensor([6.3576, 2.1694], requires_grad=True)\n",
      "step №395: loss = 30.48221778869629, weights = tensor([6.3572, 2.1720], requires_grad=True)\n",
      "step №396: loss = 30.470073699951172, weights = tensor([6.3568, 2.1747], requires_grad=True)\n",
      "step №397: loss = 30.45794105529785, weights = tensor([6.3563, 2.1774], requires_grad=True)\n",
      "step №398: loss = 30.445812225341797, weights = tensor([6.3559, 2.1800], requires_grad=True)\n",
      "step №399: loss = 30.433691024780273, weights = tensor([6.3555, 2.1827], requires_grad=True)\n",
      "step №400: loss = 30.42159080505371, weights = tensor([6.3551, 2.1854], requires_grad=True)\n",
      "step №401: loss = 30.409488677978516, weights = tensor([6.3546, 2.1880], requires_grad=True)\n",
      "step №402: loss = 30.397396087646484, weights = tensor([6.3542, 2.1907], requires_grad=True)\n",
      "step №403: loss = 30.385311126708984, weights = tensor([6.3538, 2.1933], requires_grad=True)\n",
      "step №404: loss = 30.373239517211914, weights = tensor([6.3534, 2.1960], requires_grad=True)\n",
      "step №405: loss = 30.361160278320312, weights = tensor([6.3529, 2.1986], requires_grad=True)\n",
      "step №406: loss = 30.349102020263672, weights = tensor([6.3525, 2.2013], requires_grad=True)\n",
      "step №407: loss = 30.337055206298828, weights = tensor([6.3521, 2.2040], requires_grad=True)\n",
      "step №408: loss = 30.32500648498535, weights = tensor([6.3517, 2.2066], requires_grad=True)\n",
      "step №409: loss = 30.312978744506836, weights = tensor([6.3513, 2.2093], requires_grad=True)\n",
      "step №410: loss = 30.300949096679688, weights = tensor([6.3508, 2.2119], requires_grad=True)\n",
      "step №411: loss = 30.288930892944336, weights = tensor([6.3504, 2.2146], requires_grad=True)\n",
      "step №412: loss = 30.27692222595215, weights = tensor([6.3500, 2.2172], requires_grad=True)\n",
      "step №413: loss = 30.264917373657227, weights = tensor([6.3496, 2.2199], requires_grad=True)\n",
      "step №414: loss = 30.2529296875, weights = tensor([6.3491, 2.2225], requires_grad=True)\n",
      "step №415: loss = 30.24094009399414, weights = tensor([6.3487, 2.2252], requires_grad=True)\n",
      "step №416: loss = 30.228954315185547, weights = tensor([6.3483, 2.2278], requires_grad=True)\n",
      "step №417: loss = 30.216991424560547, weights = tensor([6.3479, 2.2305], requires_grad=True)\n",
      "step №418: loss = 30.20502281188965, weights = tensor([6.3475, 2.2331], requires_grad=True)\n",
      "step №419: loss = 30.193073272705078, weights = tensor([6.3470, 2.2357], requires_grad=True)\n",
      "step №420: loss = 30.18113136291504, weights = tensor([6.3466, 2.2384], requires_grad=True)\n",
      "step №421: loss = 30.169185638427734, weights = tensor([6.3462, 2.2410], requires_grad=True)\n",
      "step №422: loss = 30.15726089477539, weights = tensor([6.3458, 2.2437], requires_grad=True)\n",
      "step №423: loss = 30.145339965820312, weights = tensor([6.3453, 2.2463], requires_grad=True)\n",
      "step №424: loss = 30.1334228515625, weights = tensor([6.3449, 2.2490], requires_grad=True)\n",
      "step №425: loss = 30.121524810791016, weights = tensor([6.3445, 2.2516], requires_grad=True)\n",
      "step №426: loss = 30.109622955322266, weights = tensor([6.3441, 2.2542], requires_grad=True)\n",
      "step №427: loss = 30.097732543945312, weights = tensor([6.3437, 2.2569], requires_grad=True)\n",
      "step №428: loss = 30.08584976196289, weights = tensor([6.3432, 2.2595], requires_grad=True)\n",
      "step №429: loss = 30.073978424072266, weights = tensor([6.3428, 2.2621], requires_grad=True)\n",
      "step №430: loss = 30.06211280822754, weights = tensor([6.3424, 2.2648], requires_grad=True)\n",
      "step №431: loss = 30.050256729125977, weights = tensor([6.3420, 2.2674], requires_grad=True)\n",
      "step №432: loss = 30.03841209411621, weights = tensor([6.3416, 2.2700], requires_grad=True)\n",
      "step №433: loss = 30.026565551757812, weights = tensor([6.3411, 2.2727], requires_grad=True)\n",
      "step №434: loss = 30.014734268188477, weights = tensor([6.3407, 2.2753], requires_grad=True)\n",
      "step №435: loss = 30.002914428710938, weights = tensor([6.3403, 2.2779], requires_grad=True)\n",
      "step №436: loss = 29.991092681884766, weights = tensor([6.3399, 2.2806], requires_grad=True)\n",
      "step №437: loss = 29.97928237915039, weights = tensor([6.3395, 2.2832], requires_grad=True)\n",
      "step №438: loss = 29.96748924255371, weights = tensor([6.3390, 2.2858], requires_grad=True)\n",
      "step №439: loss = 29.955692291259766, weights = tensor([6.3386, 2.2884], requires_grad=True)\n",
      "step №440: loss = 29.94390869140625, weights = tensor([6.3382, 2.2911], requires_grad=True)\n",
      "step №441: loss = 29.932125091552734, weights = tensor([6.3378, 2.2937], requires_grad=True)\n",
      "step №442: loss = 29.920360565185547, weights = tensor([6.3374, 2.2963], requires_grad=True)\n",
      "step №443: loss = 29.90859603881836, weights = tensor([6.3370, 2.2989], requires_grad=True)\n",
      "step №444: loss = 29.8968448638916, weights = tensor([6.3365, 2.3016], requires_grad=True)\n",
      "step №445: loss = 29.88510513305664, weights = tensor([6.3361, 2.3042], requires_grad=True)\n",
      "step №446: loss = 29.873363494873047, weights = tensor([6.3357, 2.3068], requires_grad=True)\n",
      "step №447: loss = 29.861637115478516, weights = tensor([6.3353, 2.3094], requires_grad=True)\n",
      "step №448: loss = 29.849910736083984, weights = tensor([6.3349, 2.3120], requires_grad=True)\n",
      "step №449: loss = 29.838199615478516, weights = tensor([6.3344, 2.3147], requires_grad=True)\n",
      "step №450: loss = 29.82649803161621, weights = tensor([6.3340, 2.3173], requires_grad=True)\n",
      "step №451: loss = 29.814794540405273, weights = tensor([6.3336, 2.3199], requires_grad=True)\n",
      "step №452: loss = 29.803110122680664, weights = tensor([6.3332, 2.3225], requires_grad=True)\n",
      "step №453: loss = 29.791431427001953, weights = tensor([6.3328, 2.3251], requires_grad=True)\n",
      "step №454: loss = 29.77975082397461, weights = tensor([6.3324, 2.3277], requires_grad=True)\n",
      "step №455: loss = 29.76808738708496, weights = tensor([6.3319, 2.3303], requires_grad=True)\n",
      "step №456: loss = 29.756423950195312, weights = tensor([6.3315, 2.3329], requires_grad=True)\n",
      "step №457: loss = 29.74477767944336, weights = tensor([6.3311, 2.3356], requires_grad=True)\n",
      "step №458: loss = 29.733139038085938, weights = tensor([6.3307, 2.3382], requires_grad=True)\n",
      "step №459: loss = 29.721500396728516, weights = tensor([6.3303, 2.3408], requires_grad=True)\n",
      "step №460: loss = 29.70987319946289, weights = tensor([6.3299, 2.3434], requires_grad=True)\n",
      "step №461: loss = 29.698251724243164, weights = tensor([6.3295, 2.3460], requires_grad=True)\n",
      "step №462: loss = 29.6866455078125, weights = tensor([6.3290, 2.3486], requires_grad=True)\n",
      "step №463: loss = 29.675045013427734, weights = tensor([6.3286, 2.3512], requires_grad=True)\n",
      "step №464: loss = 29.6634521484375, weights = tensor([6.3282, 2.3538], requires_grad=True)\n",
      "step №465: loss = 29.651865005493164, weights = tensor([6.3278, 2.3564], requires_grad=True)\n",
      "step №466: loss = 29.640283584594727, weights = tensor([6.3274, 2.3590], requires_grad=True)\n",
      "step №467: loss = 29.628713607788086, weights = tensor([6.3270, 2.3616], requires_grad=True)\n",
      "step №468: loss = 29.617145538330078, weights = tensor([6.3265, 2.3642], requires_grad=True)\n",
      "step №469: loss = 29.605594635009766, weights = tensor([6.3261, 2.3668], requires_grad=True)\n",
      "step №470: loss = 29.59404945373535, weights = tensor([6.3257, 2.3694], requires_grad=True)\n",
      "step №471: loss = 29.582500457763672, weights = tensor([6.3253, 2.3720], requires_grad=True)\n",
      "step №472: loss = 29.570974349975586, weights = tensor([6.3249, 2.3746], requires_grad=True)\n",
      "step №473: loss = 29.559452056884766, weights = tensor([6.3245, 2.3772], requires_grad=True)\n",
      "step №474: loss = 29.547931671142578, weights = tensor([6.3241, 2.3798], requires_grad=True)\n",
      "step №475: loss = 29.536422729492188, weights = tensor([6.3236, 2.3824], requires_grad=True)\n",
      "step №476: loss = 29.524927139282227, weights = tensor([6.3232, 2.3850], requires_grad=True)\n",
      "step №477: loss = 29.513427734375, weights = tensor([6.3228, 2.3876], requires_grad=True)\n",
      "step №478: loss = 29.501941680908203, weights = tensor([6.3224, 2.3902], requires_grad=True)\n",
      "step №479: loss = 29.490467071533203, weights = tensor([6.3220, 2.3928], requires_grad=True)\n",
      "step №480: loss = 29.478994369506836, weights = tensor([6.3216, 2.3953], requires_grad=True)\n",
      "step №481: loss = 29.4675350189209, weights = tensor([6.3212, 2.3979], requires_grad=True)\n",
      "step №482: loss = 29.45608139038086, weights = tensor([6.3208, 2.4005], requires_grad=True)\n",
      "step №483: loss = 29.444631576538086, weights = tensor([6.3203, 2.4031], requires_grad=True)\n",
      "step №484: loss = 29.433197021484375, weights = tensor([6.3199, 2.4057], requires_grad=True)\n",
      "step №485: loss = 29.421764373779297, weights = tensor([6.3195, 2.4083], requires_grad=True)\n",
      "step №486: loss = 29.41034507751465, weights = tensor([6.3191, 2.4109], requires_grad=True)\n",
      "step №487: loss = 29.398929595947266, weights = tensor([6.3187, 2.4135], requires_grad=True)\n",
      "step №488: loss = 29.38751792907715, weights = tensor([6.3183, 2.4160], requires_grad=True)\n",
      "step №489: loss = 29.37611961364746, weights = tensor([6.3179, 2.4186], requires_grad=True)\n",
      "step №490: loss = 29.36472511291504, weights = tensor([6.3175, 2.4212], requires_grad=True)\n",
      "step №491: loss = 29.35333824157715, weights = tensor([6.3170, 2.4238], requires_grad=True)\n",
      "step №492: loss = 29.341960906982422, weights = tensor([6.3166, 2.4264], requires_grad=True)\n",
      "step №493: loss = 29.33059310913086, weights = tensor([6.3162, 2.4289], requires_grad=True)\n",
      "step №494: loss = 29.31923484802246, weights = tensor([6.3158, 2.4315], requires_grad=True)\n",
      "step №495: loss = 29.307880401611328, weights = tensor([6.3154, 2.4341], requires_grad=True)\n",
      "step №496: loss = 29.29652976989746, weights = tensor([6.3150, 2.4367], requires_grad=True)\n",
      "step №497: loss = 29.28519630432129, weights = tensor([6.3146, 2.4392], requires_grad=True)\n",
      "step №498: loss = 29.273860931396484, weights = tensor([6.3142, 2.4418], requires_grad=True)\n",
      "step №499: loss = 29.26253890991211, weights = tensor([6.3138, 2.4444], requires_grad=True)\n",
      "step №500: loss = 29.251224517822266, weights = tensor([6.3133, 2.4470], requires_grad=True)\n",
      "step №501: loss = 29.239919662475586, weights = tensor([6.3129, 2.4495], requires_grad=True)\n",
      "step №502: loss = 29.22861671447754, weights = tensor([6.3125, 2.4521], requires_grad=True)\n",
      "step №503: loss = 29.21732521057129, weights = tensor([6.3121, 2.4547], requires_grad=True)\n",
      "step №504: loss = 29.206039428710938, weights = tensor([6.3117, 2.4572], requires_grad=True)\n",
      "step №505: loss = 29.194766998291016, weights = tensor([6.3113, 2.4598], requires_grad=True)\n",
      "step №506: loss = 29.183496475219727, weights = tensor([6.3109, 2.4624], requires_grad=True)\n",
      "step №507: loss = 29.172231674194336, weights = tensor([6.3105, 2.4649], requires_grad=True)\n",
      "step №508: loss = 29.160980224609375, weights = tensor([6.3101, 2.4675], requires_grad=True)\n",
      "step №509: loss = 29.149734497070312, weights = tensor([6.3097, 2.4701], requires_grad=True)\n",
      "step №510: loss = 29.138492584228516, weights = tensor([6.3093, 2.4726], requires_grad=True)\n",
      "step №511: loss = 29.127267837524414, weights = tensor([6.3088, 2.4752], requires_grad=True)\n",
      "step №512: loss = 29.116039276123047, weights = tensor([6.3084, 2.4778], requires_grad=True)\n",
      "step №513: loss = 29.10482406616211, weights = tensor([6.3080, 2.4803], requires_grad=True)\n",
      "step №514: loss = 29.093612670898438, weights = tensor([6.3076, 2.4829], requires_grad=True)\n",
      "step №515: loss = 29.082416534423828, weights = tensor([6.3072, 2.4854], requires_grad=True)\n",
      "step №516: loss = 29.071218490600586, weights = tensor([6.3068, 2.4880], requires_grad=True)\n",
      "step №517: loss = 29.06003189086914, weights = tensor([6.3064, 2.4906], requires_grad=True)\n",
      "step №518: loss = 29.04885482788086, weights = tensor([6.3060, 2.4931], requires_grad=True)\n",
      "step №519: loss = 29.037683486938477, weights = tensor([6.3056, 2.4957], requires_grad=True)\n",
      "step №520: loss = 29.026519775390625, weights = tensor([6.3052, 2.4982], requires_grad=True)\n",
      "step №521: loss = 29.015369415283203, weights = tensor([6.3048, 2.5008], requires_grad=True)\n",
      "step №522: loss = 29.004220962524414, weights = tensor([6.3044, 2.5033], requires_grad=True)\n",
      "step №523: loss = 28.993083953857422, weights = tensor([6.3040, 2.5059], requires_grad=True)\n",
      "step №524: loss = 28.981945037841797, weights = tensor([6.3035, 2.5084], requires_grad=True)\n",
      "step №525: loss = 28.970829010009766, weights = tensor([6.3031, 2.5110], requires_grad=True)\n",
      "step №526: loss = 28.959701538085938, weights = tensor([6.3027, 2.5135], requires_grad=True)\n",
      "step №527: loss = 28.948596954345703, weights = tensor([6.3023, 2.5161], requires_grad=True)\n",
      "step №528: loss = 28.937488555908203, weights = tensor([6.3019, 2.5186], requires_grad=True)\n",
      "step №529: loss = 28.9263973236084, weights = tensor([6.3015, 2.5212], requires_grad=True)\n",
      "step №530: loss = 28.91530418395996, weights = tensor([6.3011, 2.5237], requires_grad=True)\n",
      "step №531: loss = 28.90423011779785, weights = tensor([6.3007, 2.5263], requires_grad=True)\n",
      "step №532: loss = 28.89315414428711, weights = tensor([6.3003, 2.5288], requires_grad=True)\n",
      "step №533: loss = 28.882091522216797, weights = tensor([6.2999, 2.5314], requires_grad=True)\n",
      "step №534: loss = 28.871028900146484, weights = tensor([6.2995, 2.5339], requires_grad=True)\n",
      "step №535: loss = 28.859981536865234, weights = tensor([6.2991, 2.5365], requires_grad=True)\n",
      "step №536: loss = 28.848941802978516, weights = tensor([6.2987, 2.5390], requires_grad=True)\n",
      "step №537: loss = 28.837909698486328, weights = tensor([6.2983, 2.5415], requires_grad=True)\n",
      "step №538: loss = 28.82687759399414, weights = tensor([6.2979, 2.5441], requires_grad=True)\n",
      "step №539: loss = 28.815860748291016, weights = tensor([6.2975, 2.5466], requires_grad=True)\n",
      "step №540: loss = 28.804845809936523, weights = tensor([6.2971, 2.5491], requires_grad=True)\n",
      "step №541: loss = 28.793842315673828, weights = tensor([6.2966, 2.5517], requires_grad=True)\n",
      "step №542: loss = 28.7828426361084, weights = tensor([6.2962, 2.5542], requires_grad=True)\n",
      "step №543: loss = 28.7718505859375, weights = tensor([6.2958, 2.5568], requires_grad=True)\n",
      "step №544: loss = 28.760873794555664, weights = tensor([6.2954, 2.5593], requires_grad=True)\n",
      "step №545: loss = 28.749893188476562, weights = tensor([6.2950, 2.5618], requires_grad=True)\n",
      "step №546: loss = 28.738922119140625, weights = tensor([6.2946, 2.5644], requires_grad=True)\n",
      "step №547: loss = 28.72796630859375, weights = tensor([6.2942, 2.5669], requires_grad=True)\n",
      "step №548: loss = 28.717010498046875, weights = tensor([6.2938, 2.5694], requires_grad=True)\n",
      "step №549: loss = 28.706066131591797, weights = tensor([6.2934, 2.5720], requires_grad=True)\n",
      "step №550: loss = 28.695133209228516, weights = tensor([6.2930, 2.5745], requires_grad=True)\n",
      "step №551: loss = 28.6841983795166, weights = tensor([6.2926, 2.5770], requires_grad=True)\n",
      "step №552: loss = 28.673274993896484, weights = tensor([6.2922, 2.5795], requires_grad=True)\n",
      "step №553: loss = 28.662363052368164, weights = tensor([6.2918, 2.5821], requires_grad=True)\n",
      "step №554: loss = 28.65144920349121, weights = tensor([6.2914, 2.5846], requires_grad=True)\n",
      "step №555: loss = 28.640544891357422, weights = tensor([6.2910, 2.5871], requires_grad=True)\n",
      "step №556: loss = 28.629653930664062, weights = tensor([6.2906, 2.5896], requires_grad=True)\n",
      "step №557: loss = 28.618770599365234, weights = tensor([6.2902, 2.5922], requires_grad=True)\n",
      "step №558: loss = 28.607891082763672, weights = tensor([6.2898, 2.5947], requires_grad=True)\n",
      "step №559: loss = 28.597015380859375, weights = tensor([6.2894, 2.5972], requires_grad=True)\n",
      "step №560: loss = 28.58615493774414, weights = tensor([6.2890, 2.5997], requires_grad=True)\n",
      "step №561: loss = 28.575292587280273, weights = tensor([6.2886, 2.6022], requires_grad=True)\n",
      "step №562: loss = 28.5644474029541, weights = tensor([6.2882, 2.6048], requires_grad=True)\n",
      "step №563: loss = 28.553600311279297, weights = tensor([6.2878, 2.6073], requires_grad=True)\n",
      "step №564: loss = 28.542766571044922, weights = tensor([6.2874, 2.6098], requires_grad=True)\n",
      "step №565: loss = 28.531940460205078, weights = tensor([6.2870, 2.6123], requires_grad=True)\n",
      "step №566: loss = 28.521121978759766, weights = tensor([6.2866, 2.6148], requires_grad=True)\n",
      "step №567: loss = 28.51030921936035, weights = tensor([6.2862, 2.6173], requires_grad=True)\n",
      "step №568: loss = 28.499500274658203, weights = tensor([6.2858, 2.6199], requires_grad=True)\n",
      "step №569: loss = 28.488698959350586, weights = tensor([6.2854, 2.6224], requires_grad=True)\n",
      "step №570: loss = 28.4779109954834, weights = tensor([6.2850, 2.6249], requires_grad=True)\n",
      "step №571: loss = 28.46712875366211, weights = tensor([6.2846, 2.6274], requires_grad=True)\n",
      "step №572: loss = 28.45635414123535, weights = tensor([6.2842, 2.6299], requires_grad=True)\n",
      "step №573: loss = 28.44558334350586, weights = tensor([6.2838, 2.6324], requires_grad=True)\n",
      "step №574: loss = 28.434823989868164, weights = tensor([6.2834, 2.6349], requires_grad=True)\n",
      "step №575: loss = 28.424072265625, weights = tensor([6.2830, 2.6374], requires_grad=True)\n",
      "step №576: loss = 28.4133243560791, weights = tensor([6.2826, 2.6399], requires_grad=True)\n",
      "step №577: loss = 28.402578353881836, weights = tensor([6.2822, 2.6424], requires_grad=True)\n",
      "step №578: loss = 28.3918514251709, weights = tensor([6.2818, 2.6450], requires_grad=True)\n",
      "step №579: loss = 28.381122589111328, weights = tensor([6.2814, 2.6475], requires_grad=True)\n",
      "step №580: loss = 28.370403289794922, weights = tensor([6.2810, 2.6500], requires_grad=True)\n",
      "step №581: loss = 28.359695434570312, weights = tensor([6.2806, 2.6525], requires_grad=True)\n",
      "step №582: loss = 28.3489933013916, weights = tensor([6.2802, 2.6550], requires_grad=True)\n",
      "step №583: loss = 28.33829689025879, weights = tensor([6.2798, 2.6575], requires_grad=True)\n",
      "step №584: loss = 28.32760238647461, weights = tensor([6.2794, 2.6600], requires_grad=True)\n",
      "step №585: loss = 28.316925048828125, weights = tensor([6.2790, 2.6625], requires_grad=True)\n",
      "step №586: loss = 28.306249618530273, weights = tensor([6.2786, 2.6650], requires_grad=True)\n",
      "step №587: loss = 28.295581817626953, weights = tensor([6.2782, 2.6675], requires_grad=True)\n",
      "step №588: loss = 28.284915924072266, weights = tensor([6.2778, 2.6700], requires_grad=True)\n",
      "step №589: loss = 28.274267196655273, weights = tensor([6.2774, 2.6725], requires_grad=True)\n",
      "step №590: loss = 28.263622283935547, weights = tensor([6.2770, 2.6749], requires_grad=True)\n",
      "step №591: loss = 28.252986907958984, weights = tensor([6.2766, 2.6774], requires_grad=True)\n",
      "step №592: loss = 28.242351531982422, weights = tensor([6.2762, 2.6799], requires_grad=True)\n",
      "step №593: loss = 28.231725692749023, weights = tensor([6.2758, 2.6824], requires_grad=True)\n",
      "step №594: loss = 28.221118927001953, weights = tensor([6.2754, 2.6849], requires_grad=True)\n",
      "step №595: loss = 28.210498809814453, weights = tensor([6.2750, 2.6874], requires_grad=True)\n",
      "step №596: loss = 28.199901580810547, weights = tensor([6.2746, 2.6899], requires_grad=True)\n",
      "step №597: loss = 28.18931007385254, weights = tensor([6.2742, 2.6924], requires_grad=True)\n",
      "step №598: loss = 28.178720474243164, weights = tensor([6.2738, 2.6949], requires_grad=True)\n",
      "step №599: loss = 28.16813087463379, weights = tensor([6.2734, 2.6974], requires_grad=True)\n",
      "step №600: loss = 28.157562255859375, weights = tensor([6.2730, 2.6999], requires_grad=True)\n",
      "step №601: loss = 28.146991729736328, weights = tensor([6.2726, 2.7023], requires_grad=True)\n",
      "step №602: loss = 28.13643455505371, weights = tensor([6.2722, 2.7048], requires_grad=True)\n",
      "step №603: loss = 28.125885009765625, weights = tensor([6.2718, 2.7073], requires_grad=True)\n",
      "step №604: loss = 28.11533546447754, weights = tensor([6.2714, 2.7098], requires_grad=True)\n",
      "step №605: loss = 28.104801177978516, weights = tensor([6.2710, 2.7123], requires_grad=True)\n",
      "step №606: loss = 28.094268798828125, weights = tensor([6.2706, 2.7148], requires_grad=True)\n",
      "step №607: loss = 28.0837459564209, weights = tensor([6.2702, 2.7172], requires_grad=True)\n",
      "step №608: loss = 28.073230743408203, weights = tensor([6.2699, 2.7197], requires_grad=True)\n",
      "step №609: loss = 28.062719345092773, weights = tensor([6.2695, 2.7222], requires_grad=True)\n",
      "step №610: loss = 28.052215576171875, weights = tensor([6.2691, 2.7247], requires_grad=True)\n",
      "step №611: loss = 28.04172134399414, weights = tensor([6.2687, 2.7272], requires_grad=True)\n",
      "step №612: loss = 28.031230926513672, weights = tensor([6.2683, 2.7296], requires_grad=True)\n",
      "step №613: loss = 28.020751953125, weights = tensor([6.2679, 2.7321], requires_grad=True)\n",
      "step №614: loss = 28.010278701782227, weights = tensor([6.2675, 2.7346], requires_grad=True)\n",
      "step №615: loss = 27.99981117248535, weights = tensor([6.2671, 2.7371], requires_grad=True)\n",
      "step №616: loss = 27.98935317993164, weights = tensor([6.2667, 2.7395], requires_grad=True)\n",
      "step №617: loss = 27.978900909423828, weights = tensor([6.2663, 2.7420], requires_grad=True)\n",
      "step №618: loss = 27.968456268310547, weights = tensor([6.2659, 2.7445], requires_grad=True)\n",
      "step №619: loss = 27.9580135345459, weights = tensor([6.2655, 2.7469], requires_grad=True)\n",
      "step №620: loss = 27.947582244873047, weights = tensor([6.2651, 2.7494], requires_grad=True)\n",
      "step №621: loss = 27.937158584594727, weights = tensor([6.2647, 2.7519], requires_grad=True)\n",
      "step №622: loss = 27.926742553710938, weights = tensor([6.2643, 2.7543], requires_grad=True)\n",
      "step №623: loss = 27.916330337524414, weights = tensor([6.2639, 2.7568], requires_grad=True)\n",
      "step №624: loss = 27.905925750732422, weights = tensor([6.2635, 2.7593], requires_grad=True)\n",
      "step №625: loss = 27.895532608032227, weights = tensor([6.2631, 2.7617], requires_grad=True)\n",
      "step №626: loss = 27.885141372680664, weights = tensor([6.2628, 2.7642], requires_grad=True)\n",
      "step №627: loss = 27.874759674072266, weights = tensor([6.2624, 2.7667], requires_grad=True)\n",
      "step №628: loss = 27.8643856048584, weights = tensor([6.2620, 2.7691], requires_grad=True)\n",
      "step №629: loss = 27.854022979736328, weights = tensor([6.2616, 2.7716], requires_grad=True)\n",
      "step №630: loss = 27.84365463256836, weights = tensor([6.2612, 2.7741], requires_grad=True)\n",
      "step №631: loss = 27.833301544189453, weights = tensor([6.2608, 2.7765], requires_grad=True)\n",
      "step №632: loss = 27.822956085205078, weights = tensor([6.2604, 2.7790], requires_grad=True)\n",
      "step №633: loss = 27.812612533569336, weights = tensor([6.2600, 2.7814], requires_grad=True)\n",
      "step №634: loss = 27.802282333374023, weights = tensor([6.2596, 2.7839], requires_grad=True)\n",
      "step №635: loss = 27.791955947875977, weights = tensor([6.2592, 2.7864], requires_grad=True)\n",
      "step №636: loss = 27.781631469726562, weights = tensor([6.2588, 2.7888], requires_grad=True)\n",
      "step №637: loss = 27.771326065063477, weights = tensor([6.2584, 2.7913], requires_grad=True)\n",
      "step №638: loss = 27.761022567749023, weights = tensor([6.2580, 2.7937], requires_grad=True)\n",
      "step №639: loss = 27.750720977783203, weights = tensor([6.2577, 2.7962], requires_grad=True)\n",
      "step №640: loss = 27.740427017211914, weights = tensor([6.2573, 2.7986], requires_grad=True)\n",
      "step №641: loss = 27.730148315429688, weights = tensor([6.2569, 2.8011], requires_grad=True)\n",
      "step №642: loss = 27.719867706298828, weights = tensor([6.2565, 2.8035], requires_grad=True)\n",
      "step №643: loss = 27.709598541259766, weights = tensor([6.2561, 2.8060], requires_grad=True)\n",
      "step №644: loss = 27.6993408203125, weights = tensor([6.2557, 2.8084], requires_grad=True)\n",
      "step №645: loss = 27.689075469970703, weights = tensor([6.2553, 2.8109], requires_grad=True)\n",
      "step №646: loss = 27.678829193115234, weights = tensor([6.2549, 2.8133], requires_grad=True)\n",
      "step №647: loss = 27.668590545654297, weights = tensor([6.2545, 2.8158], requires_grad=True)\n",
      "step №648: loss = 27.65835189819336, weights = tensor([6.2541, 2.8182], requires_grad=True)\n",
      "step №649: loss = 27.648120880126953, weights = tensor([6.2537, 2.8207], requires_grad=True)\n",
      "step №650: loss = 27.63790512084961, weights = tensor([6.2534, 2.8231], requires_grad=True)\n",
      "step №651: loss = 27.627689361572266, weights = tensor([6.2530, 2.8256], requires_grad=True)\n",
      "step №652: loss = 27.617481231689453, weights = tensor([6.2526, 2.8280], requires_grad=True)\n",
      "step №653: loss = 27.607280731201172, weights = tensor([6.2522, 2.8305], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №654: loss = 27.59708595275879, weights = tensor([6.2518, 2.8329], requires_grad=True)\n",
      "step №655: loss = 27.586898803710938, weights = tensor([6.2514, 2.8353], requires_grad=True)\n",
      "step №656: loss = 27.576717376708984, weights = tensor([6.2510, 2.8378], requires_grad=True)\n",
      "step №657: loss = 27.566553115844727, weights = tensor([6.2506, 2.8402], requires_grad=True)\n",
      "step №658: loss = 27.556377410888672, weights = tensor([6.2502, 2.8427], requires_grad=True)\n",
      "step №659: loss = 27.54622459411621, weights = tensor([6.2499, 2.8451], requires_grad=True)\n",
      "step №660: loss = 27.53606605529785, weights = tensor([6.2495, 2.8475], requires_grad=True)\n",
      "step №661: loss = 27.525924682617188, weights = tensor([6.2491, 2.8500], requires_grad=True)\n",
      "step №662: loss = 27.51578140258789, weights = tensor([6.2487, 2.8524], requires_grad=True)\n",
      "step №663: loss = 27.505651473999023, weights = tensor([6.2483, 2.8548], requires_grad=True)\n",
      "step №664: loss = 27.495525360107422, weights = tensor([6.2479, 2.8573], requires_grad=True)\n",
      "step №665: loss = 27.48541259765625, weights = tensor([6.2475, 2.8597], requires_grad=True)\n",
      "step №666: loss = 27.475296020507812, weights = tensor([6.2471, 2.8621], requires_grad=True)\n",
      "step №667: loss = 27.465190887451172, weights = tensor([6.2468, 2.8646], requires_grad=True)\n",
      "step №668: loss = 27.455093383789062, weights = tensor([6.2464, 2.8670], requires_grad=True)\n",
      "step №669: loss = 27.445003509521484, weights = tensor([6.2460, 2.8694], requires_grad=True)\n",
      "step №670: loss = 27.434921264648438, weights = tensor([6.2456, 2.8718], requires_grad=True)\n",
      "step №671: loss = 27.424846649169922, weights = tensor([6.2452, 2.8743], requires_grad=True)\n",
      "step №672: loss = 27.41477394104004, weights = tensor([6.2448, 2.8767], requires_grad=True)\n",
      "step №673: loss = 27.404708862304688, weights = tensor([6.2444, 2.8791], requires_grad=True)\n",
      "step №674: loss = 27.394657135009766, weights = tensor([6.2440, 2.8816], requires_grad=True)\n",
      "step №675: loss = 27.38460350036621, weights = tensor([6.2437, 2.8840], requires_grad=True)\n",
      "step №676: loss = 27.374563217163086, weights = tensor([6.2433, 2.8864], requires_grad=True)\n",
      "step №677: loss = 27.36452293395996, weights = tensor([6.2429, 2.8888], requires_grad=True)\n",
      "step №678: loss = 27.354501724243164, weights = tensor([6.2425, 2.8912], requires_grad=True)\n",
      "step №679: loss = 27.3444766998291, weights = tensor([6.2421, 2.8937], requires_grad=True)\n",
      "step №680: loss = 27.334461212158203, weights = tensor([6.2417, 2.8961], requires_grad=True)\n",
      "step №681: loss = 27.324451446533203, weights = tensor([6.2413, 2.8985], requires_grad=True)\n",
      "step №682: loss = 27.314443588256836, weights = tensor([6.2410, 2.9009], requires_grad=True)\n",
      "step №683: loss = 27.3044490814209, weights = tensor([6.2406, 2.9033], requires_grad=True)\n",
      "step №684: loss = 27.29446792602539, weights = tensor([6.2402, 2.9058], requires_grad=True)\n",
      "step №685: loss = 27.284475326538086, weights = tensor([6.2398, 2.9082], requires_grad=True)\n",
      "step №686: loss = 27.27450942993164, weights = tensor([6.2394, 2.9106], requires_grad=True)\n",
      "step №687: loss = 27.264545440673828, weights = tensor([6.2390, 2.9130], requires_grad=True)\n",
      "step №688: loss = 27.25457763671875, weights = tensor([6.2386, 2.9154], requires_grad=True)\n",
      "step №689: loss = 27.244619369506836, weights = tensor([6.2383, 2.9178], requires_grad=True)\n",
      "step №690: loss = 27.234670639038086, weights = tensor([6.2379, 2.9202], requires_grad=True)\n",
      "step №691: loss = 27.2247314453125, weights = tensor([6.2375, 2.9227], requires_grad=True)\n",
      "step №692: loss = 27.214794158935547, weights = tensor([6.2371, 2.9251], requires_grad=True)\n",
      "step №693: loss = 27.20486831665039, weights = tensor([6.2367, 2.9275], requires_grad=True)\n",
      "step №694: loss = 27.1949462890625, weights = tensor([6.2363, 2.9299], requires_grad=True)\n",
      "step №695: loss = 27.18503761291504, weights = tensor([6.2360, 2.9323], requires_grad=True)\n",
      "step №696: loss = 27.175128936767578, weights = tensor([6.2356, 2.9347], requires_grad=True)\n",
      "step №697: loss = 27.165225982666016, weights = tensor([6.2352, 2.9371], requires_grad=True)\n",
      "step №698: loss = 27.155330657958984, weights = tensor([6.2348, 2.9395], requires_grad=True)\n",
      "step №699: loss = 27.145442962646484, weights = tensor([6.2344, 2.9419], requires_grad=True)\n",
      "step №700: loss = 27.135568618774414, weights = tensor([6.2340, 2.9443], requires_grad=True)\n",
      "step №701: loss = 27.12569236755371, weights = tensor([6.2336, 2.9467], requires_grad=True)\n",
      "step №702: loss = 27.11582374572754, weights = tensor([6.2333, 2.9491], requires_grad=True)\n",
      "step №703: loss = 27.1059627532959, weights = tensor([6.2329, 2.9515], requires_grad=True)\n",
      "step №704: loss = 27.096105575561523, weights = tensor([6.2325, 2.9539], requires_grad=True)\n",
      "step №705: loss = 27.086261749267578, weights = tensor([6.2321, 2.9563], requires_grad=True)\n",
      "step №706: loss = 27.076419830322266, weights = tensor([6.2317, 2.9587], requires_grad=True)\n",
      "step №707: loss = 27.06658363342285, weights = tensor([6.2314, 2.9611], requires_grad=True)\n",
      "step №708: loss = 27.0567569732666, weights = tensor([6.2310, 2.9635], requires_grad=True)\n",
      "step №709: loss = 27.04693603515625, weights = tensor([6.2306, 2.9659], requires_grad=True)\n",
      "step №710: loss = 27.037124633789062, weights = tensor([6.2302, 2.9683], requires_grad=True)\n",
      "step №711: loss = 27.02731704711914, weights = tensor([6.2298, 2.9707], requires_grad=True)\n",
      "step №712: loss = 27.01751708984375, weights = tensor([6.2294, 2.9731], requires_grad=True)\n",
      "step №713: loss = 27.00772476196289, weights = tensor([6.2291, 2.9755], requires_grad=True)\n",
      "step №714: loss = 26.9979305267334, weights = tensor([6.2287, 2.9779], requires_grad=True)\n",
      "step №715: loss = 26.988147735595703, weights = tensor([6.2283, 2.9803], requires_grad=True)\n",
      "step №716: loss = 26.978374481201172, weights = tensor([6.2279, 2.9827], requires_grad=True)\n",
      "step №717: loss = 26.96860694885254, weights = tensor([6.2275, 2.9851], requires_grad=True)\n",
      "step №718: loss = 26.958850860595703, weights = tensor([6.2272, 2.9874], requires_grad=True)\n",
      "step №719: loss = 26.949092864990234, weights = tensor([6.2268, 2.9898], requires_grad=True)\n",
      "step №720: loss = 26.939350128173828, weights = tensor([6.2264, 2.9922], requires_grad=True)\n",
      "step №721: loss = 26.92960548400879, weights = tensor([6.2260, 2.9946], requires_grad=True)\n",
      "step №722: loss = 26.919870376586914, weights = tensor([6.2256, 2.9970], requires_grad=True)\n",
      "step №723: loss = 26.910140991210938, weights = tensor([6.2253, 2.9994], requires_grad=True)\n",
      "step №724: loss = 26.900421142578125, weights = tensor([6.2249, 3.0018], requires_grad=True)\n",
      "step №725: loss = 26.890697479248047, weights = tensor([6.2245, 3.0042], requires_grad=True)\n",
      "step №726: loss = 26.880992889404297, weights = tensor([6.2241, 3.0065], requires_grad=True)\n",
      "step №727: loss = 26.87129783630371, weights = tensor([6.2237, 3.0089], requires_grad=True)\n",
      "step №728: loss = 26.86160659790039, weights = tensor([6.2234, 3.0113], requires_grad=True)\n",
      "step №729: loss = 26.851913452148438, weights = tensor([6.2230, 3.0137], requires_grad=True)\n",
      "step №730: loss = 26.842233657836914, weights = tensor([6.2226, 3.0161], requires_grad=True)\n",
      "step №731: loss = 26.832555770874023, weights = tensor([6.2222, 3.0184], requires_grad=True)\n",
      "step №732: loss = 26.822887420654297, weights = tensor([6.2218, 3.0208], requires_grad=True)\n",
      "step №733: loss = 26.813220977783203, weights = tensor([6.2215, 3.0232], requires_grad=True)\n",
      "step №734: loss = 26.803569793701172, weights = tensor([6.2211, 3.0256], requires_grad=True)\n",
      "step №735: loss = 26.79391860961914, weights = tensor([6.2207, 3.0279], requires_grad=True)\n",
      "step №736: loss = 26.784276962280273, weights = tensor([6.2203, 3.0303], requires_grad=True)\n",
      "step №737: loss = 26.774646759033203, weights = tensor([6.2199, 3.0327], requires_grad=True)\n",
      "step №738: loss = 26.7650146484375, weights = tensor([6.2196, 3.0351], requires_grad=True)\n",
      "step №739: loss = 26.755390167236328, weights = tensor([6.2192, 3.0374], requires_grad=True)\n",
      "step №740: loss = 26.745769500732422, weights = tensor([6.2188, 3.0398], requires_grad=True)\n",
      "step №741: loss = 26.736160278320312, weights = tensor([6.2184, 3.0422], requires_grad=True)\n",
      "step №742: loss = 26.7265625, weights = tensor([6.2180, 3.0445], requires_grad=True)\n",
      "step №743: loss = 26.716964721679688, weights = tensor([6.2177, 3.0469], requires_grad=True)\n",
      "step №744: loss = 26.707372665405273, weights = tensor([6.2173, 3.0493], requires_grad=True)\n",
      "step №745: loss = 26.697784423828125, weights = tensor([6.2169, 3.0517], requires_grad=True)\n",
      "step №746: loss = 26.68821144104004, weights = tensor([6.2165, 3.0540], requires_grad=True)\n",
      "step №747: loss = 26.678638458251953, weights = tensor([6.2162, 3.0564], requires_grad=True)\n",
      "step №748: loss = 26.669071197509766, weights = tensor([6.2158, 3.0588], requires_grad=True)\n",
      "step №749: loss = 26.65951919555664, weights = tensor([6.2154, 3.0611], requires_grad=True)\n",
      "step №750: loss = 26.64996337890625, weights = tensor([6.2150, 3.0635], requires_grad=True)\n",
      "step №751: loss = 26.64042091369629, weights = tensor([6.2147, 3.0658], requires_grad=True)\n",
      "step №752: loss = 26.630878448486328, weights = tensor([6.2143, 3.0682], requires_grad=True)\n",
      "step №753: loss = 26.621349334716797, weights = tensor([6.2139, 3.0706], requires_grad=True)\n",
      "step №754: loss = 26.6118221282959, weights = tensor([6.2135, 3.0729], requires_grad=True)\n",
      "step №755: loss = 26.602306365966797, weights = tensor([6.2131, 3.0753], requires_grad=True)\n",
      "step №756: loss = 26.592788696289062, weights = tensor([6.2128, 3.0776], requires_grad=True)\n",
      "step №757: loss = 26.583288192749023, weights = tensor([6.2124, 3.0800], requires_grad=True)\n",
      "step №758: loss = 26.573787689208984, weights = tensor([6.2120, 3.0824], requires_grad=True)\n",
      "step №759: loss = 26.564294815063477, weights = tensor([6.2116, 3.0847], requires_grad=True)\n",
      "step №760: loss = 26.5548038482666, weights = tensor([6.2113, 3.0871], requires_grad=True)\n",
      "step №761: loss = 26.54532241821289, weights = tensor([6.2109, 3.0894], requires_grad=True)\n",
      "step №762: loss = 26.53584861755371, weights = tensor([6.2105, 3.0918], requires_grad=True)\n",
      "step №763: loss = 26.526382446289062, weights = tensor([6.2101, 3.0941], requires_grad=True)\n",
      "step №764: loss = 26.516921997070312, weights = tensor([6.2098, 3.0965], requires_grad=True)\n",
      "step №765: loss = 26.50746726989746, weights = tensor([6.2094, 3.0988], requires_grad=True)\n",
      "step №766: loss = 26.49801254272461, weights = tensor([6.2090, 3.1012], requires_grad=True)\n",
      "step №767: loss = 26.488571166992188, weights = tensor([6.2086, 3.1035], requires_grad=True)\n",
      "step №768: loss = 26.479137420654297, weights = tensor([6.2083, 3.1059], requires_grad=True)\n",
      "step №769: loss = 26.469707489013672, weights = tensor([6.2079, 3.1082], requires_grad=True)\n",
      "step №770: loss = 26.46028709411621, weights = tensor([6.2075, 3.1106], requires_grad=True)\n",
      "step №771: loss = 26.450870513916016, weights = tensor([6.2071, 3.1129], requires_grad=True)\n",
      "step №772: loss = 26.441457748413086, weights = tensor([6.2068, 3.1153], requires_grad=True)\n",
      "step №773: loss = 26.432058334350586, weights = tensor([6.2064, 3.1176], requires_grad=True)\n",
      "step №774: loss = 26.422653198242188, weights = tensor([6.2060, 3.1200], requires_grad=True)\n",
      "step №775: loss = 26.413265228271484, weights = tensor([6.2056, 3.1223], requires_grad=True)\n",
      "step №776: loss = 26.403881072998047, weights = tensor([6.2053, 3.1246], requires_grad=True)\n",
      "step №777: loss = 26.39450454711914, weights = tensor([6.2049, 3.1270], requires_grad=True)\n",
      "step №778: loss = 26.385135650634766, weights = tensor([6.2045, 3.1293], requires_grad=True)\n",
      "step №779: loss = 26.37576675415039, weights = tensor([6.2042, 3.1317], requires_grad=True)\n",
      "step №780: loss = 26.366405487060547, weights = tensor([6.2038, 3.1340], requires_grad=True)\n",
      "step №781: loss = 26.3570556640625, weights = tensor([6.2034, 3.1364], requires_grad=True)\n",
      "step №782: loss = 26.347705841064453, weights = tensor([6.2030, 3.1387], requires_grad=True)\n",
      "step №783: loss = 26.338367462158203, weights = tensor([6.2027, 3.1410], requires_grad=True)\n",
      "step №784: loss = 26.329030990600586, weights = tensor([6.2023, 3.1434], requires_grad=True)\n",
      "step №785: loss = 26.3197021484375, weights = tensor([6.2019, 3.1457], requires_grad=True)\n",
      "step №786: loss = 26.310388565063477, weights = tensor([6.2015, 3.1480], requires_grad=True)\n",
      "step №787: loss = 26.301074981689453, weights = tensor([6.2012, 3.1504], requires_grad=True)\n",
      "step №788: loss = 26.291757583618164, weights = tensor([6.2008, 3.1527], requires_grad=True)\n",
      "step №789: loss = 26.282459259033203, weights = tensor([6.2004, 3.1550], requires_grad=True)\n",
      "step №790: loss = 26.273162841796875, weights = tensor([6.2001, 3.1574], requires_grad=True)\n",
      "step №791: loss = 26.263870239257812, weights = tensor([6.1997, 3.1597], requires_grad=True)\n",
      "step №792: loss = 26.254587173461914, weights = tensor([6.1993, 3.1620], requires_grad=True)\n",
      "step №793: loss = 26.245309829711914, weights = tensor([6.1989, 3.1644], requires_grad=True)\n",
      "step №794: loss = 26.236034393310547, weights = tensor([6.1986, 3.1667], requires_grad=True)\n",
      "step №795: loss = 26.226776123046875, weights = tensor([6.1982, 3.1690], requires_grad=True)\n",
      "step №796: loss = 26.217514038085938, weights = tensor([6.1978, 3.1713], requires_grad=True)\n",
      "step №797: loss = 26.208267211914062, weights = tensor([6.1975, 3.1737], requires_grad=True)\n",
      "step №798: loss = 26.199016571044922, weights = tensor([6.1971, 3.1760], requires_grad=True)\n",
      "step №799: loss = 26.18977928161621, weights = tensor([6.1967, 3.1783], requires_grad=True)\n",
      "step №800: loss = 26.180545806884766, weights = tensor([6.1963, 3.1806], requires_grad=True)\n",
      "step №801: loss = 26.171316146850586, weights = tensor([6.1960, 3.1830], requires_grad=True)\n",
      "step №802: loss = 26.162097930908203, weights = tensor([6.1956, 3.1853], requires_grad=True)\n",
      "step №803: loss = 26.152883529663086, weights = tensor([6.1952, 3.1876], requires_grad=True)\n",
      "step №804: loss = 26.143672943115234, weights = tensor([6.1949, 3.1899], requires_grad=True)\n",
      "step №805: loss = 26.134469985961914, weights = tensor([6.1945, 3.1922], requires_grad=True)\n",
      "step №806: loss = 26.12527847290039, weights = tensor([6.1941, 3.1946], requires_grad=True)\n",
      "step №807: loss = 26.1160888671875, weights = tensor([6.1938, 3.1969], requires_grad=True)\n",
      "step №808: loss = 26.106903076171875, weights = tensor([6.1934, 3.1992], requires_grad=True)\n",
      "step №809: loss = 26.097726821899414, weights = tensor([6.1930, 3.2015], requires_grad=True)\n",
      "step №810: loss = 26.08855628967285, weights = tensor([6.1926, 3.2038], requires_grad=True)\n",
      "step №811: loss = 26.079391479492188, weights = tensor([6.1923, 3.2061], requires_grad=True)\n",
      "step №812: loss = 26.070232391357422, weights = tensor([6.1919, 3.2085], requires_grad=True)\n",
      "step №813: loss = 26.061080932617188, weights = tensor([6.1915, 3.2108], requires_grad=True)\n",
      "step №814: loss = 26.051931381225586, weights = tensor([6.1912, 3.2131], requires_grad=True)\n",
      "step №815: loss = 26.04279136657715, weights = tensor([6.1908, 3.2154], requires_grad=True)\n",
      "step №816: loss = 26.033660888671875, weights = tensor([6.1904, 3.2177], requires_grad=True)\n",
      "step №817: loss = 26.0245304107666, weights = tensor([6.1901, 3.2200], requires_grad=True)\n",
      "step №818: loss = 26.01540756225586, weights = tensor([6.1897, 3.2223], requires_grad=True)\n",
      "step №819: loss = 26.006296157836914, weights = tensor([6.1893, 3.2246], requires_grad=True)\n",
      "step №820: loss = 25.997182846069336, weights = tensor([6.1890, 3.2269], requires_grad=True)\n",
      "step №821: loss = 25.988086700439453, weights = tensor([6.1886, 3.2293], requires_grad=True)\n",
      "step №822: loss = 25.978988647460938, weights = tensor([6.1882, 3.2316], requires_grad=True)\n",
      "step №823: loss = 25.969898223876953, weights = tensor([6.1879, 3.2339], requires_grad=True)\n",
      "step №824: loss = 25.9608154296875, weights = tensor([6.1875, 3.2362], requires_grad=True)\n",
      "step №825: loss = 25.951732635498047, weights = tensor([6.1871, 3.2385], requires_grad=True)\n",
      "step №826: loss = 25.942663192749023, weights = tensor([6.1868, 3.2408], requires_grad=True)\n",
      "step №827: loss = 25.933597564697266, weights = tensor([6.1864, 3.2431], requires_grad=True)\n",
      "step №828: loss = 25.92453384399414, weights = tensor([6.1860, 3.2454], requires_grad=True)\n",
      "step №829: loss = 25.915485382080078, weights = tensor([6.1857, 3.2477], requires_grad=True)\n",
      "step №830: loss = 25.90643882751465, weights = tensor([6.1853, 3.2500], requires_grad=True)\n",
      "step №831: loss = 25.89739418029785, weights = tensor([6.1849, 3.2523], requires_grad=True)\n",
      "step №832: loss = 25.888357162475586, weights = tensor([6.1846, 3.2546], requires_grad=True)\n",
      "step №833: loss = 25.879329681396484, weights = tensor([6.1842, 3.2569], requires_grad=True)\n",
      "step №834: loss = 25.87030601501465, weights = tensor([6.1838, 3.2592], requires_grad=True)\n",
      "step №835: loss = 25.86128807067871, weights = tensor([6.1835, 3.2615], requires_grad=True)\n",
      "step №836: loss = 25.852275848388672, weights = tensor([6.1831, 3.2638], requires_grad=True)\n",
      "step №837: loss = 25.843273162841797, weights = tensor([6.1827, 3.2661], requires_grad=True)\n",
      "step №838: loss = 25.834274291992188, weights = tensor([6.1824, 3.2684], requires_grad=True)\n",
      "step №839: loss = 25.82528305053711, weights = tensor([6.1820, 3.2707], requires_grad=True)\n",
      "step №840: loss = 25.816295623779297, weights = tensor([6.1816, 3.2729], requires_grad=True)\n",
      "step №841: loss = 25.80731773376465, weights = tensor([6.1813, 3.2752], requires_grad=True)\n",
      "step №842: loss = 25.798343658447266, weights = tensor([6.1809, 3.2775], requires_grad=True)\n",
      "step №843: loss = 25.789371490478516, weights = tensor([6.1805, 3.2798], requires_grad=True)\n",
      "step №844: loss = 25.780414581298828, weights = tensor([6.1802, 3.2821], requires_grad=True)\n",
      "step №845: loss = 25.77146339416504, weights = tensor([6.1798, 3.2844], requires_grad=True)\n",
      "step №846: loss = 25.76250648498535, weights = tensor([6.1794, 3.2867], requires_grad=True)\n",
      "step №847: loss = 25.75356101989746, weights = tensor([6.1791, 3.2890], requires_grad=True)\n",
      "step №848: loss = 25.744625091552734, weights = tensor([6.1787, 3.2913], requires_grad=True)\n",
      "step №849: loss = 25.73569107055664, weights = tensor([6.1783, 3.2935], requires_grad=True)\n",
      "step №850: loss = 25.72676658630371, weights = tensor([6.1780, 3.2958], requires_grad=True)\n",
      "step №851: loss = 25.717849731445312, weights = tensor([6.1776, 3.2981], requires_grad=True)\n",
      "step №852: loss = 25.708932876586914, weights = tensor([6.1772, 3.3004], requires_grad=True)\n",
      "step №853: loss = 25.700023651123047, weights = tensor([6.1769, 3.3027], requires_grad=True)\n",
      "step №854: loss = 25.69112777709961, weights = tensor([6.1765, 3.3050], requires_grad=True)\n",
      "step №855: loss = 25.68222999572754, weights = tensor([6.1762, 3.3072], requires_grad=True)\n",
      "step №856: loss = 25.67333984375, weights = tensor([6.1758, 3.3095], requires_grad=True)\n",
      "step №857: loss = 25.66445541381836, weights = tensor([6.1754, 3.3118], requires_grad=True)\n",
      "step №858: loss = 25.655582427978516, weights = tensor([6.1751, 3.3141], requires_grad=True)\n",
      "step №859: loss = 25.646709442138672, weights = tensor([6.1747, 3.3164], requires_grad=True)\n",
      "step №860: loss = 25.63783836364746, weights = tensor([6.1743, 3.3186], requires_grad=True)\n",
      "step №861: loss = 25.628982543945312, weights = tensor([6.1740, 3.3209], requires_grad=True)\n",
      "step №862: loss = 25.620128631591797, weights = tensor([6.1736, 3.3232], requires_grad=True)\n",
      "step №863: loss = 25.611282348632812, weights = tensor([6.1733, 3.3255], requires_grad=True)\n",
      "step №864: loss = 25.602441787719727, weights = tensor([6.1729, 3.3277], requires_grad=True)\n",
      "step №865: loss = 25.593610763549805, weights = tensor([6.1725, 3.3300], requires_grad=True)\n",
      "step №866: loss = 25.58477783203125, weights = tensor([6.1722, 3.3323], requires_grad=True)\n",
      "step №867: loss = 25.57594871520996, weights = tensor([6.1718, 3.3345], requires_grad=True)\n",
      "step №868: loss = 25.5671329498291, weights = tensor([6.1714, 3.3368], requires_grad=True)\n",
      "step №869: loss = 25.558324813842773, weights = tensor([6.1711, 3.3391], requires_grad=True)\n",
      "step №870: loss = 25.549516677856445, weights = tensor([6.1707, 3.3414], requires_grad=True)\n",
      "step №871: loss = 25.54071807861328, weights = tensor([6.1704, 3.3436], requires_grad=True)\n",
      "step №872: loss = 25.531925201416016, weights = tensor([6.1700, 3.3459], requires_grad=True)\n",
      "step №873: loss = 25.52313804626465, weights = tensor([6.1696, 3.3482], requires_grad=True)\n",
      "step №874: loss = 25.514362335205078, weights = tensor([6.1693, 3.3504], requires_grad=True)\n",
      "step №875: loss = 25.50558090209961, weights = tensor([6.1689, 3.3527], requires_grad=True)\n",
      "step №876: loss = 25.496809005737305, weights = tensor([6.1685, 3.3550], requires_grad=True)\n",
      "step №877: loss = 25.488046646118164, weights = tensor([6.1682, 3.3572], requires_grad=True)\n",
      "step №878: loss = 25.479290008544922, weights = tensor([6.1678, 3.3595], requires_grad=True)\n",
      "step №879: loss = 25.470539093017578, weights = tensor([6.1675, 3.3617], requires_grad=True)\n",
      "step №880: loss = 25.4617919921875, weights = tensor([6.1671, 3.3640], requires_grad=True)\n",
      "step №881: loss = 25.453052520751953, weights = tensor([6.1667, 3.3663], requires_grad=True)\n",
      "step №882: loss = 25.444316864013672, weights = tensor([6.1664, 3.3685], requires_grad=True)\n",
      "step №883: loss = 25.435585021972656, weights = tensor([6.1660, 3.3708], requires_grad=True)\n",
      "step №884: loss = 25.42686653137207, weights = tensor([6.1657, 3.3730], requires_grad=True)\n",
      "step №885: loss = 25.418148040771484, weights = tensor([6.1653, 3.3753], requires_grad=True)\n",
      "step №886: loss = 25.409440994262695, weights = tensor([6.1649, 3.3776], requires_grad=True)\n",
      "step №887: loss = 25.40073585510254, weights = tensor([6.1646, 3.3798], requires_grad=True)\n",
      "step №888: loss = 25.392038345336914, weights = tensor([6.1642, 3.3821], requires_grad=True)\n",
      "step №889: loss = 25.383342742919922, weights = tensor([6.1639, 3.3843], requires_grad=True)\n",
      "step №890: loss = 25.37466049194336, weights = tensor([6.1635, 3.3866], requires_grad=True)\n",
      "step №891: loss = 25.365976333618164, weights = tensor([6.1631, 3.3888], requires_grad=True)\n",
      "step №892: loss = 25.3572998046875, weights = tensor([6.1628, 3.3911], requires_grad=True)\n",
      "step №893: loss = 25.348630905151367, weights = tensor([6.1624, 3.3933], requires_grad=True)\n",
      "step №894: loss = 25.339969635009766, weights = tensor([6.1621, 3.3956], requires_grad=True)\n",
      "step №895: loss = 25.331314086914062, weights = tensor([6.1617, 3.3978], requires_grad=True)\n",
      "step №896: loss = 25.322660446166992, weights = tensor([6.1613, 3.4001], requires_grad=True)\n",
      "step №897: loss = 25.314008712768555, weights = tensor([6.1610, 3.4023], requires_grad=True)\n",
      "step №898: loss = 25.30537223815918, weights = tensor([6.1606, 3.4046], requires_grad=True)\n",
      "step №899: loss = 25.296737670898438, weights = tensor([6.1603, 3.4068], requires_grad=True)\n",
      "step №900: loss = 25.28810691833496, weights = tensor([6.1599, 3.4091], requires_grad=True)\n",
      "step №901: loss = 25.279489517211914, weights = tensor([6.1596, 3.4113], requires_grad=True)\n",
      "step №902: loss = 25.270872116088867, weights = tensor([6.1592, 3.4136], requires_grad=True)\n",
      "step №903: loss = 25.262258529663086, weights = tensor([6.1588, 3.4158], requires_grad=True)\n",
      "step №904: loss = 25.25365447998047, weights = tensor([6.1585, 3.4181], requires_grad=True)\n",
      "step №905: loss = 25.245054244995117, weights = tensor([6.1581, 3.4203], requires_grad=True)\n",
      "step №906: loss = 25.23646354675293, weights = tensor([6.1578, 3.4225], requires_grad=True)\n",
      "step №907: loss = 25.227874755859375, weights = tensor([6.1574, 3.4248], requires_grad=True)\n",
      "step №908: loss = 25.219289779663086, weights = tensor([6.1571, 3.4270], requires_grad=True)\n",
      "step №909: loss = 25.210718154907227, weights = tensor([6.1567, 3.4293], requires_grad=True)\n",
      "step №910: loss = 25.2021484375, weights = tensor([6.1563, 3.4315], requires_grad=True)\n",
      "step №911: loss = 25.193584442138672, weights = tensor([6.1560, 3.4337], requires_grad=True)\n",
      "step №912: loss = 25.18502426147461, weights = tensor([6.1556, 3.4360], requires_grad=True)\n",
      "step №913: loss = 25.176471710205078, weights = tensor([6.1553, 3.4382], requires_grad=True)\n",
      "step №914: loss = 25.167926788330078, weights = tensor([6.1549, 3.4404], requires_grad=True)\n",
      "step №915: loss = 25.159387588500977, weights = tensor([6.1546, 3.4427], requires_grad=True)\n",
      "step №916: loss = 25.150850296020508, weights = tensor([6.1542, 3.4449], requires_grad=True)\n",
      "step №917: loss = 25.142322540283203, weights = tensor([6.1538, 3.4472], requires_grad=True)\n",
      "step №918: loss = 25.133792877197266, weights = tensor([6.1535, 3.4494], requires_grad=True)\n",
      "step №919: loss = 25.125286102294922, weights = tensor([6.1531, 3.4516], requires_grad=True)\n",
      "step №920: loss = 25.116771697998047, weights = tensor([6.1528, 3.4538], requires_grad=True)\n",
      "step №921: loss = 25.108261108398438, weights = tensor([6.1524, 3.4561], requires_grad=True)\n",
      "step №922: loss = 25.099761962890625, weights = tensor([6.1521, 3.4583], requires_grad=True)\n",
      "step №923: loss = 25.09126853942871, weights = tensor([6.1517, 3.4605], requires_grad=True)\n",
      "step №924: loss = 25.082780838012695, weights = tensor([6.1514, 3.4628], requires_grad=True)\n",
      "step №925: loss = 25.07429313659668, weights = tensor([6.1510, 3.4650], requires_grad=True)\n",
      "step №926: loss = 25.065814971923828, weights = tensor([6.1506, 3.4672], requires_grad=True)\n",
      "step №927: loss = 25.05734634399414, weights = tensor([6.1503, 3.4694], requires_grad=True)\n",
      "step №928: loss = 25.048877716064453, weights = tensor([6.1499, 3.4717], requires_grad=True)\n",
      "step №929: loss = 25.04041862487793, weights = tensor([6.1496, 3.4739], requires_grad=True)\n",
      "step №930: loss = 25.031963348388672, weights = tensor([6.1492, 3.4761], requires_grad=True)\n",
      "step №931: loss = 25.023515701293945, weights = tensor([6.1489, 3.4783], requires_grad=True)\n",
      "step №932: loss = 25.015071868896484, weights = tensor([6.1485, 3.4806], requires_grad=True)\n",
      "step №933: loss = 25.006635665893555, weights = tensor([6.1482, 3.4828], requires_grad=True)\n",
      "step №934: loss = 24.99820327758789, weights = tensor([6.1478, 3.4850], requires_grad=True)\n",
      "step №935: loss = 24.989776611328125, weights = tensor([6.1475, 3.4872], requires_grad=True)\n",
      "step №936: loss = 24.981355667114258, weights = tensor([6.1471, 3.4894], requires_grad=True)\n",
      "step №937: loss = 24.97294044494629, weights = tensor([6.1467, 3.4917], requires_grad=True)\n",
      "step №938: loss = 24.964529037475586, weights = tensor([6.1464, 3.4939], requires_grad=True)\n",
      "step №939: loss = 24.956132888793945, weights = tensor([6.1460, 3.4961], requires_grad=True)\n",
      "step №940: loss = 24.947736740112305, weights = tensor([6.1457, 3.4983], requires_grad=True)\n",
      "step №941: loss = 24.939342498779297, weights = tensor([6.1453, 3.5005], requires_grad=True)\n",
      "step №942: loss = 24.930953979492188, weights = tensor([6.1450, 3.5027], requires_grad=True)\n",
      "step №943: loss = 24.92257308959961, weights = tensor([6.1446, 3.5050], requires_grad=True)\n",
      "step №944: loss = 24.914201736450195, weights = tensor([6.1443, 3.5072], requires_grad=True)\n",
      "step №945: loss = 24.905834197998047, weights = tensor([6.1439, 3.5094], requires_grad=True)\n",
      "step №946: loss = 24.8974666595459, weights = tensor([6.1436, 3.5116], requires_grad=True)\n",
      "step №947: loss = 24.889110565185547, weights = tensor([6.1432, 3.5138], requires_grad=True)\n",
      "step №948: loss = 24.880760192871094, weights = tensor([6.1429, 3.5160], requires_grad=True)\n",
      "step №949: loss = 24.872413635253906, weights = tensor([6.1425, 3.5182], requires_grad=True)\n",
      "step №950: loss = 24.864070892333984, weights = tensor([6.1422, 3.5204], requires_grad=True)\n",
      "step №951: loss = 24.855735778808594, weights = tensor([6.1418, 3.5226], requires_grad=True)\n",
      "step №952: loss = 24.847408294677734, weights = tensor([6.1415, 3.5248], requires_grad=True)\n",
      "step №953: loss = 24.839080810546875, weights = tensor([6.1411, 3.5271], requires_grad=True)\n",
      "step №954: loss = 24.830764770507812, weights = tensor([6.1407, 3.5293], requires_grad=True)\n",
      "step №955: loss = 24.822450637817383, weights = tensor([6.1404, 3.5315], requires_grad=True)\n",
      "step №956: loss = 24.814146041870117, weights = tensor([6.1400, 3.5337], requires_grad=True)\n",
      "step №957: loss = 24.805843353271484, weights = tensor([6.1397, 3.5359], requires_grad=True)\n",
      "step №958: loss = 24.79755210876465, weights = tensor([6.1393, 3.5381], requires_grad=True)\n",
      "step №959: loss = 24.78925895690918, weights = tensor([6.1390, 3.5403], requires_grad=True)\n",
      "step №960: loss = 24.780975341796875, weights = tensor([6.1386, 3.5425], requires_grad=True)\n",
      "step №961: loss = 24.772695541381836, weights = tensor([6.1383, 3.5447], requires_grad=True)\n",
      "step №962: loss = 24.764423370361328, weights = tensor([6.1379, 3.5469], requires_grad=True)\n",
      "step №963: loss = 24.756155014038086, weights = tensor([6.1376, 3.5491], requires_grad=True)\n",
      "step №964: loss = 24.74789047241211, weights = tensor([6.1372, 3.5513], requires_grad=True)\n",
      "step №965: loss = 24.739635467529297, weights = tensor([6.1369, 3.5535], requires_grad=True)\n",
      "step №966: loss = 24.73138999938965, weights = tensor([6.1365, 3.5557], requires_grad=True)\n",
      "step №967: loss = 24.7231388092041, weights = tensor([6.1362, 3.5579], requires_grad=True)\n",
      "step №968: loss = 24.714900970458984, weights = tensor([6.1358, 3.5601], requires_grad=True)\n",
      "step №969: loss = 24.706668853759766, weights = tensor([6.1355, 3.5623], requires_grad=True)\n",
      "step №970: loss = 24.698440551757812, weights = tensor([6.1351, 3.5644], requires_grad=True)\n",
      "step №971: loss = 24.69021987915039, weights = tensor([6.1348, 3.5666], requires_grad=True)\n",
      "step №972: loss = 24.681997299194336, weights = tensor([6.1344, 3.5688], requires_grad=True)\n",
      "step №973: loss = 24.673791885375977, weights = tensor([6.1341, 3.5710], requires_grad=True)\n",
      "step №974: loss = 24.66558074951172, weights = tensor([6.1337, 3.5732], requires_grad=True)\n",
      "step №975: loss = 24.657379150390625, weights = tensor([6.1334, 3.5754], requires_grad=True)\n",
      "step №976: loss = 24.649185180664062, weights = tensor([6.1330, 3.5776], requires_grad=True)\n",
      "step №977: loss = 24.6409969329834, weights = tensor([6.1327, 3.5798], requires_grad=True)\n",
      "step №978: loss = 24.6328125, weights = tensor([6.1323, 3.5820], requires_grad=True)\n",
      "step №979: loss = 24.6246337890625, weights = tensor([6.1320, 3.5842], requires_grad=True)\n",
      "step №980: loss = 24.616458892822266, weights = tensor([6.1316, 3.5863], requires_grad=True)\n",
      "step №981: loss = 24.608293533325195, weights = tensor([6.1313, 3.5885], requires_grad=True)\n",
      "step №982: loss = 24.60013198852539, weights = tensor([6.1309, 3.5907], requires_grad=True)\n",
      "step №983: loss = 24.59197425842285, weights = tensor([6.1306, 3.5929], requires_grad=True)\n",
      "step №984: loss = 24.58382797241211, weights = tensor([6.1303, 3.5951], requires_grad=True)\n",
      "step №985: loss = 24.575681686401367, weights = tensor([6.1299, 3.5973], requires_grad=True)\n",
      "step №986: loss = 24.56753921508789, weights = tensor([6.1296, 3.5994], requires_grad=True)\n",
      "step №987: loss = 24.55940818786621, weights = tensor([6.1292, 3.6016], requires_grad=True)\n",
      "step №988: loss = 24.551273345947266, weights = tensor([6.1289, 3.6038], requires_grad=True)\n",
      "step №989: loss = 24.543155670166016, weights = tensor([6.1285, 3.6060], requires_grad=True)\n",
      "step №990: loss = 24.5350341796875, weights = tensor([6.1282, 3.6082], requires_grad=True)\n",
      "step №991: loss = 24.526926040649414, weights = tensor([6.1278, 3.6103], requires_grad=True)\n",
      "step №992: loss = 24.518823623657227, weights = tensor([6.1275, 3.6125], requires_grad=True)\n",
      "step №993: loss = 24.510717391967773, weights = tensor([6.1271, 3.6147], requires_grad=True)\n",
      "step №994: loss = 24.502620697021484, weights = tensor([6.1268, 3.6169], requires_grad=True)\n",
      "step №995: loss = 24.494531631469727, weights = tensor([6.1264, 3.6190], requires_grad=True)\n",
      "step №996: loss = 24.486446380615234, weights = tensor([6.1261, 3.6212], requires_grad=True)\n",
      "step №997: loss = 24.478368759155273, weights = tensor([6.1257, 3.6234], requires_grad=True)\n",
      "step №998: loss = 24.47029685974121, weights = tensor([6.1254, 3.6256], requires_grad=True)\n",
      "step №999: loss = 24.462223052978516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#let's also use mse error and optimizer which are provided by Pytorch library  \n",
    "import torch.nn as nn \n",
    "number_of_steps = 1000 \n",
    "w = torch.zeros(2, dtype = torch.float32, requires_grad = True)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = predict(w, x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "08d78b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ty/zhdd1dkj36j5r796hbm4035c0000gn/T/ipykernel_13661/4163972926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Everything is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LinePlotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_semantics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     p = _LinePlotter(\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_semantic_mappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             plot_data, variables = self._assign_variables_longform(\n\u001b[0m\u001b[1;32m    669\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m_assign_variables_longform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;31m# Construct a tidy plot DataFrame. This will convert a number of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;31m# types automatically, aligning on index in case of pandas objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# Reduce the variables dictionary to fields with valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "sns.lineplot(x = x, y = y)\n",
    "sns.lineplot(x = x, y = y_pred.detach().numpy())\n",
    "plt.title(f'Everything is calculated by Pytorch. Slope = {w[0]}, intercept = {w[1]}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3eda2394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f8149bdfeb0>\n",
      "step №0: loss = 1407.1378173828125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №1: loss = 1311.8421630859375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №2: loss = 1223.111083984375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №3: loss = 1140.491943359375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №4: loss = 1063.5634765625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №5: loss = 991.9339599609375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №6: loss = 925.2384033203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №7: loss = 863.1365356445312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №8: loss = 805.31201171875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №9: loss = 751.4703369140625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №10: loss = 701.3368530273438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №11: loss = 654.6563110351562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №12: loss = 611.1907958984375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №13: loss = 570.7186279296875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №14: loss = 533.0337524414062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №15: loss = 497.94415283203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №16: loss = 465.27093505859375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №17: loss = 434.8478088378906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №18: loss = 406.51953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №19: loss = 380.1419677734375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №20: loss = 355.5805969238281, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №21: loss = 332.7103576660156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №22: loss = 311.41473388671875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №23: loss = 291.58526611328125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №24: loss = 273.1208801269531, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №25: loss = 255.92758178710938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №26: loss = 239.91781616210938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №27: loss = 225.01004028320312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №28: loss = 211.1283416748047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №29: loss = 198.20199584960938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №30: loss = 186.16525268554688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №31: loss = 174.9567108154297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №32: loss = 164.51943969726562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №33: loss = 154.80018615722656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №34: loss = 145.7495880126953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №35: loss = 137.32150268554688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №36: loss = 129.4730987548828, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №37: loss = 122.16441345214844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №38: loss = 115.35831451416016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №39: loss = 109.02010345458984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №40: loss = 103.11759948730469, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №41: loss = 97.62075805664062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №42: loss = 92.50160217285156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №43: loss = 87.73419952392578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №44: loss = 83.29423522949219, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №45: loss = 79.15916442871094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №46: loss = 75.30803680419922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №47: loss = 71.72122955322266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №48: loss = 68.38057708740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №49: loss = 65.26909637451172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №50: loss = 62.37102508544922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №51: loss = 59.67162322998047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №52: loss = 57.157249450683594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №53: loss = 54.815147399902344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №54: loss = 52.633399963378906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №55: loss = 50.60102081298828, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №56: loss = 48.707672119140625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №57: loss = 46.94380569458008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №58: loss = 45.30048370361328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №59: loss = 43.76943588256836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №60: loss = 42.3428955078125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №61: loss = 41.013675689697266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №62: loss = 39.77507400512695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №63: loss = 38.620853424072266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №64: loss = 37.54518508911133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №65: loss = 36.54267501831055, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №66: loss = 35.608272552490234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №67: loss = 34.7372932434082, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №68: loss = 33.925357818603516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №69: loss = 33.16841125488281, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №70: loss = 32.462646484375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №71: loss = 31.804553985595703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №72: loss = 31.190845489501953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №73: loss = 30.618484497070312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №74: loss = 30.084590911865234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №75: loss = 29.586532592773438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №76: loss = 29.1218318939209, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №77: loss = 28.688217163085938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №78: loss = 28.28351402282715, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №79: loss = 27.905746459960938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №80: loss = 27.553049087524414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №81: loss = 27.223709106445312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №82: loss = 26.91611099243164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №83: loss = 26.62876319885254, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №84: loss = 26.360260009765625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №85: loss = 26.109329223632812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №86: loss = 25.87472915649414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №87: loss = 25.655349731445312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №88: loss = 25.450149536132812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №89: loss = 25.25813865661621, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №90: loss = 25.078418731689453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №91: loss = 24.910139083862305, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №92: loss = 24.752521514892578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №93: loss = 24.60481834411621, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №94: loss = 24.466346740722656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №95: loss = 24.33648109436035, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №96: loss = 24.214630126953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №97: loss = 24.100234985351562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №98: loss = 23.992786407470703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №99: loss = 23.891809463500977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №100: loss = 23.79684829711914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №101: loss = 23.707500457763672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №102: loss = 23.623376846313477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №103: loss = 23.54410743713379, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №104: loss = 23.469371795654297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №105: loss = 23.398862838745117, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №106: loss = 23.332277297973633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №107: loss = 23.269338607788086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №108: loss = 23.20981788635254, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №109: loss = 23.15346336364746, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №110: loss = 23.100065231323242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №111: loss = 23.049423217773438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №112: loss = 23.001340866088867, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №113: loss = 22.955646514892578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №114: loss = 22.912174224853516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №115: loss = 22.870769500732422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №116: loss = 22.831296920776367, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №117: loss = 22.793617248535156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №118: loss = 22.757617950439453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №119: loss = 22.72317123413086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №120: loss = 22.690174102783203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №121: loss = 22.658533096313477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №122: loss = 22.628149032592773, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №123: loss = 22.598941802978516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №124: loss = 22.57082748413086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №125: loss = 22.543729782104492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №126: loss = 22.5175838470459, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №127: loss = 22.49232292175293, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №128: loss = 22.467876434326172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №129: loss = 22.4442081451416, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №130: loss = 22.421253204345703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №131: loss = 22.398962020874023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №132: loss = 22.377300262451172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №133: loss = 22.356212615966797, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №134: loss = 22.335660934448242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №135: loss = 22.315616607666016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №136: loss = 22.29604721069336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №137: loss = 22.276906967163086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №138: loss = 22.258180618286133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №139: loss = 22.239830017089844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №140: loss = 22.22184181213379, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №141: loss = 22.20417594909668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №142: loss = 22.186824798583984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №143: loss = 22.169757843017578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №144: loss = 22.152965545654297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №145: loss = 22.13641929626465, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №146: loss = 22.120113372802734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №147: loss = 22.104021072387695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №148: loss = 22.088130950927734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №149: loss = 22.072437286376953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №150: loss = 22.056922912597656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №151: loss = 22.04157257080078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №152: loss = 22.026378631591797, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №153: loss = 22.01132583618164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №154: loss = 21.99641990661621, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №155: loss = 21.981637954711914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №156: loss = 21.966968536376953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №157: loss = 21.952411651611328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №158: loss = 21.937965393066406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №159: loss = 21.923612594604492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №160: loss = 21.909358978271484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №161: loss = 21.895183563232422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №162: loss = 21.881088256835938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №163: loss = 21.867076873779297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №164: loss = 21.85313606262207, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №165: loss = 21.83926010131836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №166: loss = 21.825443267822266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №167: loss = 21.81167984008789, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №168: loss = 21.79798698425293, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №169: loss = 21.784334182739258, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №170: loss = 21.770740509033203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №171: loss = 21.75718879699707, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №172: loss = 21.743682861328125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №173: loss = 21.730215072631836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №174: loss = 21.71678924560547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №175: loss = 21.70340347290039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №176: loss = 21.690052032470703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №177: loss = 21.67673110961914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №178: loss = 21.663448333740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №179: loss = 21.650184631347656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №180: loss = 21.636959075927734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №181: loss = 21.623754501342773, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №182: loss = 21.610580444335938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №183: loss = 21.597429275512695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №184: loss = 21.584300994873047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №185: loss = 21.571197509765625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №186: loss = 21.55811309814453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №187: loss = 21.545047760009766, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №188: loss = 21.53200912475586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №189: loss = 21.518985748291016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №190: loss = 21.5059814453125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №191: loss = 21.492996215820312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №192: loss = 21.480022430419922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №193: loss = 21.467079162597656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №194: loss = 21.454133987426758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №195: loss = 21.44121742248535, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №196: loss = 21.42831039428711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №197: loss = 21.41541862487793, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №198: loss = 21.40254020690918, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №199: loss = 21.389678955078125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №200: loss = 21.376827239990234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №201: loss = 21.363994598388672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №202: loss = 21.35116958618164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №203: loss = 21.338359832763672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №204: loss = 21.325557708740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №205: loss = 21.312768936157227, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №206: loss = 21.29999351501465, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №207: loss = 21.287229537963867, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №208: loss = 21.274477005004883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №209: loss = 21.261735916137695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №210: loss = 21.249004364013672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №211: loss = 21.236284255981445, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №212: loss = 21.223575592041016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №213: loss = 21.210880279541016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №214: loss = 21.19818687438965, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №215: loss = 21.18551254272461, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №216: loss = 21.172840118408203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №217: loss = 21.16018295288086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №218: loss = 21.147537231445312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №219: loss = 21.134891510009766, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №220: loss = 21.122272491455078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №221: loss = 21.10965347290039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №222: loss = 21.0970401763916, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №223: loss = 21.084440231323242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №224: loss = 21.071849822998047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №225: loss = 21.059267044067383, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №226: loss = 21.046693801879883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №227: loss = 21.03413200378418, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №228: loss = 21.021575927734375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №229: loss = 21.009033203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №230: loss = 20.996498107910156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №231: loss = 20.983972549438477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №232: loss = 20.971454620361328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №233: loss = 20.958946228027344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №234: loss = 20.946443557739258, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №235: loss = 20.933956146240234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №236: loss = 20.921466827392578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №237: loss = 20.908998489379883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №238: loss = 20.896535873413086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №239: loss = 20.884075164794922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №240: loss = 20.871631622314453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №241: loss = 20.85919189453125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №242: loss = 20.84676170349121, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №243: loss = 20.834339141845703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №244: loss = 20.82192611694336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №245: loss = 20.809524536132812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №246: loss = 20.79712677001953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №247: loss = 20.784738540649414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №248: loss = 20.772361755371094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №249: loss = 20.759994506835938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №250: loss = 20.747629165649414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №251: loss = 20.735279083251953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №252: loss = 20.72293472290039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №253: loss = 20.710594177246094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №254: loss = 20.698272705078125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №255: loss = 20.685955047607422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №256: loss = 20.673641204833984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №257: loss = 20.661344528198242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №258: loss = 20.6490478515625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №259: loss = 20.636760711669922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №260: loss = 20.624479293823242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №261: loss = 20.61221694946289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №262: loss = 20.599956512451172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №263: loss = 20.587705612182617, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №264: loss = 20.575462341308594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №265: loss = 20.56322479248047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №266: loss = 20.55099868774414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №267: loss = 20.538782119750977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №268: loss = 20.526567459106445, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №269: loss = 20.51436996459961, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №270: loss = 20.502180099487305, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №271: loss = 20.489990234375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №272: loss = 20.477813720703125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №273: loss = 20.46564483642578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №274: loss = 20.453487396240234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №275: loss = 20.44133186340332, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №276: loss = 20.42919158935547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №277: loss = 20.417057037353516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №278: loss = 20.404930114746094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №279: loss = 20.392805099487305, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №280: loss = 20.380695343017578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №281: loss = 20.368595123291016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №282: loss = 20.356502532958984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №283: loss = 20.344411849975586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №284: loss = 20.33233070373535, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №285: loss = 20.320268630981445, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №286: loss = 20.308198928833008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №287: loss = 20.2961483001709, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №288: loss = 20.284103393554688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №289: loss = 20.272062301635742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №290: loss = 20.26003646850586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №291: loss = 20.24802017211914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №292: loss = 20.236003875732422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №293: loss = 20.223995208740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №294: loss = 20.21200180053711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №295: loss = 20.200016021728516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №296: loss = 20.188030242919922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №297: loss = 20.17605972290039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №298: loss = 20.164094924926758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №299: loss = 20.152141571044922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №300: loss = 20.14019203186035, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №301: loss = 20.128253936767578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №302: loss = 20.116321563720703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №303: loss = 20.104398727416992, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №304: loss = 20.092487335205078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №305: loss = 20.08057975769043, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №306: loss = 20.068683624267578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №307: loss = 20.056785583496094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №308: loss = 20.044906616210938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №309: loss = 20.033031463623047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №310: loss = 20.02116584777832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №311: loss = 20.00930404663086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №312: loss = 19.997453689575195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №313: loss = 19.985612869262695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №314: loss = 19.973777770996094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №315: loss = 19.96194839477539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №316: loss = 19.95012855529785, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №317: loss = 19.938316345214844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №318: loss = 19.926517486572266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №319: loss = 19.914724349975586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №320: loss = 19.902935028076172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №321: loss = 19.891155242919922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №322: loss = 19.879383087158203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №323: loss = 19.86762237548828, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №324: loss = 19.85586929321289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №325: loss = 19.8441219329834, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №326: loss = 19.832380294799805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №327: loss = 19.820650100708008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №328: loss = 19.808927536010742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №329: loss = 19.79720687866211, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №330: loss = 19.785507202148438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №331: loss = 19.7738094329834, weights = tensor([6.1250, 3.6277], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №332: loss = 19.76211929321289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №333: loss = 19.750431060791016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №334: loss = 19.738754272460938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №335: loss = 19.72708511352539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №336: loss = 19.71542739868164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №337: loss = 19.703777313232422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №338: loss = 19.69213104248047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №339: loss = 19.680498123168945, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №340: loss = 19.668869018554688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №341: loss = 19.65724754333496, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №342: loss = 19.64563751220703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №343: loss = 19.634033203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №344: loss = 19.622432708740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №345: loss = 19.6108455657959, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №346: loss = 19.59926986694336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №347: loss = 19.587692260742188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №348: loss = 19.576126098632812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №349: loss = 19.5645751953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №350: loss = 19.553020477294922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №351: loss = 19.54148292541504, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №352: loss = 19.52994728088379, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №353: loss = 19.518421173095703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №354: loss = 19.506898880004883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №355: loss = 19.495393753051758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №356: loss = 19.483890533447266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №357: loss = 19.472394943237305, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №358: loss = 19.46090316772461, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №359: loss = 19.44942855834961, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №360: loss = 19.43795394897461, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №361: loss = 19.426494598388672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №362: loss = 19.415035247802734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №363: loss = 19.40359115600586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №364: loss = 19.392147064208984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №365: loss = 19.380718231201172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №366: loss = 19.369291305541992, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №367: loss = 19.35787010192871, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №368: loss = 19.346464157104492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №369: loss = 19.33506202697754, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №370: loss = 19.323665618896484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №371: loss = 19.312286376953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №372: loss = 19.3009033203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №373: loss = 19.28952980041504, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №374: loss = 19.27816390991211, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №375: loss = 19.266815185546875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №376: loss = 19.25546646118164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №377: loss = 19.244125366210938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №378: loss = 19.232791900634766, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №379: loss = 19.22146987915039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №380: loss = 19.210153579711914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №381: loss = 19.198841094970703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №382: loss = 19.18754005432129, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №383: loss = 19.17624282836914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №384: loss = 19.164960861206055, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №385: loss = 19.1536808013916, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №386: loss = 19.142410278320312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №387: loss = 19.131145477294922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №388: loss = 19.119892120361328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №389: loss = 19.108642578125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №390: loss = 19.09740447998047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №391: loss = 19.086170196533203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №392: loss = 19.074947357177734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №393: loss = 19.063724517822266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №394: loss = 19.052518844604492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №395: loss = 19.04131507873535, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №396: loss = 19.030120849609375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №397: loss = 19.01893424987793, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №398: loss = 19.00775146484375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №399: loss = 18.9965763092041, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №400: loss = 18.985416412353516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №401: loss = 18.974254608154297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №402: loss = 18.96310806274414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №403: loss = 18.951961517333984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №404: loss = 18.940834045410156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №405: loss = 18.929706573486328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №406: loss = 18.9185848236084, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №407: loss = 18.907474517822266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №408: loss = 18.8963680267334, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №409: loss = 18.885272979736328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №410: loss = 18.874181747436523, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №411: loss = 18.86309814453125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №412: loss = 18.85202980041504, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №413: loss = 18.840961456298828, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №414: loss = 18.82990074157715, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №415: loss = 18.818849563598633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №416: loss = 18.807804107666016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №417: loss = 18.79676628112793, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №418: loss = 18.78573989868164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №419: loss = 18.774715423583984, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №420: loss = 18.763700485229492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №421: loss = 18.752700805664062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №422: loss = 18.741695404052734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №423: loss = 18.7307071685791, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №424: loss = 18.719715118408203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №425: loss = 18.708744049072266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №426: loss = 18.697772979736328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №427: loss = 18.686813354492188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №428: loss = 18.675857543945312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №429: loss = 18.66490936279297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №430: loss = 18.65397071838379, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №431: loss = 18.64303970336914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №432: loss = 18.632110595703125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №433: loss = 18.621196746826172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №434: loss = 18.610286712646484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №435: loss = 18.599382400512695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №436: loss = 18.588489532470703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №437: loss = 18.577590942382812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №438: loss = 18.56671714782715, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №439: loss = 18.555843353271484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №440: loss = 18.544973373413086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №441: loss = 18.534116744995117, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №442: loss = 18.52326774597168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №443: loss = 18.512422561645508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №444: loss = 18.501583099365234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №445: loss = 18.49075698852539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №446: loss = 18.479936599731445, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №447: loss = 18.4691162109375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №448: loss = 18.458314895629883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №449: loss = 18.44751739501953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №450: loss = 18.436717987060547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №451: loss = 18.425935745239258, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №452: loss = 18.4151554107666, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №453: loss = 18.404390335083008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №454: loss = 18.39362144470215, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №455: loss = 18.382862091064453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №456: loss = 18.372119903564453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №457: loss = 18.36137580871582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №458: loss = 18.35064125061035, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №459: loss = 18.33991813659668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №460: loss = 18.32918930053711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №461: loss = 18.318485260009766, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №462: loss = 18.307775497436523, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №463: loss = 18.297077178955078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №464: loss = 18.28638458251953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №465: loss = 18.27570343017578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №466: loss = 18.265029907226562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №467: loss = 18.25436019897461, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №468: loss = 18.243696212768555, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №469: loss = 18.233049392700195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №470: loss = 18.222393035888672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №471: loss = 18.211750030517578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №472: loss = 18.20112419128418, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №473: loss = 18.19049835205078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №474: loss = 18.179880142211914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №475: loss = 18.16926383972168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №476: loss = 18.158662796020508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №477: loss = 18.14806365966797, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №478: loss = 18.137479782104492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №479: loss = 18.126893997192383, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №480: loss = 18.116315841674805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №481: loss = 18.105751037597656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №482: loss = 18.095191955566406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №483: loss = 18.084638595581055, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №484: loss = 18.0740909576416, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №485: loss = 18.06355094909668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №486: loss = 18.053020477294922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №487: loss = 18.042490005493164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №488: loss = 18.03197479248047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №489: loss = 18.02146339416504, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №490: loss = 18.01095962524414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №491: loss = 18.000459671020508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №492: loss = 17.989971160888672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №493: loss = 17.979488372802734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №494: loss = 17.969011306762695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №495: loss = 17.95854377746582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №496: loss = 17.948083877563477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №497: loss = 17.937631607055664, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №498: loss = 17.927181243896484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №499: loss = 17.91674041748047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №500: loss = 17.90631103515625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №501: loss = 17.89588737487793, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №502: loss = 17.88546371459961, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №503: loss = 17.87505340576172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №504: loss = 17.864646911621094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №505: loss = 17.8542537689209, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №506: loss = 17.843860626220703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №507: loss = 17.833477020263672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №508: loss = 17.823101043701172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №509: loss = 17.81273078918457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №510: loss = 17.802370071411133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №511: loss = 17.792011260986328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №512: loss = 17.781660079956055, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №513: loss = 17.771320343017578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №514: loss = 17.760986328125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №515: loss = 17.750656127929688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №516: loss = 17.74033546447754, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №517: loss = 17.730022430419922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №518: loss = 17.719717025756836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №519: loss = 17.709421157836914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №520: loss = 17.69912338256836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №521: loss = 17.688838958740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №522: loss = 17.678556442260742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №523: loss = 17.66828727722168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №524: loss = 17.65802001953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №525: loss = 17.647769927978516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №526: loss = 17.637516021728516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №527: loss = 17.62727165222168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №528: loss = 17.617033004760742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №529: loss = 17.606807708740234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №530: loss = 17.59658432006836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №531: loss = 17.586368560791016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №532: loss = 17.576162338256836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №533: loss = 17.56595230102539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №534: loss = 17.555755615234375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №535: loss = 17.545574188232422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №536: loss = 17.535388946533203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №537: loss = 17.52521514892578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №538: loss = 17.51504898071289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №539: loss = 17.504886627197266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №540: loss = 17.494733810424805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №541: loss = 17.48458480834961, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №542: loss = 17.474445343017578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №543: loss = 17.46431541442871, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №544: loss = 17.454185485839844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №545: loss = 17.44406509399414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №546: loss = 17.433950424194336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №547: loss = 17.423847198486328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №548: loss = 17.413747787475586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №549: loss = 17.403654098510742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №550: loss = 17.393571853637695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №551: loss = 17.383495330810547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №552: loss = 17.373422622680664, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №553: loss = 17.363353729248047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №554: loss = 17.353294372558594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №555: loss = 17.343246459960938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №556: loss = 17.333202362060547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №557: loss = 17.323162078857422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №558: loss = 17.313129425048828, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №559: loss = 17.303110122680664, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №560: loss = 17.293092727661133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №561: loss = 17.283082962036133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №562: loss = 17.273082733154297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №563: loss = 17.263084411621094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №564: loss = 17.253093719482422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №565: loss = 17.243106842041016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №566: loss = 17.233135223388672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №567: loss = 17.223163604736328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №568: loss = 17.213199615478516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №569: loss = 17.203243255615234, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №570: loss = 17.19329261779785, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №571: loss = 17.183351516723633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №572: loss = 17.173416137695312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №573: loss = 17.16348648071289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №574: loss = 17.153560638427734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №575: loss = 17.143646240234375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №576: loss = 17.133737564086914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №577: loss = 17.12383460998535, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №578: loss = 17.113937377929688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №579: loss = 17.10405158996582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №580: loss = 17.094165802001953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №581: loss = 17.084291458129883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №582: loss = 17.074420928955078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №583: loss = 17.064558029174805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №584: loss = 17.05470085144043, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №585: loss = 17.044857025146484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №586: loss = 17.03501319885254, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №587: loss = 17.025175094604492, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №588: loss = 17.015352249145508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №589: loss = 17.005523681640625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №590: loss = 16.995708465576172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №591: loss = 16.985898971557617, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №592: loss = 16.976099014282227, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №593: loss = 16.966304779052734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №594: loss = 16.956510543823242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №595: loss = 16.946735382080078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №596: loss = 16.93695640563965, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №597: loss = 16.92718505859375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №598: loss = 16.917423248291016, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №599: loss = 16.907669067382812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №600: loss = 16.89792251586914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №601: loss = 16.888179779052734, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №602: loss = 16.87843894958496, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №603: loss = 16.868711471557617, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №604: loss = 16.858989715576172, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №605: loss = 16.84926986694336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №606: loss = 16.839563369750977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №607: loss = 16.82986068725586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №608: loss = 16.82015609741211, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №609: loss = 16.81047248840332, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №610: loss = 16.800790786743164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №611: loss = 16.79111099243164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №612: loss = 16.781442642211914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №613: loss = 16.771774291992188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №614: loss = 16.762121200561523, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №615: loss = 16.752471923828125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №616: loss = 16.74282455444336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №617: loss = 16.73318862915039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №618: loss = 16.723556518554688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №619: loss = 16.713932037353516, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №620: loss = 16.704313278198242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №621: loss = 16.694700241088867, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №622: loss = 16.685100555419922, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №623: loss = 16.675498962402344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №624: loss = 16.665904998779297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №625: loss = 16.656322479248047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №626: loss = 16.646745681762695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №627: loss = 16.637170791625977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №628: loss = 16.62760353088379, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №629: loss = 16.618040084838867, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №630: loss = 16.608491897583008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №631: loss = 16.59894371032715, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №632: loss = 16.58940315246582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №633: loss = 16.579872131347656, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №634: loss = 16.570344924926758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №635: loss = 16.56082534790039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №636: loss = 16.551315307617188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №637: loss = 16.54180335998535, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №638: loss = 16.53230094909668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №639: loss = 16.522809982299805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №640: loss = 16.513317108154297, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №641: loss = 16.503835678100586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №642: loss = 16.49435806274414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №643: loss = 16.484893798828125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №644: loss = 16.47542953491211, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №645: loss = 16.46596908569336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №646: loss = 16.456523895263672, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №647: loss = 16.447080612182617, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №648: loss = 16.437639236450195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №649: loss = 16.428211212158203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №650: loss = 16.418785095214844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №651: loss = 16.40936851501465, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №652: loss = 16.399951934814453, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №653: loss = 16.390548706054688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №654: loss = 16.38115119934082, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №655: loss = 16.371761322021484, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №656: loss = 16.36237335205078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №657: loss = 16.352989196777344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №658: loss = 16.343616485595703, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №659: loss = 16.33424949645996, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №660: loss = 16.324892044067383, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №661: loss = 16.315534591674805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №662: loss = 16.306184768676758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №663: loss = 16.296846389770508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №664: loss = 16.28750991821289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №665: loss = 16.27817726135254, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №666: loss = 16.268861770629883, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №667: loss = 16.25954246520996, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №668: loss = 16.250232696533203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №669: loss = 16.240924835205078, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №670: loss = 16.23162841796875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №671: loss = 16.222339630126953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №672: loss = 16.213054656982422, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №673: loss = 16.203771591186523, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №674: loss = 16.194507598876953, weights = tensor([6.1250, 3.6277], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step №675: loss = 16.18523597717285, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №676: loss = 16.175975799560547, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №677: loss = 16.166719436645508, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №678: loss = 16.157474517822266, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №679: loss = 16.14823341369629, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №680: loss = 16.138999938964844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №681: loss = 16.12976837158203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №682: loss = 16.120546340942383, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №683: loss = 16.111328125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №684: loss = 16.102123260498047, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №685: loss = 16.092918395996094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №686: loss = 16.083717346191406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №687: loss = 16.07453155517578, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №688: loss = 16.065345764160156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №689: loss = 16.056171417236328, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №690: loss = 16.046998977661133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №691: loss = 16.037830352783203, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №692: loss = 16.028669357299805, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №693: loss = 16.019515991210938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №694: loss = 16.01036834716797, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №695: loss = 16.001232147216797, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №696: loss = 15.992094039916992, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №697: loss = 15.982963562011719, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №698: loss = 15.973836898803711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №699: loss = 15.964723587036133, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №700: loss = 15.955612182617188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №701: loss = 15.946507453918457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №702: loss = 15.937411308288574, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №703: loss = 15.928321838378906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №704: loss = 15.919229507446289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №705: loss = 15.910154342651367, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №706: loss = 15.901082992553711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №707: loss = 15.892011642456055, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №708: loss = 15.882949829101562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №709: loss = 15.87389850616455, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №710: loss = 15.864847183227539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №711: loss = 15.855802536010742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №712: loss = 15.846771240234375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №713: loss = 15.837736129760742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №714: loss = 15.828715324401855, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №715: loss = 15.819694519042969, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №716: loss = 15.810684204101562, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №717: loss = 15.801676750183105, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №718: loss = 15.79267692565918, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №719: loss = 15.783681869506836, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №720: loss = 15.774694442749023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №721: loss = 15.765716552734375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №722: loss = 15.756736755371094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №723: loss = 15.747772216796875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №724: loss = 15.738802909851074, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №725: loss = 15.729848861694336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №726: loss = 15.720895767211914, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №727: loss = 15.711950302124023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №728: loss = 15.703007698059082, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №729: loss = 15.694076538085938, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №730: loss = 15.685153007507324, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №731: loss = 15.676233291625977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №732: loss = 15.667317390441895, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №733: loss = 15.658404350280762, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №734: loss = 15.649502754211426, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №735: loss = 15.640604972839355, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №736: loss = 15.63171672821045, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №737: loss = 15.622835159301758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №738: loss = 15.613950729370117, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №739: loss = 15.605081558227539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №740: loss = 15.596216201782227, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №741: loss = 15.587356567382812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №742: loss = 15.578495979309082, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №743: loss = 15.569645881652832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №744: loss = 15.560803413391113, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №745: loss = 15.551965713500977, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №746: loss = 15.54313850402832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №747: loss = 15.534314155578613, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №748: loss = 15.525497436523438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №749: loss = 15.516682624816895, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №750: loss = 15.5078763961792, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №751: loss = 15.499074935913086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №752: loss = 15.49028205871582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №753: loss = 15.481491088867188, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №754: loss = 15.47270679473877, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №755: loss = 15.463935852050781, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №756: loss = 15.455160140991211, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №757: loss = 15.446393013000488, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №758: loss = 15.437634468078613, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №759: loss = 15.42888355255127, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №760: loss = 15.420133590698242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №761: loss = 15.411392211914062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №762: loss = 15.40265941619873, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №763: loss = 15.393926620483398, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №764: loss = 15.385210037231445, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №765: loss = 15.376487731933594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №766: loss = 15.367776870727539, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №767: loss = 15.359067916870117, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №768: loss = 15.350367546081543, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №769: loss = 15.3416748046875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №770: loss = 15.332986831665039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №771: loss = 15.324302673339844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №772: loss = 15.315628051757812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №773: loss = 15.30695629119873, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №774: loss = 15.298294067382812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №775: loss = 15.289634704589844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №776: loss = 15.280982971191406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №777: loss = 15.27233600616455, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №778: loss = 15.263692855834961, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №779: loss = 15.255058288574219, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №780: loss = 15.246427536010742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №781: loss = 15.23780345916748, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №782: loss = 15.229185104370117, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №783: loss = 15.220571517944336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №784: loss = 15.211967468261719, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №785: loss = 15.203369140625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №786: loss = 15.19477367401123, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №787: loss = 15.186180114746094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №788: loss = 15.177599906921387, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №789: loss = 15.169024467468262, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №790: loss = 15.160451889038086, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №791: loss = 15.151885986328125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №792: loss = 15.143328666687012, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №793: loss = 15.134775161743164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №794: loss = 15.126226425170898, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №795: loss = 15.117683410644531, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №796: loss = 15.109148979187012, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №797: loss = 15.100618362426758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №798: loss = 15.09209156036377, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №799: loss = 15.08357048034668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №800: loss = 15.075063705444336, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №801: loss = 15.066549301147461, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №802: loss = 15.058049201965332, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №803: loss = 15.049554824829102, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №804: loss = 15.041064262390137, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №805: loss = 15.032580375671387, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №806: loss = 15.024096488952637, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №807: loss = 15.015625, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №808: loss = 15.007159233093262, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №809: loss = 14.998695373535156, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №810: loss = 14.990240097045898, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №811: loss = 14.981791496276855, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №812: loss = 14.973347663879395, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №813: loss = 14.9649076461792, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №814: loss = 14.9564790725708, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №815: loss = 14.94804859161377, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №816: loss = 14.939630508422852, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №817: loss = 14.931208610534668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №818: loss = 14.922798156738281, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №819: loss = 14.914392471313477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №820: loss = 14.905998229980469, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №821: loss = 14.897605895996094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №822: loss = 14.889215469360352, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №823: loss = 14.880834579467773, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №824: loss = 14.872459411621094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №825: loss = 14.86408805847168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №826: loss = 14.855728149414062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №827: loss = 14.847363471984863, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №828: loss = 14.839012145996094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №829: loss = 14.830662727355957, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №830: loss = 14.822321891784668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №831: loss = 14.813982963562012, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №832: loss = 14.805656433105469, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №833: loss = 14.797329902648926, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №834: loss = 14.789009094238281, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №835: loss = 14.78070068359375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №836: loss = 14.77238941192627, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №837: loss = 14.764083862304688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №838: loss = 14.755788803100586, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №839: loss = 14.74749755859375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №840: loss = 14.739217758178711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №841: loss = 14.730934143066406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №842: loss = 14.722661018371582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №843: loss = 14.714387893676758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №844: loss = 14.706128120422363, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №845: loss = 14.697870254516602, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №846: loss = 14.689615249633789, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №847: loss = 14.681370735168457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №848: loss = 14.673130989074707, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №849: loss = 14.664894104003906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №850: loss = 14.65666675567627, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №851: loss = 14.648442268371582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №852: loss = 14.640222549438477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №853: loss = 14.632006645202637, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №854: loss = 14.623804092407227, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №855: loss = 14.615598678588867, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №856: loss = 14.607404708862305, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №857: loss = 14.599215507507324, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №858: loss = 14.591026306152344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №859: loss = 14.582849502563477, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №860: loss = 14.574674606323242, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №861: loss = 14.566505432128906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №862: loss = 14.558343887329102, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №863: loss = 14.550188064575195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №864: loss = 14.542033195495605, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №865: loss = 14.53388500213623, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №866: loss = 14.52574634552002, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №867: loss = 14.517611503601074, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №868: loss = 14.509478569030762, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №869: loss = 14.50135326385498, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №870: loss = 14.493240356445312, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №871: loss = 14.485122680664062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №872: loss = 14.477015495300293, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №873: loss = 14.468914985656738, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №874: loss = 14.4608154296875, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №875: loss = 14.452728271484375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №876: loss = 14.444639205932617, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №877: loss = 14.436559677124023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №878: loss = 14.428482055664062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №879: loss = 14.42041301727295, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №880: loss = 14.412351608276367, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №881: loss = 14.404291152954102, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №882: loss = 14.39623737335205, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №883: loss = 14.388188362121582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №884: loss = 14.380149841308594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №885: loss = 14.372110366821289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №886: loss = 14.364079475402832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №887: loss = 14.356053352355957, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №888: loss = 14.348034858703613, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №889: loss = 14.340021133422852, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №890: loss = 14.332008361816406, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №891: loss = 14.324005126953125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №892: loss = 14.316006660461426, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №893: loss = 14.308012008666992, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №894: loss = 14.300023078918457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №895: loss = 14.29204273223877, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №896: loss = 14.284065246582031, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №897: loss = 14.276097297668457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №898: loss = 14.268133163452148, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №899: loss = 14.260165214538574, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №900: loss = 14.252212524414062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №901: loss = 14.24425983428955, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №902: loss = 14.23631763458252, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №903: loss = 14.228378295898438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №904: loss = 14.22044563293457, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №905: loss = 14.21251392364502, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №906: loss = 14.204594612121582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №907: loss = 14.196675300598145, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №908: loss = 14.188760757446289, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №909: loss = 14.180852890014648, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №910: loss = 14.172950744628906, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №911: loss = 14.165056228637695, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №912: loss = 14.157167434692383, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №913: loss = 14.149279594421387, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №914: loss = 14.141400337219238, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №915: loss = 14.133523941040039, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №916: loss = 14.125656127929688, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №917: loss = 14.117785453796387, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №918: loss = 14.109933853149414, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №919: loss = 14.102083206176758, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №920: loss = 14.09422779083252, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №921: loss = 14.086385726928711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №922: loss = 14.0785493850708, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №923: loss = 14.070718765258789, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №924: loss = 14.062891006469727, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №925: loss = 14.055070877075195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №926: loss = 14.047248840332031, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №927: loss = 14.03943920135498, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №928: loss = 14.031633377075195, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №929: loss = 14.023829460144043, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №930: loss = 14.016034126281738, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №931: loss = 14.008245468139648, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №932: loss = 14.000460624694824, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №933: loss = 13.992681503295898, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №934: loss = 13.984907150268555, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №935: loss = 13.977140426635742, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №936: loss = 13.969378471374512, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №937: loss = 13.961616516113281, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №938: loss = 13.953865051269531, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №939: loss = 13.94611930847168, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №940: loss = 13.938377380371094, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №941: loss = 13.930639266967773, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №942: loss = 13.9229097366333, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №943: loss = 13.915181159973145, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №944: loss = 13.907458305358887, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №945: loss = 13.899742126464844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №946: loss = 13.89202880859375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №947: loss = 13.88432502746582, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №948: loss = 13.876623153686523, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №949: loss = 13.868929862976074, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №950: loss = 13.861234664916992, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №951: loss = 13.853551864624023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №952: loss = 13.845873832702637, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №953: loss = 13.838200569152832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №954: loss = 13.830526351928711, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №955: loss = 13.822860717773438, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №956: loss = 13.815203666687012, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №957: loss = 13.807550430297852, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №958: loss = 13.799901962280273, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №959: loss = 13.792259216308594, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №960: loss = 13.78461742401123, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №961: loss = 13.776985168457031, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №962: loss = 13.76935863494873, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №963: loss = 13.761734008789062, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №964: loss = 13.754112243652344, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №965: loss = 13.746502876281738, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №966: loss = 13.738897323608398, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №967: loss = 13.731294631958008, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №968: loss = 13.723698616027832, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №969: loss = 13.716105461120605, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №970: loss = 13.708516120910645, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №971: loss = 13.700937271118164, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №972: loss = 13.693359375, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №973: loss = 13.68578815460205, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №974: loss = 13.678220748901367, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №975: loss = 13.67066478729248, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №976: loss = 13.663105964660645, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №977: loss = 13.655553817749023, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №978: loss = 13.6480073928833, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №979: loss = 13.640467643737793, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №980: loss = 13.632929801940918, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №981: loss = 13.625399589538574, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №982: loss = 13.617874145507812, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №983: loss = 13.610353469848633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №984: loss = 13.602839469909668, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №985: loss = 13.59533405303955, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №986: loss = 13.587823867797852, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №987: loss = 13.580324172973633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №988: loss = 13.572832107543945, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №989: loss = 13.565340995788574, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №990: loss = 13.55785846710205, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №991: loss = 13.550376892089844, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №992: loss = 13.542901992797852, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №993: loss = 13.535430908203125, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №994: loss = 13.52796745300293, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №995: loss = 13.52051067352295, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №996: loss = 13.513053894042969, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №997: loss = 13.505607604980469, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №998: loss = 13.49816608428955, weights = tensor([6.1250, 3.6277], requires_grad=True)\n",
      "step №999: loss = 13.490724563598633, weights = tensor([6.1250, 3.6277], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#it's time to replace prediction calculations too\n",
    "x = np.arange(10)\n",
    "y = x*5 + 10 + np.random.randint(-5, 5, 10)\n",
    "x = torch.tensor([[item] for item in x], dtype = torch.float32)\n",
    "y = torch.tensor([[item] for item in y], dtype = torch.float32)\n",
    "model = nn.Linear(1, 1)\n",
    "print(model.parameters())\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0006)\n",
    "for step in range(number_of_steps):\n",
    "    y_pred = model(x)\n",
    "    error = loss(y, y_pred)\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f'step №{step}: loss = {error}, weights = {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fe12a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for name, param in model.named_parameters():\n",
    "    weights.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "216ac528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Now absolutely verything is calculated by Pytorch. Slope = 5.793481349945068, intercept = 2.991750955581665, loss = 13.490724563598633')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y])\n",
    "sns.lineplot(x = [int(item[0]) for item in x], y = [int(item[0]) for item in y_pred])\n",
    "plt.title(f'Now absolutely verything is calculated by Pytorch. Slope = {weights[0].item()}, intercept = {weights[1].item()}, loss = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
